{
    "S4.T1": {
        "caption": "Table 1: On the left: statistics for each section of the MuST-C test set: number of segments, total duration of the English audio, number of (detokenized) source and target words. (âˆ—)(^{*}) For the Chinese side, characters are counted instead of words. On the right: statistics for each section of the Europarl-ST test set: total duration (h:mm) of the audio in the source language (top half) and number of (parallel) segments (bottom half)",
        "body": "<table class=\"ltx_tabular ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:189.8pt;\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" colspan=\"10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Europarl-ST</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" colspan=\"9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">trg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">src</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">de</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">en</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">es</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">fr</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">it</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">nl</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pl</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pt</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">ro</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">de</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">6:03</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:16</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:10</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:57</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:58</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:08</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:11</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">en</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:53</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:55</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:47</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:43</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:51</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:51</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:53</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">es</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:09</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">5:05</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:05</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:08</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:05</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:01</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:05</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">fr</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:52</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">4:40</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:55</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:52</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:01</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:55</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:51</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">it</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:43</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">5:9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:38</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:39</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:28</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:25</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:35</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">nl</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:30</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">4:01</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:24</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:21</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:14</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:18</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:13</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pl</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:08</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">5:27</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:06</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:04</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:02</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:01</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:06</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2:29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pt</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:34</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">6:28</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:32</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:34</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:31</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:26</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:22</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">ro</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:37</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">5:38</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:33</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:24</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:32</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:33</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:25</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">3:32</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">de</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2631</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1421</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1401</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1217</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1305</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1376</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1387</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1233</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">en</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1253</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1267</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1214</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1130</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1235</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1238</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1262</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1095</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">es</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1114</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1816</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1082</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1079</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1094</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1059</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1089</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">910</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">fr</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1093</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1804</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1098</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1046</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1150</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1113</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1100</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">949</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">it</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">922</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1686</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">885</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">893</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">837</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">820</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">871</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">742</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">nl</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1063</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1747</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1014</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1012</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">890</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">967</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">942</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">877</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pl</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1284</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2231</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1254</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1259</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1180</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1225</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1252</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">993</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">pt</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1271</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">2286</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1256</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1273</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1205</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1228</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1196</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1108</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">ro</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1231</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1963</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1204</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1157</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1168</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1210</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1164</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">1200</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "src",
            "side",
            "left",
            "each",
            "source",
            "words",
            "hmm",
            "audio",
            "statistics",
            "target",
            "test",
            "mustc",
            "instead",
            "english",
            "top",
            "language",
            "characters",
            "bottom",
            "trg",
            "number",
            "parallel",
            "right",
            "set",
            "europarlst",
            "detokenized",
            "half",
            "total",
            "segments",
            "counted",
            "duration",
            "chinese"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "language",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "source",
                    "audio",
                    "each",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity to the missing reference transcript</span>.\nASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "segments",
                    "target",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "language",
                    "target",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "left",
                    "bottom",
                    "segments",
                    "target",
                    "source",
                    "each",
                    "words",
                    "number",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "total",
                    "segments",
                    "mustc",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "target",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "total",
                    "number",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "set",
                    "language",
                    "target",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "english",
                    "statistics",
                    "each",
                    "mustc",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "set",
                    "europarlst",
                    "english",
                    "language",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "source",
                    "each",
                    "test",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "english",
                    "segments",
                    "source",
                    "words",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "set",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "language",
                    "total",
                    "left",
                    "source",
                    "each",
                    "words",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "detokenized",
                    "audio",
                    "segments",
                    "target",
                    "source",
                    "words",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "set",
                    "english",
                    "audio",
                    "source",
                    "each",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "audio",
                    "segments",
                    "target",
                    "source",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "audio",
                    "segments",
                    "words",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "set",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "set",
                    "source",
                    "segments",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, as discussed above, our approach relies on using source texts as proxies for input audio, which cannot be applied to languages without a standardized written form. These languages represent the majority of the thousands of languages spoken worldwide, and addressing this limitation would require the development of novel multimodal, source-aware metrics capable of directly leveraging the audio signal. However, this falls outside the scope of this work.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "english",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "set",
                    "europarlst",
                    "audio",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "left",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n",
                "matched_terms": [
                    "instead",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "left",
                    "source",
                    "words",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "number",
                    "words",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio",
                    "language",
                    "source",
                    "each",
                    "test",
                    "mustc"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: ASR (left) and MT (for the BT task, right) performance on the MuST-C and Europarl-ST test sets",
        "body": "<table class=\"ltx_tabular ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:241.5pt;\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MuST-C</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Europarl-ST</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">model</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">COMET</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MetricX</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">COMET</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MetricX</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-100<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-25<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-100<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0-25<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">38.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8562</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.398</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8810</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.439</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8510</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.326</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8627</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.891</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "metricx",
            "europarlst",
            "0100â†‘uparrow",
            "btmd",
            "task",
            "sets",
            "btnl",
            "model",
            "left",
            "test",
            "025â†“downarrow",
            "comet",
            "01â†‘uparrow",
            "asr",
            "right",
            "mustc",
            "bleu",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
            "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bleu",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "comet",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "bleu",
                    "task",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "left",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "europarlst",
                    "mustc",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "left",
                    "asr",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "right",
                    "asr",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our study, we selected two metrics that represent the two best families according to the\nWMT24 Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>. In both the official and Error Span Annotation rankings <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-etal-2024-error</span>)</cite>, metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below:</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite> is a foundation all-in-one Massively Multilingual and Multimodal Machine Translation model supporting several speech-related tasks, ASR and ST included, in nearly 100 languages. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">SeamlessM4T v2 large</span> (<span class=\"ltx_text ltx_font_bold\">ASRsm</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/seamless-m4t-v2-large\" title=\"\">https://huggingface.co/facebook/seamless-m4t-v2-large</a></span></span></span> a transformer model with 2.3B parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "btmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "btnl",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "comet",
                    "asr",
                    "mustc",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">seamlessST</span>: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "performance",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "model",
                    "comet",
                    "asr",
                    "test",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "mustc",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "mustc",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "performance",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "left",
                    "asr",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "asr",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comet",
                    "asr",
                    "bleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, if ASR can be problematic due to the intrinsic difficulties of the task even in quite common conditions (meetings, non-native speakers), the main problem for MT does not lie in the task itself but in the availability of training data. This supports our choice to conduct the investigation including also rather poor ASR models but only good quality MT models.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "left",
                    "asr",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "left",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet",
                    "asr",
                    "test",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "asr",
                    "comet"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Overview of data used: number of language pairs and of segments each system worked on",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\">system</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">MuST-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">EuroparlST</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">|src-tgt|</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">#seg</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">|src-tgt|</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">#seg</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">whisperST</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16,164</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">whsp+mdld</th>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33,295</td>\n<td class=\"ltx_td ltx_align_center\">72</td>\n<td class=\"ltx_td ltx_align_center\">88,227</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">owsmST</th>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33,295</td>\n<td class=\"ltx_td ltx_align_center\">64</td>\n<td class=\"ltx_td ltx_align_center\">79,294</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">owsm+mdld</th>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33,295</td>\n<td class=\"ltx_td ltx_align_center\">72</td>\n<td class=\"ltx_td ltx_align_center\">88,227</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">seamlessST</th>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33,295</td>\n<td class=\"ltx_td ltx_align_center\">72</td>\n<td class=\"ltx_td ltx_align_center\">88,227</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">smls+mdld</th>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33,295</td>\n<td class=\"ltx_td ltx_align_center\">72</td>\n<td class=\"ltx_td ltx_align_center\">88,227</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">166,475</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">360</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">448,366</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pairs",
            "smlsmdld",
            "whisperst",
            "whspmdld",
            "each",
            "srctgt",
            "seg",
            "system",
            "mustc",
            "seamlessst",
            "used",
            "language",
            "owsmst",
            "owsmmdld",
            "overview",
            "number",
            "europarlst",
            "total",
            "segments",
            "worked",
            "data"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
            "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "overview",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "language",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "language",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "each",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe each building block of our experiments, i.e., data, metrics, systems, and models, providing the motivations supporting each choice.</p>\n\n",
                "matched_terms": [
                    "data",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "each",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "data",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "total",
                    "segments",
                    "mustc",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "total",
                    "number",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "total",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "language",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisperst",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "whspmdld",
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "used",
                    "owsmst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">seamlessST</span>: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "seamlessst",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "europarlst",
                    "whisperst",
                    "whspmdld",
                    "language",
                    "owsmst",
                    "system",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "segments",
                    "each",
                    "system",
                    "mustc",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "whspmdld",
                    "owsmst",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "language",
                    "each",
                    "total",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "used",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "used",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "used",
                    "segments",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "each",
                    "system",
                    "mustc"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: ST performance of each system on the MuST-C and the Europarl-ST test sets. For the cascade architectures, the quality of the automatic transcription is also provided, in terms of WER. For each metric, values are arithmetically averaged on all language pairs covered by each system (see TableÂ 3)",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MuST-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Europarl-ST</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">system</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MetricX</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MetricX</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">%<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-100<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-25<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">%<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-100<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(0-25<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">whisperST</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8091</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.636</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">whsp+mdld</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8167</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.963</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8524</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.608</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">owsmST</th>\n<td class=\"ltx_td\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.6504</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.798</td>\n<td class=\"ltx_td\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.5292</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15.005</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">owsm+mdld</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8041</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.543</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8028</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.583</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">seamlessST</th>\n<td class=\"ltx_td\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7833</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.103</td>\n<td class=\"ltx_td\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7977</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.449</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">smls+mdld</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">18.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">24.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7974</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.553</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">30.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8542</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.469</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pairs",
            "metricx",
            "smlsmdld",
            "0100â†‘uparrow",
            "automatic",
            "wer",
            "whisperst",
            "whspmdld",
            "025â†“downarrow",
            "each",
            "covered",
            "â†“downarrow",
            "also",
            "terms",
            "cascade",
            "test",
            "system",
            "mustc",
            "seamlessst",
            "performance",
            "arithmetically",
            "see",
            "sets",
            "language",
            "averaged",
            "architectures",
            "comet",
            "provided",
            "owsmst",
            "owsmmdld",
            "metric",
            "bleu",
            "01â†‘uparrow",
            "values",
            "europarlst",
            "transcription",
            "all",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "automatic",
                    "language",
                    "architectures",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Translating between natural languages is a complex task even for humans, due to intrinsic linguistic challenges such as ambiguity and polysemy, dependence on context, structural differences across languages (e.g., Subject-Verb-Object vs. Verb-Object-Subject orders), the presence of idiomatic and figurative expressions, and the influence of pragmatics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jm3</span></cite>. Despite these intrinsic difficulties, human translation can be assumed to be error-free, especially for professionals, while the same cannot be said for translation performed automatically by machines, neither from text to text (machine translation, MT) nor from speech to text (ST). Therefore, for automatic translation, an additional challenge arises, that of evaluating its quality.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the quality of translations generated by machines can be performed either manually or automatically. Manual evaluation involves the annotation of systems&#8217; outputs by human professionals. While it is considered the most reliable option, it is rarely employed due to its large cost and the consequent infeasibility of performing it at scale <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span></cite>. For this reason, research advancements mostly rely on automatic metrics <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Marie2021ScientificCOA</span></cite>, the focus of this work.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "also",
                    "comet",
                    "system",
                    "metric",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "also",
                    "language",
                    "comet",
                    "provided",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "automatic",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "also",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Six ST systems, evenly divided between cascaded and direct architectures, spanning a wide performance range.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our empirical results highlight that both synthetic sources serve as effective textual proxies for input audio, enabling the application of source-aware metrics in ST (RQ1). At the same time, we also observe the superior reliability of automatic transcription over back translation, provided that its word error rate remains below 20% (RQ2). Finally, the proposed cross-lingual re-segmentation algorithm proves to support reliable evaluation, yielding only negligible degradation in the absence of audio-text alignments (RQ3).</p>\n\n",
                "matched_terms": [
                    "also",
                    "provided",
                    "transcription",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "all",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "automatic",
                    "language",
                    "comet",
                    "system",
                    "metric",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "automatic",
                    "also",
                    "language",
                    "provided",
                    "system",
                    "metric",
                    "quality",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "also",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "see",
                    "language",
                    "terms",
                    "quality",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity to the missing reference transcript</span>.\nASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "see",
                    "terms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "all",
                    "automatic",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "provided",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "provided",
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "europarlst",
                    "mustc",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "all",
                    "each",
                    "covered",
                    "test",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "europarlst",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "comet",
                    "metric",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "metricx",
                    "language",
                    "architectures",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "also",
                    "quality",
                    "bleu",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "system",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "transcription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "provided",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "values",
                    "wer",
                    "comet",
                    "all",
                    "mustc",
                    "quality",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "see",
                    "language",
                    "architectures",
                    "cascade",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisperst",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "whspmdld",
                    "language",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "owsmst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "owsmmdld",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">seamlessST</span>: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "seamlessst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "smlsmdld",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "whisperst",
                    "owsmst",
                    "each",
                    "covered",
                    "system",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All in all, our choice of ST models yields broad diversification not only in terms of architecture but also in terms of performance, including cases of particularly low quality of transcripts and translations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "all",
                    "terms",
                    "performance",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "see",
                    "sets",
                    "test",
                    "comet",
                    "all",
                    "each",
                    "system",
                    "mustc",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "see",
                    "comet",
                    "mustc",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "comet",
                    "all",
                    "mustc",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "transcription",
                    "comet",
                    "performance",
                    "quality",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "quality",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "see",
                    "whspmdld",
                    "owsmst",
                    "comet",
                    "cascade",
                    "system",
                    "metric",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their <span class=\"ltx_text ltx_font_italic\">quality</span>; (ii) the <span class=\"ltx_text ltx_font_italic\">languages</span> involved in the evaluation; and (iii) the <span class=\"ltx_text ltx_font_italic\">architecture</span> of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "wer",
                    "language",
                    "architectures",
                    "all",
                    "each",
                    "system",
                    "metric",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "also",
                    "provided",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "see",
                    "automatic",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "see",
                    "automatic",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "wer",
                    "also",
                    "provided",
                    "terms",
                    "each",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "also",
                    "provided",
                    "terms",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "wer",
                    "also",
                    "system",
                    "mustc",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "automatic",
                    "transcription",
                    "terms",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "automatic",
                    "wer",
                    "terms",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "also",
                    "europarlst",
                    "mustc",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "see"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "quality",
                    "see",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "also",
                    "europarlst",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
                "matched_terms": [
                    "also",
                    "all",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "also",
                    "quality",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "also",
                    "automatic",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "also",
                    "transcription",
                    "test",
                    "provided",
                    "system",
                    "quality",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "also",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second limitation concerns language coverage. All the languages included in the two datasets are high- or medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on low-resource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "also",
                    "transcription",
                    "performance",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "also",
                    "transcription",
                    "test",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "quality",
                    "also",
                    "language",
                    "comet",
                    "bleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, if ASR can be problematic due to the intrinsic difficulties of the task even in quite common conditions (meetings, non-native speakers), the main problem for MT does not lie in the task itself but in the availability of training data. This supports our choice to conduct the investigation including also rather poor ASR models but only good quality MT models.</p>\n\n",
                "matched_terms": [
                    "also",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "all",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "also",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "also",
                    "see"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "metricx",
                    "also",
                    "see",
                    "language",
                    "comet",
                    "all",
                    "each",
                    "test",
                    "mustc",
                    "system",
                    "metric",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "values",
                    "comet",
                    "provided",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "quality",
                    "comet",
                    "wer"
                ]
            }
        ]
    },
    "S5.T7": {
        "caption": "Table 7: For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR â€“ whisper, owsm, seamless â€“ or BT - madlad, nllb) is given here. Biased ASR MetricXs, i.e. those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">corpus</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}\\leq\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T7.m1\" intent=\":literal\"><semantics><mmultiscripts><mo>&#8804;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}\\leq</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}&gt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T7.m2\" intent=\":literal\"><semantics><mmultiscripts><mo>&gt;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}&gt;</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">MuST-C</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">206</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">78.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">56</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">21.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">18</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">100.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">224</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">80.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">56</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">20.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">Europarl-ST</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1080</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">89.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">130</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">10.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">11</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">171</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">94.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1091</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">78.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">301</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">21.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1286</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">87.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">186</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">12.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">29</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">14.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">171</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">85.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1315</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">78.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">357</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">21.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "metricx",
            "pairs",
            "counts",
            "type",
            "biased",
            "nllb",
            "corpus",
            "asrleq20",
            "excluded",
            "source",
            "two",
            "systems",
            "generation",
            "computed",
            "metricxs",
            "between",
            "mustc",
            "seamless",
            "asr20",
            "werâ‰¤ð™°ðš‚ðštt",
            "language",
            "corpora",
            "somehow",
            "comparisons",
            "possible",
            "asr",
            "metric",
            "involved",
            "number",
            "madlad",
            "here",
            "europarlst",
            "correlation",
            "wins",
            "werð™°ðš‚ðštt",
            "total",
            "given",
            "all",
            "whisper",
            "owsm",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
            "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "language",
                    "given",
                    "generation",
                    "source",
                    "possible",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "given",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "source",
                    "given",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "total",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Six ST systems, evenly divided between cascaded and direct architectures, spanning a wide performance range.</p>\n\n",
                "matched_terms": [
                    "between",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "possible",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "systems",
                    "language",
                    "comparisons",
                    "between",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "language",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the potential segmentation mismatch between the source and the reference translation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "source",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "nllb",
                    "whisper",
                    "asr",
                    "between",
                    "owsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "asr",
                    "between",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "asr",
                    "possible",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "given",
                    "all",
                    "computed",
                    "source",
                    "possible",
                    "between",
                    "number",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "europarlst",
                    "two",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "asr",
                    "all",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "europarlst",
                    "source",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "corpora",
                    "total",
                    "mustc",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our study, we selected two metrics that represent the two best families according to the\nWMT24 Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>. In both the official and Error Span Annotation rankings <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-etal-2024-error</span>)</cite>, metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below:</p>\n\n",
                "matched_terms": [
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "metric",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "language",
                    "source",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "total",
                    "computed",
                    "asr",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BLEU</span> measures the degree of overlap between a system&#8217;s output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams.</span></span></span> precisions between a translation hypothesis and reference(s), combined with a brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>signature: <span class=\"ltx_text ltx_font_typewriter\">nrefs:1|case:mixed|eff: no|tok:13a|smooth:exp|version:2.0.0</span></span></span></span></p>\n\n",
                "matched_terms": [
                    "computed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "source",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "language",
                    "nllb",
                    "generation",
                    "source",
                    "whisper",
                    "asr",
                    "owsm",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "total",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "corpus",
                    "all",
                    "between",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "two",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "synthetic",
                    "nllb",
                    "all",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "involved",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "corpus",
                    "excluded",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "europarlst",
                    "systems",
                    "language",
                    "all",
                    "mustc",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "all",
                    "computed",
                    "source",
                    "whisper",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "metric",
                    "synthetic",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "systems",
                    "generation",
                    "source",
                    "possible",
                    "between",
                    "asr",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "systems",
                    "corpora",
                    "source",
                    "between",
                    "mustc",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "all",
                    "source",
                    "between",
                    "mustc",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "biased",
                    "systems",
                    "excluded",
                    "generation",
                    "source",
                    "asr",
                    "metric",
                    "involved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their <span class=\"ltx_text ltx_font_italic\">quality</span>; (ii) the <span class=\"ltx_text ltx_font_italic\">languages</span> involved in the evaluation; and (iii) the <span class=\"ltx_text ltx_font_italic\">architecture</span> of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our findings are consistent across languages.</span> There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "between",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "systems",
                    "nllb",
                    "comparisons",
                    "source",
                    "asr",
                    "between",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "seamless",
                    "europarlst",
                    "corpus",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "source",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "whisper",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "source",
                    "whisper",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "corpora",
                    "here",
                    "source",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "correlation",
                    "owsm",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "source",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "source",
                    "asr",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "source",
                    "asr",
                    "between",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our study provides the first systematic investigation on the deployment of source-aware metrics for ST evaluation, offering practical recommendations for their selection based on specific operating conditions. The outcomes reveal that synthetic source-aware metrics provide a reliable and effective means of evaluating ST systems, achieving strong correlation with standard metrics. By addressing both empirical and pragmatic aspects, we hope to facilitate more consistent evaluation practices in ST research.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second limitation concerns language coverage. All the languages included in the two datasets are high- or medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on low-resource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation",
                    "given",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "source",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "given",
                    "number",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "all",
                    "generation",
                    "computed",
                    "source",
                    "owsm",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "corpora",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "asr"
                ]
            }
        ]
    },
    "S5.T8": {
        "caption": "Table 8: Counts of ASR/BT wins per language pair",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">src</th>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">trg</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">de</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">en</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">es</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">fr</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">it</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">nl</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">pl</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">pt</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ro</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">total</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ar</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">cs</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/4</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">de</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">25/15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">19/1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">8/12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">130/50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">en</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">24/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">22/2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">17/7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">23/1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">4/20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">4/20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">126/66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">es</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">32/8</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">19/1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">7/13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">142/38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">fa</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">fr</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">19/21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">11/9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">16/4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">134/46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">it</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">29/11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">19/1</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">17/3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">147/33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">nl</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">26/14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">18/2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">142/38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">pl</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6/4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">9/1</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6/4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71/9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">pt</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">30/10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">18/2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">17/3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">11/9</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">150/30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">24/16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">13/7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">145/35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ru</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">tr</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">18/2</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">18/2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">vi</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">14/6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">zh</th>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">20/0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">154/0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">319/111</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">152/2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">140/14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">153/1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">117/37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78/86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">110/44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">92/62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1315/357</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wins",
            "counts",
            "src",
            "language",
            "pair",
            "total",
            "trg",
            "asrbt"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "language",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pair"
                ]
            }
        ]
    },
    "S5.T9": {
        "caption": "Table 9: Counts of ASR/BT wins per system",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">direct</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">cascade</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">whisperST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#8195;&#8202;owsmST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">seamlessST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">whsp+mdld</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">owsm+mdld</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">smls+mdld</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19/13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">295/17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">261/67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">212/116</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">297/47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">231/97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">575/97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">740/260</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1315/357</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "asrbt",
            "wins",
            "smlsmdld",
            "counts",
            "whisperst",
            "whspmdld",
            "owsmst",
            "cascade",
            "owsmmdld",
            "system",
            "seamlessst",
            "direct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "direct",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "system",
                    "asrbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisperst",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "whspmdld",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "direct",
                    "owsmst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "owsmmdld",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">seamlessST</span>: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "seamlessst",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "smlsmdld",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "whisperst",
                    "system",
                    "owsmst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "whisperst",
                    "whspmdld",
                    "owsmst",
                    "system",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "whspmdld",
                    "system",
                    "cascade",
                    "owsmst"
                ]
            }
        ]
    },
    "S5.T10": {
        "caption": "Table 10: LASER scores computed on the manually segmented reference translations and the reference transcripts segmented: manually, re-segmented by either XL-Segmenter or XLR-Segmenter with respect to the BT by either MADLAD or NLLB",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\">segmentation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MuST-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Europarl-ST</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">manual</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8550</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.9008</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\">BT for reseg</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">mdld</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">XL-Segmenter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8440</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8822</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">XLR-Segmenter</td>\n<td class=\"ltx_td ltx_align_center\">0.8562</td>\n<td class=\"ltx_td ltx_align_center\">0.9013</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\">nllb</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">XL-Segmenter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8409</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8741</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">XLR-Segmenter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.8554</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.9003</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "nllb",
            "respect",
            "xlrsegmenter",
            "reseg",
            "manually",
            "laser",
            "segmented",
            "mdld",
            "computed",
            "mustc",
            "scores",
            "resegmented",
            "manual",
            "madlad",
            "europarlst",
            "transcripts",
            "translations",
            "segmentation",
            "either",
            "xlsegmenter",
            "reference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
            "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the quality of translations generated by machines can be performed either manually or automatically. Manual evaluation involves the annotation of systems&#8217; outputs by human professionals. While it is considered the most reliable option, it is rarely employed due to its large cost and the consequent infeasibility of performing it at scale <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span></cite>. For this reason, research advancements mostly rely on automatic metrics <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Marie2021ScientificCOA</span></cite>, the focus of this work.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "manual",
                    "either",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "segmentation",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "transcripts",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "segmentation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses these gaps. Building on advances in MT evaluation, we analyze source-aware metrics in the context of ST, where the speech modality challenges the availability of gold source text and its alignment with reference translations. By systematically assessing their reliability and applicability, we aim to shed light on which evaluation and re-segmentation strategies are most appropriate for ST, in scenarios where the availability of transcripts, or their alignment, cannot be assumed.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the potential segmentation mismatch between the source and the reference translation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "either",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "respect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity to the missing reference transcript</span>.\nASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "translations",
                    "reference",
                    "resegmented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "translations",
                    "segmented",
                    "segmentation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations",
                    "segmentation",
                    "xlsegmenter",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "xlrsegmenter",
                    "computed",
                    "xlsegmenter",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "translations",
                    "segmented",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "europarlst",
                    "manual",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "transcripts",
                    "manual",
                    "translations",
                    "mustc",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BLEU</span> measures the degree of overlap between a system&#8217;s output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams.</span></span></span> precisions between a translation hypothesis and reference(s), combined with a brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>signature: <span class=\"ltx_text ltx_font_typewriter\">nrefs:1|case:mixed|eff: no|tok:13a|smooth:exp|version:2.0.0</span></span></span></span></p>\n\n",
                "matched_terms": [
                    "computed",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "laser",
                    "segmented",
                    "xlrsegmenter",
                    "xlsegmenter",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "reference",
                    "nllb",
                    "madlad",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "nllb",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All in all, our choice of ST models yields broad diversification not only in terms of architecture but also in terms of performance, including cases of particularly low quality of transcripts and translations.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "manual",
                    "computed",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "either"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "translations",
                    "segmentation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "transcripts",
                    "manual",
                    "translations",
                    "mustc",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "transcripts",
                    "manual",
                    "nllb",
                    "translations",
                    "respect",
                    "segmented",
                    "segmentation",
                    "xlrsegmenter",
                    "xlsegmenter",
                    "madlad",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; XL-Segmenter approaches manual segmentation.</span> The XL-Segmenter works well, as the observed relative degradation compared to manual segmentation is limited to 1-3%. This result represents a baseline for the cross-lingual re-segmentation problem, as XL-Segmenter is a direct extension of L-Segmenter towards its application in cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation",
                    "xlsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; XLR-Segmenter closes the gap with manual segmentation.</span> The proposed boundary refinement stage allows XLR-Segmenter to completely close the gap with manual segmentation, showing consistent improvement over the XL-Segmenter baseline.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation",
                    "xlsegmenter",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "madlad",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments indicate that XL-Segmenter, the direct extension of L-Segmenter for its use across different languages, is extremely effective, and that our boundary refinement stage allows XLR-Segmenter to substantially eliminate all remaining alignment errors.</p>\n\n",
                "matched_terms": [
                    "xlrsegmenter",
                    "xlsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "resegmented",
                    "transcripts",
                    "manual",
                    "translations",
                    "segmented",
                    "segmentation",
                    "either",
                    "xlrsegmenter",
                    "xlsegmenter",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "manual",
                    "computed",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "europarlst",
                    "mustc",
                    "xlrsegmenter",
                    "xlsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "europarlst",
                    "transcripts",
                    "manual",
                    "segmentation",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "europarlst",
                    "transcripts",
                    "manual",
                    "segmentation",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "scores",
                    "computed",
                    "segmented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "translations",
                    "respect",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "computed",
                    "either",
                    "xlrsegmenter",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "reference",
                    "translations",
                    "respect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Two-Stage Re-Segmentation Algorithm at Work\n\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates how the two-stage re-segmentation algorithm works on a controlled example, where the objective is to realign the reference transcript, whose gold segmentation is disregarded, with the segments of the reference translation. The results of experiments conducted in this controlled mode are reported in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "segmentation",
                    "resegmented",
                    "segmented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "translations",
                    "computed",
                    "either",
                    "mustc",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "respect",
                    "scores"
                ]
            }
        ]
    },
    "S5.T11": {
        "caption": "Table 11: Evaluation of automatically re-segmented automatic transcripts. LASER scores are calculated with respect to the reference translation, and WER scores with respect to the reference transcript. WER is computed ignoring casing and considering (wp) or not (np) punctuation marks; in wp mode, text is tokenized, i.e., words are separated from punctuation. The LASER score for the manually segmented reference transcript is 0.8550 for MuST-C and 0.9008 for Europarl-ST",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ASR by <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T11.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">whisper</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">owsm</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">seamless</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">BT for reseg by <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T11.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">madlad</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">nllb</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">madlad</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">nllb</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">madlad</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">nllb</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:38pt;vertical-align:-15.6pt;\"><span class=\"ltx_transformed_inner\" style=\"width:38.1pt;transform:translate(-15.6pt,-15.6pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">MuST-C</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:33.5pt;vertical-align:-13.3pt;\"><span class=\"ltx_transformed_inner\" style=\"width:33.5pt;transform:translate(-13.3pt,-13.3pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">LASER</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8256</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8176</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8060</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8216</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8185</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8150</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.7286</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.7255</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8320</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8310</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8249</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8238</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.7327</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.7317</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.4pt;transform:translate(-8.8pt,-8.8pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">WER</p>\n</span></div>\n<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">23.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">23.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.4pt;transform:translate(-8.8pt,-8.8pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">WER</p>\n</span></div>\n<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" rowspan=\"9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:54.6pt;vertical-align:-24.8pt;\"><span class=\"ltx_transformed_inner\" style=\"width:54.6pt;transform:translate(-22.9pt,-22.9pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Europarl-ST</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:33.5pt;vertical-align:-13.3pt;\"><span class=\"ltx_transformed_inner\" style=\"width:33.5pt;transform:translate(-13.3pt,-13.3pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">LASER</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8849</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8604</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8878</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8642</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8563</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8361</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8276</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8648</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8569</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8833</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8824</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8530</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8517</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8816</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8807</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.4pt;transform:translate(-8.8pt,-8.8pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">WER</p>\n</span></div>\n<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">24.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">23.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">23.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.4pt;transform:translate(-8.8pt,-8.8pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">WER</p>\n</span></div>\n<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">manual seg</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XL-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">shas+XLR-Segmenter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.10</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "automatic",
            "wer",
            "nllb",
            "respect",
            "mode",
            "calculated",
            "evaluation",
            "marks",
            "words",
            "considering",
            "reseg",
            "not",
            "â†’rightarrow",
            "manually",
            "laser",
            "translation",
            "segmented",
            "tokenized",
            "automatically",
            "computed",
            "from",
            "seg",
            "mustc",
            "text",
            "casing",
            "punctuation",
            "scores",
            "seamless",
            "shasxlsegmenter",
            "resegmented",
            "ignoring",
            "manual",
            "transcript",
            "asr",
            "shasxlrsegmenter",
            "separated",
            "madlad",
            "europarlst",
            "score",
            "transcripts",
            "whisper",
            "owsm",
            "reference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "transcripts",
                    "evaluation",
                    "text",
                    "from",
                    "asr",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Translating between natural languages is a complex task even for humans, due to intrinsic linguistic challenges such as ambiguity and polysemy, dependence on context, structural differences across languages (e.g., Subject-Verb-Object vs. Verb-Object-Subject orders), the presence of idiomatic and figurative expressions, and the influence of pragmatics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jm3</span></cite>. Despite these intrinsic difficulties, human translation can be assumed to be error-free, especially for professionals, while the same cannot be said for translation performed automatically by machines, neither from text to text (machine translation, MT) nor from speech to text (ST). Therefore, for automatic translation, an additional challenge arises, that of evaluating its quality.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "automatically",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the quality of translations generated by machines can be performed either manually or automatically. Manual evaluation involves the annotation of systems&#8217; outputs by human professionals. While it is considered the most reliable option, it is rarely employed due to its large cost and the consequent infeasibility of performing it at scale <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span></cite>. For this reason, research advancements mostly rely on automatic metrics <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Marie2021ScientificCOA</span></cite>, the focus of this work.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "automatic",
                    "manual",
                    "evaluation",
                    "automatically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "evaluation",
                    "text",
                    "from",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "transcripts",
                    "manual",
                    "automatically",
                    "evaluation",
                    "from",
                    "text",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "evaluation",
                    "automatically",
                    "asr",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "evaluation",
                    "text",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation and evaluation of the proposed solutions is carried out in a rich experimental setup that leverages ST benchmarks in which human-labelled transcripts and sentence-level alignments are available. To ensure the soundness and robustness of our findings, we consider:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our empirical results highlight that both synthetic sources serve as effective textual proxies for input audio, enabling the application of source-aware metrics in ST (RQ1). At the same time, we also observe the superior reliability of automatic transcription over back translation, provided that its word error rate remains below 20% (RQ2). Finally, the proposed cross-lingual re-segmentation algorithm proves to support reliable evaluation, yielding only negligible degradation in the absence of audio-text alignments (RQ3).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "translation",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "transcripts",
                    "automatically",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "automatic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "transcripts",
                    "evaluation",
                    "text",
                    "from",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses these gaps. Building on advances in MT evaluation, we analyze source-aware metrics in the context of ST, where the speech modality challenges the availability of gold source text and its alignment with reference translations. By systematically assessing their reliability and applicability, we aim to shed light on which evaluation and re-segmentation strategies are most appropriate for ST, in scenarios where the availability of transcripts, or their alignment, cannot be assumed.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "evaluation",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the potential segmentation mismatch between the source and the reference translation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "translation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "automatically",
                    "text",
                    "asr",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "from",
                    "whisper",
                    "asr",
                    "owsm",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "asr",
                    "evaluation",
                    "respect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity to the missing reference transcript</span>.\nASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "from",
                    "transcript",
                    "asr",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "resegmented",
                    "evaluation",
                    "automatically",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "from",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "evaluation",
                    "text",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "resegmented",
                    "segmented",
                    "evaluation",
                    "automatically",
                    "from",
                    "words",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "not",
                    "asr",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ignoring",
                    "computed",
                    "from",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "manual",
                    "segmented",
                    "from",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "translation",
                    "transcripts",
                    "manual",
                    "from",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "transcripts",
                    "manual",
                    "evaluation",
                    "mustc",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our study, we selected two metrics that represent the two best families according to the\nWMT24 Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>. In both the official and Error Span Annotation rankings <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-etal-2024-error</span>)</cite>, metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below:</p>\n\n",
                "matched_terms": [
                    "text",
                    "translation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "from",
                    "translation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "evaluation",
                    "from",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "laser",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "computed",
                    "transcript",
                    "words",
                    "asr",
                    "reference",
                    "punctuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BLEU</span> measures the degree of overlap between a system&#8217;s output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams.</span></span></span> precisions between a translation hypothesis and reference(s), combined with a brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>signature: <span class=\"ltx_text ltx_font_typewriter\">nrefs:1|case:mixed|eff: no|tok:13a|smooth:exp|version:2.0.0</span></span></span></span></p>\n\n",
                "matched_terms": [
                    "computed",
                    "translation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "laser",
                    "score",
                    "translation",
                    "segmented",
                    "evaluation",
                    "automatically",
                    "from",
                    "text",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "whisper",
                    "asr",
                    "owsm",
                    "madlad",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "from",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite> is a foundation all-in-one Massively Multilingual and Multimodal Machine Translation model supporting several speech-related tasks, ASR and ST included, in nearly 100 languages. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">SeamlessM4T v2 large</span> (<span class=\"ltx_text ltx_font_bold\">ASRsm</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/seamless-m4t-v2-large\" title=\"\">https://huggingface.co/facebook/seamless-m4t-v2-large</a></span></span></span> a transformer model with 2.3B parameters.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "from",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "wer",
                    "nllb",
                    "from",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "asr",
                    "text",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "from",
                    "whisper",
                    "asr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "whisper",
                    "asr",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "translation",
                    "from",
                    "mustc",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "not",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All in all, our choice of ST models yields broad diversification not only in terms of architecture but also in terms of performance, including cases of particularly low quality of transcripts and translations.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "manual",
                    "not",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "score",
                    "manual",
                    "computed",
                    "transcript",
                    "asr",
                    "whisper",
                    "mustc",
                    "not",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "from",
                    "transcript",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "translation",
                    "transcripts",
                    "manual",
                    "not",
                    "from",
                    "asr",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "transcripts",
                    "manual",
                    "asr",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "transcripts",
                    "manual",
                    "from",
                    "words",
                    "mustc",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "from",
                    "owsm",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "manual",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "score",
                    "translation",
                    "from",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "evaluation",
                    "from",
                    "transcript",
                    "asr",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "wer",
                    "manual",
                    "from",
                    "words",
                    "asr",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our findings are consistent across languages.</span> There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "translation",
                    "automatic",
                    "transcripts",
                    "manual",
                    "nllb",
                    "respect",
                    "segmented",
                    "evaluation",
                    "automatically",
                    "from",
                    "text",
                    "words",
                    "asr",
                    "not",
                    "madlad",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "laser",
                    "automatic",
                    "transcripts",
                    "manual",
                    "segmented",
                    "automatically",
                    "computed",
                    "not",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "madlad",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "automatic",
                    "wer",
                    "resegmented",
                    "transcripts",
                    "manual",
                    "segmented",
                    "evaluation",
                    "automatically",
                    "asr",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "seamless",
                    "laser",
                    "europarlst",
                    "wer",
                    "from",
                    "whisper",
                    "words",
                    "asr",
                    "mustc",
                    "owsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "europarlst",
                    "automatic",
                    "transcripts",
                    "manual",
                    "mustc",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "europarlst",
                    "automatic",
                    "wer",
                    "transcripts",
                    "manual",
                    "from",
                    "transcript",
                    "words",
                    "mustc",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "wer",
                    "from",
                    "whisper",
                    "marks",
                    "mustc",
                    "punctuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "laser",
                    "europarlst",
                    "transcripts",
                    "manual",
                    "segmented",
                    "whisper",
                    "asr",
                    "mustc",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "manually",
                    "segmented",
                    "automatically",
                    "computed",
                    "asr",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "transcripts",
                    "manual",
                    "from",
                    "owsm",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "not",
                    "from",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "transcripts",
                    "manual",
                    "asr",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "automatic",
                    "transcripts",
                    "evaluation",
                    "asr",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "wer",
                    "transcripts",
                    "manual",
                    "automatically",
                    "evaluation",
                    "computed",
                    "from",
                    "asr",
                    "not",
                    "reference",
                    "punctuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "transcripts",
                    "respect",
                    "evaluation",
                    "asr",
                    "considering",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second limitation concerns language coverage. All the languages included in the two datasets are high- or medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on low-resource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "not",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "considering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "score",
                    "from",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, if ASR can be problematic due to the intrinsic difficulties of the task even in quite common conditions (meetings, non-native speakers), the main problem for MT does not lie in the task itself but in the availability of training data. This supports our choice to conduct the investigation including also rather poor ASR models but only good quality MT models.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n",
                "matched_terms": [
                    "laser",
                    "not",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Two-Stage Re-Segmentation Algorithm at Work\n\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates how the two-stage re-segmentation algorithm works on a controlled example, where the objective is to realign the reference transcript, whose gold segmentation is disregarded, with the segments of the reference translation. The results of experiments conducted in this controlled mode are reported in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "transcript",
                    "translation",
                    "mode"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "wer",
                    "resegmented",
                    "segmented",
                    "text",
                    "from",
                    "transcript",
                    "words",
                    "not",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "not",
                    "words",
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "owsm",
                    "asr",
                    "mustc",
                    "not",
                    "reference",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "not",
                    "asr",
                    "wer"
                ]
            }
        ]
    },
    "S5.T14": {
        "caption": "Table 14: For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR - whisper, owsm, seamless resegmented by XLR-Segmenter wrt madlad or nllb - or BT - madlad, nllb) is given here. Biased ASR MetricXs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">corpus</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}\\leq\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T14.m1\" intent=\":literal\"><semantics><mmultiscripts><mo>&#8804;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}\\leq</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}&gt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T14.m2\" intent=\":literal\"><semantics><mmultiscripts><mo>&gt;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}&gt;</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">MuST-C</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">335</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">99.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">0.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">38</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">17.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">186</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">83.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">373</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">66.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">187</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">33.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">Europarl-ST</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1657</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">71.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">675</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">28.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">74</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">13.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">474</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">86.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1731</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">60.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1149</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">39.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1992</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">74.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">676</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">25.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">112</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">14.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">660</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">85.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">2104</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">61.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1336</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">38.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "metricx",
            "pairs",
            "counts",
            "type",
            "biased",
            "nllb",
            "corpus",
            "asrleq20",
            "excluded",
            "source",
            "xlrsegmenter",
            "two",
            "systems",
            "generation",
            "computed",
            "metricxs",
            "between",
            "mustc",
            "seamless",
            "asr20",
            "werâ‰¤ð™°ðš‚ðštt",
            "resegmented",
            "language",
            "corpora",
            "wrt",
            "somehow",
            "comparisons",
            "possible",
            "asr",
            "metric",
            "involved",
            "number",
            "madlad",
            "here",
            "europarlst",
            "correlation",
            "wins",
            "werð™°ðš‚ðštt",
            "total",
            "given",
            "all",
            "whisper",
            "owsm",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
            "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "language",
                    "given",
                    "generation",
                    "source",
                    "possible",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "given",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "source",
                    "given",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "total",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Six ST systems, evenly divided between cascaded and direct architectures, spanning a wide performance range.</p>\n\n",
                "matched_terms": [
                    "between",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "possible",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "systems",
                    "language",
                    "comparisons",
                    "between",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "language",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the potential segmentation mismatch between the source and the reference translation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "source",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "nllb",
                    "whisper",
                    "asr",
                    "between",
                    "owsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "resegmented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "asr",
                    "between",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "resegmented",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "asr",
                    "possible",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "given",
                    "all",
                    "computed",
                    "source",
                    "possible",
                    "between",
                    "xlrsegmenter",
                    "number",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "europarlst",
                    "two",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "asr",
                    "all",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "europarlst",
                    "source",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "corpora",
                    "total",
                    "mustc",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our study, we selected two metrics that represent the two best families according to the\nWMT24 Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>. In both the official and Error Span Annotation rankings <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-etal-2024-error</span>)</cite>, metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below:</p>\n\n",
                "matched_terms": [
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "metric",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "language",
                    "source",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "total",
                    "computed",
                    "asr",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BLEU</span> measures the degree of overlap between a system&#8217;s output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams.</span></span></span> precisions between a translation hypothesis and reference(s), combined with a brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>signature: <span class=\"ltx_text ltx_font_typewriter\">nrefs:1|case:mixed|eff: no|tok:13a|smooth:exp|version:2.0.0</span></span></span></span></p>\n\n",
                "matched_terms": [
                    "computed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "xlrsegmenter",
                    "source",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "language",
                    "nllb",
                    "generation",
                    "source",
                    "whisper",
                    "asr",
                    "owsm",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "total",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "corpus",
                    "all",
                    "between",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "two",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "synthetic",
                    "nllb",
                    "all",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "involved",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "corpus",
                    "excluded",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "europarlst",
                    "systems",
                    "language",
                    "all",
                    "mustc",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "all",
                    "computed",
                    "source",
                    "whisper",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "metric",
                    "synthetic",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "systems",
                    "generation",
                    "source",
                    "possible",
                    "between",
                    "asr",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "systems",
                    "corpora",
                    "source",
                    "between",
                    "mustc",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "all",
                    "source",
                    "between",
                    "mustc",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "europarlst",
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "biased",
                    "systems",
                    "excluded",
                    "generation",
                    "source",
                    "asr",
                    "metric",
                    "involved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their <span class=\"ltx_text ltx_font_italic\">quality</span>; (ii) the <span class=\"ltx_text ltx_font_italic\">languages</span> involved in the evaluation; and (iii) the <span class=\"ltx_text ltx_font_italic\">architecture</span> of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "wins",
                    "counts",
                    "systems",
                    "language",
                    "total",
                    "all",
                    "source",
                    "possible",
                    "asr",
                    "metric",
                    "involved",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our findings are consistent across languages.</span> There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "between",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "systems",
                    "nllb",
                    "comparisons",
                    "source",
                    "asr",
                    "between",
                    "xlrsegmenter",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments indicate that XL-Segmenter, the direct extension of L-Segmenter for its use across different languages, is extremely effective, and that our boundary refinement stage allows XLR-Segmenter to substantially eliminate all remaining alignment errors.</p>\n\n",
                "matched_terms": [
                    "all",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "systems",
                    "source",
                    "asr",
                    "xlrsegmenter",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "seamless",
                    "metric",
                    "europarlst",
                    "corpus",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "source",
                    "between",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "whisper",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "source",
                    "whisper",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "corpora",
                    "here",
                    "source",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "correlation",
                    "owsm",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "source",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlation",
                    "source",
                    "asr",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "source",
                    "asr",
                    "between",
                    "xlrsegmenter",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "systems",
                    "language",
                    "generation",
                    "source",
                    "asr",
                    "between",
                    "metric",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our study provides the first systematic investigation on the deployment of source-aware metrics for ST evaluation, offering practical recommendations for their selection based on specific operating conditions. The outcomes reveal that synthetic source-aware metrics provide a reliable and effective means of evaluating ST systems, achieving strong correlation with standard metrics. By addressing both empirical and pragmatic aspects, we hope to facilitate more consistent evaluation practices in ST research.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second limitation concerns language coverage. All the languages included in the two datasets are high- or medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on low-resource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "metric",
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation",
                    "given",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "source",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "given",
                    "number",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "all",
                    "generation",
                    "computed",
                    "source",
                    "owsm",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "corpora",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "asr"
                ]
            }
        ]
    },
    "A0.T15": {
        "caption": "Table 15: Costs in terms of total execution time (excluding the model loading) and GPU memory usage of the various steps required for generating either ASR or BT sources. The size of models in terms of number of parameters is also shown",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">id</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">step</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">batch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">time</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GPU</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">size</td>\n<td class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">memory</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MADLAD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20m:30s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12.5GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2m:48s</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.6GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NLLB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12m:47s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.7GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3m:02s</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.6GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.55B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21m:26s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.7GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SeamlessM4T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17m:45s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.8GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3m:43s</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.7GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">L-Segmenter</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">mwerSegmenter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.6s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>efinement</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Algorithm 1 (mBERT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">180M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17m:44s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.1GB</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "algorithm",
            "models",
            "mbert",
            "generating",
            "execution",
            "23b",
            "nllb",
            "2m48s",
            "size",
            "3m02s",
            "costs",
            "06s",
            "various",
            "387gb",
            "sources",
            "180m",
            "time",
            "step",
            "required",
            "also",
            "97gb",
            "batch",
            "terms",
            "125gb",
            "gpu",
            "17m44s",
            "loading",
            "seamlessm4t",
            "11gb",
            "usage",
            "446gb",
            "12m47s",
            "155b",
            "3m43s",
            "memory",
            "108gb",
            "asr",
            "lsegmenter",
            "21m26s",
            "mwersegmenter",
            "137gb",
            "refinement",
            "number",
            "parameters",
            "madlad",
            "17m45s",
            "20m30s",
            "excluding",
            "total",
            "33b",
            "model",
            "steps",
            "either",
            "whisper",
            "426gb"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "asr",
                    "generating",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our empirical results highlight that both synthetic sources serve as effective textual proxies for input audio, enabling the application of source-aware metrics in ST (RQ1). At the same time, we also observe the superior reliability of automatic transcription over back translation, provided that its word error rate remains below 20% (RQ2). Finally, the proposed cross-lingual re-segmentation algorithm proves to support reliable evaluation, yielding only negligible degradation in the absence of audio-text alignments (RQ3).</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "also",
                    "time",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "time",
                    "models",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "also",
                    "either",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "models",
                    "nllb",
                    "terms",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity to the missing reference transcript</span>.\nASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "terms",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "step",
                    "costs",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "costs",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generating",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "mwersegmenter",
                    "lsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "lsegmenter",
                    "model",
                    "asr",
                    "required"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "mbert",
                    "step",
                    "steps",
                    "lsegmenter",
                    "refinement",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "total",
                    "number",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "also",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "total",
                    "number",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "models",
                    "nllb",
                    "whisper",
                    "asr",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "models",
                    "155b",
                    "whisper",
                    "asr",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "total",
                    "model",
                    "asr",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite> is a foundation all-in-one Massively Multilingual and Multimodal Machine Translation model supporting several speech-related tasks, ASR and ST included, in nearly 100 languages. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">SeamlessM4T v2 large</span> (<span class=\"ltx_text ltx_font_bold\">ASRsm</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/seamless-m4t-v2-large\" title=\"\">https://huggingface.co/facebook/seamless-m4t-v2-large</a></span></span></span> a transformer model with 2.3B parameters.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "23b",
                    "model",
                    "asr",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "madlad",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "nllb",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "models",
                    "nllb",
                    "whisper",
                    "asr",
                    "madlad",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "models",
                    "generating",
                    "model",
                    "asr",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "also",
                    "model",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "model",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "model",
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">seamlessST</span>: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "also",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "model",
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All in all, our choice of ST models yields broad diversification not only in terms of architecture but also in terms of performance, including cases of particularly low quality of transcripts and translations.</p>\n\n",
                "matched_terms": [
                    "also",
                    "models",
                    "terms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "also",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "model",
                    "whisper",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "either",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "total",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our findings are consistent across languages.</span> There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "also",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generating",
                    "step",
                    "nllb",
                    "asr",
                    "madlad",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "nllb",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments indicate that XL-Segmenter, the direct extension of L-Segmenter for its use across different languages, is extremely effective, and that our boundary refinement stage allows XLR-Segmenter to substantially eliminate all remaining alignment errors.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "lsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "models",
                    "also",
                    "model",
                    "terms",
                    "either",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "also",
                    "terms",
                    "asr",
                    "refinement",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "also",
                    "refinement",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "also",
                    "refinement",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "asr",
                    "various",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "also",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
                "matched_terms": [
                    "also",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "also",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "models",
                    "step",
                    "also",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "step",
                    "also",
                    "either",
                    "asr",
                    "refinement",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "also",
                    "model",
                    "terms",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "also",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "also",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "also",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, if ASR can be problematic due to the intrinsic difficulties of the task even in quite common conditions (meetings, non-native speakers), the main problem for MT does not lie in the task itself but in the availability of training data. This supports our choice to conduct the investigation including also rather poor ASR models but only good quality MT models.</p>\n\n",
                "matched_terms": [
                    "also",
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "time",
                    "loading",
                    "generating",
                    "execution",
                    "step",
                    "usage",
                    "excluding",
                    "model",
                    "memory",
                    "gpu",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mwerSegmenter vs. mweralign\n\nThe first stage of our re-segmentation algorithm (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) employs mwerSegmenter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite> for the initial source/target alignment, a code distributed only as executable binary without the source. Very recently, a new implementation, named <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, was made available as a Python module by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "mwersegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n",
                "matched_terms": [
                    "mwersegmenter",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "also",
                    "lsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "step",
                    "also",
                    "refinement",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "seamlessm4t",
                    "also",
                    "either",
                    "asr",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "algorithm",
                    "asr",
                    "various",
                    "sources"
                ]
            }
        ]
    },
    "A0.T16": {
        "caption": "Table 16: Same as TableÂ 10 but using mweralign instead of mwerSegmenter",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\">segmentation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MuST-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Europarl-ST</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">manual</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8550</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.9008</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\">BT for reseg</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">mdld</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">XL-Segmenter<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">mweralign</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8453</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8847</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">XLR-Segmenter<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">mweralign</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.8560</td>\n<td class=\"ltx_td ltx_align_center\">0.9012</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\">nllb</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">XL-Segmenter<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">mweralign</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8409</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8758</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">XLR-Segmenter<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">mweralign</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.8549</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.9001</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "europarlst",
            "same",
            "manual",
            "nllb",
            "reseg",
            "mdld",
            "segmentation",
            "xlrsegmentermweralign",
            "mweralign",
            "mustc",
            "mwersegmenter",
            "instead",
            "xlsegmentermweralign"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "same",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "mwersegmenter",
                    "same",
                    "segmentation",
                    "mweralign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "nllb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "manual",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "same",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "instead",
                    "manual",
                    "nllb",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; XL-Segmenter approaches manual segmentation.</span> The XL-Segmenter works well, as the observed relative degradation compared to manual segmentation is limited to 1-3%. This result represents a baseline for the cross-lingual re-segmentation problem, as XL-Segmenter is a direct extension of L-Segmenter towards its application in cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; XLR-Segmenter closes the gap with manual segmentation.</span> The proposed boundary refinement stage allows XLR-Segmenter to completely close the gap with manual segmentation, showing consistent improvement over the XL-Segmenter baseline.</p>\n\n",
                "matched_terms": [
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "instead",
                    "same",
                    "segmentation",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual",
                    "segmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "manual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mwerSegmenter vs. mweralign\n\nThe first stage of our re-segmentation algorithm (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) employs mwerSegmenter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite> for the initial source/target alignment, a code distributed only as executable binary without the source. Very recently, a new implementation, named <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, was made available as a Python module by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mwersegmenter",
                    "mweralign"
                ]
            }
        ]
    },
    "A0.T17": {
        "caption": "Table 17: COMET/MetricX correlations of systemsâ€™ scores averaged across language pairs in each test set",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">synthetic</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">COMET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">MetricX</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">source</th>\n<td class=\"ltx_td ltx_align_center\">EP-ST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">MuST-C</td>\n<td class=\"ltx_td ltx_align_center\">EP-ST</td>\n<td class=\"ltx_td ltx_align_center\">MuST-C</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">ASRwh</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999339</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.999225</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999987</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999865</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRos</span></th>\n<td class=\"ltx_td ltx_align_center\">0.992541</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.987877</td>\n<td class=\"ltx_td ltx_align_center\">0.999851</td>\n<td class=\"ltx_td ltx_align_center\">0.998601</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRsm</span></th>\n<td class=\"ltx_td ltx_align_center\">0.999462</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.992222</td>\n<td class=\"ltx_td ltx_align_center\">0.999980</td>\n<td class=\"ltx_td ltx_align_center\">0.998012</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.995429</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.999881</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999951</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999996</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></th>\n<td class=\"ltx_td ltx_align_center\">0.994826</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.999865</td>\n<td class=\"ltx_td ltx_align_center\">0.999953</td>\n<td class=\"ltx_td ltx_align_center\">0.999987</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">ASRwh-md</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999869</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.999935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999997</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.999986</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRwh-nl</span></th>\n<td class=\"ltx_td ltx_align_center\">0.999872</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.999935</td>\n<td class=\"ltx_td ltx_align_center\">0.999997</td>\n<td class=\"ltx_td ltx_align_center\">0.999983</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRos-md</span></th>\n<td class=\"ltx_td ltx_align_center\">0.997725</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.999175</td>\n<td class=\"ltx_td ltx_align_center\">0.999943</td>\n<td class=\"ltx_td ltx_align_center\">0.999901</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRos-nl</span></th>\n<td class=\"ltx_td ltx_align_center\">0.997721</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.999188</td>\n<td class=\"ltx_td ltx_align_center\">0.999944</td>\n<td class=\"ltx_td ltx_align_center\">0.999901</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span></th>\n<td class=\"ltx_td ltx_align_center\">0.999957</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.999906</td>\n<td class=\"ltx_td ltx_align_center\">0.999997</td>\n<td class=\"ltx_td ltx_align_center\">0.999932</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.999955</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.999909</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.999997</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.999936</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pairs",
            "metricx",
            "systemsâ€™",
            "asros",
            "asrosnl",
            "asrsmnl",
            "each",
            "source",
            "cometmetricx",
            "btnl",
            "epst",
            "asrwhmd",
            "correlations",
            "test",
            "mustc",
            "scores",
            "across",
            "btmd",
            "language",
            "asrsmmd",
            "averaged",
            "comet",
            "asrwhnl",
            "set",
            "asrosmd",
            "asrsm",
            "asrwh",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "comet",
                    "language",
                    "across",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "source",
                    "each",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "across",
                    "language",
                    "comet",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "set",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "source",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlations",
                    "across",
                    "language",
                    "comet",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "metricx",
                    "across",
                    "language",
                    "source",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asrwh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "across",
                    "btmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "btnl",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "synthetic",
                    "mustc",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "set",
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "set",
                    "language",
                    "averaged",
                    "mustc",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlations",
                    "comet",
                    "source",
                    "each",
                    "test",
                    "mustc",
                    "synthetic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "comet",
                    "source",
                    "mustc",
                    "synthetic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "comet",
                    "source",
                    "mustc",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "asros",
                    "comet",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "source",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "comet",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "set",
                    "comet",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their <span class=\"ltx_text ltx_font_italic\">quality</span>; (ii) the <span class=\"ltx_text ltx_font_italic\">languages</span> involved in the evaluation; and (iii) the <span class=\"ltx_text ltx_font_italic\">architecture</span> of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "set",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "across",
                    "language",
                    "source",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "set",
                    "source",
                    "each",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "asrwhmd",
                    "asrosnl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "source",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "set",
                    "comet",
                    "source",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "across",
                    "comet",
                    "source",
                    "synthetic",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "set",
                    "source",
                    "synthetic",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "comet",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "set",
                    "each",
                    "synthetic",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the adoption of the new code would substantially affect our results, we applied it to a subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T16\" title=\"Table 16 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> corresponds to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than <math alttext=\"0.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p15.m1\" intent=\":literal\"><semantics><mrow><mn>0.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.3\\%</annotation></semantics></math>. Such a minimal difference is not expected to affect the overall validity of our findings.</p>\n\n",
                "matched_terms": [
                    "source",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "correlations",
                    "comet",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "source",
                    "comet"
                ]
            }
        ]
    },
    "A0.T20": {
        "caption": "Table 20: \nFor all possible comparisons between the COMET correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR - whisper, owsm, seamless resegmented by XLR-Segmenter wrt madlad or nllb - or BT - madlad, nllb) is given here. Biased ASR COMETs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">corpus</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}\\leq\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A0.T20.m1\" intent=\":literal\"><semantics><mmultiscripts><mo>&#8804;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}\\leq</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">WER<math alttext=\"{}_{\\tt ASR}&gt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A0.T20.m2\" intent=\":literal\"><semantics><mmultiscripts><mo>&gt;</mo><mprescripts/><mi>&#120432;&#120450;&#120449;</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\tt ASR}&gt;</annotation></semantics></math>20%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">ASR wins</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">BT wins</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">counts</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">MuST-C</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">300</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">89.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">36</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">10.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">56</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">25.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">168</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">75.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">356</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">63.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">204</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">36.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">Europarl-ST</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1822</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">78.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">510</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">21.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">541</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">98.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1829</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">63.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1051</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">36.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">total</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">2122</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">79.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">546</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">20.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">63</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">8.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">709</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">91.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">2185</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">63.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">1255</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.7pt;padding-right:1.7pt;\">36.5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pairs",
            "counts",
            "type",
            "biased",
            "nllb",
            "corpus",
            "asrleq20",
            "excluded",
            "source",
            "xlrsegmenter",
            "two",
            "systems",
            "generation",
            "computed",
            "between",
            "mustc",
            "seamless",
            "asr20",
            "werâ‰¤ð™°ðš‚ðštt",
            "resegmented",
            "language",
            "corpora",
            "wrt",
            "comets",
            "comet",
            "somehow",
            "comparisons",
            "possible",
            "asr",
            "metric",
            "involved",
            "number",
            "madlad",
            "here",
            "europarlst",
            "correlation",
            "wins",
            "werð™°ðš‚ðštt",
            "total",
            "given",
            "all",
            "whisper",
            "owsm",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F7\" title=\"Figure 7 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, histograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F8\" title=\"Figure 8 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T20\" title=\"Table 20 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, respectively, related to COMET metric instead of MetricX.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, automatic MT metrics rely on comparing a system output against one or more human reference translations assumed to represent the &#8220;correct&#8221; rendering of the source sentence to translate. This is the case of BLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, the most widespread MT metric in the scientific community over the last two decades <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span></cite>, which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bojar-etal-2018-findings</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrault-etal-2019-findings</span>)</cite>, the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define a &#8220;perfect&#8221; holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since 2008, the conference on Machine Translation (WMT) has organized a shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts a large part of the scientific community working on it. The link to the 2025 edition is: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www2.statmt.org/wmt25/mteval-subtask.html\" title=\"\">https://www2.statmt.org/wmt25/mteval-subtask.html</a></span></span></span> recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, which demonstrated its effectiveness from its first participation in the MT Metrics shared task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-results</span></cite> within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "comet",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require a textual source, not audio. As a solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on a textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agostinelli-etal-2025-findings</span>)</cite>. In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is a viable option when manual transcripts are not available, a typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is&#160;(<span class=\"ltx_text ltx_font_bold\">RQ1</span>): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics?</p>\n\n",
                "matched_terms": [
                    "language",
                    "given",
                    "comet",
                    "generation",
                    "source",
                    "possible",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To answer RQ1, we study two alternative ways to generate a synthetic textual source: the automatic transcription of the audio with an automatic speech recognition&#160;(ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (<span class=\"ltx_text ltx_font_bold\">RQ2</span>): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in a given setting?</p>\n\n",
                "matched_terms": [
                    "given",
                    "source",
                    "asr",
                    "between",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires a cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (<span class=\"ltx_text ltx_font_bold\">RQ3</span>): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose a novel two-stage, cross-lingual algorithm, XLR-Segmenter,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/source-resegmenter\" title=\"\">https://github.com/hlt-mt/source-resegmenter</a>) and on PyPi (<span class=\"ltx_text ltx_font_typewriter\">pip install source_resegmenter</span>).</span></span></span> which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "source",
                    "given",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two ST benchmarks covering 79 language pairs from different language families, for a total of over 120,000 sentences;</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "total",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments;</p>\n\n",
                "matched_terms": [
                    "comet",
                    "correlation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Six ST systems, evenly divided between cascaded and direct architectures, spanning a wide performance range.</p>\n\n",
                "matched_terms": [
                    "between",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S2\" title=\"2 Related Works &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). It then describes the solutions we propose (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3\" title=\"3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and provides the experimental setup (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4\" title=\"4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). The experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are divided into four blocks. The first two blocks (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. A thorough discussion (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and the examination of the limitations of the work (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S7\" title=\"7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) conclude the paper.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "possible",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The automatic evaluation of MT outputs has been a central topic of research since the early days of statistical approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown-etal-1990-statistical</span></cite>. The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papineni-etal-2002-bleu</span></cite>, TER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snover-etal-2006-study</span></cite>, and chrF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span></cite>. While computationally efficient, early n-gram- or character-based metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banerjee-lavie-2005-meteor</span></cite> incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BERTScore</span></cite>, BLEURT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sellam-etal-2020-bleurt</span></cite>, COMET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite>, and MetricX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2022-results</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2023-results</span></cite>. Since 2008, WMT has hosted a dedicated Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ws-2008-statistical</span></cite>, providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2022-machine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2023</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wmt-2024-1</span></cite>. Despite the current dominance of COMET in system ranking, meta-evaluation studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathur-etal-2020-tangled</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2021-experts</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moghe-etal-2025-machine</span></cite> have highlighted the limitations of relying on a single metric, including problems in reproducing and comparing scores across works <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zouhar-etal-2024-pitfalls</span>)</cite>, echoing earlier critiques of BLEU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">callison-burch-etal-2006-evaluating</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>. Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "language",
                    "comet",
                    "comparisons",
                    "between",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akiba-etal-2004-overview</span></cite>, its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, however, introduces two distinctive challenges for automatic evaluation: <span class=\"ltx_text ltx_font_italic\">i)</span> segmentation mismatches between speech and text, which complicate direct comparison&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">papi-etal-2021-dealing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fukuda22b_interspeech</span></cite>, and <span class=\"ltx_text ltx_font_italic\">ii)</span> the possible absence of gold source transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng21_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fang-feng-2023-back</span></cite>, which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both source- and reference-free, could be employed (e.g. GEMBA-MQM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-federmann-2023-gemba</span></cite>), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">larionov2025batchgemba</span></cite>. Among ST-specific metrics, BLASER&#160;2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dale-costa-jussa-2024-blaser</span></cite> is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into a shared multilingual embedding space to assess translation quality. However, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han-etal-2024-speechqe</span></cite> report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER&#160;2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sperber-etal-2024-evaluating</span></cite>. In a similar vein, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite> analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "language",
                    "source",
                    "possible",
                    "between",
                    "metric",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the potential segmentation mismatch between the source and the reference translation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "source",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, <span class=\"ltx_text ltx_font_italic\">synthetic</span> source text can be created either by automatically transcribing the input audio (<span class=\"ltx_text ltx_font_bold\">ASR</span>) or by back-translating the reference translation into the source language (<span class=\"ltx_text ltx_font_bold\">BT</span>). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coverage and Quality</span>.\nAlthough there are many ASR models available, even more than MT ones (e.g., on Huggingface<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a></span></span></span>), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than a few dozen high-resource languages are Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite>, SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>, XLS-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">babu22_interspeech</span></cite>, and OWSM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite>. Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or a few language pairs. From the first group, we can mention: Madlad-400&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, NLLB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite>, mBART-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2020multilingual</span></cite>, M2M100&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">M2M100</span></cite>, DeltaLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deltalm</span></cite>, and SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">communication2023seamlessm4tmassivelymultilingual</span></cite>. From the second one: the Helsinki-NLP/opus-mt family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tiedemann2023democratizing</span></cite>, which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gala2023indictrans</span></cite>, which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix&#160;A) consistently show that MT systems typically achieve higher overall quality than ASR models.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "nllb",
                    "whisper",
                    "asr",
                    "between",
                    "owsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutrality with respect to third-party system evaluation</span>.\nWhen evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only a residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment with the segmentation of the reference translations</span>.\nFor evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "source",
                    "synthetic",
                    "resegmented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cost</span>.\nThe ASR approach requires a speech recognition system and often a re-segmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only a machine translation system, typically less demanding in terms of computational resources. Appendix&#160;B reports on specific experiments performed to compare the two approaches from this perspective.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5\" title=\"5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendices B, D, and E, while the final discussion in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S6\" title=\"6 Discussion and Conclusions &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is organized around them.</p>\n\n",
                "matched_terms": [
                    "all",
                    "source",
                    "asr",
                    "between",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such a scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matusov-etal-2005-evaluating</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span> For the sake of simplicity, we name it <span class=\"ltx_text ltx_font_bold\">L-Segmenter</span>, where the <span class=\"ltx_text ltx_font_bold\">L</span> highlights its <span class=\"ltx_text ltx_font_italic\">intra-lingual</span> nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>A comparison between <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>, the original implementation of this method, and <span class=\"ltx_text ltx_font_typewriter\">mweralign</span>, its new re-implementation by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>, is provided in Appendix&#160;C.</span></span></span> re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-hoang-2025-effects</span></cite>. The upper table of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix D shows an example of L-Segmenter output, highlighting this problem.</p>\n\n",
                "matched_terms": [
                    "language",
                    "resegmented",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, a cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. A possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as <span class=\"ltx_text ltx_font_bold\">XL-Segmenter</span>, where <span class=\"ltx_text ltx_font_bold\">X</span> emphasizes its cross-lingual nature.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "asr",
                    "possible",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words&#8217; surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a high-level description of the procedure. Given a pair of source/target sentences &lt;<math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math>,<math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>&gt;, each element is concatenated with its immediate successor (<math alttext=\"s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">s_{i+1}</annotation></semantics></math> and <math alttext=\"t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">t_{i+1}</annotation></semantics></math>, respectively); the tokens of the resulting concatenations are then aligned using SimAlign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jalili-sabet-etal-2020-simalign</span></cite> (step&#160;3). At this point, while keeping the original bipartition of <math alttext=\"t_{i}+t_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}+t_{i+1}</annotation></semantics></math> fixed, the number of cross-alignments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>A cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa).</span></span></span> is computed for all possible bipartitions of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math>; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> (step&#160;4). If the optimal segmentation of <math alttext=\"s_{i}+s_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}+s_{i+1}</annotation></semantics></math> differs from the original one, words are moved accordingly from one segment to the other (steps&#160;5 and&#160;6). The strength of the proposed algorithm lies in its use of SimAlign, a word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span></cite>, which supports 104 languages. The two tables at the bottom of Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.F6\" title=\"Figure 6 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> in Appendix&#160;D illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name <span class=\"ltx_text ltx_font_bold\">XLR-Segmenter</span> the whole segmentation procedure depicted in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where the <span class=\"ltx_text ltx_font_bold\">R</span> indicates the boundary Refinement stage, and its effectiveness is shown in Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "given",
                    "all",
                    "computed",
                    "source",
                    "possible",
                    "between",
                    "xlrsegmenter",
                    "number",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">di-gangi-etal-2019-must</span></cite> and Europarl-ST v1.1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jairsan2020a</span></cite>.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "europarlst",
                    "two",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span> (Multilingual Speech Translation Corpus) is a large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.ted.com\" title=\"\">www.ted.com</a></span></span></span> ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (left) provides statistics of the MuST-C test set, named tst-COMMON.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "asr",
                    "all",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Europarl-ST</span> is a multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koehn-2005-europarl</span></cite> of European Parliament proceedings, which features textual translations of formal, structured speech across a range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentence-aligned translations into the other eight languages, thus covering 72 translation directions. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (right) provides statistics of the Europarl-ST test set.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "europarlst",
                    "source",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since both corpora provide reference transcripts aligned to reference translations, they enable a systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides a solid empirical basis, ensuring that the results reported in the following sections are grounded on a broad and statistically reliable pool of observations.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "corpora",
                    "total",
                    "mustc",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our study, we selected two metrics that represent the two best families according to the\nWMT24 Metrics Shared Task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">freitag-etal-2024-llms</span></cite>. In both the official and Error Span Annotation rankings <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kocmi-etal-2024-error</span>)</cite>, metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below:</p>\n\n",
                "matched_terms": [
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2020-comet</span></cite> is a learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau-etal-2020-unsupervised</span></cite>), and then scoring the translation quality through a regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is <span class=\"ltx_text ltx_font_typewriter\">wmt22-comet-da</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rei-etal-2022-comet</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> which covers about 100 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "comet",
                    "source",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MetricX</span> family consists of state-of-the-art neural models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2023-metricx</span></cite> that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The <span class=\"ltx_text ltx_font_typewriter\">Hybrid</span> variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">juraska-etal-2024-metricx</span></cite> combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the <span class=\"ltx_text ltx_font_typewriter\">MetricX-24-Hybrid-XL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google/metricx-24-hybrid-xl-v2p6\" title=\"\">https://huggingface.co/google/metricx-24-hybrid-xl-v2p6</a></span></span></span></span> model, which supports 101 languages.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER:</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jitsi/jiwer\" title=\"\">https://github.com/jitsi/jiwer</a></span></span></span> Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "total",
                    "computed",
                    "asr",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BLEU</span> measures the degree of overlap between a system&#8217;s output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams.</span></span></span> precisions between a translation hypothesis and reference(s), combined with a brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">post-2018-call</span></cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>signature: <span class=\"ltx_text ltx_font_typewriter\">nrefs:1|case:mixed|eff: no|tok:13a|smooth:exp|version:2.0.0</span></span></span></span></p>\n\n",
                "matched_terms": [
                    "computed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">artetxe-schwenk-2019-massively</span></cite> is not an evaluation metric, but rather a sentence embedding model that produces multilingual vector representations of sentences. We used <span class=\"ltx_text ltx_font_typewriter\">laserembeddings 1.1.2</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/laserembeddings/\" title=\"\">https://pypi.org/project/laserembeddings/</a></span></span></span> the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "xlrsegmenter",
                    "source",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below:</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "language",
                    "nllb",
                    "generation",
                    "source",
                    "whisper",
                    "asr",
                    "owsm",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Whisper</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pmlr-v202-radford23a</span></cite> is a family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon a Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters <span class=\"ltx_text ltx_font_typewriter\">v3-large multilingual</span> model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> (<span class=\"ltx_text ltx_font_bold\">ASRwh</span>) to perform the speech transcription.</p>\n\n",
                "matched_terms": [
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OWSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm</span></cite> is a family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used <span class=\"ltx_text ltx_font_typewriter\">OWSM v3.1 medium</span> (<span class=\"ltx_text ltx_font_bold\">ASRos</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owsm-v31</span></cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/owsm_v3.1_ebf\" title=\"\">https://huggingface.co/espnet/owsm_v3.1_ebf</a></span></span></span> a model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages.</p>\n\n",
                "matched_terms": [
                    "total",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MADLAD</span> is a family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are low-resource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from a wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400-3B-MT</span> (<span class=\"ltx_text ltx_font_bold\">BTmd</span>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kudugunta2023madlad400</span></cite>, a 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on a Transformer architecture and is trained with a multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "systems",
                    "language",
                    "corpus",
                    "all",
                    "between",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLLB</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nllbteam2022languageleftbehindscaling</span></cite> is a family of multilingual MT models, aimed at enabling translation across a wide spectrum of languages, including low-resource and underrepresented ones. <span class=\"ltx_text ltx_font_typewriter\">NLLB-200-3.3B</span> (<span class=\"ltx_text ltx_font_bold\">BTnl</span>) is a 3.3-billion-parameter Transformer-based model that supports direct translation between any pair of the 200 covered languages.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average quality of the three ASR models and of the two MT models for the BT task is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "two",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4T&#8217;s ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span>While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works.</span></span></span> Concerning OWSM, it is quite strong in transcribing English (WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os</span></sub>=12.70% vs. WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh</span></sub>=11.33 and WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">sm</span></sub>=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "nllb",
                    "comet",
                    "all",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the broad validity of our findings, we complemented the variety of data and domains (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) with a heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote18\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">18</sup><span class=\"ltx_tag ltx_tag_note\">18</span>Direct ST systems map input speech directly into the target language text in a single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bentivogli-etal-2021-cascade</span></cite> for a comparison of the two architectures.</span></span></span> Specifically, the six ST systems are:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "language",
                    "source",
                    "asr",
                    "involved",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whisperST</span>: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">whsp+mdld</span>: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "language",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsmST</span>: We employed the same OWSM model used to perform ASR to also generate the direct translations.</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">owsm+mdld</span>: Cascade of the OWSM model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">smls+mdld</span>: Cascade of the SeamlessM4T model for ASR and MADLAD for MT.</p>\n\n",
                "matched_terms": [
                    "madlad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since <span class=\"ltx_text ltx_font_typewriter\">whisperST</span> supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> does not cover the translation into Polish; therefore, the eight to-Polish tasks are excluded from experiments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "corpus",
                    "excluded",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the overall performance of the six ST systems on the two benchmarks.\nTo better contextualize these values, we compare the MuST-C scores with those reported by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas-etal-2024-pushing</span></cite>, which spans 35 recent systems. The best BLEU, averaged on 8&#160;language pairs, is 33.2. Our <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span> system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is <span class=\"ltx_text ltx_font_typewriter\">whisperST</span>, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>&#8217;s average performance is particularly low, especially on Europarl-ST. In fact, while <span class=\"ltx_text ltx_font_typewriter\">owsmST</span> translates the English speech section of Europarl-ST similarly to how it does in MuST-C (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "europarlst",
                    "systems",
                    "language",
                    "all",
                    "mustc",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our goal is to verify the effectiveness of using synthetic sources to compute source-aware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pearson1895note</span>)</cite> of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The correlation scores are computed independently for the two benchmarks (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS1\" title=\"4.1 Data &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), for each metric (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS2\" title=\"4.2 Metrics &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), for each method used to generate synthetic sources (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) and for each ST system (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS4\" title=\"4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as a benchmark, COMET as a metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated to the &#8220;correct&#8221; score (if the score is close to 0). Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite>, we consider correlations <math alttext=\"&gt;0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.80</annotation></semantics></math> as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (<math alttext=\"&gt;0.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">&gt;0.99</annotation></semantics></math>) for all methods and metrics, as we show in Appendix&#160;E.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "comet",
                    "all",
                    "computed",
                    "source",
                    "whisper",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From here on, for simplicity, we name <span class=\"ltx_text ltx_font_italic\">standard</span> the metric that uses the manual transcript, and either <span class=\"ltx_text ltx_font_italic\">BT</span>, <span class=\"ltx_text ltx_font_italic\">ASR</span> or simply <span class=\"ltx_text ltx_font_italic\">synthetic</span> the one with, respectively, the BT source, the ASR source or any of them.</p>\n\n",
                "matched_terms": [
                    "source",
                    "asr",
                    "metric",
                    "synthetic",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). This preliminary investigation has a two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in a realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "systems",
                    "generation",
                    "source",
                    "possible",
                    "between",
                    "asr",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this first set of experiments, we exploit the <span class=\"ltx_text ltx_font_italic\">manual audio segmentation</span>, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T3\" title=\"Table 3 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). To define a lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (<span class=\"ltx_text ltx_font_typewriter\">shuff</span>), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "correlation",
                    "systems",
                    "corpora",
                    "comet",
                    "source",
                    "between",
                    "mustc",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Both synthetic sources are effective.</span> Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 indicate a very strong correlation <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cohen1988spa</span></cite> between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "correlation",
                    "comet",
                    "all",
                    "source",
                    "between",
                    "mustc",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR is better than BT.</span> In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The correlation gap is almost fully recovered.</span> Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from a minimum of 67.63 to a maximum of 97.07), and high for COMET, with the exception of <span class=\"ltx_text ltx_font_typewriter\">ASRos</span> on Europarl-ST, due to the poor performance of the recognizer. These numbers confirm the general effectiveness of synthetic sources as a substitute of manual sources, although conditioned to transcription quality.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "europarlst",
                    "correlation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Random sources have different impact.</span> COMET&#8217;s correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it a definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using a source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "metric",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Biased conditions.</span> The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by a system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., <span class=\"ltx_text ltx_font_typewriter\">whsp+mdld</span>) or the ST system itself (e.g., <span class=\"ltx_text ltx_font_typewriter\">owsmST</span>). In these biased conditions (see the scores marked with a dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "biased",
                    "systems",
                    "comet",
                    "generation",
                    "excluded",
                    "source",
                    "asr",
                    "metric",
                    "involved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with a randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "source",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their <span class=\"ltx_text ltx_font_italic\">quality</span>; (ii) the <span class=\"ltx_text ltx_font_italic\">languages</span> involved in the evaluation; and (iii) the <span class=\"ltx_text ltx_font_italic\">architecture</span> of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concerning the possible dependency on the <span class=\"ltx_text ltx_font_italic\">quality</span>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for a more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is a better substitute of the ideal source than any BT is over 87%, otherwise the choice of a BT is winning at almost 86%.\nFrom the <span class=\"ltx_text ltx_font_italic\">language</span> perspective, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T8\" title=\"Table 8 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process.\nIn relation to the ST system <span class=\"ltx_text ltx_font_italic\">architecture</span>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T9\" title=\"Table 9 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows a breakdown of the total counts of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "wins",
                    "counts",
                    "systems",
                    "language",
                    "total",
                    "all",
                    "source",
                    "possible",
                    "asr",
                    "metric",
                    "involved",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The quality of ASR impacts the effectiveness of synthetic metrics.</span> Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our findings are consistent across languages.</span> There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated.</p>\n\n",
                "matched_terms": [
                    "involved",
                    "between",
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S1\" title=\"1 Introduction &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (RQ3), this represents a simplistic assumption that might not hold in real-world conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS2\" title=\"3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), in this section we assess its ability to re-segment the <span class=\"ltx_text ltx_font_italic\">manual transcripts</span>. This controlled setup enables us to precisely measure how well our re-segmenters (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.F1\" title=\"Figure 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) can recover the original segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources<span class=\"ltx_note ltx_role_footnote\" id=\"footnote19\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">19</sup><span class=\"ltx_tag ltx_tag_note\">19</span>Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</span></span></span> and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span>We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite>, most likely due to the fact that these alignments are automatically produced. While <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang-etal-2022-impact</span></cite> showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text.</span></span></span> Since LASER scores are based on the cosine similarity between the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "systems",
                    "nllb",
                    "comparisons",
                    "source",
                    "asr",
                    "between",
                    "xlrsegmenter",
                    "madlad",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote21\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">21</sup><span class=\"ltx_tag ltx_tag_note\">21</span>It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks.</span></span></span> The minimal differences observed between the manual and automatic segmentation scores demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "possible",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; Our automatic re-segmentation is effective and robust to variable BT quality.</span> Using a higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), yields only a slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on.</p>\n\n",
                "matched_terms": [
                    "nllb",
                    "madlad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments indicate that XL-Segmenter, the direct extension of L-Segmenter for its use across different languages, is extremely effective, and that our boundary refinement stage allows XLR-Segmenter to substantially eliminate all remaining alignment errors.</p>\n\n",
                "matched_terms": [
                    "all",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS3\" title=\"5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tsiamas22_interspeech</span></cite>, using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. Finally, automatic transcripts are re-segmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.SS3\" title=\"4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we compare BT sources with those obtained on <span class=\"ltx_text ltx_font_italic\">automatic transcripts</span> of <span class=\"ltx_text ltx_font_italic\">automatically segmented audio</span>. This setup enables a comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild.</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "systems",
                    "source",
                    "asr",
                    "xlrsegmenter",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The boundaries refiner handles real-world conditions.</span> The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub>, WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>). The observed WER improvement, up to more than 7 absolute points (from WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub>=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter.</p>\n\n",
                "matched_terms": [
                    "seamless",
                    "metric",
                    "europarlst",
                    "corpus",
                    "whisper",
                    "asr",
                    "owsm",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of automatic transcripts is fully preserved.</span> In terms of LASER, the re-segmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such a way as to recover the semantic correspondence between the source and target segments.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "source",
                    "between",
                    "mustc",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; A few words are still misplaced.</span>\nIn terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">np</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wh-md</span></sub> on Europarl-ST (from 10.40 to 13.48), to 6.20 WER<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">wp</span></sub> of ASR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">os-nl</span></sub> on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words&#160;(3-6%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "xlrsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The refiner correctly repositions punctuation marks. </span>\nThe refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLAD&#8217;s BT, the refinement lowers the <math alttext=\"{\\tt WER_{np}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{np}}</annotation></semantics></math> from 15.13 to 11.55 on MuST-C (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.58</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt np}=-3.58</annotation></semantics></math>) and from 18.43 to 13.48 on Europarl-ST (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120471;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>4.95</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt np}=-4.95</annotation></semantics></math>). The <math alttext=\"{\\tt WER_{wp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m4\" intent=\":literal\"><semantics><msub><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi></msub><annotation encoding=\"application/x-tex\">{\\tt WER_{wp}}</annotation></semantics></math> is instead brought from 21.32 to 17.60 (<math alttext=\"\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120444;&#120478;&#120450;&#120451;</mi><mo>&#8722;</mo><mi>&#120434;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>3.72</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt MuST-C}_{\\tt wp}=-3.72</annotation></semantics></math>) and from 23.96 to 18.62 (<math alttext=\"\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120480;&#120473;</mi><mrow><mi>&#120436;&#120447;</mi><mo>&#8722;</mo><mi>&#120450;&#120451;</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>5.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt EP-ST}_{\\tt wp}=-5.34</annotation></semantics></math>), respectively. Since the difference between <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt np}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120471;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt np}</annotation></semantics></math> and <math alttext=\"\\Delta{\\tt WER}^{\\tt X}{\\tt wp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px2.p6.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>&#120454;&#120436;&#120449;</mi><mi>&#120455;</mi></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#120480;&#120473;</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta{\\tt WER}^{\\tt X}{\\tt wp}</annotation></semantics></math> stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "europarlst",
                    "whisper",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The semantics of manual transcripts is almost fully preserved.</span> The fully automated process is able to generate segmented ASR texts (with <span class=\"ltx_text ltx_font_typewriter\">whisper</span>), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T10\" title=\"Table 10 &#8227; Results &#8227; 5.3 Source Re-segmentation of Manual Transcripts &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This further demonstrates the effectiveness of our source re-segmentation solution.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "source",
                    "whisper",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only a minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Due to the observed lower sensitivity of COMET to the source content (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), here we focus on MetricX, omitting COMET results (which are reported in Appendix&#160;F for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and MetricX<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> scores that are reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> correspond to those originally reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> are equivalent to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F2\" title=\"Figure 2 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F3\" title=\"Figure 3 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, but related to realistic conditions under investigation in this last set of experiments.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "comet",
                    "here",
                    "source",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR sources remain effective.</span>\nIn general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with a minimum value of 0.8140), which still reflects a strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "correlation",
                    "owsm",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; ASR confirms to be a better proxy.</span>\nIn 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating a generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "systems",
                    "source",
                    "asr",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">&#8211; The 20% threshold is still discriminative.</span>\nFigures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F4\" title=\"Figure 4 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.F5\" title=\"Figure 5 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T14\" title=\"Table 14 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, not too dissimilar from those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases.</p>\n\n",
                "matched_terms": [
                    "all",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains a strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides a better alternative.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "source",
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigated the reliability of source-aware evaluation metrics for speech-to-text translation&#160;(ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, a critical step for computing reliable evaluation scores.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS2\" title=\"5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>) and in the wild (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). This supports the use of source-aware metrics in ST evaluation (RQ1), a practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote22\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">22</sup><span class=\"ltx_tag ltx_tag_note\">22</span>We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, a situation that rarely occurs, or measured on a suitable test set, whose availability is far from guaranteed.</span></span></span> Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "source",
                    "asr",
                    "between",
                    "xlrsegmenter",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Regarding the <span class=\"ltx_text ltx_font_italic\">language coverage</span>, MT systems currently offer a broader range of supported languages than ASR models, making BT a more versatile solution for low-resource or less commonly studied languages (see also Appendix&#160;A). Concerning <span class=\"ltx_text ltx_font_italic\">model quality</span>, our experiments show that deploying strong MT models is generally easier and more consistent (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T2\" title=\"Table 2 &#8227; 4.3 ASR and BT Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), while achieving comparable ASR quality remains more challenging (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S4.T4\" title=\"Table 4 &#8227; 4.4 ST Models &#8227; 4 Experimental Setting &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). When considering the <span class=\"ltx_text ltx_font_italic\">neutrality of synthetic sources with respect to the ST system under evaluation</span>, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The <span class=\"ltx_text ltx_font_italic\">similarity between the synthetic source and the original transcripts</span> further influences evaluation reliability, and the results in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS1\" title=\"5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T7\" title=\"Table 7 &#8227; Results &#8227; 5.2 Insights into Synthetic Source Comparison &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the <span class=\"ltx_text ltx_font_italic\">alignment between synthetic sources and reference translations</span> that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and a measurable drop in evaluation accuracy (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.SS4\" title=\"5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>). Therefore, BT sources should be preferred when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall <span class=\"ltx_text ltx_font_italic\">cost</span> of creating synthetic sources, we carried out experiments on a subset of data (Appendix&#160;B). As expected, results show that the ASR generation is more demanding than BT generation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "systems",
                    "language",
                    "generation",
                    "source",
                    "asr",
                    "between",
                    "metric",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our study provides the first systematic investigation on the deployment of source-aware metrics for ST evaluation, offering practical recommendations for their selection based on specific operating conditions. The outcomes reveal that synthetic source-aware metrics provide a reliable and effective means of evaluating ST systems, achieving strong correlation with standard metrics. By addressing both empirical and pragmatic aspects, we hope to facilitate more consistent evaluation practices in ST research.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "synthetic",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second limitation concerns language coverage. All the languages included in the two datasets are high- or medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on low-resource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such a broad range of languages (i.e., 79 language pairs).</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in\nAppendix&#160;C to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "metric",
                    "source",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in a reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "source",
                    "asr",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Quality of ASR and MT Models\n\nIn Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S3.SS1\" title=\"3.1 Synthetic Source Generation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, transcription quality is influenced by a wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> split of LibriSpeech), current systems achieve WERs of 1-3%&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gulati2020ConformerCT</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wav2vecBaevskiEtAl2020</span></cite>.\nOn spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conformerZeineldeenEtAl2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ng21b_interspeech</span></cite>, as also reflected on the Hugging Face Open ASR leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">open-asr-leaderboard</span></cite>.\nFor instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote23\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">23</sup><span class=\"ltx_tag ltx_tag_note\">23</span>Visited on 29 Aug 2025</span></span></span> but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canaryWangEtAl2024</span></cite>, showing the substantial degradation under less controlled conditions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote24\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">24</sup><span class=\"ltx_tag ltx_tag_note\">24</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chimechallenge.org/workshops/chime2024/\" title=\"\">https://www.chimechallenge.org/workshops/chime2024/</a></span></span></span> two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote25\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">25</sup><span class=\"ltx_tag ltx_tag_note\">25</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge\" title=\"\">https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge</a></span></span></span>\nTranscribing non-native speech or speech with strong regional accents is also similarly challenging: <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">accentedAsrDoEtAl2024</span></cite> reports WERs above 30% on a test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech.\nFinally, dysarthric speech represents another critical condition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricQianEtAl2023</span></cite>, with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dysarthricAlmadhorEtAl2023</span></cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard<span class=\"ltx_note ltx_role_footnote\" id=\"footnote26\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">26</sup><span class=\"ltx_tag ltx_tag_note\">26</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://opus.nlpl.eu/dashboard/\" title=\"\">https://opus.nlpl.eu/dashboard/</a>, visited on 29 Aug 2025</span></span></span> shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "language",
                    "asr",
                    "comet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cost of Generating Synthetic Sources\n\nTo estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on a single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>it Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting values, reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, allow to compare the resource demands of the ASR- and BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T15\" title=\"Table 15 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of a segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more cost-effective to generate than ASR sources.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation",
                    "given",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#9600A8;\"> red violet</span>; the original segmentation point is also shown but ignored (<span class=\"ltx_text ltx_ulem_sout\" style=\"--ltx-fg-color:#0080FF;\">EoS</span>), in order to evaluate the algorithm&#8217;s ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2.\nThe contents of rows 1 and 2 represent the input to L-Segmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>orrect word matches, <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>ubstitutions, <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions, and <span class=\"ltx_text ltx_framed ltx_framed_underline\">D</span>eletions. Notably, near the <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">Are not</span>\", that are not aligned to any word in the back-translation and are therefore labeled as <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nsertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">incorrectly</span>) assigned to the left segment.</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "source",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in a more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#alg1\" title=\"Algorithm 1 &#8227; 3.2 Source Re-Segmentation &#8227; 3 Methodology &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#cross-alignments=0) precisely when the moving <span class=\"ltx_text\" style=\"--ltx-fg-color:#0080FF;\">EoS</span> tag is placed to (<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0080FF;\">correctly</span>) reassign the two words \"Are not\" to the right segment.</p>\n\n",
                "matched_terms": [
                    "given",
                    "number",
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASR- or BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T17\" title=\"Table 17 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (<span class=\"ltx_text ltx_font_typewriter\">ASRsm-md</span>, <span class=\"ltx_text ltx_font_typewriter\">ASRsm-nl</span>) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems&#8217; rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "correlation",
                    "systems",
                    "language",
                    "comet",
                    "all",
                    "generation",
                    "computed",
                    "source",
                    "owsm",
                    "asr",
                    "between",
                    "mustc",
                    "metric",
                    "synthetic",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show for COMET the scores that Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T12\" title=\"Table 12 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T13\" title=\"Table 13 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as <span class=\"ltx_text ltx_font_typewriter\">shas+XLR-Segmenter</span> of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T11\" title=\"Table 11 &#8227; In-the-wild Re-Segmentation: Results &#8227; 5.4 In-the-Wild Evaluation &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nValues have to be compared to those of Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTmd</span></sub> and Comet<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_typewriter\">BTnl</span></sub> in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T5\" title=\"Table 5 &#8227; Outline of experiments &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#S5.T6\" title=\"Table 6 &#8227; Results &#8227; 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments &#8227; 5 Experiments &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which are shown in Tables &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T18\" title=\"Table 18 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">18</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03295v2#A0.T19\" title=\"Table 19 &#8227; 7 Limitations &#8227; How to Evaluate Speech Translation with Source-Aware Neural MT Metrics\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> as well for ease of reading.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "corpora",
                    "asr",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "source",
                    "asr"
                ]
            }
        ]
    }
}