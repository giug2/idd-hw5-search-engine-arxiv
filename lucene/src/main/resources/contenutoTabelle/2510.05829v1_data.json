{
    "S4.T1": {
        "source_file": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author’s email: riccardofosco.gramaccioni@uniroma1.it",
        "caption": "TABLE I: Results for FoleyGRAM and comparison with other SOTA models on Greatest Hits. HRC stands for Human Readable Control and refers to the use of time conditionings signals that sound designers can use to control the generation process (i.e., envelope or onsets). Our model provides the best results on all objective metrics compared to the baselines.",
        "body": "Model\nHRC\nFAD-C ↓\nFAD-LC ↓\nCLAP ↑\nFAVD ↓\n\n\n\nSpecVQGAN [15]\n\n✗\n1001\n0.7102\n0.1418\n0.1418\n\n\n\nDiff-Foley [16]\n\n✗\n654\n0.4690\n0.3733\n0.3733\n\n\n\nCondFoleyGen [17]\n\n✗\n650\n0.4883\n0.4879\n0.4879\n\n\n\nSyncFusion [2] (Audio)\n✓\n591\n0.4365\n0.5154\n0.5154\n\n\n\nSyncFusion (Text)\n✓\n542\n0.2793\n0.6621\n0.6621\n\n\n\nVideo-Foley [40] (Audio)\n✓\n644\n0.4997\n0.3680\n0.3680\n\n\n\nVideo-Foley (Text)\n✓\n435\n0.1671\n0.6779\n0.6779\n\n\n\nFoleyGRAM (Ours)\n✓\n235\n0.0720\n0.7083\n0.8912",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">HRC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAD-C &#8595;</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAD-LC &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CLAP &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAVD &#8595;</span></td>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SpecVQGAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib15\" title=\"\">15</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">&#10007;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1001</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.7102</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1418</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1418</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Diff-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib16\" title=\"\">16</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">&#10007;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">654</th>\n<td class=\"ltx_td ltx_align_center\">0.4690</td>\n<td class=\"ltx_td ltx_align_center\">0.3733</td>\n<td class=\"ltx_td ltx_align_center\">0.3733</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CondFoleyGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib17\" title=\"\">17</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">&#10007;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">650</th>\n<td class=\"ltx_td ltx_align_center\">0.4883</td>\n<td class=\"ltx_td ltx_align_center\">0.4879</td>\n<td class=\"ltx_td ltx_align_center\">0.4879</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SyncFusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib2\" title=\"\">2</a>]</cite> (Audio)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">591</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.4365</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5154</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5154</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SyncFusion (Text)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">542</th>\n<td class=\"ltx_td ltx_align_center\">0.2793</td>\n<td class=\"ltx_td ltx_align_center\">0.6621</td>\n<td class=\"ltx_td ltx_align_center\">0.6621</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Video-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib40\" title=\"\">40</a>]</cite> (Audio)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">644</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.4997</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.3680</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.3680</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Video-Foley (Text)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">435</th>\n<td class=\"ltx_td ltx_align_center\">0.1671</td>\n<td class=\"ltx_td ltx_align_center\">0.6779</td>\n<td class=\"ltx_td ltx_align_center\">0.6779</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">FoleyGRAM (Ours)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">235</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0720</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.7083</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.8912</span></td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "difffoley",
            "text",
            "ours",
            "greatest",
            "process",
            "refers",
            "designers",
            "our",
            "objective",
            "readable",
            "hits",
            "conditionings",
            "all",
            "metrics",
            "condfoleygen",
            "specvqgan",
            "human",
            "clap",
            "sota",
            "results",
            "generation",
            "envelope",
            "model",
            "foleygram",
            "stands",
            "fadlc",
            "videofoley",
            "favd",
            "syncfusion",
            "fadc",
            "time",
            "signals",
            "models",
            "sound",
            "compared",
            "best",
            "use",
            "onsets",
            "hrc",
            "control",
            "baselines",
            "comparison",
            "audio",
            "provides"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S4.T1\" title=\"TABLE I &#8227; IV-C Training and Inference Details &#8227; IV Experiments &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, FoleyGRAM demonstrates substantial improvements in semantic quality of the generated audio compared to all baseline models. This enhancement is primarily attributed to the integration of the multimodal-aligned encoder GRAM for conditioning the state-of-the-art audio generation model, Stable Audio. Unlike Video-Foley and SyncFusion, which rely on CLAP as the audio encoder for semantic conditioning, our approach leverages GRAM to ensure alignment across audio, video, and text modalities. This alignment enables FoleyGRAM to better capture the semantic features required for precise audio generation.\nOur model is also able to provide strong results on CLAP-based metrics, surpassing even Video-Foley and SyncFusion, despite the latter directly rely on CLAP for their semantic encoders.\nThe improved evaluation metrics scores of FoleyGRAM confirm the advantages of employing a unified multimodal encoder like GRAM for conditioning, particularly in scenarios where cross-modal consistency is essential.\nAdditionally, the use of Stable Audio as the backbone for audio generation ensures high-definition, stereo audio at 44.1 kHz, aligning with professional audio standards. The integration of ControlNet within our architecture further enhances the ability of the model to incorporate temporal conditioning through envelopes, ensuring precise timing and dynamic for the generated waveforms. Notably, FoleyGRAM achieves these results while being lightweight and efficient, requiring only approximately six hours of data of which the Greatest Hits dataset is composed and a limited number of training steps. This efficiency underscores the robustness and practicality of our approach for real-world sound design applications.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we present FoleyGRAM, a novel approach to video-to-audio generation that emphasizes semantic conditioning through the use of aligned multimodal encoders. Building on prior advancements in video-to-audio generation, FoleyGRAM leverages the Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities, enabling precise semantic control over the audio generation process. The core of FoleyGRAM is a diffusion-based audio synthesis model conditioned on GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness and temporal alignment with the corresponding input video. We evaluate FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio models. Our experiments demonstrate that aligning multimodal encoders using GRAM enhances the system&#8217;s ability to semantically align generated audio with video content, advancing the state of the art in video-to-audio synthesis.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "hits",
                    "model",
                    "foleygram",
                    "text",
                    "models",
                    "greatest",
                    "process",
                    "use",
                    "control",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, transforming visual information into audio representations, known as video-to-audio (V2A) generation task, has gained increasing attention. V2A task is discovering extremely attractive applications in fields concerning sound design in cinema and video games, enhancing accessibility tools, and creating immersive multimedia experiences. Central to this challenge is the ability to generate audio that not only matches the temporal and structural properties of the visual input but also captures its semantics.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "sound",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Usually, multiple semantic inputs can be used in the process of generating audio, as different semantic conditioning may allow the control of diverse aspects of the generated waveform <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib1\" title=\"\">1</a>]</cite>. Typically, for V2A task, semantics is controlled through video, audio, or text conditioning, and existing methods rely on encoder architectures to condition the audio generation process on such relevant visual and semantic cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib6\" title=\"\">6</a>]</cite>. While effective in some cases, this approach has severe limitations that undermine the effective control of semantics in generated audio. A significant limitation of these approaches lies in the lack of joint training for the encoders used across different modalities. This disjoint training paradigm often results in the creation of separate latent spaces for each modality, leading to misaligned embeddings that the generative model may semantically badly interpret <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib7\" title=\"\">7</a>]</cite>. Additionally, even in the case of jointly-trained encoders, misalignment in the latent space may occur, as all previous methods solely rely on cosine similarities that can only be computed between pairs of modalities <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>]</cite>. More specifically, state-of-the-art models select an anchor modality and align all other modalities to the anchor. Examples are ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib9\" title=\"\">9</a>]</cite> that selects the image modality as anchor, or LanguageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib10\" title=\"\">10</a>]</cite>, selecting the text modality instead. Although promising, this approach does not provide any geometrical guarantees that the other modalities are aligned with each other and, in practice, they are not <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite>. Therefore, during training, such encoders may end up in a local minimum or may not guarantee all the modalities&#8217; true geometric alignment together. Such misalignment can compromise the semantic coherence of the generated audio, reducing the model&#8217;s ability to faithfully represent the desired audiovisual relationship.</p>\n\n",
                "matched_terms": [
                    "other",
                    "generation",
                    "model",
                    "text",
                    "models",
                    "all",
                    "process",
                    "control",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we propose FoleyGRAM, a novel approach that leverages the Gramian Representation Alignment Measure (GRAM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite> to ensure aligned latent representations across multiple modalities. GRAM enables the construction of a shared latent space that is jointly trained and optimized, providing a robust framework for embedding alignment. Indeed, GRAM relies on the computation of the volume of the high-dimensional parallelotope defined by the modalities embeddings, which provides direct insights into the joint alignment of all the modalities at once, avoiding pairwise computations. By aligning the latent spaces of video, text, and audio modalities, FoleyGRAM facilitates precise semantic conditioning, enhancing the quality and relevance of the generated audio.\nAt the generative core of FoleyGRAM is a diffusion-based audio synthesis model, conditioned on GRAM-aligned embeddings and additional waveform envelope information. This dual conditioning mechanism ensures semantic fidelity through GRAM and also temporal synchronization between the input video and the generated audio by means of the envelope. The effectiveness of our approach is demonstrated on the Greatest Hits dataset, a benchmark for video-to-audio generation. Experimental results show that FoleyGRAM achieves superior results compared with common baseline methods for V2A tasks, with better semantic alignment and audio quality. An example of a result is shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "envelope",
                    "hits",
                    "model",
                    "foleygram",
                    "text",
                    "all",
                    "provides",
                    "compared",
                    "greatest",
                    "results",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose FoleyGRAM, a novel V2A model able to generate semantically meaningful and temporally aligned audio from video.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "foleygram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use GRAM for producing highly semantically aligned embeddings for the generative model conditioning, resulting in unified semantic controls.</p>\n\n",
                "matched_terms": [
                    "model",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">FoleyGRAM achieves enhanced semantic fidelity through the use of such unified, jointly trained and optimized latent space. Comprehensive evaluations validate the effectiveness of our approach, demonstrating advancements in semantic alignment and generative quality.</p>\n\n",
                "matched_terms": [
                    "use",
                    "foleygram",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these contributions, FoleyGRAM represents a significant step forward in video-to-audio generation, offering new solutions for multimodal semantic conditioning in generative models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "foleygram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-to-Audio Generation.</span> The task of generating audio aligned with video has gained increasing attraction in multimedia post-production, driven by recent advancements in deep learning. Several state-of-the-art models have been proposed, aiming to achieve both semantic coherence and temporal alignment between the visual input and the generated audio. Early approaches, such as Im2Wav <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib12\" title=\"\">12</a>]</cite>, utilized transformer-based architectures conditioned on visual features extracted using CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite>, while models like RegNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib14\" title=\"\">14</a>]</cite> employed GANs with video encoders to synthesize temporally aligned audio from video inputs. These efforts demonstrated the potential of multimodal learning but often suffered from limitations in alignment precision and semantic control.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "control",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent innovations, including SpecVQGAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib15\" title=\"\">15</a>]</cite> and Diff-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib16\" title=\"\">16</a>]</cite>, have further improved temporal and semantic alignment by leveraging optical flow features and contrastive learning strategies. For instance, Diff-Foley employs Contrastive Audio-Visual Pretraining (CAVP) to align video and audio embeddings before conditioning a latent diffusion model. Similarly, CondFoleyGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib17\" title=\"\">17</a>]</cite> demonstrates the utility of training directly on benchmark datasets, achieving improved alignment through Transformer-based architectures. However, these methods lack human-intelligible controls, limiting their utility in practical sound design applications <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib18\" title=\"\">18</a>]</cite>.\nSyncFusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib2\" title=\"\">2</a>]</cite> addresses some of these challenges by introducing a human-readable control mechanism based on onset tracks. While this approach provides temporal guidance for audio generation, it requires manually annotated datasets and may not capture finer semantic details, such as sound intensity or duration. Finally, models like T-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib19\" title=\"\">19</a>]</cite> demonstrated the effectiveness of envelope-based conditioning for precise temporal alignment but lacked the flexibility to integrate semantic controls across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "difffoley",
                    "models",
                    "control",
                    "condfoleygen",
                    "sound",
                    "specvqgan",
                    "syncfusion",
                    "audio",
                    "provides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Alignment.</span> The alignment of multiple modalities is a crucial and challenging task for enabling deep learning models to understand surrounding reality and generate content accordingly. The introduction of foundational models for two modalities like CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite> for text and images has significantly influenced cross-modal alignment, inspiring subsequent works such as CLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib20\" title=\"\">20</a>]</cite> for audio-text alignment. Such works rely on the cosine similarity between the two modalities and establish the conventional receipt for multimodal alignment. Indeed, the pairwise cosine similarity has been leveraged in following works like CLIP4VLA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib21\" title=\"\">21</a>]</cite> integrating text, images, and audio samples, ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib9\" title=\"\">9</a>]</cite>, LanguageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib10\" title=\"\">10</a>]</cite>, and VAST <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib22\" title=\"\">22</a>]</cite> scaling up to 5 modalities. Despite the improved performance, these methods rely on the same cosine similarity loss function and align all the modalities to a select anchor one, providing no guarantees that all other modalities are aligned with each other, thus limiting the expressiveness of the latent space and resulting in modalities that may not be aligned in practice.</p>\n\n",
                "matched_terms": [
                    "other",
                    "text",
                    "models",
                    "all",
                    "clap",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conventionally, multimodal models align their representations according to the cosine similarity score between pairs of modalities. The cosine similarity is incorporated into the InfoNCE loss <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib23\" title=\"\">23</a>]</cite> as done for two modalities by CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite>. However, when scaling to more than two modalities like in the video-audio-text case, the cosine similarity-based loss has severe limitations and fails to learn a unified latent space, obtaining suboptimal performance in downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>]</cite>.\nTo avoid such limitations, we involve GRAM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite>, a recent multimodal model able to learn a unified latent space by means of a brand-new loss function. The GRAM loss function is based on the intuition that modalities embedding vectors lie in a hypersphere with unitary norm and that those vectors act as the edges of a high-dimensional parallelotope. Then, the volume of such parallelotope provides direct information about the alignment of the vectors, being small in the case of aligned data and large in the case of vectors representing different semantic concepts, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.F2\" title=\"Figure 2 &#8227; III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nMore formally, consider the three latent representations of audio <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, video <math alttext=\"\\mathbf{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119855;</mi><annotation encoding=\"application/x-tex\">\\mathbf{v}</annotation></semantics></math>, and text <math alttext=\"\\mathbf{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119853;</mi><annotation encoding=\"application/x-tex\">\\mathbf{t}</annotation></semantics></math> be vectors in <math alttext=\"\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#8477;</mi><mi>n</mi></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{n}</annotation></semantics></math> arranged in a matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> containing its dot products. From <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> we can easily compute the Gram matrix as <math alttext=\"\\textbf{G}(\\textbf{t},\\textbf{a},\\textbf{v})\\in\\mathbb{R}^{3\\times 3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mtext class=\"ltx_mathvariant_bold\">G</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext class=\"ltx_mathvariant_bold\">t</mtext><mo>,</mo><mtext class=\"ltx_mathvariant_bold\">a</mtext><mo>,</mo><mtext class=\"ltx_mathvariant_bold\">v</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textbf{G}(\\textbf{t},\\textbf{a},\\textbf{v})\\in\\mathbb{R}^{3\\times 3}</annotation></semantics></math> is defined:</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "audio",
                    "provides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">According to the GRAM loss function in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E3\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), the GRAM model consists of three encoders to encode the different modalities into the latent space. The video modality is encoded with EVAClip-ViT-G <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib25\" title=\"\">25</a>]</cite>, the text one with BERT-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib26\" title=\"\">26</a>]</cite>, while the audio with BEATs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio synthesis model leverages Stable Audio Open <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib28\" title=\"\">28</a>]</cite>, a state-of-the-art latent diffusion model (LDM) for generating high-quality, stereo audio at 44.1 kHz.\nWhile Stable Audio excels at generating semantically rich audio from text prompts, it lacks explicit mechanisms for temporal and multimodal conditioning, making it unsuitable for video-to-audio (V2A) tasks.\nTo address this limitation, we introduce novel conditioning strategies leveraging the Gramian Representation Alignment Measure (GRAM) to guide the synthesis process semantically.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "process",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The temporal alignment is provided using directly the envelope extracted with librosa library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/main/generated/librosa.feature.rms.html\" title=\"\">https://librosa.org/doc/main/generated/librosa.feature.rms.html</a></span></span></span> from the ground truth audio, such as the main scope of this work is focusing on the semantic alignment and not introducing novel methods for temporal synchrony.</p>\n\n",
                "matched_terms": [
                    "envelope",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">our novel approach lies in the use of GRAM-aligned embeddings as conditioning inputs for the audio synthesis model. Unlike previous methods that rely on separately trained encoders (e.g., CLAP or CAVP) with unaligned latent spaces, our approach integrates GRAM-trained encoders to produce a unified latent representation for video, text, and audio modalities. This alignment ensures consistent and semantically meaningful interactions across modalities, enabling precise control over the audio generation process.\nSpecifically, we condition the audio synthesis model on a set of multimodal embeddings <math alttext=\"\\mathbf{F}={\\mathbf{f}_{1},\\mathbf{f}_{2},\\mathbf{f}_{3}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119813;</mi><mo>=</mo><mrow><msub><mi>&#119839;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119839;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#119839;</mi><mn>3</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{F}={\\mathbf{f}_{1},\\mathbf{f}_{2},\\mathbf{f}_{3}}</annotation></semantics></math>, where each <math alttext=\"\\mathbf{f}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119839;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{f}_{i}</annotation></semantics></math> represents a semantic embedding derived from GRAM encoders trained jointly across the three modalities. These embeddings are integrated into the diffusion process through cross-attention mechanisms, as originally proposed for global conditioning in Stable Audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib29\" title=\"\">29</a>]</cite>.\nDuring inference, we can use all the modality togheter, as done during training, or we can use them separately.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "text",
                    "all",
                    "process",
                    "use",
                    "clap",
                    "control",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the temporal alignment is provided by an envelope extracted directly from the ground truth audio.\nThe <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th sample of the temporal sequence representing the envelope is then calculated on a window of the audio signal <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> as follows:</p>\n\n",
                "matched_terms": [
                    "envelope",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> is the window size and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the hop size. In our experiments we set <math alttext=\"W=512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">W=512</annotation></semantics></math> and <math alttext=\"h=128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">h=128</annotation></semantics></math>.\nThe envelope serves as a coarse temporal guide, providing information about the timing and intensity of audio events. To encode this temporal control, we utilize the pre-trained VAE from Stable Audio, which downsamples the input stereo audio by a factor of 1024, mapping it into a compact latent space. The latent representation of the envelope, <math alttext=\"\\mathbf{r_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{r_{c}}</annotation></semantics></math>, is processed through a ControlNet-inspired architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib30\" title=\"\">30</a>]</cite>, allowing fine-grained temporal adjustments during audio generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "envelope",
                    "control",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">our audio model is based on Stable Audio and follows the standard latent denoising diffusion formulation. Given a noisy latent representation <math alttext=\"\\mathbf{z}=\\mathcal{E}(\\mathbf{y})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119859;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}=\\mathcal{E}(\\mathbf{y})</annotation></semantics></math> at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model learns to estimate the noise <math alttext=\"\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{F},\\mathbf{r_{c}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>t</mi></msub><mo>,</mo><mi>t</mi><mo>,</mo><mi>&#119813;</mi><mo>,</mo><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{F},\\mathbf{r_{c}})</annotation></semantics></math> conditioned on semantic embeddings <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mi>&#119813;</mi><annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation></semantics></math> and the temporal control signal <math alttext=\"\\mathbf{r_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{r_{c}}</annotation></semantics></math>.\nIn the forward process, Gaussian noise is slowly added to the original data distribution with a fixed schedule <math alttext=\"\\alpha_{1},\\ldots,\\alpha_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#945;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#945;</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1},\\ldots,\\alpha_{T}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the total timesteps, and <math alttext=\"\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8719;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mi>&#945;</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "time",
                    "model",
                    "process",
                    "control",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training objective is the the same L2 loss on which Stable Audio\nmodels are trained<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib31\" title=\"\">31</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We freeze the pre-trained weights of the diffusion model and only train the ControlNet layers, which process the RMS envelope, and the linear projections that align GRAM embeddings to the conditioning dimensions of Stable Audio. By jointly leveraging GRAM-aligned embeddings for semantic control and the ControlNet mechanism for temporal alignment, our model ensures that the generated audio aligns both semantically and temporally with the input video. A block diagram of the proposed architecture is shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.F3\" title=\"Figure 3 &#8227; III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "envelope",
                    "model",
                    "process",
                    "control",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We work with the <span class=\"ltx_text ltx_font_italic\">Greatest Hits</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib32\" title=\"\">32</a>]</cite>, a well-known and widespread benchmark for video-to-audio generation tasks. The dataset contains videos of people using a drumstick to strike or rub different surfaces and objects. The choice of a drumstick as main object in motion allows the scene&#8217;s action to remain clearly visible with minimal occlusion of the frame. Each video captures the sound of these interactions with a shotgun microphone attached to the camera, and the audio is later processed to remove noise. Metadata provided for each video is used to create textual prompts following the structure proposed in Fol&#183;AI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib18\" title=\"\">18</a>]</cite>: &#8220;A person {<span class=\"ltx_text ltx_font_italic\">action</span>} {<span class=\"ltx_text ltx_font_italic\">frequency</span>} on {<span class=\"ltx_text ltx_font_italic\">material</span>} with a wooden stick.&#8221; The placeholders {<span class=\"ltx_text ltx_font_italic\">action</span>} (e.g., &#8220;hit&#8221; or &#8220;scratch&#8221;), {<span class=\"ltx_text ltx_font_italic\">frequency</span>} (e.g., &#8220;multiple times&#8221; or &#8220;once&#8221;), and {<span class=\"ltx_text ltx_font_italic\">material</span>} are populated based on the metadata details.\nThe carefully curated samples of this dataset are crucial for V2A model training, as real-world video datasets often lack both the audiovisual alignment and the quality required to make models understand how to produce audio that is semantically and temporally consistent with the input video. This dataset contains 977 video recordings captured in diverse settings, both indoors and outdoors. Indoor videos showcase materials like metal, plastic, and cloth, while outdoor recordings feature dynamic materials such as water, leaves, and grass. We extract 10-second-long chunks from each sample to train and test our model. On average, each video includes 48 distinct actions, split between striking and rubbing, ensuring that each extracted chunk has sufficient activity.\nFor our experiments, we split the dataset into 732 videos for training, 49 for validation, and 196 for testing.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "hits",
                    "models",
                    "sound",
                    "greatest",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For an objective evaluation of our model, we utilize the most commonly adopted metrics to assess semantic quality in V2A tasks:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "model",
                    "our",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fr&#233;chet Audio Distance (FAD):\nFAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib33\" title=\"\">33</a>]</cite> is a metric designed to assess the quality and realism of generated audio by comparing it to reference audio. It evaluates the similarity between the statistical distributions of embeddings extracted from real and generated waveforms. The choice of the audio encoder for extracting these embeddings plays a crucial role, as different encoders emphasize various audio features, affecting how well the metric aligns with human perception <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib34\" title=\"\">34</a>]</cite>. To account for this, we calculate FAD using two distinct audio encoders: Microsoft CLAP (FAD-C) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib35\" title=\"\">35</a>]</cite>, and Laion-CLAP (FAD-LC) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib36\" title=\"\">36</a>]</cite>. The FAD scores are computed using the <span class=\"ltx_text ltx_font_italic\">fadtk<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/fadtk\" title=\"\">https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/fadtk</a></span></span></span></span> library.</p>\n\n",
                "matched_terms": [
                    "fadc",
                    "fadlc",
                    "human",
                    "clap",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLAP-score:\nThe CLAP-score evaluates the overall quality of the generated waveforms, also used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib28\" title=\"\">28</a>]</cite>. It calculates the cosine similarity between embeddings of ground truth and generated audio, which are obtained using the CLAP model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib36\" title=\"\">36</a>]</cite>. Given that the majority of baseline models employs CLAP as the primary audio representation, this metric serves as a key indicator of how effectively the conditioning features contribute to generating the final output for a fair comparison.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "clap",
                    "comparison",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fr&#233;chet Audio-Visual Distance (FAVD):\nFAVD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib37\" title=\"\">37</a>]</cite> is increasingly recognized for evaluating video-to-audio (V2A) models. It measures the alignment, both temporal and semantic, between the audio and video modalities. This metric calculates the Fr&#233;chet Distance between video embeddings and audio embeddings. For our evaluation, we use I3D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib38\" title=\"\">38</a>]</cite> as the video encoder and VGGish <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib39\" title=\"\">39</a>]</cite> as the audio encoder, extracting embeddings from both ground truth videos and the generated audio to determine their alignment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "favd",
                    "use",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For training FoleyGRAM, we initialize the model weights using the Stable Audio Open repository and its associated checkpoint. The ground truth audio used in our experiments is 44.1 kHz stereo recordings from the Greatest Hits dataset. The model is trained on a single Nvidia RTX A6000 GPU (48 GB) with a batch size of 12 for 20,000 steps. The training process employs the AdamW optimizer, with parameters configured as those in Stable Audio Open, and uses a fixed learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "hits",
                    "model",
                    "foleygram",
                    "greatest",
                    "process",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To initialize GRAM encoders, we use the official associated repository and its relative checkpoints. Three GRAM encoders, which are EVAClip-ViT-G for video, BEATs for audio, and BERT-B for text with a total number of parameters equal to 1B, have been previously pretrained on the VAST27M dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib22\" title=\"\">22</a>]</cite> with conventional contrastive loss functions. Later, the learned latent space is rearranged and pretrained on a subset of such dataset comprising 150k samples with the GRAM losses in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E3\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E4\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and finally fine-tuned on the Greatest Hits dataset to make the encoders aware of the particular cases of this dataset. The pertaining on the subset of VAST27M dataset has been carried on for one epoch with learning rate <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> with a batch size of 256 on 4 NVIDIA A100 cards, and the same configuration holds for the fine-tuning on&#160;Greatest Hits.</p>\n\n",
                "matched_terms": [
                    "hits",
                    "text",
                    "greatest",
                    "use",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, envelopes extracted directly from ground truth audio are interpolated to match the target sample rate, and fed into the ControlNet from the audio synthesis model as inputs. The model then generates the final output in 150 sampling steps, applying classifier-free guidance with a guidance scale set to 2.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our model against the main publicly available V2A models at the time of this study.</p>\n\n",
                "matched_terms": [
                    "time",
                    "models",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model uses a similar architecture respect to SpecVQGAN, adding additional controls on the final output conditioning with audio and video features from the semantic target. The model is trained directly using Greatest Hits, succeeding in achieving an efficient alignment in both content and\ntiming with the reference video.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hits",
                    "greatest",
                    "specvqgan",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">leverages Contrastive Audio-Visual Pretraining (CAVP) to achieve temporal and semantic alignment between audio and video modalities, enabling the generation of video embeddings with features pertinent to the associated audio. These embeddings are then employed as direct conditioning inputs for Stable-Diffusion.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model is the first to introduce a human-readable control mechanism for the V2A task. It utilizes a ResNet(2+1)D-18 based video encoder, which processes video frames to generate an onset track. This onset track is subsequently fed into a time-domain diffusion model to produce the final audio output.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model uses a video encoder through which the RMS of the audio signal associated with the input video can be mapped, which is then used as the control signal for the temporal alignment of the model. In contrast, the semantics of the final output is controlled by embeddings produced by the CLAP audio/text encoder. These control signals are used to generate 16kHz mono audio through the use of AudioLDM.</p>\n\n",
                "matched_terms": [
                    "model",
                    "signals",
                    "use",
                    "clap",
                    "control",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all of the above models, we use the official released codes provided on GitHub and relative checkpoints.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GRAM allows the alignment of three modalities, audio video and text, which can be used together to provide meaningful semantic information to the synthesis model. Conditioning with multiple modalities allows for better control over the semantics of the generated waveform. To demonstrate this assertion also in our generation task, at inference time we conditioned our model in seven different ways: first using all three modalities simultaneously (AVT), then audio and video (AV), audio and text (AT) and video and text (VT), and finally the single modalities audio (A), Video (V) and text (T). The results shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S4.T2\" title=\"TABLE II &#8227; IV-C Training and Inference Details &#8227; IV Experiments &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> demonstrate that using multiple modalities simultaneously succeeds in providing the model with more semantic information, achieving the best results in the case of AVT conditioning.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "time",
                    "text",
                    "all",
                    "best",
                    "control",
                    "results",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced FoleyGRAM, a novel V2A synthesis model that combines a state-of-the-art audio generation framework, Stable Audio, with GRAM, a unified multimodal encoder designed for cross-modal alignment. Our results demonstrate significant advancements in the semantic accuracy, achieving strong performances across key semantic metrics. By leveraging GRAM as the primary encoder for semantic conditioning, FoleyGRAM can use multiple aligned modalities simultaneously in order to leverage as much information as possible to generate waveforms with rich semantic information. Additionally, the integration of ControlNet allows for precise temporal control through envelopes, enabling the generation of high-quality 44.1 kHz stereo audio.\nFoleyGRAM achieves these results with a lightweight architecture and efficient training, requiring only a small dataset and limited computational resources. This makes our model a powerful tool for sound designers and also a practical solution for real-world applications where resource constraints are a factor.\nThe proposed model wants to encourage further exploration of multimodal deep learning in V2A tasks, highlighting the potential of unified embeddings and advanced generative models to bridge the gap between visual and audio modalities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "foleygram",
                    "models",
                    "metrics",
                    "sound",
                    "use",
                    "control",
                    "designers",
                    "audio",
                    "our",
                    "results"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author’s email: riccardofosco.gramaccioni@uniroma1.it",
        "caption": "TABLE II: Ablation studies: conditioning FoleyGRAM with all modalities (AVT), audio and video (AV), audio and text (AT), video and text (VT), audio (A), video (V) and text (T) modalities. For all the experiments, the conditioning modalities are the ground truth samples, even though at inference time any kind of sample can be used to condition the semantics of the waveforms.",
        "body": "Conditionings\nFAD-C ↓\\downarrow\nFAD-LC ↓\\downarrow\nFAVD ↓\\downarrow\nCLAP ↑\\uparrow\n\n\n\n\nAVT\n235\n0.072\n0.8912\n0.7083\n\n\nAV\n238\n0.074\n0.9309\n0.7007\n\n\nAT\n287\n0.093\n0.9978\n0.6814\n\n\nVT\n269\n0.119\n1.1739\n0.6623\n\n\nA\n325\n0.135\n1.6513\n0.6155\n\n\nV\n271\n0.122\n1.2003\n0.6543\n\n\nT\n1069\n0.797\n6.1288\n0.1962",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Conditionings</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAD-C <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAD-LC <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FAVD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CLAP <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">AVT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">235</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.072</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.8912</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.7083</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">AV</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">238</th>\n<td class=\"ltx_td ltx_align_center\">0.074</td>\n<td class=\"ltx_td ltx_align_center\">0.9309</td>\n<td class=\"ltx_td ltx_align_center\">0.7007</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">AT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">287</th>\n<td class=\"ltx_td ltx_align_center\">0.093</td>\n<td class=\"ltx_td ltx_align_center\">0.9978</td>\n<td class=\"ltx_td ltx_align_center\">0.6814</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">VT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">269</th>\n<td class=\"ltx_td ltx_align_center\">0.119</td>\n<td class=\"ltx_td ltx_align_center\">1.1739</td>\n<td class=\"ltx_td ltx_align_center\">0.6623</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">325</th>\n<td class=\"ltx_td ltx_align_center\">0.135</td>\n<td class=\"ltx_td ltx_align_center\">1.6513</td>\n<td class=\"ltx_td ltx_align_center\">0.6155</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">V</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">271</th>\n<td class=\"ltx_td ltx_align_center\">0.122</td>\n<td class=\"ltx_td ltx_align_center\">1.2003</td>\n<td class=\"ltx_td ltx_align_center\">0.6543</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">1069</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.797</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.1288</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1962</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "though",
            "avt",
            "ablation",
            "text",
            "↓downarrow",
            "modalities",
            "video",
            "even",
            "ground",
            "studies",
            "used",
            "conditionings",
            "all",
            "sample",
            "conditioning",
            "clap",
            "waveforms",
            "foleygram",
            "fadlc",
            "favd",
            "truth",
            "↑uparrow",
            "samples",
            "semantics",
            "fadc",
            "time",
            "experiments",
            "any",
            "inference",
            "condition",
            "kind",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">GRAM allows the alignment of three modalities, audio video and text, which can be used together to provide meaningful semantic information to the synthesis model. Conditioning with multiple modalities allows for better control over the semantics of the generated waveform. To demonstrate this assertion also in our generation task, at inference time we conditioned our model in seven different ways: first using all three modalities simultaneously (AVT), then audio and video (AV), audio and text (AT) and video and text (VT), and finally the single modalities audio (A), Video (V) and text (T). The results shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S4.T2\" title=\"TABLE II &#8227; IV-C Training and Inference Details &#8227; IV Experiments &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> demonstrate that using multiple modalities simultaneously succeeds in providing the model with more semantic information, achieving the best results in the case of AVT conditioning.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we present FoleyGRAM, a novel approach to video-to-audio generation that emphasizes semantic conditioning through the use of aligned multimodal encoders. Building on prior advancements in video-to-audio generation, FoleyGRAM leverages the Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities, enabling precise semantic control over the audio generation process. The core of FoleyGRAM is a diffusion-based audio synthesis model conditioned on GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness and temporal alignment with the corresponding input video. We evaluate FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio models. Our experiments demonstrate that aligning multimodal encoders using GRAM enhances the system&#8217;s ability to semantically align generated audio with video content, advancing the state of the art in video-to-audio synthesis.</p>\n\n",
                "matched_terms": [
                    "foleygram",
                    "text",
                    "experiments",
                    "conditioning",
                    "video",
                    "modalities",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, transforming visual information into audio representations, known as video-to-audio (V2A) generation task, has gained increasing attention. V2A task is discovering extremely attractive applications in fields concerning sound design in cinema and video games, enhancing accessibility tools, and creating immersive multimedia experiences. Central to this challenge is the ability to generate audio that not only matches the temporal and structural properties of the visual input but also captures its semantics.</p>\n\n",
                "matched_terms": [
                    "semantics",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Usually, multiple semantic inputs can be used in the process of generating audio, as different semantic conditioning may allow the control of diverse aspects of the generated waveform <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib1\" title=\"\">1</a>]</cite>. Typically, for V2A task, semantics is controlled through video, audio, or text conditioning, and existing methods rely on encoder architectures to condition the audio generation process on such relevant visual and semantic cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib6\" title=\"\">6</a>]</cite>. While effective in some cases, this approach has severe limitations that undermine the effective control of semantics in generated audio. A significant limitation of these approaches lies in the lack of joint training for the encoders used across different modalities. This disjoint training paradigm often results in the creation of separate latent spaces for each modality, leading to misaligned embeddings that the generative model may semantically badly interpret <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib7\" title=\"\">7</a>]</cite>. Additionally, even in the case of jointly-trained encoders, misalignment in the latent space may occur, as all previous methods solely rely on cosine similarities that can only be computed between pairs of modalities <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>]</cite>. More specifically, state-of-the-art models select an anchor modality and align all other modalities to the anchor. Examples are ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib9\" title=\"\">9</a>]</cite> that selects the image modality as anchor, or LanguageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib10\" title=\"\">10</a>]</cite>, selecting the text modality instead. Although promising, this approach does not provide any geometrical guarantees that the other modalities are aligned with each other and, in practice, they are not <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite>. Therefore, during training, such encoders may end up in a local minimum or may not guarantee all the modalities&#8217; true geometric alignment together. Such misalignment can compromise the semantic coherence of the generated audio, reducing the model&#8217;s ability to faithfully represent the desired audiovisual relationship.</p>\n\n",
                "matched_terms": [
                    "semantics",
                    "text",
                    "all",
                    "any",
                    "video",
                    "modalities",
                    "conditioning",
                    "condition",
                    "even",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we propose FoleyGRAM, a novel approach that leverages the Gramian Representation Alignment Measure (GRAM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite> to ensure aligned latent representations across multiple modalities. GRAM enables the construction of a shared latent space that is jointly trained and optimized, providing a robust framework for embedding alignment. Indeed, GRAM relies on the computation of the volume of the high-dimensional parallelotope defined by the modalities embeddings, which provides direct insights into the joint alignment of all the modalities at once, avoiding pairwise computations. By aligning the latent spaces of video, text, and audio modalities, FoleyGRAM facilitates precise semantic conditioning, enhancing the quality and relevance of the generated audio.\nAt the generative core of FoleyGRAM is a diffusion-based audio synthesis model, conditioned on GRAM-aligned embeddings and additional waveform envelope information. This dual conditioning mechanism ensures semantic fidelity through GRAM and also temporal synchronization between the input video and the generated audio by means of the envelope. The effectiveness of our approach is demonstrated on the Greatest Hits dataset, a benchmark for video-to-audio generation. Experimental results show that FoleyGRAM achieves superior results compared with common baseline methods for V2A tasks, with better semantic alignment and audio quality. An example of a result is shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "foleygram",
                    "text",
                    "all",
                    "video",
                    "modalities",
                    "conditioning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose FoleyGRAM, a novel V2A model able to generate semantically meaningful and temporally aligned audio from video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "audio",
                    "foleygram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these contributions, FoleyGRAM represents a significant step forward in video-to-audio generation, offering new solutions for multimodal semantic conditioning in generative models.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "foleygram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-to-Audio Generation.</span> The task of generating audio aligned with video has gained increasing attraction in multimedia post-production, driven by recent advancements in deep learning. Several state-of-the-art models have been proposed, aiming to achieve both semantic coherence and temporal alignment between the visual input and the generated audio. Early approaches, such as Im2Wav <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib12\" title=\"\">12</a>]</cite>, utilized transformer-based architectures conditioned on visual features extracted using CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite>, while models like RegNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib14\" title=\"\">14</a>]</cite> employed GANs with video encoders to synthesize temporally aligned audio from video inputs. These efforts demonstrated the potential of multimodal learning but often suffered from limitations in alignment precision and semantic control.</p>\n\n",
                "matched_terms": [
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent innovations, including SpecVQGAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib15\" title=\"\">15</a>]</cite> and Diff-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib16\" title=\"\">16</a>]</cite>, have further improved temporal and semantic alignment by leveraging optical flow features and contrastive learning strategies. For instance, Diff-Foley employs Contrastive Audio-Visual Pretraining (CAVP) to align video and audio embeddings before conditioning a latent diffusion model. Similarly, CondFoleyGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib17\" title=\"\">17</a>]</cite> demonstrates the utility of training directly on benchmark datasets, achieving improved alignment through Transformer-based architectures. However, these methods lack human-intelligible controls, limiting their utility in practical sound design applications <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib18\" title=\"\">18</a>]</cite>.\nSyncFusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib2\" title=\"\">2</a>]</cite> addresses some of these challenges by introducing a human-readable control mechanism based on onset tracks. While this approach provides temporal guidance for audio generation, it requires manually annotated datasets and may not capture finer semantic details, such as sound intensity or duration. Finally, models like T-Foley <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib19\" title=\"\">19</a>]</cite> demonstrated the effectiveness of envelope-based conditioning for precise temporal alignment but lacked the flexibility to integrate semantic controls across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "video",
                    "conditioning",
                    "modalities",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Alignment.</span> The alignment of multiple modalities is a crucial and challenging task for enabling deep learning models to understand surrounding reality and generate content accordingly. The introduction of foundational models for two modalities like CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite> for text and images has significantly influenced cross-modal alignment, inspiring subsequent works such as CLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib20\" title=\"\">20</a>]</cite> for audio-text alignment. Such works rely on the cosine similarity between the two modalities and establish the conventional receipt for multimodal alignment. Indeed, the pairwise cosine similarity has been leveraged in following works like CLIP4VLA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib21\" title=\"\">21</a>]</cite> integrating text, images, and audio samples, ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib9\" title=\"\">9</a>]</cite>, LanguageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib10\" title=\"\">10</a>]</cite>, and VAST <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib22\" title=\"\">22</a>]</cite> scaling up to 5 modalities. Despite the improved performance, these methods rely on the same cosine similarity loss function and align all the modalities to a select anchor one, providing no guarantees that all other modalities are aligned with each other, thus limiting the expressiveness of the latent space and resulting in modalities that may not be aligned in practice.</p>\n\n",
                "matched_terms": [
                    "text",
                    "all",
                    "modalities",
                    "clap",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conventionally, multimodal models align their representations according to the cosine similarity score between pairs of modalities. The cosine similarity is incorporated into the InfoNCE loss <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib23\" title=\"\">23</a>]</cite> as done for two modalities by CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib13\" title=\"\">13</a>]</cite>. However, when scaling to more than two modalities like in the video-audio-text case, the cosine similarity-based loss has severe limitations and fails to learn a unified latent space, obtaining suboptimal performance in downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib8\" title=\"\">8</a>]</cite>.\nTo avoid such limitations, we involve GRAM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib11\" title=\"\">11</a>]</cite>, a recent multimodal model able to learn a unified latent space by means of a brand-new loss function. The GRAM loss function is based on the intuition that modalities embedding vectors lie in a hypersphere with unitary norm and that those vectors act as the edges of a high-dimensional parallelotope. Then, the volume of such parallelotope provides direct information about the alignment of the vectors, being small in the case of aligned data and large in the case of vectors representing different semantic concepts, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.F2\" title=\"Figure 2 &#8227; III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nMore formally, consider the three latent representations of audio <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, video <math alttext=\"\\mathbf{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119855;</mi><annotation encoding=\"application/x-tex\">\\mathbf{v}</annotation></semantics></math>, and text <math alttext=\"\\mathbf{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119853;</mi><annotation encoding=\"application/x-tex\">\\mathbf{t}</annotation></semantics></math> be vectors in <math alttext=\"\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#8477;</mi><mi>n</mi></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{n}</annotation></semantics></math> arranged in a matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> containing its dot products. From <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> we can easily compute the Gram matrix as <math alttext=\"\\textbf{G}(\\textbf{t},\\textbf{a},\\textbf{v})\\in\\mathbb{R}^{3\\times 3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mtext class=\"ltx_mathvariant_bold\">G</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext class=\"ltx_mathvariant_bold\">t</mtext><mo>,</mo><mtext class=\"ltx_mathvariant_bold\">a</mtext><mo>,</mo><mtext class=\"ltx_mathvariant_bold\">v</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textbf{G}(\\textbf{t},\\textbf{a},\\textbf{v})\\in\\mathbb{R}^{3\\times 3}</annotation></semantics></math> is defined:</p>\n\n",
                "matched_terms": [
                    "text",
                    "video",
                    "modalities",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">According to the GRAM loss function in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E3\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), the GRAM model consists of three encoders to encode the different modalities into the latent space. The video modality is encoded with EVAClip-ViT-G <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib25\" title=\"\">25</a>]</cite>, the text one with BERT-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib26\" title=\"\">26</a>]</cite>, while the audio with BEATs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "video",
                    "modalities",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio synthesis model leverages Stable Audio Open <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib28\" title=\"\">28</a>]</cite>, a state-of-the-art latent diffusion model (LDM) for generating high-quality, stereo audio at 44.1 kHz.\nWhile Stable Audio excels at generating semantically rich audio from text prompts, it lacks explicit mechanisms for temporal and multimodal conditioning, making it unsuitable for video-to-audio (V2A) tasks.\nTo address this limitation, we introduce novel conditioning strategies leveraging the Gramian Representation Alignment Measure (GRAM) to guide the synthesis process semantically.</p>\n\n",
                "matched_terms": [
                    "text",
                    "conditioning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The temporal alignment is provided using directly the envelope extracted with librosa library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/main/generated/librosa.feature.rms.html\" title=\"\">https://librosa.org/doc/main/generated/librosa.feature.rms.html</a></span></span></span> from the ground truth audio, such as the main scope of this work is focusing on the semantic alignment and not introducing novel methods for temporal synchrony.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">our novel approach lies in the use of GRAM-aligned embeddings as conditioning inputs for the audio synthesis model. Unlike previous methods that rely on separately trained encoders (e.g., CLAP or CAVP) with unaligned latent spaces, our approach integrates GRAM-trained encoders to produce a unified latent representation for video, text, and audio modalities. This alignment ensures consistent and semantically meaningful interactions across modalities, enabling precise control over the audio generation process.\nSpecifically, we condition the audio synthesis model on a set of multimodal embeddings <math alttext=\"\\mathbf{F}={\\mathbf{f}_{1},\\mathbf{f}_{2},\\mathbf{f}_{3}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119813;</mi><mo>=</mo><mrow><msub><mi>&#119839;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119839;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#119839;</mi><mn>3</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{F}={\\mathbf{f}_{1},\\mathbf{f}_{2},\\mathbf{f}_{3}}</annotation></semantics></math>, where each <math alttext=\"\\mathbf{f}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119839;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{f}_{i}</annotation></semantics></math> represents a semantic embedding derived from GRAM encoders trained jointly across the three modalities. These embeddings are integrated into the diffusion process through cross-attention mechanisms, as originally proposed for global conditioning in Stable Audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib29\" title=\"\">29</a>]</cite>.\nDuring inference, we can use all the modality togheter, as done during training, or we can use them separately.</p>\n\n",
                "matched_terms": [
                    "text",
                    "all",
                    "video",
                    "modalities",
                    "conditioning",
                    "inference",
                    "clap",
                    "condition",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the temporal alignment is provided by an envelope extracted directly from the ground truth audio.\nThe <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th sample of the temporal sequence representing the envelope is then calculated on a window of the audio signal <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sample",
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> is the window size and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the hop size. In our experiments we set <math alttext=\"W=512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">W=512</annotation></semantics></math> and <math alttext=\"h=128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">h=128</annotation></semantics></math>.\nThe envelope serves as a coarse temporal guide, providing information about the timing and intensity of audio events. To encode this temporal control, we utilize the pre-trained VAE from Stable Audio, which downsamples the input stereo audio by a factor of 1024, mapping it into a compact latent space. The latent representation of the envelope, <math alttext=\"\\mathbf{r_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{r_{c}}</annotation></semantics></math>, is processed through a ControlNet-inspired architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib30\" title=\"\">30</a>]</cite>, allowing fine-grained temporal adjustments during audio generation.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">our audio model is based on Stable Audio and follows the standard latent denoising diffusion formulation. Given a noisy latent representation <math alttext=\"\\mathbf{z}=\\mathcal{E}(\\mathbf{y})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119859;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}=\\mathcal{E}(\\mathbf{y})</annotation></semantics></math> at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model learns to estimate the noise <math alttext=\"\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{F},\\mathbf{r_{c}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>t</mi></msub><mo>,</mo><mi>t</mi><mo>,</mo><mi>&#119813;</mi><mo>,</mo><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{F},\\mathbf{r_{c}})</annotation></semantics></math> conditioned on semantic embeddings <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mi>&#119813;</mi><annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation></semantics></math> and the temporal control signal <math alttext=\"\\mathbf{r_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119851;</mi><mi>&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{r_{c}}</annotation></semantics></math>.\nIn the forward process, Gaussian noise is slowly added to the original data distribution with a fixed schedule <math alttext=\"\\alpha_{1},\\ldots,\\alpha_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#945;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#945;</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1},\\ldots,\\alpha_{T}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the total timesteps, and <math alttext=\"\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8719;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mi>&#945;</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "time",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We freeze the pre-trained weights of the diffusion model and only train the ControlNet layers, which process the RMS envelope, and the linear projections that align GRAM embeddings to the conditioning dimensions of Stable Audio. By jointly leveraging GRAM-aligned embeddings for semantic control and the ControlNet mechanism for temporal alignment, our model ensures that the generated audio aligns both semantically and temporally with the input video. A block diagram of the proposed architecture is shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.F3\" title=\"Figure 3 &#8227; III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We work with the <span class=\"ltx_text ltx_font_italic\">Greatest Hits</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib32\" title=\"\">32</a>]</cite>, a well-known and widespread benchmark for video-to-audio generation tasks. The dataset contains videos of people using a drumstick to strike or rub different surfaces and objects. The choice of a drumstick as main object in motion allows the scene&#8217;s action to remain clearly visible with minimal occlusion of the frame. Each video captures the sound of these interactions with a shotgun microphone attached to the camera, and the audio is later processed to remove noise. Metadata provided for each video is used to create textual prompts following the structure proposed in Fol&#183;AI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib18\" title=\"\">18</a>]</cite>: &#8220;A person {<span class=\"ltx_text ltx_font_italic\">action</span>} {<span class=\"ltx_text ltx_font_italic\">frequency</span>} on {<span class=\"ltx_text ltx_font_italic\">material</span>} with a wooden stick.&#8221; The placeholders {<span class=\"ltx_text ltx_font_italic\">action</span>} (e.g., &#8220;hit&#8221; or &#8220;scratch&#8221;), {<span class=\"ltx_text ltx_font_italic\">frequency</span>} (e.g., &#8220;multiple times&#8221; or &#8220;once&#8221;), and {<span class=\"ltx_text ltx_font_italic\">material</span>} are populated based on the metadata details.\nThe carefully curated samples of this dataset are crucial for V2A model training, as real-world video datasets often lack both the audiovisual alignment and the quality required to make models understand how to produce audio that is semantically and temporally consistent with the input video. This dataset contains 977 video recordings captured in diverse settings, both indoors and outdoors. Indoor videos showcase materials like metal, plastic, and cloth, while outdoor recordings feature dynamic materials such as water, leaves, and grass. We extract 10-second-long chunks from each sample to train and test our model. On average, each video includes 48 distinct actions, split between striking and rubbing, ensuring that each extracted chunk has sufficient activity.\nFor our experiments, we split the dataset into 732 videos for training, 49 for validation, and 196 for testing.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "experiments",
                    "video",
                    "used",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fr&#233;chet Audio Distance (FAD):\nFAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib33\" title=\"\">33</a>]</cite> is a metric designed to assess the quality and realism of generated audio by comparing it to reference audio. It evaluates the similarity between the statistical distributions of embeddings extracted from real and generated waveforms. The choice of the audio encoder for extracting these embeddings plays a crucial role, as different encoders emphasize various audio features, affecting how well the metric aligns with human perception <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib34\" title=\"\">34</a>]</cite>. To account for this, we calculate FAD using two distinct audio encoders: Microsoft CLAP (FAD-C) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib35\" title=\"\">35</a>]</cite>, and Laion-CLAP (FAD-LC) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib36\" title=\"\">36</a>]</cite>. The FAD scores are computed using the <span class=\"ltx_text ltx_font_italic\">fadtk<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/fadtk\" title=\"\">https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/fadtk</a></span></span></span></span> library.</p>\n\n",
                "matched_terms": [
                    "fadc",
                    "fadlc",
                    "clap",
                    "waveforms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLAP-score:\nThe CLAP-score evaluates the overall quality of the generated waveforms, also used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib28\" title=\"\">28</a>]</cite>. It calculates the cosine similarity between embeddings of ground truth and generated audio, which are obtained using the CLAP model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib36\" title=\"\">36</a>]</cite>. Given that the majority of baseline models employs CLAP as the primary audio representation, this metric serves as a key indicator of how effectively the conditioning features contribute to generating the final output for a fair comparison.</p>\n\n",
                "matched_terms": [
                    "ground",
                    "conditioning",
                    "truth",
                    "clap",
                    "waveforms",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fr&#233;chet Audio-Visual Distance (FAVD):\nFAVD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib37\" title=\"\">37</a>]</cite> is increasingly recognized for evaluating video-to-audio (V2A) models. It measures the alignment, both temporal and semantic, between the audio and video modalities. This metric calculates the Fr&#233;chet Distance between video embeddings and audio embeddings. For our evaluation, we use I3D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib38\" title=\"\">38</a>]</cite> as the video encoder and VGGish <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib39\" title=\"\">39</a>]</cite> as the audio encoder, extracting embeddings from both ground truth videos and the generated audio to determine their alignment.</p>\n\n",
                "matched_terms": [
                    "favd",
                    "video",
                    "modalities",
                    "truth",
                    "ground",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For training FoleyGRAM, we initialize the model weights using the Stable Audio Open repository and its associated checkpoint. The ground truth audio used in our experiments is 44.1 kHz stereo recordings from the Greatest Hits dataset. The model is trained on a single Nvidia RTX A6000 GPU (48 GB) with a batch size of 12 for 20,000 steps. The training process employs the AdamW optimizer, with parameters configured as those in Stable Audio Open, and uses a fixed learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "foleygram",
                    "experiments",
                    "truth",
                    "ground",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To initialize GRAM encoders, we use the official associated repository and its relative checkpoints. Three GRAM encoders, which are EVAClip-ViT-G for video, BEATs for audio, and BERT-B for text with a total number of parameters equal to 1B, have been previously pretrained on the VAST27M dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#bib.bib22\" title=\"\">22</a>]</cite> with conventional contrastive loss functions. Later, the learned latent space is rearranged and pretrained on a subset of such dataset comprising 150k samples with the GRAM losses in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E3\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S3.E4\" title=\"In III-A Gramian Representation Learning &#8227; III FoleyGRAM &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and finally fine-tuned on the Greatest Hits dataset to make the encoders aware of the particular cases of this dataset. The pertaining on the subset of VAST27M dataset has been carried on for one epoch with learning rate <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> with a batch size of 256 on 4 NVIDIA A100 cards, and the same configuration holds for the fine-tuning on&#160;Greatest Hits.</p>\n\n",
                "matched_terms": [
                    "text",
                    "samples",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, envelopes extracted directly from ground truth audio are interpolated to match the target sample rate, and fed into the ControlNet from the audio synthesis model as inputs. The model then generates the final output in 150 sampling steps, applying classifier-free guidance with a guidance scale set to 2.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "inference",
                    "truth",
                    "ground",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it extracts RGB and optical flow features of a video and leverages a Transformer-based autoregressive architecture to generate temporally and semantically aligned audio to the reference video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model uses a similar architecture respect to SpecVQGAN, adding additional controls on the final output conditioning with audio and video features from the semantic target. The model is trained directly using Greatest Hits, succeeding in achieving an efficient alignment in both content and\ntiming with the reference video.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">leverages Contrastive Audio-Visual Pretraining (CAVP) to achieve temporal and semantic alignment between audio and video modalities, enabling the generation of video embeddings with features pertinent to the associated audio. These embeddings are then employed as direct conditioning inputs for Stable-Diffusion.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "video",
                    "modalities",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model is the first to introduce a human-readable control mechanism for the V2A task. It utilizes a ResNet(2+1)D-18 based video encoder, which processes video frames to generate an onset track. This onset track is subsequently fed into a time-domain diffusion model to produce the final audio output.</p>\n\n",
                "matched_terms": [
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">this model uses a video encoder through which the RMS of the audio signal associated with the input video can be mapped, which is then used as the control signal for the temporal alignment of the model. In contrast, the semantics of the final output is controlled by embeddings produced by the CLAP audio/text encoder. These control signals are used to generate 16kHz mono audio through the use of AudioLDM.</p>\n\n",
                "matched_terms": [
                    "semantics",
                    "video",
                    "clap",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05829v1#S4.T1\" title=\"TABLE I &#8227; IV-C Training and Inference Details &#8227; IV Experiments &#8227; FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders * Equal contribution. Corresponding author&#8217;s email: riccardofosco.gramaccioni@uniroma1.it\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, FoleyGRAM demonstrates substantial improvements in semantic quality of the generated audio compared to all baseline models. This enhancement is primarily attributed to the integration of the multimodal-aligned encoder GRAM for conditioning the state-of-the-art audio generation model, Stable Audio. Unlike Video-Foley and SyncFusion, which rely on CLAP as the audio encoder for semantic conditioning, our approach leverages GRAM to ensure alignment across audio, video, and text modalities. This alignment enables FoleyGRAM to better capture the semantic features required for precise audio generation.\nOur model is also able to provide strong results on CLAP-based metrics, surpassing even Video-Foley and SyncFusion, despite the latter directly rely on CLAP for their semantic encoders.\nThe improved evaluation metrics scores of FoleyGRAM confirm the advantages of employing a unified multimodal encoder like GRAM for conditioning, particularly in scenarios where cross-modal consistency is essential.\nAdditionally, the use of Stable Audio as the backbone for audio generation ensures high-definition, stereo audio at 44.1 kHz, aligning with professional audio standards. The integration of ControlNet within our architecture further enhances the ability of the model to incorporate temporal conditioning through envelopes, ensuring precise timing and dynamic for the generated waveforms. Notably, FoleyGRAM achieves these results while being lightweight and efficient, requiring only approximately six hours of data of which the Greatest Hits dataset is composed and a limited number of training steps. This efficiency underscores the robustness and practicality of our approach for real-world sound design applications.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "foleygram",
                    "text",
                    "all",
                    "video",
                    "modalities",
                    "conditioning",
                    "even",
                    "waveforms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced FoleyGRAM, a novel V2A synthesis model that combines a state-of-the-art audio generation framework, Stable Audio, with GRAM, a unified multimodal encoder designed for cross-modal alignment. Our results demonstrate significant advancements in the semantic accuracy, achieving strong performances across key semantic metrics. By leveraging GRAM as the primary encoder for semantic conditioning, FoleyGRAM can use multiple aligned modalities simultaneously in order to leverage as much information as possible to generate waveforms with rich semantic information. Additionally, the integration of ControlNet allows for precise temporal control through envelopes, enabling the generation of high-quality 44.1 kHz stereo audio.\nFoleyGRAM achieves these results with a lightweight architecture and efficient training, requiring only a small dataset and limited computational resources. This makes our model a powerful tool for sound designers and also a practical solution for real-world applications where resource constraints are a factor.\nThe proposed model wants to encourage further exploration of multimodal deep learning in V2A tasks, highlighting the potential of unified embeddings and advanced generative models to bridge the gap between visual and audio modalities.</p>\n\n",
                "matched_terms": [
                    "foleygram",
                    "conditioning",
                    "modalities",
                    "waveforms",
                    "audio"
                ]
            }
        ]
    }
}