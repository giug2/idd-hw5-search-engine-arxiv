{
    "S4.T1": {
        "source_file": "Towards Multimodal Query-Based Spatial Audio Source Extraction",
        "caption": "Table 1: Median SI-SDR and SDR performance (dB) on different channel configurations.",
        "body": "Channel\nAudio Condition\nText Condition\n\n\nSI-SDR\nSDR\nSI-SDR\nSDR\n\n\nwxyz (full FOA)\n7.296\n8.595\n4.098\n5.664\n\n\nw (omni channel only)\n5.833\n6.785\n4.101\n4.557",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Channel</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio Condition</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text Condition</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SI-SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SI-SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDR</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">wxyz (full FOA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.296</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.595</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.098</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.664</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">w (omni channel only)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.833</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.785</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.101</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.557</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "channel",
            "sisdr",
            "sdr",
            "text",
            "different",
            "only",
            "configurations",
            "foa",
            "median",
            "condition",
            "performance",
            "wxyz",
            "audio",
            "omni"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first evaluate the model&#8217;s ability to exploit spatial cues for audio source extraction. We trained two versions of BSAST: a full version that takes the four FOA channels (wxyz) as input, thereby leveraging the spatial information in multi-channel audio, and an ablated version that uses only the single omnidirectional channel (w), corresponding to the absence of spatial cues. Experimental results demonstrate that the model achieves better performance when complete spatial information is available, thereby validating the effectiveness of the proposed time&#8211;frequency&#8211;spatial modeling strategy in capturing temporal, frequential, and spatial dependencies for separating complex spatial audio mixtures under reverberant conditions.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Query-based audio source extraction seeks to recover a target source from a mixture conditioned on a query. Existing approaches are largely confined to single-channel audio, leaving the spatial information in multi-channel recordings underexploited. We introduce a query-based spatial audio source extraction framework for recovering dry target signals from first-order ambisonics (FOA) mixtures. Our method accepts either an audio prompt or a text prompt as condition input, enabling flexible end-to-end extraction. The core of our proposed model lies in a tri-axial Transformer that jointly models temporal, frequency, and spatial channel dependencies. The model uses contrastive\nlanguage&#8211;audio pretraining (CLAP) embeddings to enable unified audio&#8211;text conditioning via feature-wise linear modulation (FiLM). To eliminate costly annotations and improve generalization, we propose a label-free data pipeline that dynamically generates spatial mixtures and corresponding targets for training. The result of our experiment with high separation quality demonstrates the efficacy of multimodal conditioning and tri-axial modeling. This work establishes a new paradigm for high-fidelity spatial audio separation in immersive applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "channel",
                    "text",
                    "foa",
                    "condition",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio source separation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a fundamental problem in audio signal processing, aiming to recover individual sound events from complex mixtures. The task has recently gained renewed importance due to the growing demand for spatialized audio processing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in applications such as immersive media, augmented and virtual reality (AR/VR), hearing aids, and human&#8211;robot interaction. In these scenarios, separation models must not only achieve accurate source separation but also make effective use of spatial cues to distinguish direct sound from reverberation. This increasing demand calls for models that can operate reliably in reverberant, cluttered, and dynamic real-world acoustic environments.</span>\n</p>\n\n",
                "matched_terms": [
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have significantly improved separation performance in both monaural and stereophonic conditions. However, most existing approaches mainly emphasize time-domain modeling or time-frequency representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while insufficiently exploiting spatial cues that are essential to human auditory perception. Moreover, many separation systems are trained in a class-specific manner&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (e.g., focusing on speech or a definite set of sound events), which restricts their generalizability and hinders their applicability to diverse real-world scenarios.\nMeanwhile, although some recent studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have explored target sound separation with multimodal cues, these efforts remain confined to single-channel audio and fail to exploit spatial information.\nConventional spatial filtering or beamforming methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further struggle when spatial reverberation is strong. Consequently, designing a framework that can jointly capture temporal and spatial dependencies while also supporting end-to-end, query-based separation remains an open challenge.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">and-split </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">patial </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">udio </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eparation </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ransformer (BSAST), a novel framework for query-based spatial audio extraction in reverberant and acoustically complex environments. Our model operates on first-order ambisonics (FOA) inputs, explicitly integrating spatial-channel cues alongside time-frequency representations to capture multi-dimensional dependencies. To enhance spectral modeling, we employ a band-split strategy&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dividing the input spectrum into multiple non-overlapping frequency bands. A tri-axial rotary positional encoding (RoPE) transformer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> then applies attention sequentially along the time, frequency, and spatial&#8211;channel dimensions, enabling the model to capture complex interactions across all three axes. BSAST is designed to extract target sound events from reverberation and background interference, achieving robust and high-fidelity separation performance under realistic acoustic conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "foa",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, our framework supports flexible, open-domain source extraction by accepting either audio exemplars or text descriptions as queries. We achieve this versatility through contrastive language&#8211;audio pretraining (CLAP) embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which inject semantic guidance directly into the extraction process. To scale training without reliance on strongly labeled data, we introduce a simple yet effective label-free data generation approach, where controlled noise is injected into the CLAP embeddings of target events to generate diverse query conditions on the fly. This strategy removes the need for paired audio&#8211;text annotations, substantially increases training diversity, and lowers the barrier to developing separation models in underexplored spatial audio scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We address query-based spatial audio source extraction, where the goal is to recover a dry target signal from a multichannel mixture. The input mixture, given in first-order ambisonics (FOA) format, contains multiple simultaneously active sound events convolved with room impulse responses (RIRs) and overlaid with background noise and interference. The key challenge lies in disentangling overlapping sources in time and space, suppressing reverberation, and enabling flexible extraction of any target source specified by a query.</span>\n</p>\n\n",
                "matched_terms": [
                    "foa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Formally, let\n</span>\n  <math alttext=\"{\\bm{X}}\\in\\mathbb{R}^{C\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119935;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">L</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\bm{X}}\\in\\mathbb{R}^{C\\times L}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\ndenote a multichannel FOA mixture, where </span>\n  <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">C</mi>\n      <annotation encoding=\"application/x-tex\">C</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the numbers of channels and audio samples, respectively. The mixture is generated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "foa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a query </span>\n  <math alttext=\"{\\bm{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119954;</mi>\n      <annotation encoding=\"application/x-tex\">{\\bm{q}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (either an audio exemplar or a text description), the goal is to estimate the corresponding dry target signal </span>\n  <math alttext=\"\\hat{{\\bm{s}}}_{q}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119956;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">T</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{{\\bm{s}}}_{q}\\in\\mathbb{R}^{T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the mixture:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the overall system architecture.\nSpecifically, given a multichannel FOA mixture </span>\n  <math alttext=\"{\\bm{X}}\\in\\mathbb{R}^{C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119935;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\bm{X}}\\in\\mathbb{R}^{C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the system first transforms the mixture into the time&#8211;frequency (T-F) representation </span>\n  <math alttext=\"\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119987;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by applying the short-time Fourier transform (STFT) independently for each channel, where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">F</mi>\n      <annotation encoding=\"application/x-tex\">F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the number of frames and frequency bins, respectively. Subsequently, a band-split encoder module divides the spectrum into multiple subbands and extracts latent embeddings </span>\n  <math alttext=\"\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for each subband </span>\n  <math alttext=\"n\\in\\{1,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">n\\in\\{1,\\dots,N\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the numbers of subbands and latent features, respectively. We stack all </span>\n  <math alttext=\"\\mathcal{Z}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> along the subband axis to obtain a stack feature map </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Meanwhile, the query condition is encoded using a pretrained CLAP encoder and injected via FiLM, producing query-adaptive feature maps. Then, the tri-axial RoPE Transformer models dependencies along the time, frequency, and channel dimensions. The output is denoted as </span>\n  <math alttext=\"\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Following tri-axial modeling, the spectrum estimation module predicts the target spectrum </span>\n  <math alttext=\"\\mathcal{\\hat{S}}_{q}\\in\\mathbb{R}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}_{q}\\in\\mathbb{R}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> directly for each channel, and the channel merge module combines multi-channel outputs into a single-channel spectrum </span>\n  <math alttext=\"\\hat{{\\bm{S}}}_{q}\\in\\mathbb{C}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119930;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{{\\bm{S}}}_{q}\\in\\mathbb{C}^{T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The final target waveform is obtained via iSTFT. During training, a label-free data construction pipeline generates diverse mixtures on-the-fly using dry sources, RIRs, and background noises, with pseudo-query embeddings derived from target audio CLAP embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "channel",
                    "foa",
                    "audio",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Previous studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated that splitting the input spectrum into multiple subbands can improve the performance while reducing computational complexity. The intuition is that the mixture signal spans the entire frequency range, where the low- and high-frequency components often exhibit distinct statistical and perceptual properties. By decomposing the spectrum into subbands, the model can better capture band-specific structures and mitigate interference across distant frequency regions. Building on this principle, we adopt a band-split module tailored for spatial audio. Specifically, we restructure the frequency division to better align with the multi-dimensional processing in the subsequent module. After STFT, we first map the complex spectrogram </span>\n  <math alttext=\"\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119987;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to the real domain by separating and concatenating the real and imaginary parts along the frequency dimension. Then the spectrogram is partitioned into </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uneven, non-overlapping subbands along the frequency axis to get </span>\n  <math alttext=\"\\mathcal{B}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">F</mi>\n              <mi mathsize=\"0.900em\">n</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{B}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"F_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of frequency bins in band </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For each subband, we employ a lightweight feature extractor composed of RMS normalization&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a linear projection, producing embeddings </span>\n  <math alttext=\"\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the hidden dimension. Finally, all subband embeddings are concatenated along the band axis, yielding the stacked representation </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is used as the input to the subsequent tri-axial RoFormer blocks.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable query-based extraction, we employ semantic conditioning derived from CLAP embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CLAP provides a joint representation space for both audio and text queries, which allows our model to accept either an audio exemplar or a text description as conditioning input. A key design choice lies in how to inject these embeddings into the extraction backbone.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopt the official dataset released for the detection and classification of\nacoustic scenes and events (DCASE) 2025 Task 4&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The dataset consists of anechoic dry sources covering a wide range of sound event classes (Anechoic Sound Event 1K, FSD50K, and EARS dataset), room impulse responses (RIRs) recorded in FOA format, and nondirectional background noise and interference events (FOA-MEIR, FSD50K, ESC-50, DISCO). All audio is standardized to 32 kHz and 16-bit.</span>\n</p>\n\n",
                "matched_terms": [
                    "foa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Here </span>\n  <math alttext=\"\\alpha s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">s</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha s</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the target signal projected onto the direction of </span>\n  <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, removing scale ambiguity and making the metric invariant to gain. The SI-SDR loss is defined as the negative of the computed SI-SDR value. Additionally, to promote waveform fidelity in the reconstructed audio waveforms, an </span>\n  <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss is incorporated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We perform STFT on each FOA channel with a Hanning window length of 2048 and a hop length of 1024. The frequency axis is partitioned into 25 non-overlapping subbands according to a pre-defined band-split scheme: 11 low-frequency bands of 6 bins each, followed by 6 mid-frequency bands of 32 bins each, 4 high-frequency bands of 64 bins each, and 3 ultra-high-frequency bands of 128, 128, 128, and 127 bins respectively, resulting in 25 bands in total.\nThe feature dimension is set to 128, and the backbone consists of 8 RoPE Transformer blocks. Each block employs 4 attention heads with a head dimension of 64. The spectrum estimator is implemented with a multi-layer perceptron (MLP) of depth 2 and an expansion factor of 4.\nOptimization is performed using the AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with an initial learning rate (LR) </span>\n  <math alttext=\"3\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a weight decay of </span>\n  <math alttext=\"1\\times 10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">2</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Mixed-precision training is adapted to improve efficiency. The model is trained for a maximum of 300 epochs on an NVIDIA H100-80GB GPU with a batch size of 4. Each epoch contains 2,000 mixture samples. We apply gradient accumulation with an accumulation step of 2.</span>\n</p>\n\n",
                "matched_terms": [
                    "channel",
                    "foa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate BSAST on the official test split using SI-SDR and SDR as evaluation metrics. The experiments are designed to assess the capability of our proposed method in multi-modal query-based spatial audio extraction. Results are reported under two types of query conditioning: a noise-added audio query and a clean text query.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "sisdr",
                    "audio",
                    "sdr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we then examine the influence of model depth. Increasing the number of Transformer blocks leads to steady performance gains, with the best results obtained with 8 blocks. This trend highlights the scalability of BSAST, showing that deeper architectures can better capture the intricate structures of complex spatial audio mixtures. Moreover, the consistent improvements suggest that the model design can effectively leverage additional capacity without signs of saturation, pointing to strong robustness and potential for further scaling.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Notably, although BSAST is trained exclusively with audio queries using the label-free data construction strategy, it also achieves competitive results when tested with clean text queries. This demonstrates the versatility of the CLAP-based FiLM conditioning, which enables the framework to support both audio- and text-driven extraction beyond fixed-class settings. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates a representative test example, showing that BSAST can recover individual sources with high fidelity and clarity.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced a flexible framework for query-based spatial audio extraction that enables direct dry source recovery from multichannel mixtures using either audio or text queries. By unifying spatial, temporal, and spectral reasoning with modality-agnostic query conditioning, our approach simplifies source extraction without relying on predefined classes or labeled datasets. The proposed label-free data construction further enhances practicality, making it well-suited for real-world acoustic scenarios. This work demonstrates the promise of query-based extraction for advancing interactive and immersive audio technologies.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Towards Multimodal Query-Based Spatial Audio Source Extraction",
        "caption": "Table 2: Median SI-SDR and SDR performance (dB) across different Transformer block configurations.",
        "body": "Blocks\nAudio Condition\nText Condition\n\n\nSI-SDR\nSDR\nSI-SDR\nSDR\n\n\n4\n4.791\n6.273\n2.435\n3.052\n\n\n6\n6.426\n7.752\n3.871\n4.459\n\n\n8\n7.296\n8.595\n4.098\n5.664",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Blocks</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio Condition</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text Condition</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SI-SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SI-SDR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDR</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.791</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.273</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.435</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.052</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.426</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.752</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.871</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.459</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.296</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.595</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.098</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.664</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "across",
            "text",
            "sisdr",
            "sdr",
            "blocks",
            "different",
            "block",
            "configurations",
            "median",
            "transformer",
            "condition",
            "performance",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we then examine the influence of model depth. Increasing the number of Transformer blocks leads to steady performance gains, with the best results obtained with 8 blocks. This trend highlights the scalability of BSAST, showing that deeper architectures can better capture the intricate structures of complex spatial audio mixtures. Moreover, the consistent improvements suggest that the model design can effectively leverage additional capacity without signs of saturation, pointing to strong robustness and potential for further scaling.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Query-based audio source extraction seeks to recover a target source from a mixture conditioned on a query. Existing approaches are largely confined to single-channel audio, leaving the spatial information in multi-channel recordings underexploited. We introduce a query-based spatial audio source extraction framework for recovering dry target signals from first-order ambisonics (FOA) mixtures. Our method accepts either an audio prompt or a text prompt as condition input, enabling flexible end-to-end extraction. The core of our proposed model lies in a tri-axial Transformer that jointly models temporal, frequency, and spatial channel dependencies. The model uses contrastive\nlanguage&#8211;audio pretraining (CLAP) embeddings to enable unified audio&#8211;text conditioning via feature-wise linear modulation (FiLM). To eliminate costly annotations and improve generalization, we propose a label-free data pipeline that dynamically generates spatial mixtures and corresponding targets for training. The result of our experiment with high separation quality demonstrates the efficacy of multimodal conditioning and tri-axial modeling. This work establishes a new paradigm for high-fidelity spatial audio separation in immersive applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "condition",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have significantly improved separation performance in both monaural and stereophonic conditions. However, most existing approaches mainly emphasize time-domain modeling or time-frequency representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while insufficiently exploiting spatial cues that are essential to human auditory perception. Moreover, many separation systems are trained in a class-specific manner&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (e.g., focusing on speech or a definite set of sound events), which restricts their generalizability and hinders their applicability to diverse real-world scenarios.\nMeanwhile, although some recent studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have explored target sound separation with multimodal cues, these efforts remain confined to single-channel audio and fail to exploit spatial information.\nConventional spatial filtering or beamforming methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further struggle when spatial reverberation is strong. Consequently, designing a framework that can jointly capture temporal and spatial dependencies while also supporting end-to-end, query-based separation remains an open challenge.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">and-split </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">patial </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">udio </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eparation </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ransformer (BSAST), a novel framework for query-based spatial audio extraction in reverberant and acoustically complex environments. Our model operates on first-order ambisonics (FOA) inputs, explicitly integrating spatial-channel cues alongside time-frequency representations to capture multi-dimensional dependencies. To enhance spectral modeling, we employ a band-split strategy&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dividing the input spectrum into multiple non-overlapping frequency bands. A tri-axial rotary positional encoding (RoPE) transformer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> then applies attention sequentially along the time, frequency, and spatial&#8211;channel dimensions, enabling the model to capture complex interactions across all three axes. BSAST is designed to extract target sound events from reverberation and background interference, achieving robust and high-fidelity separation performance under realistic acoustic conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio",
                    "across",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, our framework supports flexible, open-domain source extraction by accepting either audio exemplars or text descriptions as queries. We achieve this versatility through contrastive language&#8211;audio pretraining (CLAP) embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which inject semantic guidance directly into the extraction process. To scale training without reliance on strongly labeled data, we introduce a simple yet effective label-free data generation approach, where controlled noise is injected into the CLAP embeddings of target events to generate diverse query conditions on the fly. This strategy removes the need for paired audio&#8211;text annotations, substantially increases training diversity, and lowers the barrier to developing separation models in underexplored spatial audio scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a query </span>\n  <math alttext=\"{\\bm{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119954;</mi>\n      <annotation encoding=\"application/x-tex\">{\\bm{q}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (either an audio exemplar or a text description), the goal is to estimate the corresponding dry target signal </span>\n  <math alttext=\"\\hat{{\\bm{s}}}_{q}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119956;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">T</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{{\\bm{s}}}_{q}\\in\\mathbb{R}^{T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the mixture:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the overall system architecture.\nSpecifically, given a multichannel FOA mixture </span>\n  <math alttext=\"{\\bm{X}}\\in\\mathbb{R}^{C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119935;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\bm{X}}\\in\\mathbb{R}^{C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the system first transforms the mixture into the time&#8211;frequency (T-F) representation </span>\n  <math alttext=\"\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119987;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by applying the short-time Fourier transform (STFT) independently for each channel, where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">F</mi>\n      <annotation encoding=\"application/x-tex\">F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the number of frames and frequency bins, respectively. Subsequently, a band-split encoder module divides the spectrum into multiple subbands and extracts latent embeddings </span>\n  <math alttext=\"\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for each subband </span>\n  <math alttext=\"n\\in\\{1,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">n\\in\\{1,\\dots,N\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the numbers of subbands and latent features, respectively. We stack all </span>\n  <math alttext=\"\\mathcal{Z}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> along the subband axis to obtain a stack feature map </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Meanwhile, the query condition is encoded using a pretrained CLAP encoder and injected via FiLM, producing query-adaptive feature maps. Then, the tri-axial RoPE Transformer models dependencies along the time, frequency, and channel dimensions. The output is denoted as </span>\n  <math alttext=\"\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Following tri-axial modeling, the spectrum estimation module predicts the target spectrum </span>\n  <math alttext=\"\\mathcal{\\hat{S}}_{q}\\in\\mathbb{R}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}_{q}\\in\\mathbb{R}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> directly for each channel, and the channel merge module combines multi-channel outputs into a single-channel spectrum </span>\n  <math alttext=\"\\hat{{\\bm{S}}}_{q}\\in\\mathbb{C}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119930;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">q</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{{\\bm{S}}}_{q}\\in\\mathbb{C}^{T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The final target waveform is obtained via iSTFT. During training, a label-free data construction pipeline generates diverse mixtures on-the-fly using dry sources, RIRs, and background noises, with pseudo-query embeddings derived from target audio CLAP embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "condition",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our BSAST backbone mainly contains a band-split encoder, a FiLM condition module, a tri-axial RoPE Transformer, and a spectrum estimate module.</span>\n</p>\n\n",
                "matched_terms": [
                    "condition",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Previous studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated that splitting the input spectrum into multiple subbands can improve the performance while reducing computational complexity. The intuition is that the mixture signal spans the entire frequency range, where the low- and high-frequency components often exhibit distinct statistical and perceptual properties. By decomposing the spectrum into subbands, the model can better capture band-specific structures and mitigate interference across distant frequency regions. Building on this principle, we adopt a band-split module tailored for spatial audio. Specifically, we restructure the frequency division to better align with the multi-dimensional processing in the subsequent module. After STFT, we first map the complex spectrogram </span>\n  <math alttext=\"\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119987;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{X}\\in\\mathbb{C}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to the real domain by separating and concatenating the real and imaginary parts along the frequency dimension. Then the spectrogram is partitioned into </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uneven, non-overlapping subbands along the frequency axis to get </span>\n  <math alttext=\"\\mathcal{B}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">F</mi>\n              <mi mathsize=\"0.900em\">n</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{B}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"F_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of frequency bins in band </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For each subband, we employ a lightweight feature extractor composed of RMS normalization&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a linear projection, producing embeddings </span>\n  <math alttext=\"\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}_{n}\\in\\mathbb{R}^{C\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the hidden dimension. Finally, all subband embeddings are concatenated along the band axis, yielding the stacked representation </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is used as the input to the subsequent tri-axial RoFormer blocks.</span>\n</p>\n\n",
                "matched_terms": [
                    "blocks",
                    "performance",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable query-based extraction, we employ semantic conditioning derived from CLAP embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CLAP provides a joint representation space for both audio and text queries, which allows our model to accept either an audio exemplar or a text description as conditioning input. A key design choice lies in how to inject these embeddings into the extraction backbone.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopted FiLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to inject conditioning, as it provides a lightweight yet effective mechanism by modulating feature distributions without altering the backbone design. Concretely, given a CLAP embedding </span>\n  <math alttext=\"{\\bm{e}}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119942;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">d</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\bm{e}}\\in\\mathbb{R}^{d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first map it through a two-layer fully connected network with a ReLU activation, yielding a vector of size </span>\n  <math alttext=\"2D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">D</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This vector is then split into two parts, </span>\n  <math alttext=\"\\bm{\\gamma},\\bm{\\beta}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">&#120632;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">&#120631;</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">D</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bm{\\gamma},\\bm{\\beta}\\in\\mathbb{R}^{D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the hidden dimension of the extraction backbone. For an intermediate feature map </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the band-split encoder, the modulation parameters are broadcast across non-feature dimensions and applied as\n</span>\n  <math alttext=\"\\ FiLM(\\mathcal{Z},\\bm{\\gamma},\\bm{\\beta})=\\bm{\\gamma}\\odot\\mathcal{Z}+\\bm{\\beta},\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mrow>\n            <mi mathsize=\"0.900em\">F</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">i</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">M</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">&#120632;</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">&#120631;</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mrow>\n            <mrow>\n              <mi mathsize=\"0.900em\">&#120632;</mi>\n              <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8857;</mo>\n              <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mi mathsize=\"0.900em\">&#120631;</mi>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\ FiLM(\\mathcal{Z},\\bm{\\gamma},\\bm{\\beta})=\\bm{\\gamma}\\odot\\mathcal{Z}+\\bm{\\beta},</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwhere </span>\n  <math alttext=\"\\bm{\\gamma}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#120632;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{\\gamma}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> scales and </span>\n  <math alttext=\"\\bm{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#120631;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{\\beta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shifts the latent features, and </span>\n  <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8857;</mo>\n      <annotation encoding=\"application/x-tex\">\\odot</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes element-wise multiplication. In our implementation, the FiLM layer is inserted immediately after the band-split encoder and before each RoFormer block.</span>\n</p>\n\n",
                "matched_terms": [
                    "block",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopt a tri-axial RoPE Transformer as the main extraction module. Each block sequentially applies axial attention along the time, frequency, and channel axes, enabling the model to explicitly model interactions and efficiently exchange information within and across these dimensions. Additionally, we apply RoPE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to encode relative positional dependencies along each axis.</span>\n</p>\n\n",
                "matched_terms": [
                    "block",
                    "across",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Formally, given </span>\n  <math alttext=\"\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is the query-adaptive feature map after FiLM modulation, each Transformer block first normalizes the input using RMSNorm&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and encodes positional information via RoPE. Multi-head attention is then performed sequentially along each axis in the following order: time, frequency, and channel. When computing attention along a specific axis, the other dimensions are stacked together to form the input sequence. In a single Transformer block, a multi-head attention module is followed by a standard feedforward module, and both of them include residual connections. Stacking </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> such blocks produces a final latent representation for downstream source extraction. This tri-axial RoPE Transformer design effectively integrates temporal dynamics, frequency bands structure, and spatial-channel dependencies in a unified framework.</span>\n</p>\n\n",
                "matched_terms": [
                    "block",
                    "blocks",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">After the tri-axial RoPE Transformer blocks, the model produces latent features that capture temporal, spectral, and spatial-channel dependencies. These features are then passed to the spectrum estimate module, which directly predicts the magnitude spectrum of the target source for each subband, rather than estimating a time-frequency mask. Such complex spectral mapping has been shown to improve convergence stability and reconstruction fidelity, especially in highly overlapped or reverberant mixtures&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "blocks",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Specifically, given the output features from the tri-axial RoPE Transformer blocks </span>\n  <math alttext=\"\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z^{\\prime}}\\in\\mathbb{R}^{C\\times T\\times N\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first unbind along the subband axis, yielding </span>\n  <math alttext=\"\\mathcal{Z^{\\prime}}_{n}\\in\\mathbb{R}^{C\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mmultiscripts>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119989;</mi>\n          <mrow/>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n          <mi mathsize=\"0.900em\">n</mi>\n          <mrow/>\n        </mmultiscripts>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{Z^{\\prime}}_{n}\\in\\mathbb{R}^{C\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for each band </span>\n  <math alttext=\"n\\in\\{1,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">n\\in\\{1,\\dots,N\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Each band is processed by an MLP with gated linear units (GLU) to generate the estimated spectrum </span>\n  <math alttext=\"\\mathcal{\\hat{B}}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">n</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">F</mi>\n              <mi mathsize=\"0.900em\">n</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{\\hat{B}}_{n}\\in\\mathbb{R}^{C\\times T\\times F_{n}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The outputs from all subbands are then concatenated along the frequency axis to obtain the estimated full-band spectrum </span>\n  <math alttext=\"\\mathcal{\\hat{S}}\\in\\mathbb{R}^{C\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}\\in\\mathbb{R}^{C\\times T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To aggregate information across channels, the concatenated spectrum is further passed through a channel merge Module, implemented as a convolution network with reduction to a single channel. This module effectively integrates multi-channel information while preserving the spatially-resolved spectral content, producing the final spectrum estimate </span>\n  <math alttext=\"\\hat{{\\bm{S}}}\\in\\mathbb{C}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119930;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8450;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{{\\bm{S}}}\\in\\mathbb{C}^{T\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "blocks",
                    "across",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Here </span>\n  <math alttext=\"\\alpha s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">s</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha s</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the target signal projected onto the direction of </span>\n  <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, removing scale ambiguity and making the metric invariant to gain. The SI-SDR loss is defined as the negative of the computed SI-SDR value. Additionally, to promote waveform fidelity in the reconstructed audio waveforms, an </span>\n  <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss is incorporated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We perform STFT on each FOA channel with a Hanning window length of 2048 and a hop length of 1024. The frequency axis is partitioned into 25 non-overlapping subbands according to a pre-defined band-split scheme: 11 low-frequency bands of 6 bins each, followed by 6 mid-frequency bands of 32 bins each, 4 high-frequency bands of 64 bins each, and 3 ultra-high-frequency bands of 128, 128, 128, and 127 bins respectively, resulting in 25 bands in total.\nThe feature dimension is set to 128, and the backbone consists of 8 RoPE Transformer blocks. Each block employs 4 attention heads with a head dimension of 64. The spectrum estimator is implemented with a multi-layer perceptron (MLP) of depth 2 and an expansion factor of 4.\nOptimization is performed using the AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with an initial learning rate (LR) </span>\n  <math alttext=\"3\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a weight decay of </span>\n  <math alttext=\"1\\times 10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">2</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Mixed-precision training is adapted to improve efficiency. The model is trained for a maximum of 300 epochs on an NVIDIA H100-80GB GPU with a batch size of 4. Each epoch contains 2,000 mixture samples. We apply gradient accumulation with an accumulation step of 2.</span>\n</p>\n\n",
                "matched_terms": [
                    "block",
                    "blocks",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate BSAST on the official test split using SI-SDR and SDR as evaluation metrics. The experiments are designed to assess the capability of our proposed method in multi-modal query-based spatial audio extraction. Results are reported under two types of query conditioning: a noise-added audio query and a clean text query.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "sisdr",
                    "audio",
                    "sdr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first evaluate the model&#8217;s ability to exploit spatial cues for audio source extraction. We trained two versions of BSAST: a full version that takes the four FOA channels (wxyz) as input, thereby leveraging the spatial information in multi-channel audio, and an ablated version that uses only the single omnidirectional channel (w), corresponding to the absence of spatial cues. Experimental results demonstrate that the model achieves better performance when complete spatial information is available, thereby validating the effectiveness of the proposed time&#8211;frequency&#8211;spatial modeling strategy in capturing temporal, frequential, and spatial dependencies for separating complex spatial audio mixtures under reverberant conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Notably, although BSAST is trained exclusively with audio queries using the label-free data construction strategy, it also achieves competitive results when tested with clean text queries. This demonstrates the versatility of the CLAP-based FiLM conditioning, which enables the framework to support both audio- and text-driven extraction beyond fixed-class settings. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13308v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.4 Results &#8227; 4 EXPERIMENTS &#8227; Towards Multimodal Query-Based Spatial Audio Source Extraction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates a representative test example, showing that BSAST can recover individual sources with high fidelity and clarity.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Together, these results underscore the three core innovations of BSAST&#8212;time&#8211;frequency&#8211;spatial dependency modeling, CLAP-based multi-modal query conditioning, and a label-free training pipeline&#8212;while also establishing a robust and extensible foundation for high-fidelity spatial source extraction. Building on this foundation, further scaling of both model capacity and training data is expected to yield even stronger performance across diverse and challenging acoustic scenes.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced a flexible framework for query-based spatial audio extraction that enables direct dry source recovery from multichannel mixtures using either audio or text queries. By unifying spatial, temporal, and spectral reasoning with modality-agnostic query conditioning, our approach simplifies source extraction without relying on predefined classes or labeled datasets. The proposed label-free data construction further enhances practicality, making it well-suited for real-world acoustic scenarios. This work demonstrates the promise of query-based extraction for advancing interactive and immersive audio technologies.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            }
        ]
    }
}