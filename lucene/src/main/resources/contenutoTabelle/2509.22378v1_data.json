{
    "S3.T1": {
        "source_file": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach",
        "caption": "TABLE I: Human Evaluation Results: The bold text indicates the best result, while underlined text represents the second-best result. This formatting will be consistent in the following tables.",
        "body": "Methods\n\n\n\n\nMetrics\n\n\nMusic Quality\nMusic-Image Consistency\n\n\nOverall\nMelody\nRhythm\nAuthenticity\nHarmony\nAverage\nOverall\nSemantics\nEmotion\nAverage\n\n\n\n\n\nSynesthesia [6]\n\n3.65\n3.52\n4.10\n4.04\n3.47\n3.75\n2.80\n2.75\n2.83\n2.79\n\n\n\nMozart’s Touch [24]\n\n3.69\n3.57\n4.06\n3.53\n3.82\n3.73\n3.40\n3.14\n3.53\n3.35\n\n\nOurs\n4.02\n3.73\n4.22\n3.68\n4.02\n3.93\n3.88\n3.58\n4.01\n3.82",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_nopad\"><svg height=\"19.22\" overflow=\"visible\" version=\"1.1\" width=\"121.88\"><g transform=\"translate(0,19.22) scale(1,-1)\"><path d=\"M 0,19.22 121.88,0\" stroke=\"#000000\" stroke-width=\"0.4\" style=\"--ltx-stroke-color:#000000;\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"60.94\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_left\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Methods</span></span>\n</span>\n</span></span></span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(68.97,9.61)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"52.91\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_right\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></span>\n</span>\n</span></span></span></foreignobject></g></g></g></svg></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">Music Quality</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Music-Image Consistency</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Melody</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Rhythm</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Authenticity</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Harmony</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Average</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Semantics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Average</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Synesthesia</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">3.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">Mozart&#8217;s Touch</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.57</span></td>\n<td class=\"ltx_td ltx_align_center\">4.06</td>\n<td class=\"ltx_td ltx_align_center\">3.53</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.73</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.82</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "secondbest",
            "underlined",
            "mozart’s",
            "emotion",
            "musicimage",
            "will",
            "touch",
            "text",
            "synesthesia",
            "evaluation",
            "best",
            "methods",
            "results",
            "formatting",
            "bold",
            "overall",
            "metrics",
            "while",
            "indicates",
            "music",
            "tables",
            "following",
            "harmony",
            "average",
            "melody",
            "rhythm",
            "authenticity",
            "result",
            "consistent",
            "consistency",
            "ours",
            "human",
            "quality",
            "represents",
            "semantics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The experimental results are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T1\" title=\"TABLE I &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. It is evident that our method achieves the highest scores across almost all metrics, indicating its superior performance compared to previous methods. The machine evaluation by the VLM (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>) and the SongEval benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>) further demonstrate the efficacy of the proposed multi-modal RAG and self-refinement mechanisms through an ablation study, where our method consistently scores higher with their assistance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at <span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/RS2002/Image2Music</span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "consistency",
                    "methods",
                    "human",
                    "quality",
                    "results",
                    "emotion",
                    "musicimage",
                    "represents",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advancements, the field faces several critical challenges: (i) The mapping from images to music is inherently ambiguous and subjective <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib4\" title=\"\">4</a>]</cite>. Although various approaches, such as emotion-based mappings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib8\" title=\"\">8</a>]</cite>, have been proposed, they often lack comprehensiveness and rationality <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib10\" title=\"\">10</a>]</cite>. (ii) While there are a limited number of datasets available <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib11\" title=\"\">11</a>]</cite>, assessing their quality remains challenging. The performance of models trained on these datasets can be significantly affected. Furthermore, even if a dataset is suitable for training an end-to-end model, the resulting system may lack interpretability, hindering users&#8217; understanding of the connections between images and generated music. (iii) Most I2M systems depend on complex model architectures <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib12\" title=\"\">12</a>]</cite>, which require substantial training and computational resources, especially those that involve fine-tuning large pre-trained models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "while",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we propose a novel framework based on Vision Language Models (VLMs) that offers high interpretability without the need for additional training. Our approach leverages the image understanding and text generation capabilities of pre-trained VLMs by utilizing ABC music notation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib16\" title=\"\">16</a>]</cite> to bridge natural language and symbolic music. By introducing a Multi-Modal Retrieval-Augmented Generation (RAG) mechanism, we equip the VLM with sufficient knowledge about music generation, thereby eliminating the need for external training. This framework not only provides a multi-modal explanation for the motivation behind music creation, where the textual explanation derives from the VLM output and the image explanation is based on a processed attention map within the VLM, but also enhances music quality through a novel self-reflection mechanism. This mechanism allows the VLM to refine its generated music according to various quality metrics. The contributions of this letter can be summarized as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "metrics",
                    "quality",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a novel framework for the I2M task characterized by low computational cost and high interpretability.\nOur method directly utilizes pre-trained VLMs to generate text-based music in ABC notation from images, while ensuring interpretability through two complementary forms: the linguistic explanations provided by the VLM&#8217;s textual output and the visual reasoning revealed in the attention map.</p>\n\n",
                "matched_terms": [
                    "while",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the quality of generated music, we present a multi-modal RAG method that supplies the VLM with external knowledge about music. Additionally, we implement a self-reflection generation process, employing an evaluator to guide the VLM in refining its musical outputs.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We validate our method through both human studies and machine evaluations, comparing it with previous I2M methods and conducting several ablation studies. The results show that our method achieves the highest scores in terms of both music quality and music-image consistency, demonstrating the effectiveness of our design.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "methods",
                    "human",
                    "quality",
                    "results",
                    "musicimage",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The workflow of our method is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprising three main phases: (i) primary music generation with the multi-modal RAG; (ii) music refinement using a model-based evaluator; and (iii) explanation generation with text output and an image attention map. In this section, we will provide a detailed introduction to the process and underlying motivation for each step.</p>\n\n",
                "matched_terms": [
                    "text",
                    "will",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the training corpus of many current large models contains limited information about music, particularly regarding music generation. This limitation makes it challenging for them to produce ABC music directly. To develop domain-specific models from large models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib17\" title=\"\">17</a>]</cite>, fine-tuning and RAG are the two predominant approaches. Fine-tuning often requires substantial training resources and data. In contrast, RAG offers a plug-and-play method that demonstrates high flexibility and efficiency with promising results. Below, we formally describe our RAG-based music generation process.</p>\n\n",
                "matched_terms": [
                    "results",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider a database containing pieces of music along with their descriptions (which may be text or images). For VLM understanding, we first convert the music into ABC notation, represented as <math alttext=\"M=[m_{1},m_{2},\\ldots,m_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>m</mi><mn>1</mn></msub><mo>,</mo><msub><mi>m</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>m</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M=[m_{1},m_{2},\\ldots,m_{n}]</annotation></semantics></math> where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the number of music pieces. To utilize RAG, we employ a trained Contrastive Language-Image Pre-training (CLIP) model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib18\" title=\"\">18</a>]</cite> to encode the descriptions <math alttext=\"D=[d_{1},d_{2},\\ldots,d_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D=[d_{1},d_{2},\\ldots,d_{n}]</annotation></semantics></math> into embeddings <math alttext=\"E=[e_{1},e_{2},\\ldots,e_{n}]\\in\\mathbb{R}^{n,h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo>,</mo><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E=[e_{1},e_{2},\\ldots,e_{n}]\\in\\mathbb{R}^{n,h}</annotation></semantics></math>, where <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the dimension of the hidden features. Next, given an input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, we first use the VLM to generate a text description <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and then encode it with the CLIP to obtain embeddings <math alttext=\"\\mathcal{E}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{I}</annotation></semantics></math> and <math alttext=\"\\mathcal{E}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{T}</annotation></semantics></math>, respectively. We can then calculate the similarity between the input image and the music in the database using cosine similarity among the embeddings:</p>\n\n",
                "matched_terms": [
                    "text",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"||\\cdot||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p4.m10\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||\\cdot||</annotation></semantics></math> denotes the Euclidean norm. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib19\" title=\"\">19</a>]</cite>, we select the top <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> music pieces with the highest similarity to both the text description <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m12\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and the original image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m13\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. We then incorporate these music pieces along with their descriptions into the prompt and ask the VLM to generate corresponding music for the input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m14\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> using these references.</p>\n\n",
                "matched_terms": [
                    "text",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-round generation has become a popular method in generative models for quality refinement. To achieve this, we propose two approaches for VLM-based music generation. First, we utilize a parser to convert the generated ABC music into MIDI files. If there are any grammatical errors, the parser will report them, prompting the VLM to regenerate the music <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib20\" title=\"\">20</a>]</cite>. Second, we introduce an evaluator-based method that allows the VLM to self-reflect and refine the generated music. Specifically, we employ MusPy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib21\" title=\"\">21</a>]</cite> to assess the generated MIDI music using several common metrics, including Pitch Range (PR), Number of Pitches Used (NPU), Number of Pitch Classes Used (NPCU), Polyphony, Scale Consistency (SC), Pitch Entropy (PE), Pitch Class Entropy (PCE), and Empty Beat Rate (EBR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib22\" title=\"\">22</a>]</cite>. These metrics are style-independent: for example, higher entropy is suited for serialist compositions that aim for atonal equality, while lower entropy benefits pop music, where predictable pitch centers create catchy melodies. As a result, we provide the evaluated metric results, along with detailed explanations, back to the VLM. This enables the model to refine the music according to the given conditions (image and text description) based on its own judgment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "result",
                    "consistency",
                    "quality",
                    "results",
                    "while",
                    "metrics",
                    "will",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the low interpretability problem of most current music generation models, we propose two approaches based on text and image modalities, respectively. For the text modality, we employ a straightforward method by directly asking the VLM to provide the motivation behind its creation using text. For the image modality, we utilize attention maps to analyze which parts of the input image the model focuses on when generating music, which has been widely used in various studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib23\" title=\"\">23</a>]</cite>. For example, for a specific part of the generated text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we can obtain the attention map to the image, denoted as <math alttext=\"A\\in\\mathbb{R}^{l,h,|I|,|t|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>l</mi><mo>,</mo><mi>h</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A\\in\\mathbb{R}^{l,h,|I|,|t|}</annotation></semantics></math>, where <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is the number of layers, <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the number of heads, and <math alttext=\"|I|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|I|</annotation></semantics></math> and <math alttext=\"|t|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|t|</annotation></semantics></math> represent the token counts for the input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> and the generated text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, respectively. We then calculate the average attention to the image as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "average",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement our framework, we utilize the latest open-source model, Keye-8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib26\" title=\"\">26</a>]</cite>, as the Vision Language Model (VLM). For the multi-modal CLIP component, we employ LongCLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib27\" title=\"\">27</a>]</cite>, and we use MidiCaps <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib28\" title=\"\">28</a>]</cite> as the external database, which contains 168,385 MIDI music files paired with descriptive text captions. The experiments were conducted using the PyTorch framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib29\" title=\"\">29</a>]</cite> on a server running Ubuntu 22.04.5 LTS, equipped with an Intel(R) Xeon(R) Gold 6133 CPU @ 2.50 GHz and one NVIDIA A100 GPU.</p>\n\n",
                "matched_terms": [
                    "text",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the proposed method, we follow the experimental design outlined by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>, incorporating both human and machine evaluations using the same testing images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib30\" title=\"\">30</a>]</cite>. For human evaluation, we ask 31 participants to assess three image-music pairs for each method (informed consent was obtained), scoring them from 1 to 7 based on the following metrics inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib32\" title=\"\">32</a>]</cite>: (i) music quality level, including overall quality, melody, rhythm, authenticity, and harmony; (ii) music-image consistency, encompassing overall correspondence, semantic consistency, and emotional consistency. For machine evaluation, we adopt the paradigm of LLMs for judgment. Specifically, we utilize the latest multi-modal LLM, Grok4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib33\" title=\"\">33</a>]</cite>, and pose the same questions to it that human participants answer, converting the music into ABC notation for input into Grok4. Additionally, we utilize the latest music evaluation benchmark method, SongEval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib25\" title=\"\">25</a>]</cite>, to assess music quality in terms of coherence, musicality, memorability, clarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "rhythm",
                    "consistency",
                    "evaluation",
                    "human",
                    "following",
                    "harmony",
                    "quality",
                    "melody",
                    "musicimage",
                    "overall",
                    "metrics",
                    "authenticity",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As noted by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite>, there are very few open-source I2M methods available, particularly for symbolic music generation. Since we require music to be formatted in ABC notation for evaluation with the LLM, audio-based methods may introduce errors and biases. Consequently, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite> in selecting the open-source Synesthesia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite> as our benchmark, along with a variant of Mozart&#8217;s Touch <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>, utilizing the LLM to replace the music generator and enabling the generation of symbolic music in ABC notation. Additionally, we conduct a series of ablation studies to explore the efficacy of our proposed multi-modal RAG method and music refinement mechanism through machine evaluation. The primary reason we omit these ablation study from human evaluation is that increasing the number of comparative methods complicates data collection and may discourage user participation, as longer questionnaires can lead to fatigue and confusion among participants.</p>\n\n",
                "matched_terms": [
                    "synesthesia",
                    "evaluation",
                    "methods",
                    "human",
                    "mozart’s",
                    "touch",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph\">Note that while there are music quality metrics such as PCE and EBR, it is not deterministic whether higher or lower values are better, as this depends on the music style <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib34\" title=\"\">34</a>]</cite>. In previous music generation research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib35\" title=\"\">35</a>]</cite>, these metrics have been used to measure how closely the generated results align with ground truth. Since our dataset does not have a ground truth, we ignore these metrics for evaluation.</em>\n  <span class=\"ltx_text ltx_font_italic\"/>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality",
                    "results",
                    "metrics",
                    "while",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the performance of our method more directly, we present a generation case in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2\" title=\"Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, using the image in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> as input, which depicts a coastal sunset with a lighthouse. We notice that the VLM provides vivid text motivation (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2.sf2\" title=\"In Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2b</span></a>) to explain the generated music (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2.sf1\" title=\"In Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2a</span></a>) alongside the corresponding imagery. Additionally, the generated music features a complete structure, including an intro, development, climax, and fade-out. Moreover, the attention map in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates which parts of the image the VLM focused on, highlighting the close-up lighthouse, the distant sunset, and the background of the sky and sea, all of which are reflected in the text motivation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This letter proposes a novel framework for the image-to-music generation task, offering advantages in terms of low cost and high interpretability. Based on a trained VLM, we introduce a series of methods, including multi-modal RAG, self-refinement, and prompt engineering, to generate high-quality music without the need for external training. Additionally, we leverage the motivations generated by the VLM in text format and the attention maps from images to provide explanations for the generated results. To evaluate the proposed method, we conduct both human and machine evaluations. The results demonstrate that our method achieves promising performance in music quality and image-music consistency, suggesting an efficient design. In the future, it would be worthwhile to further explore the potential of the proposed method using more powerful VLMs or music domain-specific models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "consistency",
                    "methods",
                    "human",
                    "quality",
                    "results",
                    "music"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach",
        "caption": "TABLE II: Machine Evaluation by VLM Judger",
        "body": "Methods\n\n\n\n\nMetrics\n\n\nMusic Quality\nMusic-Image Consistency\n\n\nOverall\nMelody\nRhythm\nAuthenticity\nHarmony\nAverage\nOverall\nSemantics\nEmotion\nAverage\n\n\n\nSynesthesia [6]\n\n3.0\n2.4\n3.6\n2.8\n2.2\n2.8\n3.8\n2.8\n3.8\n3.5\n\n\n\nMozart’s Touch [24]\n\n4.4\n3.8\n4.6\n4.6\n4.2\n4.3\n5.4\n4.0\n5.6\n5.0\n\n\nOurs\n5.2\n5.2\n5.2\n5.2\n5.0\n5.2\n6.0\n5.2\n6.0\n5.7\n\n\nw/o RAG\n4.8\n4.4\n4.4\n4.8\n4.4\n4.6\n5.6\n5.0\n5.4\n5.3\n\n\nw/o Refinement\n5.2\n5.0\n4.6\n5.0\n5.0\n5.0\n5.8\n5.0\n5.8\n5.5\n\n\nw/o RAG, Refinement\n4.8\n4.4\n4.6\n5.0\n4.4\n4.6\n5.4\n4.8\n5.4\n5.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_nopad\"><svg height=\"19.22\" overflow=\"visible\" version=\"1.1\" width=\"121.88\"><g transform=\"translate(0,19.22) scale(1,-1)\"><path d=\"M 0,19.22 121.88,0\" stroke=\"#000000\" stroke-width=\"0.4\" style=\"--ltx-stroke-color:#000000;\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"60.94\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_left\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Methods</span></span>\n</span>\n</span></span></span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(68.97,9.61)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"52.91\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_right\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></span>\n</span>\n</span></span></span></foreignobject></g></g></g></svg></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">Music Quality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Music-Image Consistency</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Overall</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Melody</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Rhythm</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Authenticity</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Harmony</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Average</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Overall</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Semantics</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Synesthesia</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">Mozart&#8217;s Touch</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">3.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center\">4.6</td>\n<td class=\"ltx_td ltx_align_center\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.3</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\">5.6</td>\n<td class=\"ltx_td ltx_align_center\">5.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o RAG</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">4.8</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">4.8</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.6</td>\n<td class=\"ltx_td ltx_align_center\">5.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n<td class=\"ltx_td ltx_align_center\">5.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o Refinement</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o RAG, Refinement</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\">4.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">4.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "vlm",
            "rag",
            "mozart’s",
            "emotion",
            "musicimage",
            "refinement",
            "touch",
            "synesthesia",
            "evaluation",
            "methods",
            "overall",
            "metrics",
            "music",
            "harmony",
            "average",
            "judger",
            "melody",
            "rhythm",
            "authenticity",
            "ours",
            "consistency",
            "quality",
            "machine",
            "semantics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The experimental results are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T1\" title=\"TABLE I &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. It is evident that our method achieves the highest scores across almost all metrics, indicating its superior performance compared to previous methods. The machine evaluation by the VLM (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>) and the SongEval benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>) further demonstrate the efficacy of the proposed multi-modal RAG and self-refinement mechanisms through an ablation study, where our method consistently scores higher with their assistance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at <span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/RS2002/Image2Music</span>.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "vlm",
                    "methods",
                    "rag",
                    "quality",
                    "emotion",
                    "machine",
                    "musicimage",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advancements, the field faces several critical challenges: (i) The mapping from images to music is inherently ambiguous and subjective <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib4\" title=\"\">4</a>]</cite>. Although various approaches, such as emotion-based mappings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib8\" title=\"\">8</a>]</cite>, have been proposed, they often lack comprehensiveness and rationality <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib10\" title=\"\">10</a>]</cite>. (ii) While there are a limited number of datasets available <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib11\" title=\"\">11</a>]</cite>, assessing their quality remains challenging. The performance of models trained on these datasets can be significantly affected. Furthermore, even if a dataset is suitable for training an end-to-end model, the resulting system may lack interpretability, hindering users&#8217; understanding of the connections between images and generated music. (iii) Most I2M systems depend on complex model architectures <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib12\" title=\"\">12</a>]</cite>, which require substantial training and computational resources, especially those that involve fine-tuning large pre-trained models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we propose a novel framework based on Vision Language Models (VLMs) that offers high interpretability without the need for additional training. Our approach leverages the image understanding and text generation capabilities of pre-trained VLMs by utilizing ABC music notation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib16\" title=\"\">16</a>]</cite> to bridge natural language and symbolic music. By introducing a Multi-Modal Retrieval-Augmented Generation (RAG) mechanism, we equip the VLM with sufficient knowledge about music generation, thereby eliminating the need for external training. This framework not only provides a multi-modal explanation for the motivation behind music creation, where the textual explanation derives from the VLM output and the image explanation is based on a processed attention map within the VLM, but also enhances music quality through a novel self-reflection mechanism. This mechanism allows the VLM to refine its generated music according to various quality metrics. The contributions of this letter can be summarized as follows:</p>\n\n",
                "matched_terms": [
                    "vlm",
                    "rag",
                    "quality",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the quality of generated music, we present a multi-modal RAG method that supplies the VLM with external knowledge about music. Additionally, we implement a self-reflection generation process, employing an evaluator to guide the VLM in refining its musical outputs.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "quality",
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We validate our method through both human studies and machine evaluations, comparing it with previous I2M methods and conducting several ablation studies. The results show that our method achieves the highest scores in terms of both music quality and music-image consistency, demonstrating the effectiveness of our design.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "methods",
                    "quality",
                    "machine",
                    "musicimage",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The workflow of our method is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprising three main phases: (i) primary music generation with the multi-modal RAG; (ii) music refinement using a model-based evaluator; and (iii) explanation generation with text output and an image attention map. In this section, we will provide a detailed introduction to the process and underlying motivation for each step.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "rag",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the training corpus of many current large models contains limited information about music, particularly regarding music generation. This limitation makes it challenging for them to produce ABC music directly. To develop domain-specific models from large models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib17\" title=\"\">17</a>]</cite>, fine-tuning and RAG are the two predominant approaches. Fine-tuning often requires substantial training resources and data. In contrast, RAG offers a plug-and-play method that demonstrates high flexibility and efficiency with promising results. Below, we formally describe our RAG-based music generation process.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider a database containing pieces of music along with their descriptions (which may be text or images). For VLM understanding, we first convert the music into ABC notation, represented as <math alttext=\"M=[m_{1},m_{2},\\ldots,m_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>m</mi><mn>1</mn></msub><mo>,</mo><msub><mi>m</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>m</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M=[m_{1},m_{2},\\ldots,m_{n}]</annotation></semantics></math> where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the number of music pieces. To utilize RAG, we employ a trained Contrastive Language-Image Pre-training (CLIP) model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib18\" title=\"\">18</a>]</cite> to encode the descriptions <math alttext=\"D=[d_{1},d_{2},\\ldots,d_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D=[d_{1},d_{2},\\ldots,d_{n}]</annotation></semantics></math> into embeddings <math alttext=\"E=[e_{1},e_{2},\\ldots,e_{n}]\\in\\mathbb{R}^{n,h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo>,</mo><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E=[e_{1},e_{2},\\ldots,e_{n}]\\in\\mathbb{R}^{n,h}</annotation></semantics></math>, where <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the dimension of the hidden features. Next, given an input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, we first use the VLM to generate a text description <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and then encode it with the CLIP to obtain embeddings <math alttext=\"\\mathcal{E}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{I}</annotation></semantics></math> and <math alttext=\"\\mathcal{E}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{T}</annotation></semantics></math>, respectively. We can then calculate the similarity between the input image and the music in the database using cosine similarity among the embeddings:</p>\n\n",
                "matched_terms": [
                    "rag",
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"||\\cdot||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p4.m10\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||\\cdot||</annotation></semantics></math> denotes the Euclidean norm. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib19\" title=\"\">19</a>]</cite>, we select the top <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> music pieces with the highest similarity to both the text description <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m12\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and the original image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m13\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. We then incorporate these music pieces along with their descriptions into the prompt and ask the VLM to generate corresponding music for the input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m14\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> using these references.</p>\n\n",
                "matched_terms": [
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-round generation has become a popular method in generative models for quality refinement. To achieve this, we propose two approaches for VLM-based music generation. First, we utilize a parser to convert the generated ABC music into MIDI files. If there are any grammatical errors, the parser will report them, prompting the VLM to regenerate the music <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib20\" title=\"\">20</a>]</cite>. Second, we introduce an evaluator-based method that allows the VLM to self-reflect and refine the generated music. Specifically, we employ MusPy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib21\" title=\"\">21</a>]</cite> to assess the generated MIDI music using several common metrics, including Pitch Range (PR), Number of Pitches Used (NPU), Number of Pitch Classes Used (NPCU), Polyphony, Scale Consistency (SC), Pitch Entropy (PE), Pitch Class Entropy (PCE), and Empty Beat Rate (EBR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib22\" title=\"\">22</a>]</cite>. These metrics are style-independent: for example, higher entropy is suited for serialist compositions that aim for atonal equality, while lower entropy benefits pop music, where predictable pitch centers create catchy melodies. As a result, we provide the evaluated metric results, along with detailed explanations, back to the VLM. This enables the model to refine the music according to the given conditions (image and text description) based on its own judgment.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "vlm",
                    "quality",
                    "refinement",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the low interpretability problem of most current music generation models, we propose two approaches based on text and image modalities, respectively. For the text modality, we employ a straightforward method by directly asking the VLM to provide the motivation behind its creation using text. For the image modality, we utilize attention maps to analyze which parts of the input image the model focuses on when generating music, which has been widely used in various studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib23\" title=\"\">23</a>]</cite>. For example, for a specific part of the generated text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we can obtain the attention map to the image, denoted as <math alttext=\"A\\in\\mathbb{R}^{l,h,|I|,|t|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>l</mi><mo>,</mo><mi>h</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A\\in\\mathbb{R}^{l,h,|I|,|t|}</annotation></semantics></math>, where <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is the number of layers, <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> is the number of heads, and <math alttext=\"|I|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|I|</annotation></semantics></math> and <math alttext=\"|t|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|t|</annotation></semantics></math> represent the token counts for the input image <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> and the generated text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, respectively. We then calculate the average attention to the image as follows:</p>\n\n",
                "matched_terms": [
                    "average",
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This allows us to identify which parts of the image the model primarily utilizes when generating each track or segment of music. However, this method requires tracking the entire attention matrix during inference, which leads to high GPU utilization. In practice, this aspect should be considered based on the actual device capacity and the scale of the VLM used.</p>\n\n",
                "matched_terms": [
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement our framework, we utilize the latest open-source model, Keye-8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib26\" title=\"\">26</a>]</cite>, as the Vision Language Model (VLM). For the multi-modal CLIP component, we employ LongCLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib27\" title=\"\">27</a>]</cite>, and we use MidiCaps <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib28\" title=\"\">28</a>]</cite> as the external database, which contains 168,385 MIDI music files paired with descriptive text captions. The experiments were conducted using the PyTorch framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib29\" title=\"\">29</a>]</cite> on a server running Ubuntu 22.04.5 LTS, equipped with an Intel(R) Xeon(R) Gold 6133 CPU @ 2.50 GHz and one NVIDIA A100 GPU.</p>\n\n",
                "matched_terms": [
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the proposed method, we follow the experimental design outlined by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>, incorporating both human and machine evaluations using the same testing images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib30\" title=\"\">30</a>]</cite>. For human evaluation, we ask 31 participants to assess three image-music pairs for each method (informed consent was obtained), scoring them from 1 to 7 based on the following metrics inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib32\" title=\"\">32</a>]</cite>: (i) music quality level, including overall quality, melody, rhythm, authenticity, and harmony; (ii) music-image consistency, encompassing overall correspondence, semantic consistency, and emotional consistency. For machine evaluation, we adopt the paradigm of LLMs for judgment. Specifically, we utilize the latest multi-modal LLM, Grok4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib33\" title=\"\">33</a>]</cite>, and pose the same questions to it that human participants answer, converting the music into ABC notation for input into Grok4. Additionally, we utilize the latest music evaluation benchmark method, SongEval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib25\" title=\"\">25</a>]</cite>, to assess music quality in terms of coherence, musicality, memorability, clarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "rhythm",
                    "consistency",
                    "machine",
                    "evaluation",
                    "quality",
                    "harmony",
                    "melody",
                    "musicimage",
                    "overall",
                    "metrics",
                    "authenticity",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As noted by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite>, there are very few open-source I2M methods available, particularly for symbolic music generation. Since we require music to be formatted in ABC notation for evaluation with the LLM, audio-based methods may introduce errors and biases. Consequently, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite> in selecting the open-source Synesthesia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite> as our benchmark, along with a variant of Mozart&#8217;s Touch <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>, utilizing the LLM to replace the music generator and enabling the generation of symbolic music in ABC notation. Additionally, we conduct a series of ablation studies to explore the efficacy of our proposed multi-modal RAG method and music refinement mechanism through machine evaluation. The primary reason we omit these ablation study from human evaluation is that increasing the number of comparative methods complicates data collection and may discourage user participation, as longer questionnaires can lead to fatigue and confusion among participants.</p>\n\n",
                "matched_terms": [
                    "synesthesia",
                    "evaluation",
                    "methods",
                    "rag",
                    "mozart’s",
                    "machine",
                    "refinement",
                    "touch",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph\">Note that while there are music quality metrics such as PCE and EBR, it is not deterministic whether higher or lower values are better, as this depends on the music style <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib34\" title=\"\">34</a>]</cite>. In previous music generation research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib35\" title=\"\">35</a>]</cite>, these metrics have been used to measure how closely the generated results align with ground truth. Since our dataset does not have a ground truth, we ignore these metrics for evaluation.</em>\n  <span class=\"ltx_text ltx_font_italic\"/>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "quality",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the performance of our method more directly, we present a generation case in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2\" title=\"Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, using the image in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> as input, which depicts a coastal sunset with a lighthouse. We notice that the VLM provides vivid text motivation (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2.sf2\" title=\"In Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2b</span></a>) to explain the generated music (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F2.sf1\" title=\"In Figure 2 &#8227; II-C Multi-Modal Explanation Generation &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">2a</span></a>) alongside the corresponding imagery. Additionally, the generated music features a complete structure, including an intro, development, climax, and fade-out. Moreover, the attention map in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates which parts of the image the VLM focused on, highlighting the close-up lighthouse, the distant sunset, and the background of the sky and sea, all of which are reflected in the text motivation.</p>\n\n",
                "matched_terms": [
                    "vlm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This letter proposes a novel framework for the image-to-music generation task, offering advantages in terms of low cost and high interpretability. Based on a trained VLM, we introduce a series of methods, including multi-modal RAG, self-refinement, and prompt engineering, to generate high-quality music without the need for external training. Additionally, we leverage the motivations generated by the VLM in text format and the attention maps from images to provide explanations for the generated results. To evaluate the proposed method, we conduct both human and machine evaluations. The results demonstrate that our method achieves promising performance in music quality and image-music consistency, suggesting an efficient design. In the future, it would be worthwhile to further explore the potential of the proposed method using more powerful VLMs or music domain-specific models.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "vlm",
                    "methods",
                    "rag",
                    "quality",
                    "machine",
                    "music"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach",
        "caption": "TABLE III: Machine Evaluation by SongEval [25]",
        "body": "Methods\n\n\n\n\nMetrics\n\n\nCoherence\nMusicality\nMemorability\nClarity\nNaturalness\nAverage\n\n\n\n\n\nSynesthesia [6]\n\n2.31\n2.38\n2.10\n2.15\n2.10\n2.21\n\n\n\nMozart’s Touch [24]\n\n2.84\n2.82\n2.66\n2.63\n2.59\n2.71\n\n\nOurs\n2.90\n2.84\n2.60\n2.66\n2.60\n2.72\n\n\nw/o RAG\n2.76\n2.62\n2.52\n2.46\n2.41\n2.56\n\n\nw/o Refinement\n2.81\n2.79\n2.56\n2.63\n2.57\n2.67\n\n\nw/o RAG, Refinement\n2.78\n2.63\n2.54\n2.49\n2.48\n2.58",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><svg height=\"19.22\" overflow=\"visible\" version=\"1.1\" width=\"121.88\"><g transform=\"translate(0,19.22) scale(1,-1)\"><path d=\"M 0,19.22 121.88,0\" stroke=\"#000000\" stroke-width=\"0.4\" style=\"--ltx-stroke-color:#000000;\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"60.94\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_left\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Methods</span></span>\n</span>\n</span></span></span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(68.97,9.61)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"52.91\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_right\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></span>\n</span>\n</span></span></span></foreignobject></g></g></g></svg></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Coherence</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Musicality</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Memorability</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Clarity</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Naturalness</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Average</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Synesthesia</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">2.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">Mozart&#8217;s Touch</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o RAG</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">2.76</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\">2.46</td>\n<td class=\"ltx_td ltx_align_center\">2.41</td>\n<td class=\"ltx_td ltx_align_center\">2.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o Refinement</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">2.81</td>\n<td class=\"ltx_td ltx_align_center\">2.79</td>\n<td class=\"ltx_td ltx_align_center\">2.56</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n<td class=\"ltx_td ltx_align_center\">2.57</td>\n<td class=\"ltx_td ltx_align_center\">2.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">w/o RAG, Refinement</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\">2.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.58</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "ours",
            "songeval",
            "synesthesia",
            "evaluation",
            "musicality",
            "methods",
            "rag",
            "mozart’s",
            "coherence",
            "clarity",
            "average",
            "memorability",
            "machine",
            "refinement",
            "iii",
            "metrics",
            "touch"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The experimental results are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T1\" title=\"TABLE I &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. It is evident that our method achieves the highest scores across almost all metrics, indicating its superior performance compared to previous methods. The machine evaluation by the VLM (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T2\" title=\"TABLE II &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>) and the SongEval benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S3.T3\" title=\"TABLE III &#8227; III Experiment &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>) further demonstrate the efficacy of the proposed multi-modal RAG and self-refinement mechanisms through an ablation study, where our method consistently scores higher with their assistance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at <span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/RS2002/Image2Music</span>.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "rag",
                    "machine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we propose a novel framework based on Vision Language Models (VLMs) that offers high interpretability without the need for additional training. Our approach leverages the image understanding and text generation capabilities of pre-trained VLMs by utilizing ABC music notation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib16\" title=\"\">16</a>]</cite> to bridge natural language and symbolic music. By introducing a Multi-Modal Retrieval-Augmented Generation (RAG) mechanism, we equip the VLM with sufficient knowledge about music generation, thereby eliminating the need for external training. This framework not only provides a multi-modal explanation for the motivation behind music creation, where the textual explanation derives from the VLM output and the image explanation is based on a processed attention map within the VLM, but also enhances music quality through a novel self-reflection mechanism. This mechanism allows the VLM to refine its generated music according to various quality metrics. The contributions of this letter can be summarized as follows:</p>\n\n",
                "matched_terms": [
                    "rag",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We validate our method through both human studies and machine evaluations, comparing it with previous I2M methods and conducting several ablation studies. The results show that our method achieves the highest scores in terms of both music quality and music-image consistency, demonstrating the effectiveness of our design.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "machine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The workflow of our method is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#S2.F1\" title=\"Figure 1 &#8227; II Methodology &#8227; Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprising three main phases: (i) primary music generation with the multi-modal RAG; (ii) music refinement using a model-based evaluator; and (iii) explanation generation with text output and an image attention map. In this section, we will provide a detailed introduction to the process and underlying motivation for each step.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "iii",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-round generation has become a popular method in generative models for quality refinement. To achieve this, we propose two approaches for VLM-based music generation. First, we utilize a parser to convert the generated ABC music into MIDI files. If there are any grammatical errors, the parser will report them, prompting the VLM to regenerate the music <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib20\" title=\"\">20</a>]</cite>. Second, we introduce an evaluator-based method that allows the VLM to self-reflect and refine the generated music. Specifically, we employ MusPy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib21\" title=\"\">21</a>]</cite> to assess the generated MIDI music using several common metrics, including Pitch Range (PR), Number of Pitches Used (NPU), Number of Pitch Classes Used (NPCU), Polyphony, Scale Consistency (SC), Pitch Entropy (PE), Pitch Class Entropy (PCE), and Empty Beat Rate (EBR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib22\" title=\"\">22</a>]</cite>. These metrics are style-independent: for example, higher entropy is suited for serialist compositions that aim for atonal equality, while lower entropy benefits pop music, where predictable pitch centers create catchy melodies. As a result, we provide the evaluated metric results, along with detailed explanations, back to the VLM. This enables the model to refine the music according to the given conditions (image and text description) based on its own judgment.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the proposed method, we follow the experimental design outlined by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite>, incorporating both human and machine evaluations using the same testing images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib30\" title=\"\">30</a>]</cite>. For human evaluation, we ask 31 participants to assess three image-music pairs for each method (informed consent was obtained), scoring them from 1 to 7 based on the following metrics inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib32\" title=\"\">32</a>]</cite>: (i) music quality level, including overall quality, melody, rhythm, authenticity, and harmony; (ii) music-image consistency, encompassing overall correspondence, semantic consistency, and emotional consistency. For machine evaluation, we adopt the paradigm of LLMs for judgment. Specifically, we utilize the latest multi-modal LLM, Grok4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib33\" title=\"\">33</a>]</cite>, and pose the same questions to it that human participants answer, converting the music into ABC notation for input into Grok4. Additionally, we utilize the latest music evaluation benchmark method, SongEval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib25\" title=\"\">25</a>]</cite>, to assess music quality in terms of coherence, musicality, memorability, clarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "songeval",
                    "evaluation",
                    "musicality",
                    "coherence",
                    "clarity",
                    "memorability",
                    "machine",
                    "naturalness",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As noted by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite>, there are very few open-source I2M methods available, particularly for symbolic music generation. Since we require music to be formatted in ABC notation for evaluation with the LLM, audio-based methods may introduce errors and biases. Consequently, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib15\" title=\"\">15</a>]</cite> in selecting the open-source Synesthesia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib6\" title=\"\">6</a>]</cite> as our benchmark, along with a variant of Mozart&#8217;s Touch <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib24\" title=\"\">24</a>]</cite>, utilizing the LLM to replace the music generator and enabling the generation of symbolic music in ABC notation. Additionally, we conduct a series of ablation studies to explore the efficacy of our proposed multi-modal RAG method and music refinement mechanism through machine evaluation. The primary reason we omit these ablation study from human evaluation is that increasing the number of comparative methods complicates data collection and may discourage user participation, as longer questionnaires can lead to fatigue and confusion among participants.</p>\n\n",
                "matched_terms": [
                    "synesthesia",
                    "evaluation",
                    "methods",
                    "rag",
                    "mozart’s",
                    "machine",
                    "refinement",
                    "touch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph\">Note that while there are music quality metrics such as PCE and EBR, it is not deterministic whether higher or lower values are better, as this depends on the music style <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib34\" title=\"\">34</a>]</cite>. In previous music generation research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22378v1#bib.bib35\" title=\"\">35</a>]</cite>, these metrics have been used to measure how closely the generated results align with ground truth. Since our dataset does not have a ground truth, we ignore these metrics for evaluation.</em>\n  <span class=\"ltx_text ltx_font_italic\"/>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This letter proposes a novel framework for the image-to-music generation task, offering advantages in terms of low cost and high interpretability. Based on a trained VLM, we introduce a series of methods, including multi-modal RAG, self-refinement, and prompt engineering, to generate high-quality music without the need for external training. Additionally, we leverage the motivations generated by the VLM in text format and the attention maps from images to provide explanations for the generated results. To evaluate the proposed method, we conduct both human and machine evaluations. The results demonstrate that our method achieves promising performance in music quality and image-music consistency, suggesting an efficient design. In the future, it would be worthwhile to further explore the potential of the proposed method using more powerful VLMs or music domain-specific models.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "rag",
                    "machine"
                ]
            }
        ]
    }
}