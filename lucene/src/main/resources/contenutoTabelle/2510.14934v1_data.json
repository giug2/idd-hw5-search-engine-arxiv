{
    "S5.T1": {
        "source_file": "TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation",
        "caption": "Table 1: Experimental Results for Prosody Metrics. We report prosody metrics across datasets at text-aligned speech token architectures.\nOur proposed method, TASLA, outperforms the ablations and baselines on nearly all prosody metrics. The rows marked in light red denote the upper bound (S3 topline).",
        "body": "Model\nEne. RMSE ↓\\downarrow\n\n\n\nF0-PCC ↑\\uparrow\n\n\n\n\nPhr. Cos. ↑\\uparrow\n\n\n\n\nEne. PCC ↑\\uparrow\n\n\n\n\nPhr. L2 ↓\\downarrow\n\n\n\n\nVDE ↓\\downarrow\n\n\n\n\nGPE ↓\\downarrow\n\n\n\n\n\nLibriSpeech (in-domain)\n\n\n\n\\rowcolorred!10 S3 Topline\n6.94\n\n\n0.91\n\n\n\n\n0.92\n\n\n\n\n0.95\n\n\n\n\n5.41\n\n\n\n\n0.15\n\n\n\n\n0.03\n\n\n\n\nText-only Baseline\n10.08\n\n\n0.33\n\n\n\n\n0.89\n\n\n\n\n0.81\n\n\n\n\n7.06\n\n\n\n\n0.25\n\n\n\n\n0.32\n\n\n\n\nTASTE\n8.63\n\n\n0.80\n\n\n\n\n0.91\n\n\n\n\n0.88\n\n\n\n\n5.79\n\n\n\n\n0.19\n\n\n\n\n0.08\n\n\n\n\nTASLA\n6.97\n\n\n0.87\n\n\n\n\n0.90\n\n\n\n\n0.92\n\n\n\n\n6.67\n\n\n\n\n0.17\n\n\n\n\n0.05\n\n\n\n\n\nVoxceleb (noisy/natural, OOD)\n\n\n\n\\rowcolorred!10 S3 Topline\n5.31\n\n\n0.86\n\n\n\n\n0.94\n\n\n\n\n0.94\n\n\n\n\n4.46\n\n\n\n\n0.17\n\n\n\n\n0.03\n\n\n\n\nText-only Baseline\n8.73\n\n\n0.19\n\n\n\n\n0.90\n\n\n\n\n0.57\n\n\n\n\n6.48\n\n\n\n\n0.33\n\n\n\n\n0.35\n\n\n\n\nTASTE\n7.68\n\n\n0.64\n\n\n\n\n0.93\n\n\n\n\n0.74\n\n\n\n\n5.16\n\n\n\n\n0.26\n\n\n\n\n0.13\n\n\n\n\nTASLA\n6.53\n\n\n0.71\n\n\n\n\n0.93\n\n\n\n\n0.81\n\n\n\n\n4.92\n\n\n\n\n0.22\n\n\n\n\n0.09\n\n\n\n\n\nExpresso (emotion-rich, OOD)\n\n\n\n\\rowcolorred!10 S3 Topline\n8.57\n\n\n0.91\n\n\n\n\n0.82\n\n\n\n\n0.93\n\n\n\n\n7.19\n\n\n\n\n0.13\n\n\n\n\n0.04\n\n\n\n\nText-only Baseline\n8.95\n\n\n0.21\n\n\n\n\n0.68\n\n\n\n\n0.80\n\n\n\n\n11.77\n\n\n\n\n0.27\n\n\n\n\n0.45\n\n\n\n\nTASTE\n8.90\n\n\n0.73\n\n\n\n\n0.76\n\n\n\n\n0.86\n\n\n\n\n8.73\n\n\n\n\n0.20\n\n\n\n\n0.19\n\n\n\n\nTASLA\n7.87\n\n\n0.84\n\n\n\n\n0.82\n\n\n\n\n0.91\n\n\n\n\n7.43\n\n\n\n\n0.15\n\n\n\n\n0.10",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 4.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">Ene. RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">F0-PCC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Phr. Cos. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Ene. PCC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Phr. L2 <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">VDE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GPE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> (in-domain)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 S3 Topline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">6.94</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.92</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.41</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.15</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.03</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Text-only Baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">10.08</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.89</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.81</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.06</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.32</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASTE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.63</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.88</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.79</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.19</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.08</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASLA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">6.97</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.90</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.67</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.17</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.05</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> (noisy/natural, OOD)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 S3 Topline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">5.31</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.86</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.94</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.94</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.46</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.17</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.03</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Text-only Baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.73</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.19</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.90</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.57</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.35</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASTE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">7.68</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.93</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.74</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.26</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.13</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASLA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">6.53</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.93</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.22</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.09</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> (emotion-rich, OOD)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 S3 Topline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.57</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.82</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.93</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.19</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.13</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.04</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Text-only Baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.95</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.21</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.68</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">11.77</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.45</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASTE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.90</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.73</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.76</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.86</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">8.73</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.20</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.19</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASLA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.87</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.82</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.91</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">7.43</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.10</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "topline",
            "voxceleb",
            "taste",
            "speech",
            "upper",
            "datasets",
            "indomain",
            "proposed",
            "ene",
            "nearly",
            "architectures",
            "↑uparrow",
            "f0pcc",
            "token",
            "gpe",
            "prosody",
            "results",
            "pcc",
            "cos",
            "marked",
            "expresso",
            "metrics",
            "light",
            "ood",
            "bound",
            "vde",
            "method",
            "↓downarrow",
            "rowcolorred10",
            "outperforms",
            "ablations",
            "rows",
            "noisynatural",
            "emotionrich",
            "experimental",
            "tasla",
            "textaligned",
            "across",
            "all",
            "librispeech",
            "report",
            "baseline",
            "rmse",
            "denote",
            "phr",
            "our",
            "textonly",
            "model",
            "red"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We report the main experimental results of the quality metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> and prosody metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe details of the weight analysis could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.SS2\" title=\"6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a>.\nThe details of the training and parameters can be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A2\" title=\"Appendix B Training Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nThe details of metrics formulation and explanation could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, TASLA consistently surpasses other text-aligned speech token frameworks on nearly every prosody indicator.\nAnd it is also closer to the S3 tokens&#8217; topline.\nThis pattern shows that TASLA preserves paralinguistic cues, intonation, rhythm, and emphasis, rather than merely reconstructing clean audio, and does so under much stronger compression.\nFor pitch-related behavior, a higher F0-PCC means TASLA&#8217;s generated pitch contour follows the reference more faithfully.\nAt the same time, lower GPE and VDE indicate fewer octave (pitch halving/doubling) mistakes and more stable voiced/unvoiced decisions, yielding more natural intonation and fewer pitch glitches.\nFor loudness dynamics and phrasing, higher Energy-PCC reflects a closer alignment of energy fluctuations over time, while lower Energy-RMSE shows that the absolute loudness levels better match the reference, rather than just the trend.\nHigher Phrase-Cos. and lower Phrase-L2 further indicate that pauses, timing, and rhythmic structure align more precisely with the ground truth.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">T</span>ext-<span class=\"ltx_text ltx_font_bold\">A</span>ligned <span class=\"ltx_text ltx_font_bold\">S</span>peech Tokens with Multiple <span class=\"ltx_text ltx_font_bold\">L</span>ayer-<span class=\"ltx_text ltx_font_bold\">A</span>ggregation (TASLA), which is a text&#8209;aligned speech tokenization framework that aims to address the problem that under a low-frame-rate and text-aligned regime, single-source speech tokens may lose acoustic details during reconstruction.\nOn the other hand, this paper further explains how different encoder layers collaborate to capture comprehensive acoustic features for tokenization.\nPrevious work, TASTE, proposed the text-aligned speech tokenization framework, which is a LM-friendly architecture, but struggles to capture acoustic details.\nWe address this trade&#8209;off with two components: Multi&#8209;Layer Dynamic Attention (MLDA), which lets each text position adaptively mix shallow/deep features from a frozen speech encoder, and Finite Scalar Quantization (FSQ), a simple per&#8209;dimension discretization with smooth optimization.\nAt about 2.62 Hz (tokens/s), TASLA consistently improves prosody and achieves competitive quality over TASTE on in-domain (<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>) and OOD (<span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>) sets.\nWe further demonstrate that dynamic layer mixing is correlated with spectral flux and explains why MLDA preserves prosody under a low frame rate with extreme feature compression.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "voxceleb",
                    "librispeech",
                    "taste",
                    "speech",
                    "indomain",
                    "prosody",
                    "proposed",
                    "expresso",
                    "tasla",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation</span>\n</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "speech",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have recently transformed text understanding and generation.\nTo pursue a more natural interaction mode with LMs, researchers have started to research Spoken Language Models (SLM)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib5\" title=\"\">2024</a>); Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>); KimiTeam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib13\" title=\"\">2025</a>); Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib21\" title=\"\">2024a</a>)</cite>, where models process and generate speech as a first-class modality.\nIf we want to use voice mode to interact with the LMs, one option is a cascade pipeline that first generates text with an LM and then converts text to speech using a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib31\" title=\"\">2025</a>); Shikhar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib26\" title=\"\">2025</a>)</cite>.\nIn contrast, SLM offers a better approach, as it captures acoustic details from context and generates speech that more naturally aligns with the context&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Arora et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib1\" title=\"\">2025</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib10\" title=\"\">2025</a>)</cite>.\nThe key to enabling SLMs to have such an ability is speech tokenization.\nSpeech tokenization involves mapping continuous waveforms into discrete tokens that are compact, learnable, and acoustically informative, enabling a vocoder to reconstruct speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, there are two practical problems with joint modeling of text tokens and speech tokens for an SLM.\nThe first problem is the trade-off between acoustic richness and LM-friendliness.\nNeural speech tokenizers that generate more tokens per second could provide fine-grained acoustic details since they use a shorter time frame for speech reconstruction, and these speech tokenizers often capture only acoustic details without text information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nTherefore, they could generate very high-quality speech, but their speech token sequence is often very long&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib4\" title=\"\">2023</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib29\" title=\"\">2023</a>)</cite>, which is not suitable for transformer-based architectures if we want to generate longer speech.</p>\n\n",
                "matched_terms": [
                    "architectures",
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second problem is the speech token sequence length mismatch with the text sequence length.\nStandard codecs operate at fixed frame rates that yield token streams far longer than text (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12.5&#8211;50 speech Hz vs. <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2&#8211;3 text Hz), making joint speech-text modeling harder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nMaking the LMs need to learn two modalities separately, which also increases the difficulty of the training process.\nHowever, aggressive sequence compression helps alignment but risks losing prosodic detail.\nPrior work addresses the mismatch by aligning speech tokens to text via an attention mechanism during tokenization, enabling straightforward joint modeling at very low bitrates&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, under strong compression of the token length, it still shows limitations in preserving fine-grained acoustics and in out-of-domain generalization. To overcome these problems and find a balance between bitrate and token sequence length.\nWe propose <span class=\"ltx_text ltx_font_bold\">TASLA</span>, a text-aligned speech tokenization framework that preserves acoustic detail at text length.\nTASLA introduces Multi-Layer Dynamic Attention (MLDA), which enables text tokens to query a frozen speech encoder and adaptively combine shallow and deep representations, allowing each text position to gather the most predictive acoustic evidence for content, prosody, and speaker cues.\nTo discretize reliably, we replace Residual Vector Quantization (RVQ) with Finite Scalar Quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mentzer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib18\" title=\"\">2024</a>)</cite>, which provides smooth optimization and resilience to codebook pathologies.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "token",
                    "speech",
                    "prosody",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work shows that <em class=\"ltx_emph ltx_font_italic\">dynamic, per-token fusion across encoder depth</em> is the missing piece for text-aligned speech tokens: MLDA lets each word position select the most predictive mixture of shallow and deep features, thereby retaining prosody and speaker cues at a <em class=\"ltx_emph ltx_font_italic\">text-length</em> rate.\nUnder rate parity with prior text-aligned tokenizers, MLDA is consistently comparable in quality and outperforms on prosody metrics in both in-domain and OOD settings, and yields better spoken continuation than strong text-only and speech-token baselines, demonstrating that adaptive layer mixing effectively narrows the gap between alignment convenience and acoustic expressivity.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "textaligned",
                    "across",
                    "speech",
                    "outperforms",
                    "prosody",
                    "indomain",
                    "textonly",
                    "metrics",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CNN-based neural codecs typically use RVQ and multi-scale spectral/adversarial losses.\nThey often excel at high-fidelity reconstruction and preserve prosodic cues.\nEnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>)</cite> and DAC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kumar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib14\" title=\"\">2023</a>)</cite> are canonical convolutional codecs with RVQ, which have strong perceptual quality and real-time operation, but are not designed for text alignment.\nThen, BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib30\" title=\"\">2024</a>)</cite> and TAAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Parker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib24\" title=\"\">2025</a>)</cite> achieve extremely high quality at extremely low bitrates using a convolution codec architecture.\nHowever, they are still fixed-rate and not LM-friendly for speech&#8211;text joint modeling.\nTherefore, Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib7\" title=\"\">2024</a>)</cite> deliberately reduces token rates to be more LM-friendly, which is a useful bridge from pure reconstruction toward joint modeling.\nIn contrast, our method goes beyond purely acoustic reconstruction by jointly modeling speech tokens in both acoustic and semantic information.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "method",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate linguistic and semantic information and enable unified modeling, recent work blends semantic and acoustic representations.\nSpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite> distills semantics into upper RVQ codebooks using SSL features, yielding tokens that carry both semantic and acoustic content, but its sequences remain relatively long and not text-aligned.\nDualCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib15\" title=\"\">2025</a>)</cite> uses dual branches, semantic and acoustic, to balance intelligibility and fidelity, with lower rates but still not explicitly aligned to text.\nSpirit-LM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib22\" title=\"\">2024b</a>)</cite> interleaves speech and text tokens for joint modeling; effective but depends on extra interleaving rules rather than addressing alignment at tokenization.\nThese methods add semantics or interleaving strategies, yet do not solve alignment at the tokenization stage, leaving training and inference complexity and potential prosody loss.\nOur method differs by enforcing speech-text length alignment during tokenization, while still preserving both acoustic and semantic information.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "method",
                    "upper",
                    "speech",
                    "prosody",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-aligned speech tokens use text positions as queries over speech representations so that tokenization itself aligns speech tokens to the text length.\nSequences become LM-friendly while retaining prosodic and paralinguistic cues.\nBecause of alignment, the token rate is about <math alttext=\"1/20\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">1/20</annotation></semantics></math>&#8211;<math alttext=\"1/5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">1/5</annotation></semantics></math> of traditional tokenizers.\nTASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> introduces the text-aligned paradigm, where text-query cross-attention over speech encoder features produces time-variant, text-length-matched tokens, thereby eliminating speech&#8211;text length mismatch and enabling straightforward SLM training.\nHowever, the extreme compression of the bitrate and token rate results in the loss of acoustic details.\nUnlike prior text-aligned approaches, our method maintains the extremely low frame rate while preserving richer acoustic detail in reconstruction.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "method",
                    "token",
                    "taste",
                    "speech",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech and text provide two complementary but structurally different views of language.\nLet a spoken utterance be denoted as a continuous waveform <math alttext=\"u\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">u\\in\\mathbb{R}^{T}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of acoustic frames.\nOn the other hand, let the corresponding transcript be a discrete token sequence <math alttext=\"v=[v_{1},\\dots,v_{N}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">v=[v_{1},\\dots,v_{N}]</annotation></semantics></math> of length <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>.\nThe asymmetry between <math alttext=\"|T|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>T</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|T|</annotation></semantics></math> and <math alttext=\"|N|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>N</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|N|</annotation></semantics></math> is striking: while text unfolds sparsely with clear boundaries, speech evolves densely, carrying layered information including acoustic details and prosody.\nThis discrepancy yields the <em class=\"ltx_emph ltx_font_italic\">length mismatch problem</em> that most speech tokenizations produce sequences far longer than text, making it hard to jointly model text and speech.\nPrior remedies, such as interleaving speech and text tokens or introducing alignment heuristics, reduce the mismatch but do not directly resolve the modality gap, and often sacrifice prosodic fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "token",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> proposes to construct speech tokens that are <em class=\"ltx_emph ltx_font_italic\">explicitly aligned</em> with their text counterpart, which is called text-aligned speech tokens.\nFormally, in text-aligned speech token architectures, each time a text token is generated, a corresponding speech token is also generated, and the two token sequences are identical in length and order.\nIn practice, TASTE uses distilled Whisper-large-v3&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gandhi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib9\" title=\"\">2023</a>)</cite> encoder part as its encoder, which is a 32-layer speech encoder.\nThen, given the encoder&#8217;s hidden states, <math alttext=\"\\{h^{(1)},\\dots,h^{(L)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{h^{(1)},\\dots,h^{(L)}\\}</annotation></semantics></math> where each <math alttext=\"h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>h</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}</annotation></semantics></math>, text-aligned speech tokens aim to produce a compressed representation <math alttext=\"z\\in\\mathbb{R}^{N\\times d_{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>z</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>z</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">z\\in\\mathbb{R}^{N\\times d_{z}}</annotation></semantics></math> whose length matches that of the text sequence.\nA cross-attention mechanism achieves this by letting the text embeddings <math alttext=\"E(v)\\in\\mathbb{R}^{N\\times d_{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>q</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E(v)\\in\\mathbb{R}^{N\\times d_{q}}</annotation></semantics></math> act as queries, the final-layer states <math alttext=\"h^{(L)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(L)}</annotation></semantics></math> provide keys, and selected shallow layer <math alttext=\"h^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(\\ell)}</annotation></semantics></math> provide values.\nThe aggregated representation can thus be written as</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "token",
                    "taste",
                    "speech",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation aligns speech and text sequences at the token level, enabling straightforward joint modeling without heuristic synchronization.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We distinguish two quantities: the <em class=\"ltx_emph ltx_font_italic\">frame rate</em> <math alttext=\"R_{\\text{tok}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>tok</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{tok}}</annotation></semantics></math> (Hz) and the <em class=\"ltx_emph ltx_font_italic\">bitrate</em> <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (bits/s).\nThe token rate measures how many discrete speech tokens are produced per second for reconstruction, while the bitrate measures how many bits per second are actually needed to encode those tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the text-aligned speech token sequence length is aligned to the text token length, it is not a fixed token rate tokenizer.\nTherefore, we use the <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> test set to estimate the frame rate and calculate the bitrate.\nIn the <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> test set, there are about 19,805.2 seconds and a total of 51,903 text tokens; therefore, the average frame rate for text-aligned speech tokens is about 2.62.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "speech",
                    "token",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to design a text-aligned speech tokenization framework that captures richer acoustic and prosodic cues while remaining simple to train and easy to control. Concretely, we (i) aggregate multi-layer encoder features with <em class=\"ltx_emph ltx_font_italic\">Multi-Layer Dynamic Attention</em> (MLDA) to compress frame-level speech into text-length representations and (ii) discretize these representations by <em class=\"ltx_emph ltx_font_italic\">Finite Scalar Quantization</em> (FSQ) to control bitrate with minimal parameters.\nFinally, we train the whole system with a lightweight objective that combines a unit-level cross-entropy and a masked reconstruction loss.</p>\n\n",
                "matched_terms": [
                    "our",
                    "textaligned",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We turn frame-level speech features into a text-length sequence by letting text tokens query the speech encoder&#8217;s hidden states.\nThe keys come from the last encoder layer, while the values come from shallower layers.\nAn MLP produces per-layer mixture weights so the model can adapt each layer&#8217;s mix ratio for each frame.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"E(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">E(\\cdot)</annotation></semantics></math> be a token embedding function. We embed the text sequence <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> into queries <math alttext=\"Q=E(v)\\in\\mathbb{R}^{N\\times d_{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>q</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">Q=E(v)\\in\\mathbb{R}^{N\\times d_{q}}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the text length and <math alttext=\"d_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">d_{q}</annotation></semantics></math> is the query dimension.\nFinally, we compute the cross-attention between text tokens and aggregated speech tokens represented as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After we get the text token <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> and speech token <math alttext=\"\\hat{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{z}</annotation></semantics></math>, we uses a transformer-based unit decoder to autoregressively decode the text and speech token to speech units and then use the CosyVoice speech vocoder to convert it into speech.\nTherefore, we calculate the cross-entropy loss of the target speech units.\nWhen a target speech unit sequence <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> is available, a unit decoder consumes <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> and predicts <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> autoregressively with the usual next-token cross-entropy:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on both in-domain and out-of-domain datasets to comprehensively evaluate our framework.\n<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib23\" title=\"\">2015</a>)</cite> is used for model training and primary evaluation, while <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib20\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nagrani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib19\" title=\"\">2017</a>)</cite> are adopted for out-of-domain evaluation, as they contain richer acoustic variability aligned with our objectives.\nSpecifically, <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> emphasizes emotional expressiveness and prosodic variation, whereas <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> features diverse recording conditions with natural background noise.\nThe detailed dataset introduction could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A5\" title=\"Appendix E Dataset &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a></p>\n\n",
                "matched_terms": [
                    "voxceleb",
                    "librispeech",
                    "datasets",
                    "indomain",
                    "our",
                    "expresso",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study focuses on <em class=\"ltx_emph ltx_font_italic\">text-aligned</em> speech tokenization, aiming to produce token sequences whose length matches the text while keeping the bitrate low enough for joint speech-text modeling. To evaluate TASLA, we consider representative tokenizers from three major categories: Compression Bitrate Neural Codecs, Semantic-Acoustic Joint Modeling Tokenizers, and Text-Aligned Speech Tokenizers.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "token",
                    "speech",
                    "our",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because TASLA compresses the token rate to an extremely low level while preserving text alignment, there is no directly comparable tokenizer besides TASTE. We therefore include state-of-the-art codecs that either operate at, or can be adjusted to, similar bitrates: EnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>)</cite>, Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib7\" title=\"\">2024</a>)</cite>, SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite>, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite>, DualCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib15\" title=\"\">2025</a>)</cite>, and BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib30\" title=\"\">2024</a>)</cite>. These baselines vary in training data and parameter scales, which we summarize in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A1\" title=\"Appendix A Baselines and Implementation Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. Our conclusions are thus scoped to this targeted bitrate regime rather than claiming universal superiority.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "token",
                    "taste",
                    "our",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ablation studies, we further introduce the S3 topline and a text-only baseline. The S3 topline reconstructs speech directly from ground-truth S3 units, while the text-only baseline predicts S3 tokens from text without joint modeling.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "topline",
                    "speech",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our framework along two complementary dimensions: <em class=\"ltx_emph ltx_font_italic\">quality</em>, which reflects the overall naturalness and fidelity of the reconstructed audio, and <em class=\"ltx_emph ltx_font_italic\">prosody</em>, which measures how well prosodic patterns such as pitch, rhythm, and energy are preserved.</p>\n\n",
                "matched_terms": [
                    "our",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosody metrics, we report Emotion Consistency, F0-PCC, Energy-PCC, Energy-RMSE, Phrase Cosine Similarity (Phr. Cos.), Phrase L2 (Phr. L2), Voicing Decision Error (VDE), and Gross Pitch Error (GPE). Emotion Consistency checks whether the emotional intent is preserved after reconstruction. F0-PCC measures the correlation of pitch contours between hypothesis and reference after voicing masking and time alignment, while Energy-PCC and Energy-RMSE quantify how well loudness dynamics over time are retained. Phr. Cos. and Phr. L2 summarize phrase-level intonation by converting F0 to semitones, fitting degree-3 Legendre polynomials on normalized time, and comparing the resulting coefficient vectors of reference and reconstruction. VDE is the fraction of frames with mismatched voiced/unvoiced decisions (lower is better), and GPE is the proportion of voiced frames where predicted F0 deviates from the reference by a large relative margin, reflecting robustness of pitch accuracy.</p>\n\n",
                "matched_terms": [
                    "vde",
                    "report",
                    "gpe",
                    "prosody",
                    "phr",
                    "cos",
                    "metrics",
                    "f0pcc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> summarizes the quality results across in-domain <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and two out-of-domain datasets, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> and <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>.\nOverall, TASLA achieves superior performance compared to other commonly used speech codecs under most of the quality metrics under a significantly low frame rate.\nCompared to other low frame rate methods, TASLA is also closer to the S3 topline.\nSpecifically, on the OOD sets, the higher frame rate speech codecs get more advantage on WER since they could reconstruct more acoustic details, and the training data of the ASR models mostly contains <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and a large amount of training data.\nTherefore, even though the quality of the reconstructed speech of other methods is lower than TASLA, they could also get lower WER on these evaluation sets.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "tasla",
                    "across",
                    "voxceleb",
                    "librispeech",
                    "speech",
                    "datasets",
                    "indomain",
                    "results",
                    "expresso",
                    "metrics",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These improvements match our design goal: text-aligned speech tokens that retain prosody while remaining LM-friendly under aggressive compression.\nMechanistic analysis for this behavior is provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.SS2\" title=\"6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "textaligned",
                    "speech",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze 500 utterances from <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> to characterize the dynamic weighting module, since <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> is richer with prosody.\nThe per-frame normalized weight means are <math alttext=\"\\bar{w}_{0}=0.573,\\ \\bar{w}_{1}=0.325,\\ \\bar{w}_{2}=0.100,\\ \\bar{w}_{3}=0.0017\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0.573</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>1</mn></msub><mo>=</mo><mn>0.325</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>2</mn></msub><mo>=</mo><mn>0.100</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>3</mn></msub><mo>=</mo><mn>0.0017</mn></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{w}_{0}=0.573,\\ \\bar{w}_{1}=0.325,\\ \\bar{w}_{2}=0.100,\\ \\bar{w}_{3}=0.0017</annotation></semantics></math>.\nThus the stream is carried predominantly by <math alttext=\"w_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math>, with <math alttext=\"w_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">w_{1}</annotation></semantics></math> and <math alttext=\"w_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">w_{2}</annotation></semantics></math> providing substantial secondary contribution.\nTo quantify how many layers act at once, we compute frame-wise entropy</p>\n\n",
                "matched_terms": [
                    "expresso",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F3\" title=\"Figure 3 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we observe that TASLA predicts the S3 units more accurately than TASTE and text-only ablations.\nThese results indicate that TASLA is better at jointly modeling acoustic and semantic features, leading to more accurate S3 unit predictions.\nHowever, this improvement is not directly reflected in the quality metrics but instead appears in the prosody metrics.\nThis limitation may be due to the upper bound of the S3 units&#8217; performance and the size of the training data.</p>\n\n",
                "matched_terms": [
                    "bound",
                    "tasla",
                    "taste",
                    "upper",
                    "prosody",
                    "results",
                    "ablations",
                    "textonly",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce TASLA, a text-aligned speech tokenization framework designed to preserve fine-grained acoustic detail under an extremely low frame rate while remaining compatible with text-token alignment.\nOur experiments demonstrate that, despite the time-variant nature of the speech tokens, TASLA consistently maintains rich acoustic cues, achieving prosody metrics close to the upper-bound topline and outperforming prior text-aligned speech tokenizers on out-of-domain tasks. Quantitative analyses reveal how different encoder layers contribute to various acoustic aspects, offering interpretability that helps explain why certain layers dominate under specific speaking conditions.\nFuture work includes improving time efficiency through low-latency variants, and exploring single-stage generation approaches to overcome the performance ceiling imposed by S3 speech units.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "textaligned",
                    "speech",
                    "prosody",
                    "metrics",
                    "our",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the performance ceiling of the S3 units and the limited dataset size, TASLA has untapped potential to model speech more effectively than other text-aligned speech tokenizers, achieving higher accuracy in predicting S3 units. While this advantage does not manifest directly in quality metrics, it is reflected in improved prosody performance and a narrower gap to the S3 units&#8217; topline.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "textaligned",
                    "speech",
                    "prosody",
                    "metrics",
                    "model",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While TASLA focuses on text-aligned speech tokenization rather than speech generation, discrete speech representations could be misused for voice spoofing or cloning. All models and data used in this study are for research purposes only, and all datasets are public and released under academic licenses. We encourage responsible use and adherence to ethical standards when reproducing or extending this work.\nWe used AI assistants for grammar polishing and document formatting only. All experimental design, implementation, and data analysis were manually verified by the authors.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "all",
                    "speech",
                    "datasets",
                    "experimental",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study targets <em class=\"ltx_emph ltx_font_italic\">text-aligned</em> speech tokenization suitable for joint speech&#8211;text modeling at low bitrates and short sequences. Because only TASTE is directly text-aligned at word length, we include it and compare against state-of-the-art neural codecs that either operate near, or can be adjusted to, a similar low-bitrate regime (via retaining fewer RVQ layers and/or applying temporal striding) to ensure a fair comparison. We also report two ablations (S3 topline and text-only). The baselines used throughout the paper are: EnCodec, Mimi, SpeechTokenizer, DualCodec, BigCodec, and TASTE, plus the S3 topline and our text-only baseline.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "topline",
                    "textaligned",
                    "baseline",
                    "taste",
                    "report",
                    "speech",
                    "ablations",
                    "our",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baselines are run under a unified path-based pipeline: mono <span class=\"ltx_text ltx_font_typewriter\">float32</span> waveforms are loaded from disk, resampled to each model&#8217;s native sampling rate, encoded into discrete codes/units, and decoded back to waveforms. For RVQ/stacked-codebook models, we probe multiple operating points by (i) keeping only the first <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> quantizer layers and/or (ii) applying temporal sub-sampling, to bring effective rates into a comparable low-bitrate regime for joint modeling.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional encoder&#8211;decoder with RVQ, widely adopted for high-fidelity neural audio compression. We use the official Transformers implementation at 24&#8201;kHz and evaluate standard bandwidths (including 1.5&#8201;kbps) via the model&#8217;s built-in encode/decode APIs. In our tables, we report the configured bandwidth along with token/frame statistics measured from emitted code streams.</p>\n\n",
                "matched_terms": [
                    "our",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A modern low-latency neural codec used in speech&#8211;text foundation models. We employ the <span class=\"ltx_text ltx_font_italic\">kyutai/mimi</span> Transformers port at 24&#8201;kHz; encoding yields discrete <span class=\"ltx_text ltx_font_typewriter\">audio_codes</span> and decoding reconstructs waveforms. Frame/token statistics are computed from code length per second; when codebook details are not exposed, we also report the bandwidth indicated by the model card.</p>\n\n",
                "matched_terms": [
                    "report",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A unified discrete tokenizer designed for speech LMs (multi-stage VQ with LM-friendly objectives). We load the official package/config and use the provided <span class=\"ltx_text ltx_font_typewriter\">encode</span>/<span class=\"ltx_text ltx_font_typewriter\">decode</span> APIs. To reach low-bitrate operating points without retraining, we retain only the first <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> RVQ layers and optionally apply a time stride; we report the resulting measured token/frame statistics under these settings.</p>\n\n",
                "matched_terms": [
                    "report",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech tokenizer explicitly designed for joint modeling: text tokens query a frozen speech encoder (keys from the last layer; values from selected shallow layers), producing <em class=\"ltx_emph ltx_font_italic\">text-length</em> representations that are then discretized before unit decoding and vocoding. We use TASTE as the canonical text-aligned baseline in our low-rate regime.</p>\n\n",
                "matched_terms": [
                    "textaligned",
                    "baseline",
                    "taste",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">S3 Topline:</span> reconstructs speech directly from ground-truth S3 units with the same unit-to-speech vocoder stack; it serves as an upper-bound reference at word-level frame rates. <span class=\"ltx_text ltx_font_italic\">Text-only baseline:</span> predicts S3 tokens from text alone (no speech tokens) to quantify the value of speech tokenization under the same decoder/vocoder. Both are reported in the main results and ablations.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "baseline",
                    "speech",
                    "results",
                    "ablations",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each baseline, we use official or widely adopted checkpoints and libraries (Transformers or the authors&#8217; packages), adhere to their documented sampling rates/hop sizes, and expose only non-retraining knobs (layer keep, time stride) when needed to reach the shared low-bitrate regime. All models are run under the same I/O wrapper described above.</p>\n\n",
                "matched_terms": [
                    "all",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train TASLA on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> (train-clean-100/360 and train-other-500) and validate/test on dev-clean/dev-other and test-clean/test-other.\nTraining uses dynamic batching with a 2000-frame budget and gradient accumulation of 2, distributed across 4 Nvidia A800 GPUs.\nThe model is initialized from a text-only baseline and employs an FSQ audio quantizer (codebook dimension = 64, codebook size = 8).\nWe optimize with Adam at a learning rate of <math alttext=\"1.6\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.6\\times 10^{-4}</annotation></semantics></math> using a warmup scheduler with 5k warmup steps and gradient clipping of 5, for up to 5 epochs.\nWe evaluate and save every 2000 steps and select the best weights by development-set accuracy.\nThe random seed is fixed to 1986 for random, NumPy, and PyTorch.</p>\n\n",
                "matched_terms": [
                    "across",
                    "baseline",
                    "librispeech",
                    "textonly",
                    "model",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report only the prosody metrics used in our tables. Unless noted, audio is resampled to 16&#8201;kHz. Pairwise statistics are computed on a shared, DTW-aligned time axis: we extract MFCCs from reference/hypothesis, obtain a DTW path, and time-warp hypothesis-side prosody features to the reference timeline. Let aligned F0 (Hz) be <math alttext=\"f^{\\mathrm{Hz}}_{\\mathrm{ref}},f^{\\mathrm{Hz}}_{\\mathrm{hyp}}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>ref</mi><mi>Hz</mi></msubsup><mo>,</mo><msubsup><mi>f</mi><mi>hyp</mi><mi>Hz</mi></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">f^{\\mathrm{Hz}}_{\\mathrm{ref}},f^{\\mathrm{Hz}}_{\\mathrm{hyp}}\\in\\mathbb{R}^{L}</annotation></semantics></math> with voiced masks <math alttext=\"v_{\\mathrm{ref}},v_{\\mathrm{hyp}}\\in\\{0,1\\}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>v</mi><mi>ref</mi></msub><mo>,</mo><msub><mi>v</mi><mi>hyp</mi></msub></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">v_{\\mathrm{ref}},v_{\\mathrm{hyp}}\\in\\{0,1\\}^{L}</annotation></semantics></math>. Define the jointly voiced mask <math alttext=\"b=v_{\\mathrm{ref}}\\land v_{\\mathrm{hyp}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msub><mi>v</mi><mi>ref</mi></msub><mo>&#8743;</mo><msub><mi>v</mi><mi>hyp</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">b=v_{\\mathrm{ref}}\\land v_{\\mathrm{hyp}}</annotation></semantics></math> and <math alttext=\"n_{v}=\\sum b\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mi>v</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><mo>&#8721;</mo><mi>b</mi></mrow></mrow><annotation encoding=\"application/x-tex\">n_{v}=\\sum b</annotation></semantics></math>. Frame RMS sequences are <math alttext=\"\\rho_{\\mathrm{ref}},\\rho_{\\mathrm{hyp}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#961;</mi><mi>ref</mi></msub><mo>,</mo><msub><mi>&#961;</mi><mi>hyp</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\rho_{\\mathrm{ref}},\\rho_{\\mathrm{hyp}}</annotation></semantics></math>; their dB versions use <math alttext=\"E(\\cdot)=20\\log_{10}(\\max(\\cdot,\\epsilon))\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>20</mn><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mi>log</mi><mn>10</mn></msub><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>,</mo><mi>&#1013;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">E(\\cdot)=20\\log_{10}(\\max(\\cdot,\\epsilon))</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "report",
                    "metrics",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F0 uses CREPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib12\" title=\"\">2018</a>)</cite>; energy uses frame RMS; all pairwise metrics share the same DTW-aligned time base to avoid length/misalignment artifacts. Degree 3 for Legendre fits is fixed across systems/datasets for fair comparison.</p>\n\n",
                "matched_terms": [
                    "across",
                    "metrics",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib23\" title=\"\">2015</a>)</cite> is a widely used English corpus containing 585 hours of read speech from 2,458 speakers. We use the training and development subsets during training and report evaluation results on the test subset.</p>\n\n",
                "matched_terms": [
                    "results",
                    "report",
                    "speech",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib20\" title=\"\">2023</a>)</cite> is an expressive English speech corpus designed to capture a wide range of emotions and prosodic patterns. We adopt it for out-of-domain evaluation to assess whether our method preserves emotional cues and prosodic richness.</p>\n\n",
                "matched_terms": [
                    "our",
                    "expresso",
                    "speech",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nagrani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib19\" title=\"\">2017</a>)</cite> is a large-scale, text-independent speaker identification dataset collected from unconstrained YouTube videos.\nSince transcripts are not provided, we do not calculate WER for this dataset.\nWe include <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> as it provides challenging scenarios with varied speakers, spontaneous speaking styles, and background noise, making it suitable for testing robustness and naturalness in speech reconstruction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxceleb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All datasets used in this work are publicly released for academic research (<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> under CC BY 4.0, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> under YouTube TOS, <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> under research license).</p>\n\n",
                "matched_terms": [
                    "voxceleb",
                    "librispeech",
                    "all",
                    "datasets",
                    "expresso"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use <em class=\"ltx_emph ltx_font_italic\">S3 units</em> as the discrete supervision and the reconstruction target in our pipeline.\nConcretely, an external unit recognizer from the CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib8\" title=\"\">2024</a>)</cite> stack converts a waveform into a\none-dimensional sequence of discrete unit IDs <math alttext=\"s=[s_{1},\\dots,s_{M}]\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s=[s_{1},\\dots,s_{M}]</annotation></semantics></math>, <math alttext=\"s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>S3</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}</annotation></semantics></math>,\nat roughly word-/syllable-level time resolution. These units carry high-level linguistic content along\nwith coarse prosodic cues and are paired with a unit-to-speech vocoder that maps <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> back to a waveform.\nIn TASLA, the unit decoder consumes the text-aligned speech tokens <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> and predicts the S3 sequence\nautoregressively (cross-entropy objective), after which the CosyVoice unit-to-speech vocoder reconstructs\naudio. We also report an <em class=\"ltx_emph ltx_font_italic\">S3 topline</em> that bypasses tokenization by feeding ground-truth S3 units\nto the same vocoder, providing an informative upper bound under the same unitized reconstruction stack.</p>\n\n",
                "matched_terms": [
                    "bound",
                    "topline",
                    "textaligned",
                    "report",
                    "upper",
                    "speech",
                    "our",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the shallow-layer ablation study, we select layers 3, 6, 9, and 32 and evaluate the model on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A7.T3\" title=\"Table 3 &#8227; Appendix G Shallow Layer Abaltion &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the shallow-layer ablation achieves performance comparable to TASLA on the PCC (contour) metrics, but it performs significantly worse on Energy RMSE.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "rmse",
                    "pcc",
                    "metrics",
                    "model",
                    "tasla"
                ]
            }
        ]
    },
    "S5.SS3.SSS2.tab1": {
        "source_file": "TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation",
        "caption": "Table 2: Experimental Results for Quality Metrics. Lower is better for WER; higher is better for the others. Voxceleb has no WER column because no transcripts are available. Our proposed method (TASLA) outperforms or performs competitively against the baselines over different evaluation sets.\nThe out-of-domain (OOD) tag is applied to TASTE and TASLA in this table, since the two OOD datasets may overlap with the training data of other speech tokenizers.",
        "body": "LibriSpeech\n\nVoxceleb (OOD)\n\nExpresso (OOD)\n\n\nModel\nFrame Rate\n\n\nBitrate\n\n\n\n\nWER\n\n\n\n\nUTMOS\n\n\n\n\nSpk. Sim.\n\n\n\n\nUTMOS\n\n\n\n\nSpk. Sim.\n\n\n\n\nWER\n\n\n\n\nUTMOS\n\n\n\n\nSpk. Sim.\n\n\n\n\nGround Truth\n—\n\n\n256000\n\n\n\n\n0.05\n\n\n\n\n3.41\n\n\n\n\n—\n\n\n\n\n2.82\n\n\n\n\n—\n\n\n\n\n0.11\n\n\n\n\n2.98\n\n\n\n\n—\n\n\n\n\n\n\\rowcolorgray!20                frame rate less than 150\n\n\n\nEnCodec\n75\n\n\n1500\n\n\n\n\n0.07\n\n\n\n\n1.40\n\n\n\n\n0.63\n\n\n\n\n1.44\n\n\n\n\n0.61\n\n\n\n\n0.13\n\n\n\n\n1.11\n\n\n\n\n0.56\n\n\n\n\nSpeechTokenizer\n50\n\n\n1000\n\n\n\n\n0.11\n\n\n\n\n1.72\n\n\n\n\n0.34\n\n\n\n\n1.80\n\n\n\n\n0.39\n\n\n\n\n0.30\n\n\n\n\n1.51\n\n\n\n\n0.34\n\n\n\n\nBigCodec\n80\n\n\n1040\n\n\n\n\n0.05\n\n\n\n\n3.29\n\n\n\n\n1.00\n\n\n\n\n2.76\n\n\n\n\n0.99\n\n\n\n\n0.12\n\n\n\n\n2.79\n\n\n\n\n0.99\n\n\n\n\nDualCodec\n12.5\n\n\n1225\n\n\n\n\n0.04\n\n\n\n\n3.16\n\n\n\n\n0.88\n\n\n\n\n2.60\n\n\n\n\n0.84\n\n\n\n\n0.09\n\n\n\n\n2.80\n\n\n\n\n0.84\n\n\n\n\nMimi\n12.5\n\n\n1100\n\n\n\n\n0.04\n\n\n\n\n3.01\n\n\n\n\n0.93\n\n\n\n\n2.53\n\n\n\n\n0.93\n\n\n\n\n0.10\n\n\n\n\n2.45\n\n\n\n\n0.89\n\n\n\n\n\n\\rowcolorgray!20                frame rate less than 10 (word-level)\n\n\n\nS3 Topline\n—\n\n\n600\n\n\n\n\n0.04\n\n\n\n\n3.40\n\n\n\n\n0.87\n\n\n\n\n2.93\n\n\n\n\n0.84\n\n\n\n\n0.12\n\n\n\n\n3.17\n\n\n\n\n0.82\n\n\n\n\nText-only Baseline\n—\n\n\n50\n\n\n\n\n0.23\n\n\n\n\n3.42\n\n\n\n\n0.77\n\n\n\n\n3.33\n\n\n\n\n0.68\n\n\n\n\n0.41\n\n\n\n\n3.28\n\n\n\n\n0.64\n\n\n\n\nTASTE\n\n∼\\sim2.62\n\n\n∼\\sim150\n\n\n\n\n0.10\n\n\n\n\n3.51\n\n\n\n\n0.85\n\n\n\n\n3.31\n\n\n\n\n0.78\n\n\n\n\n0.27\n\n\n\n\n3.33\n\n\n\n\n0.75\n\n\n\n\nTASLA\n\n∼\\sim2.62\n\n\n∼\\sim600\n\n\n\n\n0.12\n\n\n\n\n3.43\n\n\n\n\n0.87\n\n\n\n\n3.10\n\n\n\n\n0.81\n\n\n\n\n0.28\n\n\n\n\n3.12\n\n\n\n\n0.80",
        "html_code": "<table class=\"ltx_tabular ltx_figure_panel ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.4pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.4pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" style=\"padding:0.4pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> (OOD)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> (OOD)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">Frame Rate</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Bitrate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">WER</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">UTMOS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Spk. Sim.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">UTMOS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Spk. Sim.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">WER</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">UTMOS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Spk. Sim.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">256000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.41</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.82</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.98</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"11\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20 &#8197;&#8197; &#8197; &#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197; <em class=\"ltx_emph ltx_font_italic\">frame rate less than 150</em>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">EnCodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">75</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.07</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.63</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.44</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.61</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.13</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.56</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">SpeechTokenizer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">50</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.34</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.39</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.51</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.34</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">BigCodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">80</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1040</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.29</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.76</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.99</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.12</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.79</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.99</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">DualCodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">12.5</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1225</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.88</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.60</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.84</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.09</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.84</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">Mimi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">12.5</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1100</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.01</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.93</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.53</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.93</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.89</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"11\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20 &#8197;&#8197; &#8197; &#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197;&#8197; <em class=\"ltx_emph ltx_font_italic\">frame rate less than 10 (word-level)</em>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">S3 Topline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">600</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.87</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.93</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.84</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.12</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.17</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.82</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">Text-only Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">50</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.42</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.77</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.68</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.41</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.28</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.64</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">TASTE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.62</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>150</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.51</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.85</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.31</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.78</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.75</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">TASLA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.62</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>600</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.12</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.43</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.87</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.81</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.28</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.12</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.80</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "frame",
            "topline",
            "voxceleb",
            "wer",
            "taste",
            "has",
            "speech",
            "rowcolorgray20",
            "datasets",
            "performs",
            "data",
            "lower",
            "proposed",
            "∼sim262",
            "∼sim600",
            "utmos",
            "over",
            "than",
            "transcripts",
            "∼sim150",
            "mimi",
            "evaluation",
            "because",
            "rate",
            "encodec",
            "bigcodec",
            "different",
            "results",
            "outofdomain",
            "truth",
            "column",
            "expresso",
            "metrics",
            "ood",
            "sets",
            "method",
            "speechtokenizer",
            "tag",
            "sim",
            "outperforms",
            "dualcodec",
            "overlap",
            "training",
            "competitively",
            "two",
            "experimental",
            "others",
            "applied",
            "tasla",
            "less",
            "higher",
            "available",
            "spk",
            "librispeech",
            "baseline",
            "ground",
            "against",
            "wordlevel",
            "quality",
            "better",
            "tokenizers",
            "other",
            "our",
            "bitrate",
            "textonly",
            "model",
            "since"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">T</span>ext-<span class=\"ltx_text ltx_font_bold\">A</span>ligned <span class=\"ltx_text ltx_font_bold\">S</span>peech Tokens with Multiple <span class=\"ltx_text ltx_font_bold\">L</span>ayer-<span class=\"ltx_text ltx_font_bold\">A</span>ggregation (TASLA), which is a text&#8209;aligned speech tokenization framework that aims to address the problem that under a low-frame-rate and text-aligned regime, single-source speech tokens may lose acoustic details during reconstruction.\nOn the other hand, this paper further explains how different encoder layers collaborate to capture comprehensive acoustic features for tokenization.\nPrevious work, TASTE, proposed the text-aligned speech tokenization framework, which is a LM-friendly architecture, but struggles to capture acoustic details.\nWe address this trade&#8209;off with two components: Multi&#8209;Layer Dynamic Attention (MLDA), which lets each text position adaptively mix shallow/deep features from a frozen speech encoder, and Finite Scalar Quantization (FSQ), a simple per&#8209;dimension discretization with smooth optimization.\nAt about 2.62 Hz (tokens/s), TASLA consistently improves prosody and achieves competitive quality over TASTE on in-domain (<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>) and OOD (<span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>) sets.\nWe further demonstrate that dynamic layer mixing is correlated with spectral flux and explains why MLDA preserves prosody under a low frame rate with extreme feature compression.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "sets",
                    "voxceleb",
                    "librispeech",
                    "taste",
                    "rate",
                    "speech",
                    "quality",
                    "different",
                    "proposed",
                    "other",
                    "two",
                    "expresso",
                    "over",
                    "tasla",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have recently transformed text understanding and generation.\nTo pursue a more natural interaction mode with LMs, researchers have started to research Spoken Language Models (SLM)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib5\" title=\"\">2024</a>); Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>); KimiTeam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib13\" title=\"\">2025</a>); Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib21\" title=\"\">2024a</a>)</cite>, where models process and generate speech as a first-class modality.\nIf we want to use voice mode to interact with the LMs, one option is a cascade pipeline that first generates text with an LM and then converts text to speech using a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib31\" title=\"\">2025</a>); Shikhar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib26\" title=\"\">2025</a>)</cite>.\nIn contrast, SLM offers a better approach, as it captures acoustic details from context and generates speech that more naturally aligns with the context&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Arora et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib1\" title=\"\">2025</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib10\" title=\"\">2025</a>)</cite>.\nThe key to enabling SLMs to have such an ability is speech tokenization.\nSpeech tokenization involves mapping continuous waveforms into discrete tokens that are compact, learnable, and acoustically informative, enabling a vocoder to reconstruct speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, there are two practical problems with joint modeling of text tokens and speech tokens for an SLM.\nThe first problem is the trade-off between acoustic richness and LM-friendliness.\nNeural speech tokenizers that generate more tokens per second could provide fine-grained acoustic details since they use a shorter time frame for speech reconstruction, and these speech tokenizers often capture only acoustic details without text information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nTherefore, they could generate very high-quality speech, but their speech token sequence is often very long&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib4\" title=\"\">2023</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib29\" title=\"\">2023</a>)</cite>, which is not suitable for transformer-based architectures if we want to generate longer speech.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "speech",
                    "tokenizers",
                    "two",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second problem is the speech token sequence length mismatch with the text sequence length.\nStandard codecs operate at fixed frame rates that yield token streams far longer than text (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12.5&#8211;50 speech Hz vs. <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2&#8211;3 text Hz), making joint speech-text modeling harder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nMaking the LMs need to learn two modalities separately, which also increases the difficulty of the training process.\nHowever, aggressive sequence compression helps alignment but risks losing prosodic detail.\nPrior work addresses the mismatch by aligning speech tokens to text via an attention mechanism during tokenization, enabling straightforward joint modeling at very low bitrates&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "speech",
                    "training",
                    "two",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, under strong compression of the token length, it still shows limitations in preserving fine-grained acoustics and in out-of-domain generalization. To overcome these problems and find a balance between bitrate and token sequence length.\nWe propose <span class=\"ltx_text ltx_font_bold\">TASLA</span>, a text-aligned speech tokenization framework that preserves acoustic detail at text length.\nTASLA introduces Multi-Layer Dynamic Attention (MLDA), which enables text tokens to query a frozen speech encoder and adaptively combine shallow and deep representations, allowing each text position to gather the most predictive acoustic evidence for content, prosody, and speaker cues.\nTo discretize reliably, we replace Residual Vector Quantization (RVQ) with Finite Scalar Quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mentzer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib18\" title=\"\">2024</a>)</cite>, which provides smooth optimization and resilience to codebook pathologies.</p>\n\n",
                "matched_terms": [
                    "bitrate",
                    "outofdomain",
                    "speech",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work shows that <em class=\"ltx_emph ltx_font_italic\">dynamic, per-token fusion across encoder depth</em> is the missing piece for text-aligned speech tokens: MLDA lets each word position select the most predictive mixture of shallow and deep features, thereby retaining prosody and speaker cues at a <em class=\"ltx_emph ltx_font_italic\">text-length</em> rate.\nUnder rate parity with prior text-aligned tokenizers, MLDA is consistently comparable in quality and outperforms on prosody metrics in both in-domain and OOD settings, and yields better spoken continuation than strong text-only and speech-token baselines, demonstrating that adaptive layer mixing effectively narrows the gap between alignment convenience and acoustic expressivity.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "rate",
                    "speech",
                    "outperforms",
                    "quality",
                    "better",
                    "tokenizers",
                    "textonly",
                    "ood",
                    "metrics",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CNN-based neural codecs typically use RVQ and multi-scale spectral/adversarial losses.\nThey often excel at high-fidelity reconstruction and preserve prosodic cues.\nEnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>)</cite> and DAC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kumar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib14\" title=\"\">2023</a>)</cite> are canonical convolutional codecs with RVQ, which have strong perceptual quality and real-time operation, but are not designed for text alignment.\nThen, BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib30\" title=\"\">2024</a>)</cite> and TAAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Parker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib24\" title=\"\">2025</a>)</cite> achieve extremely high quality at extremely low bitrates using a convolution codec architecture.\nHowever, they are still fixed-rate and not LM-friendly for speech&#8211;text joint modeling.\nTherefore, Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib7\" title=\"\">2024</a>)</cite> deliberately reduces token rates to be more LM-friendly, which is a useful bridge from pure reconstruction toward joint modeling.\nIn contrast, our method goes beyond purely acoustic reconstruction by jointly modeling speech tokens in both acoustic and semantic information.</p>\n\n",
                "matched_terms": [
                    "mimi",
                    "method",
                    "bigcodec",
                    "encodec",
                    "speech",
                    "quality",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate linguistic and semantic information and enable unified modeling, recent work blends semantic and acoustic representations.\nSpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite> distills semantics into upper RVQ codebooks using SSL features, yielding tokens that carry both semantic and acoustic content, but its sequences remain relatively long and not text-aligned.\nDualCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib15\" title=\"\">2025</a>)</cite> uses dual branches, semantic and acoustic, to balance intelligibility and fidelity, with lower rates but still not explicitly aligned to text.\nSpirit-LM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib22\" title=\"\">2024b</a>)</cite> interleaves speech and text tokens for joint modeling; effective but depends on extra interleaving rules rather than addressing alignment at tokenization.\nThese methods add semantics or interleaving strategies, yet do not solve alignment at the tokenization stage, leaving training and inference complexity and potential prosody loss.\nOur method differs by enforcing speech-text length alignment during tokenization, while still preserving both acoustic and semantic information.</p>\n\n",
                "matched_terms": [
                    "method",
                    "speechtokenizer",
                    "speech",
                    "dualcodec",
                    "lower",
                    "training",
                    "our",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-aligned speech tokens use text positions as queries over speech representations so that tokenization itself aligns speech tokens to the text length.\nSequences become LM-friendly while retaining prosodic and paralinguistic cues.\nBecause of alignment, the token rate is about <math alttext=\"1/20\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">1/20</annotation></semantics></math>&#8211;<math alttext=\"1/5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">1/5</annotation></semantics></math> of traditional tokenizers.\nTASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> introduces the text-aligned paradigm, where text-query cross-attention over speech encoder features produces time-variant, text-length-matched tokens, thereby eliminating speech&#8211;text length mismatch and enabling straightforward SLM training.\nHowever, the extreme compression of the bitrate and token rate results in the loss of acoustic details.\nUnlike prior text-aligned approaches, our method maintains the extremely low frame rate while preserving richer acoustic detail in reconstruction.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "method",
                    "because",
                    "taste",
                    "speech",
                    "rate",
                    "results",
                    "tokenizers",
                    "training",
                    "our",
                    "bitrate",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech and text provide two complementary but structurally different views of language.\nLet a spoken utterance be denoted as a continuous waveform <math alttext=\"u\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">u\\in\\mathbb{R}^{T}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of acoustic frames.\nOn the other hand, let the corresponding transcript be a discrete token sequence <math alttext=\"v=[v_{1},\\dots,v_{N}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">v=[v_{1},\\dots,v_{N}]</annotation></semantics></math> of length <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>.\nThe asymmetry between <math alttext=\"|T|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>T</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|T|</annotation></semantics></math> and <math alttext=\"|N|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>N</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|N|</annotation></semantics></math> is striking: while text unfolds sparsely with clear boundaries, speech evolves densely, carrying layered information including acoustic details and prosody.\nThis discrepancy yields the <em class=\"ltx_emph ltx_font_italic\">length mismatch problem</em> that most speech tokenizations produce sequences far longer than text, making it hard to jointly model text and speech.\nPrior remedies, such as interleaving speech and text tokens or introducing alignment heuristics, reduce the mismatch but do not directly resolve the modality gap, and often sacrifice prosodic fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "other",
                    "two",
                    "model",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> proposes to construct speech tokens that are <em class=\"ltx_emph ltx_font_italic\">explicitly aligned</em> with their text counterpart, which is called text-aligned speech tokens.\nFormally, in text-aligned speech token architectures, each time a text token is generated, a corresponding speech token is also generated, and the two token sequences are identical in length and order.\nIn practice, TASTE uses distilled Whisper-large-v3&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gandhi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib9\" title=\"\">2023</a>)</cite> encoder part as its encoder, which is a 32-layer speech encoder.\nThen, given the encoder&#8217;s hidden states, <math alttext=\"\\{h^{(1)},\\dots,h^{(L)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{h^{(1)},\\dots,h^{(L)}\\}</annotation></semantics></math> where each <math alttext=\"h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>h</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}</annotation></semantics></math>, text-aligned speech tokens aim to produce a compressed representation <math alttext=\"z\\in\\mathbb{R}^{N\\times d_{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>z</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>z</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">z\\in\\mathbb{R}^{N\\times d_{z}}</annotation></semantics></math> whose length matches that of the text sequence.\nA cross-attention mechanism achieves this by letting the text embeddings <math alttext=\"E(v)\\in\\mathbb{R}^{N\\times d_{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>q</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E(v)\\in\\mathbb{R}^{N\\times d_{q}}</annotation></semantics></math> act as queries, the final-layer states <math alttext=\"h^{(L)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(L)}</annotation></semantics></math> provide keys, and selected shallow layer <math alttext=\"h^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(\\ell)}</annotation></semantics></math> provide values.\nThe aggregated representation can thus be written as</p>\n\n",
                "matched_terms": [
                    "two",
                    "taste",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> is the number of quantizer stages and <math alttext=\"\\mathcal{C}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{C}_{r}</annotation></semantics></math> is the <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>-th codebook.\nAlthough RVQ provides fine-grained reconstruction, it often requires large codebooks and multiple stages, which inflate bitrate and introduce redundancy.\nThis motivates alternative quantization strategies that achieve comparable expressiveness with fewer parameters, lower bitrate, and better preservation of prosodic cues.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "bitrate",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We distinguish two quantities: the <em class=\"ltx_emph ltx_font_italic\">frame rate</em> <math alttext=\"R_{\\text{tok}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>tok</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{tok}}</annotation></semantics></math> (Hz) and the <em class=\"ltx_emph ltx_font_italic\">bitrate</em> <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (bits/s).\nThe token rate measures how many discrete speech tokens are produced per second for reconstruction, while the bitrate measures how many bits per second are actually needed to encode those tokens.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "bitrate",
                    "rate",
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For residual vector quantization (RVQ) with <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> quantizers and a codebook size <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> per layer, each token consumes <math alttext=\"R\\log_{2}K,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>R</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace=\"0.167em\">&#8289;</mo><mi>K</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">R\\log_{2}K,</annotation></semantics></math> and the bitrate is <math alttext=\"\\textstyle b=R_{\\text{tok}}\\cdot R\\log_{2}K.\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo>=</mo><mrow><mrow><msub><mi>R</mi><mtext>tok</mtext></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>R</mi></mrow><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace=\"0.167em\">&#8289;</mo><mi>K</mi></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\textstyle b=R_{\\text{tok}}\\cdot R\\log_{2}K.</annotation></semantics></math>\nIntuitively, increasing either the token frequency <math alttext=\"R_{\\text{tok}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>tok</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{tok}}</annotation></semantics></math> or the per-token code payload <math alttext=\"R\\log_{2}K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>R</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace=\"0.167em\">&#8289;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">R\\log_{2}K</annotation></semantics></math> raises the bitrate.\nThis RVQ counting is standard in neural codecs/speech tokenizers that output layered code indices.</p>\n\n",
                "matched_terms": [
                    "bitrate",
                    "tokenizers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the text-aligned speech token sequence length is aligned to the text token length, it is not a fixed token rate tokenizer.\nTherefore, we use the <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> test set to estimate the frame rate and calculate the bitrate.\nIn the <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> test set, there are about 19,805.2 seconds and a total of 51,903 text tokens; therefore, the average frame rate for text-aligned speech tokens is about 2.62.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "librispeech",
                    "rate",
                    "speech",
                    "bitrate",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to design a text-aligned speech tokenization framework that captures richer acoustic and prosodic cues while remaining simple to train and easy to control. Concretely, we (i) aggregate multi-layer encoder features with <em class=\"ltx_emph ltx_font_italic\">Multi-Layer Dynamic Attention</em> (MLDA) to compress frame-level speech into text-length representations and (ii) discretize these representations by <em class=\"ltx_emph ltx_font_italic\">Finite Scalar Quantization</em> (FSQ) to control bitrate with minimal parameters.\nFinally, we train the whole system with a lightweight objective that combines a unit-level cross-entropy and a masked reconstruction loss.</p>\n\n",
                "matched_terms": [
                    "our",
                    "bitrate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We turn frame-level speech features into a text-length sequence by letting text tokens query the speech encoder&#8217;s hidden states.\nThe keys come from the last encoder layer, while the values come from shallower layers.\nAn MLP produces per-layer mixture weights so the model can adapt each layer&#8217;s mix ratio for each frame.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"(\\cdot)_{\\mathrm{sg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><mi>sg</mi></msub><annotation encoding=\"application/x-tex\">(\\cdot)_{\\mathrm{sg}}</annotation></semantics></math> stops gradients. The decoder maps <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> back to the feature space; training minimizes MSE on valid frames (with masking if sequences are padded).\nIn our experiments, we select the <math alttext=\"d=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">d=64</annotation></semantics></math> and <math alttext=\"L=8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">L=8</annotation></semantics></math> for a balance of bitrate and performance.</p>\n\n",
                "matched_terms": [
                    "our",
                    "bitrate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After we get the text token <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> and speech token <math alttext=\"\\hat{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{z}</annotation></semantics></math>, we uses a transformer-based unit decoder to autoregressively decode the text and speech token to speech units and then use the CosyVoice speech vocoder to convert it into speech.\nTherefore, we calculate the cross-entropy loss of the target speech units.\nWhen a target speech unit sequence <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> is available, a unit decoder consumes <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> and predicts <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> autoregressively with the usual next-token cross-entropy:</p>\n\n",
                "matched_terms": [
                    "available",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further help the FSQ module learn how to reconstruct tokens back to latent vectors, we use a reconstruction loss to further help the training process.\nIndependent of speech units, we mask the padded frames and calculate the MSE loss of the pre-FSQ feature <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> and its dequantized feature <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on both in-domain and out-of-domain datasets to comprehensively evaluate our framework.\n<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib23\" title=\"\">2015</a>)</cite> is used for model training and primary evaluation, while <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib20\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nagrani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib19\" title=\"\">2017</a>)</cite> are adopted for out-of-domain evaluation, as they contain richer acoustic variability aligned with our objectives.\nSpecifically, <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> emphasizes emotional expressiveness and prosodic variation, whereas <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> features diverse recording conditions with natural background noise.\nThe detailed dataset introduction could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A5\" title=\"Appendix E Dataset &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a></p>\n\n",
                "matched_terms": [
                    "voxceleb",
                    "librispeech",
                    "evaluation",
                    "datasets",
                    "outofdomain",
                    "training",
                    "our",
                    "expresso",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study focuses on <em class=\"ltx_emph ltx_font_italic\">text-aligned</em> speech tokenization, aiming to produce token sequences whose length matches the text while keeping the bitrate low enough for joint speech-text modeling. To evaluate TASLA, we consider representative tokenizers from three major categories: Compression Bitrate Neural Codecs, Semantic-Acoustic Joint Modeling Tokenizers, and Text-Aligned Speech Tokenizers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokenizers",
                    "our",
                    "bitrate",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since many existing codecs operate at bitrates much higher than 1 kbps, we downsample their outputs&#8212;by retaining fewer RVQ layers or applying temporal striding&#8212;to approximately 1 kbps. This allows us to ensure fair comparisons under a comparable bitrate regime. The implementation details are provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A1\" title=\"Appendix A Baselines and Implementation Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "bitrate",
                    "since",
                    "than",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because TASLA compresses the token rate to an extremely low level while preserving text alignment, there is no directly comparable tokenizer besides TASTE. We therefore include state-of-the-art codecs that either operate at, or can be adjusted to, similar bitrates: EnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>)</cite>, Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib7\" title=\"\">2024</a>)</cite>, SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite>, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite>, DualCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib15\" title=\"\">2025</a>)</cite>, and BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib30\" title=\"\">2024</a>)</cite>. These baselines vary in training data and parameter scales, which we summarize in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A1\" title=\"Appendix A Baselines and Implementation Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. Our conclusions are thus scoped to this targeted bitrate regime rather than claiming universal superiority.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "mimi",
                    "speechtokenizer",
                    "taste",
                    "because",
                    "rate",
                    "encodec",
                    "bigcodec",
                    "data",
                    "dualcodec",
                    "training",
                    "our",
                    "bitrate",
                    "tasla",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ablation studies, we further introduce the S3 topline and a text-only baseline. The S3 topline reconstructs speech directly from ground-truth S3 units, while the text-only baseline predicts S3 tokens from text without joint modeling.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "topline",
                    "speech",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our framework along two complementary dimensions: <em class=\"ltx_emph ltx_font_italic\">quality</em>, which reflects the overall naturalness and fidelity of the reconstructed audio, and <em class=\"ltx_emph ltx_font_italic\">prosody</em>, which measures how well prosodic patterns such as pitch, rhythm, and energy are preserved.</p>\n\n",
                "matched_terms": [
                    "our",
                    "two",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosody metrics, we report Emotion Consistency, F0-PCC, Energy-PCC, Energy-RMSE, Phrase Cosine Similarity (Phr. Cos.), Phrase L2 (Phr. L2), Voicing Decision Error (VDE), and Gross Pitch Error (GPE). Emotion Consistency checks whether the emotional intent is preserved after reconstruction. F0-PCC measures the correlation of pitch contours between hypothesis and reference after voicing masking and time alignment, while Energy-PCC and Energy-RMSE quantify how well loudness dynamics over time are retained. Phr. Cos. and Phr. L2 summarize phrase-level intonation by converting F0 to semitones, fitting degree-3 Legendre polynomials on normalized time, and comparing the resulting coefficient vectors of reference and reconstruction. VDE is the fraction of frames with mismatched voiced/unvoiced decisions (lower is better), and GPE is the proportion of voiced frames where predicted F0 deviates from the reference by a large relative margin, reflecting robustness of pitch accuracy.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "over",
                    "metrics",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For quality metrics, we use Word Error Rate (WER), UTMOS, and Speaker Similarity to evaluate the reconstructed speech&#8217;s quality.\nWord Error Rate (WER) is computed between the ASR transcription of the reconstructed audio and the reference transcript.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baba et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib2\" title=\"\">2024</a>)</cite> is a non-intrusive, neural MOS predictor that estimates perceived naturalness and quality directly from the waveform.\nSpeaker similarity is quantified as the cosine similarity between speaker embeddings extracted from reference and reconstructed audio.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "rate",
                    "quality",
                    "utmos",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the main experimental results of the quality metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> and prosody metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe details of the weight analysis could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.SS2\" title=\"6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a>.\nThe details of the training and parameters can be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A2\" title=\"Appendix B Training Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nThe details of metrics formulation and explanation could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "results",
                    "training",
                    "experimental",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, TASLA consistently surpasses other text-aligned speech token frameworks on nearly every prosody indicator.\nAnd it is also closer to the S3 tokens&#8217; topline.\nThis pattern shows that TASLA preserves paralinguistic cues, intonation, rhythm, and emphasis, rather than merely reconstructing clean audio, and does so under much stronger compression.\nFor pitch-related behavior, a higher F0-PCC means TASLA&#8217;s generated pitch contour follows the reference more faithfully.\nAt the same time, lower GPE and VDE indicate fewer octave (pitch halving/doubling) mistakes and more stable voiced/unvoiced decisions, yielding more natural intonation and fewer pitch glitches.\nFor loudness dynamics and phrasing, higher Energy-PCC reflects a closer alignment of energy fluctuations over time, while lower Energy-RMSE shows that the absolute loudness levels better match the reference, rather than just the trend.\nHigher Phrase-Cos. and lower Phrase-L2 further indicate that pauses, timing, and rhythmic structure align more precisely with the ground truth.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "ground",
                    "speech",
                    "better",
                    "lower",
                    "truth",
                    "other",
                    "over",
                    "tasla",
                    "than",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> summarizes the quality results across in-domain <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and two out-of-domain datasets, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> and <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>.\nOverall, TASLA achieves superior performance compared to other commonly used speech codecs under most of the quality metrics under a significantly low frame rate.\nCompared to other low frame rate methods, TASLA is also closer to the S3 topline.\nSpecifically, on the OOD sets, the higher frame rate speech codecs get more advantage on WER since they could reconstruct more acoustic details, and the training data of the ASR models mostly contains <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and a large amount of training data.\nTherefore, even though the quality of the reconstructed speech of other methods is lower than TASLA, they could also get lower WER on these evaluation sets.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "topline",
                    "voxceleb",
                    "wer",
                    "speech",
                    "datasets",
                    "data",
                    "lower",
                    "than",
                    "evaluation",
                    "rate",
                    "results",
                    "outofdomain",
                    "expresso",
                    "metrics",
                    "ood",
                    "sets",
                    "training",
                    "two",
                    "tasla",
                    "higher",
                    "librispeech",
                    "quality",
                    "other",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These improvements match our design goal: text-aligned speech tokens that retain prosody while remaining LM-friendly under aggressive compression.\nMechanistic analysis for this behavior is provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.SS2\" title=\"6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze 500 utterances from <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> to characterize the dynamic weighting module, since <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> is richer with prosody.\nThe per-frame normalized weight means are <math alttext=\"\\bar{w}_{0}=0.573,\\ \\bar{w}_{1}=0.325,\\ \\bar{w}_{2}=0.100,\\ \\bar{w}_{3}=0.0017\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>0</mn></msub><mo>=</mo><mn>0.573</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>1</mn></msub><mo>=</mo><mn>0.325</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>2</mn></msub><mo>=</mo><mn>0.100</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><msub><mover accent=\"true\"><mi>w</mi><mo>&#175;</mo></mover><mn>3</mn></msub><mo>=</mo><mn>0.0017</mn></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{w}_{0}=0.573,\\ \\bar{w}_{1}=0.325,\\ \\bar{w}_{2}=0.100,\\ \\bar{w}_{3}=0.0017</annotation></semantics></math>.\nThus the stream is carried predominantly by <math alttext=\"w_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math>, with <math alttext=\"w_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">w_{1}</annotation></semantics></math> and <math alttext=\"w_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">w_{2}</annotation></semantics></math> providing substantial secondary contribution.\nTo quantify how many layers act at once, we compute frame-wise entropy</p>\n\n",
                "matched_terms": [
                    "expresso",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">and report <math alttext=\"\\bar{H}=0.484\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>H</mi><mo>&#175;</mo></mover><mo>=</mo><mn>0.484</mn></mrow><annotation encoding=\"application/x-tex\">\\bar{H}=0.484</annotation></semantics></math>.\nThe effective number of layers <math alttext=\"\\mathrm{ENL}=\\exp(\\bar{H}){\\approx}1.62\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>ENL</mi><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>H</mi><mo>&#175;</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mn>1.62</mn></mrow><annotation encoding=\"application/x-tex\">\\mathrm{ENL}=\\exp(\\bar{H}){\\approx}1.62</annotation></semantics></math> indicates about one to two meaningful contributors per frame.</p>\n\n",
                "matched_terms": [
                    "two",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We relate weights to short-time spectral dynamics.\nLet spectral flux be\n<math alttext=\"F_{t}=\\lVert\\mathbf{m}_{t}-\\mathbf{m}_{t-1}\\rVert_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>t</mi></msub><mo rspace=\"0.1389em\">=</mo><msub><mrow><mo fence=\"true\" lspace=\"0.1389em\" rspace=\"0em\">&#8741;</mo><mrow><msub><mi>&#119846;</mi><mi>t</mi></msub><mo>&#8722;</mo><msub><mi>&#119846;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo fence=\"true\" lspace=\"0em\">&#8741;</mo></mrow><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">F_{t}=\\lVert\\mathbf{m}_{t}-\\mathbf{m}_{t-1}\\rVert_{2}</annotation></semantics></math>, where <math alttext=\"\\mathbf{m}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119846;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{m}_{t}</annotation></semantics></math> denotes mel magnitudes.\nHigh <math alttext=\"F_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">F_{t}</annotation></semantics></math> marks onsets/offsets and low <math alttext=\"F_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">F_{t}</annotation></semantics></math> marks steady vowels or near-silence.\nFrom Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F2\" title=\"Figure 2 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we observe <math alttext=\"\\mathrm{corr}(w_{0},F)=-0.116\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.116</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{0},F)=-0.116</annotation></semantics></math>, <math alttext=\"\\mathrm{corr}(w_{1},F)=+0.464\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>+</mo><mn>0.464</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{1},F)=+0.464</annotation></semantics></math>, <math alttext=\"\\mathrm{corr}(w_{2},F)=-0.379\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.379</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{2},F)=-0.379</annotation></semantics></math>, and <math alttext=\"\\mathrm{corr}(w_{3},F)=-0.183\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.183</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{3},F)=-0.183</annotation></semantics></math>.\nPrevious work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baby et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib3\" title=\"\">2020</a>)</cite> denotes that the spectrum flux has a positive relation with the phoneme or sub-word boundaries.\nCombining with the observation of the average weight and the relation from Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F3\" title=\"Figure 3 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, deeper layers (16th and 24th layer, <math alttext=\"w_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m9\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">w_{1}</annotation></semantics></math> and <math alttext=\"w_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m10\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">w_{2}</annotation></semantics></math>) are more related to the flux; therefore, the deeper layers may relate to the phoneme or sub-word boundaries.\nOn the other hand, the <math alttext=\"w_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m11\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math> seems not obviously related to the spectrum flux, but has a high weight ratio, which shows that shallow layers provide more acoustic cues, but are not obviously related to phoneme or sub-word boundaries.\nTherefore, since the semantic information has been carried with the text tokens, the deeper layer&#8217;s weight becomes smaller since it carries lower acoustic cues.\nThis observation also matches the conclusion of&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib17\" title=\"\">2025</a>)</cite> and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nTherefore, we provide an ablation study that mainly uses shallower layers for MLDA; see the experiments in&#160;Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A7\" title=\"Appendix G Shallow Layer Abaltion &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "has",
                    "since",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F3\" title=\"Figure 3 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we observe that TASLA predicts the S3 units more accurately than TASTE and text-only ablations.\nThese results indicate that TASLA is better at jointly modeling acoustic and semantic features, leading to more accurate S3 unit predictions.\nHowever, this improvement is not directly reflected in the quality metrics but instead appears in the prosody metrics.\nThis limitation may be due to the upper bound of the S3 units&#8217; performance and the size of the training data.</p>\n\n",
                "matched_terms": [
                    "tasla",
                    "taste",
                    "data",
                    "better",
                    "results",
                    "quality",
                    "training",
                    "textonly",
                    "metrics",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce TASLA, a text-aligned speech tokenization framework designed to preserve fine-grained acoustic detail under an extremely low frame rate while remaining compatible with text-token alignment.\nOur experiments demonstrate that, despite the time-variant nature of the speech tokens, TASLA consistently maintains rich acoustic cues, achieving prosody metrics close to the upper-bound topline and outperforming prior text-aligned speech tokenizers on out-of-domain tasks. Quantitative analyses reveal how different encoder layers contribute to various acoustic aspects, offering interpretability that helps explain why certain layers dominate under specific speaking conditions.\nFuture work includes improving time efficiency through low-latency variants, and exploring single-stage generation approaches to overcome the performance ceiling imposed by S3 speech units.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "topline",
                    "tasla",
                    "rate",
                    "speech",
                    "different",
                    "outofdomain",
                    "tokenizers",
                    "our",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the performance ceiling of the S3 units and the limited dataset size, TASLA has untapped potential to model speech more effectively than other text-aligned speech tokenizers, achieving higher accuracy in predicting S3 units. While this advantage does not manifest directly in quality metrics, it is reflected in improved prosody performance and a narrower gap to the S3 units&#8217; topline.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "tasla",
                    "speech",
                    "has",
                    "quality",
                    "tokenizers",
                    "other",
                    "model",
                    "metrics",
                    "than",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While TASLA focuses on text-aligned speech tokenization rather than speech generation, discrete speech representations could be misused for voice spoofing or cloning. All models and data used in this study are for research purposes only, and all datasets are public and released under academic licenses. We encourage responsible use and adherence to ethical standards when reproducing or extending this work.\nWe used AI assistants for grammar polishing and document formatting only. All experimental design, implementation, and data analysis were manually verified by the authors.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets",
                    "data",
                    "experimental",
                    "tasla",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study targets <em class=\"ltx_emph ltx_font_italic\">text-aligned</em> speech tokenization suitable for joint speech&#8211;text modeling at low bitrates and short sequences. Because only TASTE is directly text-aligned at word length, we include it and compare against state-of-the-art neural codecs that either operate near, or can be adjusted to, a similar low-bitrate regime (via retaining fewer RVQ layers and/or applying temporal striding) to ensure a fair comparison. We also report two ablations (S3 topline and text-only). The baselines used throughout the paper are: EnCodec, Mimi, SpeechTokenizer, DualCodec, BigCodec, and TASTE, plus the S3 topline and our text-only baseline.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "topline",
                    "mimi",
                    "speechtokenizer",
                    "baseline",
                    "taste",
                    "because",
                    "speech",
                    "encodec",
                    "bigcodec",
                    "against",
                    "dualcodec",
                    "our",
                    "two",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baselines are run under a unified path-based pipeline: mono <span class=\"ltx_text ltx_font_typewriter\">float32</span> waveforms are loaded from disk, resampled to each model&#8217;s native sampling rate, encoded into discrete codes/units, and decoded back to waveforms. For RVQ/stacked-codebook models, we probe multiple operating points by (i) keeping only the first <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> quantizer layers and/or (ii) applying temporal sub-sampling, to bring effective rates into a comparable low-bitrate regime for joint modeling.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A capacity-scaled neural codec targeting very low bitrates with strong reconstruction quality. We call the official repository&#8217;s encoder/decoder at 16&#8201;kHz, retaining exposed VQ indices (<span class=\"ltx_text ltx_font_typewriter\">vq_code</span>) for rate accounting; any block-size padding mandated by the model is trimmed post-decoding.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech tokenizer explicitly designed for joint modeling: text tokens query a frozen speech encoder (keys from the last layer; values from selected shallow layers), producing <em class=\"ltx_emph ltx_font_italic\">text-length</em> representations that are then discretized before unit decoding and vocoding. We use TASTE as the canonical text-aligned baseline in our low-rate regime.</p>\n\n",
                "matched_terms": [
                    "our",
                    "taste",
                    "speech",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">S3 Topline:</span> reconstructs speech directly from ground-truth S3 units with the same unit-to-speech vocoder stack; it serves as an upper-bound reference at word-level frame rates. <span class=\"ltx_text ltx_font_italic\">Text-only baseline:</span> predicts S3 tokens from text alone (no speech tokens) to quantify the value of speech tokenization under the same decoder/vocoder. Both are reported in the main results and ablations.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "topline",
                    "baseline",
                    "wordlevel",
                    "speech",
                    "results",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train TASLA on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> (train-clean-100/360 and train-other-500) and validate/test on dev-clean/dev-other and test-clean/test-other.\nTraining uses dynamic batching with a 2000-frame budget and gradient accumulation of 2, distributed across 4 Nvidia A800 GPUs.\nThe model is initialized from a text-only baseline and employs an FSQ audio quantizer (codebook dimension = 64, codebook size = 8).\nWe optimize with Adam at a learning rate of <math alttext=\"1.6\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.6\\times 10^{-4}</annotation></semantics></math> using a warmup scheduler with 5k warmup steps and gradient clipping of 5, for up to 5 epochs.\nWe evaluate and save every 2000 steps and select the best weights by development-set accuracy.\nThe random seed is fixed to 1986 for random, NumPy, and PyTorch.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "librispeech",
                    "rate",
                    "training",
                    "textonly",
                    "model",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WER is a standard metric for evaluating speech recognition quality, defined as the minimum edit distance between the hypothesis and reference transcripts, normalized by the reference length. Specifically, it computes the total number of substitutions (<math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>), insertions (<math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>), and deletions (<math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>) required to transform the hypothesis into the reference, divided by the number of words in the reference (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>):</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "quality",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A lower WER indicates higher transcription accuracy and better preservation of linguistic content.\nWe use Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib25\" title=\"\">2023</a>)</cite> for transcription and apply standard text normalization before computing WER. This metric reflects intelligibility and alignment with the reference text.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "lower",
                    "better",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baba et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib2\" title=\"\">2024</a>)</cite> is a non-intrusive, neural MOS predictor that estimates perceived naturalness and quality directly from the waveform.\nUnlike WER, which focuses on content accuracy, UTMOS approximates human subjective ratings of audio quality, providing a complementary measure of perceptual fidelity.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "quality",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report only the prosody metrics used in our tables. Unless noted, audio is resampled to 16&#8201;kHz. Pairwise statistics are computed on a shared, DTW-aligned time axis: we extract MFCCs from reference/hypothesis, obtain a DTW path, and time-warp hypothesis-side prosody features to the reference timeline. Let aligned F0 (Hz) be <math alttext=\"f^{\\mathrm{Hz}}_{\\mathrm{ref}},f^{\\mathrm{Hz}}_{\\mathrm{hyp}}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>ref</mi><mi>Hz</mi></msubsup><mo>,</mo><msubsup><mi>f</mi><mi>hyp</mi><mi>Hz</mi></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">f^{\\mathrm{Hz}}_{\\mathrm{ref}},f^{\\mathrm{Hz}}_{\\mathrm{hyp}}\\in\\mathbb{R}^{L}</annotation></semantics></math> with voiced masks <math alttext=\"v_{\\mathrm{ref}},v_{\\mathrm{hyp}}\\in\\{0,1\\}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>v</mi><mi>ref</mi></msub><mo>,</mo><msub><mi>v</mi><mi>hyp</mi></msub></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">v_{\\mathrm{ref}},v_{\\mathrm{hyp}}\\in\\{0,1\\}^{L}</annotation></semantics></math>. Define the jointly voiced mask <math alttext=\"b=v_{\\mathrm{ref}}\\land v_{\\mathrm{hyp}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msub><mi>v</mi><mi>ref</mi></msub><mo>&#8743;</mo><msub><mi>v</mi><mi>hyp</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">b=v_{\\mathrm{ref}}\\land v_{\\mathrm{hyp}}</annotation></semantics></math> and <math alttext=\"n_{v}=\\sum b\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mi>v</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><mo>&#8721;</mo><mi>b</mi></mrow></mrow><annotation encoding=\"application/x-tex\">n_{v}=\\sum b</annotation></semantics></math>. Frame RMS sequences are <math alttext=\"\\rho_{\\mathrm{ref}},\\rho_{\\mathrm{hyp}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#961;</mi><mi>ref</mi></msub><mo>,</mo><msub><mi>&#961;</mi><mi>hyp</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\rho_{\\mathrm{ref}},\\rho_{\\mathrm{hyp}}</annotation></semantics></math>; their dB versions use <math alttext=\"E(\\cdot)=20\\log_{10}(\\max(\\cdot,\\epsilon))\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>20</mn><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mi>log</mi><mn>10</mn></msub><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>,</mo><mi>&#1013;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">E(\\cdot)=20\\log_{10}(\\max(\\cdot,\\epsilon))</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "frame",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F0 uses CREPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib12\" title=\"\">2018</a>)</cite>; energy uses frame RMS; all pairwise metrics share the same DTW-aligned time base to avoid length/misalignment artifacts. Degree 3 for Legendre fits is fixed across systems/datasets for fair comparison.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select three dataset for evaluation and training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib23\" title=\"\">2015</a>)</cite> is a widely used English corpus containing 585 hours of read speech from 2,458 speakers. We use the training and development subsets during training and report evaluation results on the test subset.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "evaluation",
                    "speech",
                    "results",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib20\" title=\"\">2023</a>)</cite> is an expressive English speech corpus designed to capture a wide range of emotions and prosodic patterns. We adopt it for out-of-domain evaluation to assess whether our method preserves emotional cues and prosodic richness.</p>\n\n",
                "matched_terms": [
                    "method",
                    "evaluation",
                    "speech",
                    "outofdomain",
                    "our",
                    "expresso"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Nagrani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib19\" title=\"\">2017</a>)</cite> is a large-scale, text-independent speaker identification dataset collected from unconstrained YouTube videos.\nSince transcripts are not provided, we do not calculate WER for this dataset.\nWe include <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> as it provides challenging scenarios with varied speakers, spontaneous speaking styles, and background noise, making it suitable for testing robustness and naturalness in speech reconstruction.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "voxceleb",
                    "wer",
                    "speech",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All datasets used in this work are publicly released for academic research (<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> under CC BY 4.0, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> under YouTube TOS, <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span> under research license).</p>\n\n",
                "matched_terms": [
                    "expresso",
                    "datasets",
                    "voxceleb",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use <em class=\"ltx_emph ltx_font_italic\">S3 units</em> as the discrete supervision and the reconstruction target in our pipeline.\nConcretely, an external unit recognizer from the CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib8\" title=\"\">2024</a>)</cite> stack converts a waveform into a\none-dimensional sequence of discrete unit IDs <math alttext=\"s=[s_{1},\\dots,s_{M}]\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s=[s_{1},\\dots,s_{M}]</annotation></semantics></math>, <math alttext=\"s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>S3</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}</annotation></semantics></math>,\nat roughly word-/syllable-level time resolution. These units carry high-level linguistic content along\nwith coarse prosodic cues and are paired with a unit-to-speech vocoder that maps <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> back to a waveform.\nIn TASLA, the unit decoder consumes the text-aligned speech tokens <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> and predicts the S3 sequence\nautoregressively (cross-entropy objective), after which the CosyVoice unit-to-speech vocoder reconstructs\naudio. We also report an <em class=\"ltx_emph ltx_font_italic\">S3 topline</em> that bypasses tokenization by feeding ground-truth S3 units\nto the same vocoder, providing an informative upper bound under the same unitized reconstruction stack.</p>\n\n",
                "matched_terms": [
                    "our",
                    "topline",
                    "speech",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the shallow-layer ablation study, we select layers 3, 6, 9, and 32 and evaluate the model on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A7.T3\" title=\"Table 3 &#8227; Appendix G Shallow Layer Abaltion &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the shallow-layer ablation achieves performance comparable to TASLA on the PCC (contour) metrics, but it performs significantly worse on Energy RMSE.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "performs",
                    "metrics",
                    "model",
                    "tasla"
                ]
            }
        ]
    },
    "A7.T3": {
        "source_file": "TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation",
        "caption": "Table 3: Experimental Results for Shallow Layer Ablation.",
        "body": "Model\nEne. RMSE ↓\\downarrow\n\n\n\nF0-PCC ↑\\uparrow\n\n\n\n\nPhr. Cos. ↑\\uparrow\n\n\n\n\nEne. PCC ↑\\uparrow\n\n\n\n\nUTMOS ↑\\uparrow\n\n\n\n\nWER ↓\\downarrow\n\n\n\n\n\n\nS3 Topline\n6.94\n\n\n0.91\n\n\n\n\n0.92\n\n\n\n\n0.95\n\n\n\n\n3.40\n\n\n\n\n0.04\n\n\n\n\nText-only Baseline\n10.08\n\n\n0.33\n\n\n\n\n0.89\n\n\n\n\n0.81\n\n\n\n\n3.51\n\n\n\n\n0.23\n\n\n\n\nTASTE\n8.63\n\n\n0.80\n\n\n\n\n0.91\n\n\n\n\n0.88\n\n\n\n\n3.42\n\n\n\n\n0.10\n\n\n\n\nTASLA\n6.97\n\n\n0.87\n\n\n\n\n0.90\n\n\n\n\n0.92\n\n\n\n\n3.43\n\n\n\n\n0.12\n\n\n\n\nTASLA (Shallow Layer)\n7.65\n\n\n0.86\n\n\n\n\n0.91\n\n\n\n\n0.92\n\n\n\n\n3.44\n\n\n\n\n0.10",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 4.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">Ene. RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">F0-PCC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Phr. Cos. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Ene. PCC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">S3 Topline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">6.94</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.92</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.04</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Text-only Baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">10.08</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.89</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.81</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.51</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.23</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASTE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">8.63</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.88</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.42</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.10</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASLA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:0.5pt 4.0pt;\">6.97</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.87</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.90</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.92</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.43</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.12</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">TASLA (Shallow Layer)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">7.65</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.86</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.91</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.92</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.44</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.10</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "topline",
            "wer",
            "taste",
            "ene",
            "utmos",
            "↑uparrow",
            "f0pcc",
            "results",
            "pcc",
            "cos",
            "layer",
            "ablation",
            "↓downarrow",
            "experimental",
            "tasla",
            "shallow",
            "baseline",
            "rmse",
            "phr",
            "textonly",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the shallow-layer ablation study, we select layers 3, 6, 9, and 32 and evaluate the model on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A7.T3\" title=\"Table 3 &#8227; Appendix G Shallow Layer Abaltion &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the shallow-layer ablation achieves performance comparable to TASLA on the PCC (contour) metrics, but it performs significantly worse on Energy RMSE.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">T</span>ext-<span class=\"ltx_text ltx_font_bold\">A</span>ligned <span class=\"ltx_text ltx_font_bold\">S</span>peech Tokens with Multiple <span class=\"ltx_text ltx_font_bold\">L</span>ayer-<span class=\"ltx_text ltx_font_bold\">A</span>ggregation (TASLA), which is a text&#8209;aligned speech tokenization framework that aims to address the problem that under a low-frame-rate and text-aligned regime, single-source speech tokens may lose acoustic details during reconstruction.\nOn the other hand, this paper further explains how different encoder layers collaborate to capture comprehensive acoustic features for tokenization.\nPrevious work, TASTE, proposed the text-aligned speech tokenization framework, which is a LM-friendly architecture, but struggles to capture acoustic details.\nWe address this trade&#8209;off with two components: Multi&#8209;Layer Dynamic Attention (MLDA), which lets each text position adaptively mix shallow/deep features from a frozen speech encoder, and Finite Scalar Quantization (FSQ), a simple per&#8209;dimension discretization with smooth optimization.\nAt about 2.62 Hz (tokens/s), TASLA consistently improves prosody and achieves competitive quality over TASTE on in-domain (<span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span>) and OOD (<span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span>) sets.\nWe further demonstrate that dynamic layer mixing is correlated with spectral flux and explains why MLDA preserves prosody under a low frame rate with extreme feature compression.</p>\n\n",
                "matched_terms": [
                    "taste",
                    "layer",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, under strong compression of the token length, it still shows limitations in preserving fine-grained acoustics and in out-of-domain generalization. To overcome these problems and find a balance between bitrate and token sequence length.\nWe propose <span class=\"ltx_text ltx_font_bold\">TASLA</span>, a text-aligned speech tokenization framework that preserves acoustic detail at text length.\nTASLA introduces Multi-Layer Dynamic Attention (MLDA), which enables text tokens to query a frozen speech encoder and adaptively combine shallow and deep representations, allowing each text position to gather the most predictive acoustic evidence for content, prosody, and speaker cues.\nTo discretize reliably, we replace Residual Vector Quantization (RVQ) with Finite Scalar Quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mentzer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib18\" title=\"\">2024</a>)</cite>, which provides smooth optimization and resilience to codebook pathologies.</p>\n\n",
                "matched_terms": [
                    "shallow",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work shows that <em class=\"ltx_emph ltx_font_italic\">dynamic, per-token fusion across encoder depth</em> is the missing piece for text-aligned speech tokens: MLDA lets each word position select the most predictive mixture of shallow and deep features, thereby retaining prosody and speaker cues at a <em class=\"ltx_emph ltx_font_italic\">text-length</em> rate.\nUnder rate parity with prior text-aligned tokenizers, MLDA is consistently comparable in quality and outperforms on prosody metrics in both in-domain and OOD settings, and yields better spoken continuation than strong text-only and speech-token baselines, demonstrating that adaptive layer mixing effectively narrows the gap between alignment convenience and acoustic expressivity.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "layer",
                    "shallow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-aligned speech tokens use text positions as queries over speech representations so that tokenization itself aligns speech tokens to the text length.\nSequences become LM-friendly while retaining prosodic and paralinguistic cues.\nBecause of alignment, the token rate is about <math alttext=\"1/20\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">1/20</annotation></semantics></math>&#8211;<math alttext=\"1/5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">1/5</annotation></semantics></math> of traditional tokenizers.\nTASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> introduces the text-aligned paradigm, where text-query cross-attention over speech encoder features produces time-variant, text-length-matched tokens, thereby eliminating speech&#8211;text length mismatch and enabling straightforward SLM training.\nHowever, the extreme compression of the bitrate and token rate results in the loss of acoustic details.\nUnlike prior text-aligned approaches, our method maintains the extremely low frame rate while preserving richer acoustic detail in reconstruction.</p>\n\n",
                "matched_terms": [
                    "results",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite> proposes to construct speech tokens that are <em class=\"ltx_emph ltx_font_italic\">explicitly aligned</em> with their text counterpart, which is called text-aligned speech tokens.\nFormally, in text-aligned speech token architectures, each time a text token is generated, a corresponding speech token is also generated, and the two token sequences are identical in length and order.\nIn practice, TASTE uses distilled Whisper-large-v3&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gandhi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib9\" title=\"\">2023</a>)</cite> encoder part as its encoder, which is a 32-layer speech encoder.\nThen, given the encoder&#8217;s hidden states, <math alttext=\"\\{h^{(1)},\\dots,h^{(L)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{h^{(1)},\\dots,h^{(L)}\\}</annotation></semantics></math> where each <math alttext=\"h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>h</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h^{(\\ell)}\\in\\mathbb{R}^{T\\times d_{h}}</annotation></semantics></math>, text-aligned speech tokens aim to produce a compressed representation <math alttext=\"z\\in\\mathbb{R}^{N\\times d_{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>z</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>z</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">z\\in\\mathbb{R}^{N\\times d_{z}}</annotation></semantics></math> whose length matches that of the text sequence.\nA cross-attention mechanism achieves this by letting the text embeddings <math alttext=\"E(v)\\in\\mathbb{R}^{N\\times d_{q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>q</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E(v)\\in\\mathbb{R}^{N\\times d_{q}}</annotation></semantics></math> act as queries, the final-layer states <math alttext=\"h^{(L)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(L)}</annotation></semantics></math> provide keys, and selected shallow layer <math alttext=\"h^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">h^{(\\ell)}</annotation></semantics></math> provide values.\nThe aggregated representation can thus be written as</p>\n\n",
                "matched_terms": [
                    "taste",
                    "shallow",
                    "layer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We turn frame-level speech features into a text-length sequence by letting text tokens query the speech encoder&#8217;s hidden states.\nThe keys come from the last encoder layer, while the values come from shallower layers.\nAn MLP produces per-layer mixture weights so the model can adapt each layer&#8217;s mix ratio for each frame.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because TASLA compresses the token rate to an extremely low level while preserving text alignment, there is no directly comparable tokenizer besides TASTE. We therefore include state-of-the-art codecs that either operate at, or can be adjusted to, similar bitrates: EnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib6\" title=\"\">2023</a>)</cite>, Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib7\" title=\"\">2024</a>)</cite>, SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib32\" title=\"\">2023</a>)</cite>, TASTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tseng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib28\" title=\"\">2025</a>)</cite>, DualCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib15\" title=\"\">2025</a>)</cite>, and BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib30\" title=\"\">2024</a>)</cite>. These baselines vary in training data and parameter scales, which we summarize in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A1\" title=\"Appendix A Baselines and Implementation Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. Our conclusions are thus scoped to this targeted bitrate regime rather than claiming universal superiority.</p>\n\n",
                "matched_terms": [
                    "taste",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ablation studies, we further introduce the S3 topline and a text-only baseline. The S3 topline reconstructs speech directly from ground-truth S3 units, while the text-only baseline predicts S3 tokens from text without joint modeling.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "topline",
                    "ablation",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosody metrics, we report Emotion Consistency, F0-PCC, Energy-PCC, Energy-RMSE, Phrase Cosine Similarity (Phr. Cos.), Phrase L2 (Phr. L2), Voicing Decision Error (VDE), and Gross Pitch Error (GPE). Emotion Consistency checks whether the emotional intent is preserved after reconstruction. F0-PCC measures the correlation of pitch contours between hypothesis and reference after voicing masking and time alignment, while Energy-PCC and Energy-RMSE quantify how well loudness dynamics over time are retained. Phr. Cos. and Phr. L2 summarize phrase-level intonation by converting F0 to semitones, fitting degree-3 Legendre polynomials on normalized time, and comparing the resulting coefficient vectors of reference and reconstruction. VDE is the fraction of frames with mismatched voiced/unvoiced decisions (lower is better), and GPE is the proportion of voiced frames where predicted F0 deviates from the reference by a large relative margin, reflecting robustness of pitch accuracy.</p>\n\n",
                "matched_terms": [
                    "phr",
                    "cos",
                    "f0pcc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For quality metrics, we use Word Error Rate (WER), UTMOS, and Speaker Similarity to evaluate the reconstructed speech&#8217;s quality.\nWord Error Rate (WER) is computed between the ASR transcription of the reconstructed audio and the reference transcript.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baba et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib2\" title=\"\">2024</a>)</cite> is a non-intrusive, neural MOS predictor that estimates perceived naturalness and quality directly from the waveform.\nSpeaker similarity is quantified as the cosine similarity between speaker embeddings extracted from reference and reconstructed audio.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the main experimental results of the quality metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> and prosody metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe details of the weight analysis could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.SS2\" title=\"6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a>.\nThe details of the training and parameters can be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A2\" title=\"Appendix B Training Details &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nThe details of metrics formulation and explanation could be found in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.T1\" title=\"Table 1 &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, TASLA consistently surpasses other text-aligned speech token frameworks on nearly every prosody indicator.\nAnd it is also closer to the S3 tokens&#8217; topline.\nThis pattern shows that TASLA preserves paralinguistic cues, intonation, rhythm, and emphasis, rather than merely reconstructing clean audio, and does so under much stronger compression.\nFor pitch-related behavior, a higher F0-PCC means TASLA&#8217;s generated pitch contour follows the reference more faithfully.\nAt the same time, lower GPE and VDE indicate fewer octave (pitch halving/doubling) mistakes and more stable voiced/unvoiced decisions, yielding more natural intonation and fewer pitch glitches.\nFor loudness dynamics and phrasing, higher Energy-PCC reflects a closer alignment of energy fluctuations over time, while lower Energy-RMSE shows that the absolute loudness levels better match the reference, rather than just the trend.\nHigher Phrase-Cos. and lower Phrase-L2 further indicate that pauses, timing, and rhythmic structure align more precisely with the ground truth.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "tasla",
                    "f0pcc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S5.SS3.SSS2\" title=\"5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">5.3.2</span></a> summarizes the quality results across in-domain <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and two out-of-domain datasets, <span class=\"ltx_text ltx_font_smallcaps\">Voxceleb</span> and <span class=\"ltx_text ltx_font_smallcaps\">Expresso</span>.\nOverall, TASLA achieves superior performance compared to other commonly used speech codecs under most of the quality metrics under a significantly low frame rate.\nCompared to other low frame rate methods, TASLA is also closer to the S3 topline.\nSpecifically, on the OOD sets, the higher frame rate speech codecs get more advantage on WER since they could reconstruct more acoustic details, and the training data of the ASR models mostly contains <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> and a large amount of training data.\nTherefore, even though the quality of the reconstructed speech of other methods is lower than TASLA, they could also get lower WER on these evaluation sets.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "topline",
                    "tasla",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We relate weights to short-time spectral dynamics.\nLet spectral flux be\n<math alttext=\"F_{t}=\\lVert\\mathbf{m}_{t}-\\mathbf{m}_{t-1}\\rVert_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>t</mi></msub><mo rspace=\"0.1389em\">=</mo><msub><mrow><mo fence=\"true\" lspace=\"0.1389em\" rspace=\"0em\">&#8741;</mo><mrow><msub><mi>&#119846;</mi><mi>t</mi></msub><mo>&#8722;</mo><msub><mi>&#119846;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo fence=\"true\" lspace=\"0em\">&#8741;</mo></mrow><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">F_{t}=\\lVert\\mathbf{m}_{t}-\\mathbf{m}_{t-1}\\rVert_{2}</annotation></semantics></math>, where <math alttext=\"\\mathbf{m}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119846;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{m}_{t}</annotation></semantics></math> denotes mel magnitudes.\nHigh <math alttext=\"F_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">F_{t}</annotation></semantics></math> marks onsets/offsets and low <math alttext=\"F_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">F_{t}</annotation></semantics></math> marks steady vowels or near-silence.\nFrom Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F2\" title=\"Figure 2 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we observe <math alttext=\"\\mathrm{corr}(w_{0},F)=-0.116\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.116</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{0},F)=-0.116</annotation></semantics></math>, <math alttext=\"\\mathrm{corr}(w_{1},F)=+0.464\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>+</mo><mn>0.464</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{1},F)=+0.464</annotation></semantics></math>, <math alttext=\"\\mathrm{corr}(w_{2},F)=-0.379\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.379</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{2},F)=-0.379</annotation></semantics></math>, and <math alttext=\"\\mathrm{corr}(w_{3},F)=-0.183\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>corr</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>,</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.183</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{corr}(w_{3},F)=-0.183</annotation></semantics></math>.\nPrevious work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baby et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib3\" title=\"\">2020</a>)</cite> denotes that the spectrum flux has a positive relation with the phoneme or sub-word boundaries.\nCombining with the observation of the average weight and the relation from Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F3\" title=\"Figure 3 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, deeper layers (16th and 24th layer, <math alttext=\"w_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m9\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">w_{1}</annotation></semantics></math> and <math alttext=\"w_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m10\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">w_{2}</annotation></semantics></math>) are more related to the flux; therefore, the deeper layers may relate to the phoneme or sub-word boundaries.\nOn the other hand, the <math alttext=\"w_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p3.m11\" intent=\":literal\"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math> seems not obviously related to the spectrum flux, but has a high weight ratio, which shows that shallow layers provide more acoustic cues, but are not obviously related to phoneme or sub-word boundaries.\nTherefore, since the semantic information has been carried with the text tokens, the deeper layer&#8217;s weight becomes smaller since it carries lower acoustic cues.\nThis observation also matches the conclusion of&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib17\" title=\"\">2025</a>)</cite> and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nTherefore, we provide an ablation study that mainly uses shallower layers for MLDA; see the experiments in&#160;Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#A7\" title=\"Appendix G Shallow Layer Abaltion &#8227; Ethical Considerations and Potential Risks. &#8227; 8 Limitations &#8227; 7 Conclusion &#8227; 6.3 S3 Unit Accuracy &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "shallow",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#S6.F3\" title=\"Figure 3 &#8227; 6.2 Analysis of Dynamic Weights &#8227; 6 Experimental Results &#8227; 5.3.2 Quality Metrics &#8227; 5.3 Evaluation Metrics &#8227; 5 Experimental Setup &#8227; TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we observe that TASLA predicts the S3 units more accurately than TASTE and text-only ablations.\nThese results indicate that TASLA is better at jointly modeling acoustic and semantic features, leading to more accurate S3 unit predictions.\nHowever, this improvement is not directly reflected in the quality metrics but instead appears in the prosody metrics.\nThis limitation may be due to the upper bound of the S3 units&#8217; performance and the size of the training data.</p>\n\n",
                "matched_terms": [
                    "results",
                    "taste",
                    "textonly",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce TASLA, a text-aligned speech tokenization framework designed to preserve fine-grained acoustic detail under an extremely low frame rate while remaining compatible with text-token alignment.\nOur experiments demonstrate that, despite the time-variant nature of the speech tokens, TASLA consistently maintains rich acoustic cues, achieving prosody metrics close to the upper-bound topline and outperforming prior text-aligned speech tokenizers on out-of-domain tasks. Quantitative analyses reveal how different encoder layers contribute to various acoustic aspects, offering interpretability that helps explain why certain layers dominate under specific speaking conditions.\nFuture work includes improving time efficiency through low-latency variants, and exploring single-stage generation approaches to overcome the performance ceiling imposed by S3 speech units.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the performance ceiling of the S3 units and the limited dataset size, TASLA has untapped potential to model speech more effectively than other text-aligned speech tokenizers, achieving higher accuracy in predicting S3 units. While this advantage does not manifest directly in quality metrics, it is reflected in improved prosody performance and a narrower gap to the S3 units&#8217; topline.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "model",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While TASLA focuses on text-aligned speech tokenization rather than speech generation, discrete speech representations could be misused for voice spoofing or cloning. All models and data used in this study are for research purposes only, and all datasets are public and released under academic licenses. We encourage responsible use and adherence to ethical standards when reproducing or extending this work.\nWe used AI assistants for grammar polishing and document formatting only. All experimental design, implementation, and data analysis were manually verified by the authors.</p>\n\n",
                "matched_terms": [
                    "experimental",
                    "tasla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study targets <em class=\"ltx_emph ltx_font_italic\">text-aligned</em> speech tokenization suitable for joint speech&#8211;text modeling at low bitrates and short sequences. Because only TASTE is directly text-aligned at word length, we include it and compare against state-of-the-art neural codecs that either operate near, or can be adjusted to, a similar low-bitrate regime (via retaining fewer RVQ layers and/or applying temporal striding) to ensure a fair comparison. We also report two ablations (S3 topline and text-only). The baselines used throughout the paper are: EnCodec, Mimi, SpeechTokenizer, DualCodec, BigCodec, and TASTE, plus the S3 topline and our text-only baseline.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "taste",
                    "topline",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech tokenizer explicitly designed for joint modeling: text tokens query a frozen speech encoder (keys from the last layer; values from selected shallow layers), producing <em class=\"ltx_emph ltx_font_italic\">text-length</em> representations that are then discretized before unit decoding and vocoding. We use TASTE as the canonical text-aligned baseline in our low-rate regime.</p>\n\n",
                "matched_terms": [
                    "taste",
                    "shallow",
                    "layer",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">S3 Topline:</span> reconstructs speech directly from ground-truth S3 units with the same unit-to-speech vocoder stack; it serves as an upper-bound reference at word-level frame rates. <span class=\"ltx_text ltx_font_italic\">Text-only baseline:</span> predicts S3 tokens from text alone (no speech tokens) to quantify the value of speech tokenization under the same decoder/vocoder. Both are reported in the main results and ablations.</p>\n\n",
                "matched_terms": [
                    "results",
                    "textonly",
                    "topline",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each baseline, we use official or widely adopted checkpoints and libraries (Transformers or the authors&#8217; packages), adhere to their documented sampling rates/hop sizes, and expose only non-retraining knobs (layer keep, time stride) when needed to reach the shared low-bitrate regime. All models are run under the same I/O wrapper described above.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train TASLA on <span class=\"ltx_text ltx_font_smallcaps\">LibriSpeech</span> (train-clean-100/360 and train-other-500) and validate/test on dev-clean/dev-other and test-clean/test-other.\nTraining uses dynamic batching with a 2000-frame budget and gradient accumulation of 2, distributed across 4 Nvidia A800 GPUs.\nThe model is initialized from a text-only baseline and employs an FSQ audio quantizer (codebook dimension = 64, codebook size = 8).\nWe optimize with Adam at a learning rate of <math alttext=\"1.6\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.6\\times 10^{-4}</annotation></semantics></math> using a warmup scheduler with 5k warmup steps and gradient clipping of 5, for up to 5 epochs.\nWe evaluate and save every 2000 steps and select the best weights by development-set accuracy.\nThe random seed is fixed to 1986 for random, NumPy, and PyTorch.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "model",
                    "tasla",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baba et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib2\" title=\"\">2024</a>)</cite> is a non-intrusive, neural MOS predictor that estimates perceived naturalness and quality directly from the waveform.\nUnlike WER, which focuses on content accuracy, UTMOS approximates human subjective ratings of audio quality, providing a complementary measure of perceptual fidelity.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use <em class=\"ltx_emph ltx_font_italic\">S3 units</em> as the discrete supervision and the reconstruction target in our pipeline.\nConcretely, an external unit recognizer from the CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14934v1#bib.bib8\" title=\"\">2024</a>)</cite> stack converts a waveform into a\none-dimensional sequence of discrete unit IDs <math alttext=\"s=[s_{1},\\dots,s_{M}]\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s=[s_{1},\\dots,s_{M}]</annotation></semantics></math>, <math alttext=\"s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>S3</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}\\in\\{1,\\dots,|\\mathcal{V}_{\\mathrm{S3}}|\\}</annotation></semantics></math>,\nat roughly word-/syllable-level time resolution. These units carry high-level linguistic content along\nwith coarse prosodic cues and are paired with a unit-to-speech vocoder that maps <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> back to a waveform.\nIn TASLA, the unit decoder consumes the text-aligned speech tokens <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> and predicts the S3 sequence\nautoregressively (cross-entropy objective), after which the CosyVoice unit-to-speech vocoder reconstructs\naudio. We also report an <em class=\"ltx_emph ltx_font_italic\">S3 topline</em> that bypasses tokenization by feeding ground-truth S3 units\nto the same vocoder, providing an informative upper bound under the same unitized reconstruction stack.</p>\n\n",
                "matched_terms": [
                    "topline",
                    "tasla"
                ]
            }
        ]
    }
}