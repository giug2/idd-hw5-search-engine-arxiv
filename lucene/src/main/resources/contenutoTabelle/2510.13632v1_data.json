{
    "S3.T1": {
        "caption": "Table 1: Univariate and partial R2R^{2} explaining the variance in speech and text performance, attributed to forgetting and misalignment.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Predictor</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Speech perf. <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Text perf. <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Forgetting</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.455</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.744</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Misalignment</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.750</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.614</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Misalignment <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> Forgetting</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.563</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-0.001</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Forgetting <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> Misalignment</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.049</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.323</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "attributed",
            "univariate",
            "âˆ£mid",
            "r2r2",
            "forgetting",
            "predictor",
            "perf",
            "speech",
            "partial",
            "misalignment",
            "text",
            "variance",
            "explaining",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "speech",
                    "text",
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "speech",
                    "text",
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify forgetting and cross-modal misalignment as the statistical distances between the outputs of a speech-adapted LLM and its text-based LLM backbone on matched text&#8211;speech inputs from a broad-domain pretraining corpus, and show that these measures are highly predictive of the text&#8211;speech understanding gap on broad-domain benchmarks.</p>\n\n",
                "matched_terms": [
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We find that training on narrow-domain speech corpora with standard objectives used in prior work worsens both forgetting and broad-domain misalignment as the amount of training data increases. In contrast, using a cross-modal knowledge distillation objective&#8212;where the text-based LLM backbone serves as the teacher&#8212;improves alignment and mitigates forgetting, even when trained on narrow-domain data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show that cross-modal distillation alone leaves residual misalignment when trained only on narrow-domain data. To further address this misalignment, we introduce an active learning algorithm that targets domain mismatches between natural speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent end-to-end methods for speech-adapted LLMs typically generate text as an intermediate representation conditioned on speech inputs, and then generate speech conditioned on that text&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite>. In this work, we focus on generating intermediate text conditioned on speech input as our primary task since it directly reflects language capability, leaving the task of generating speech for future work.\nSpecifically,\nwe aim to build a speech-adapted language model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>, parameterized by weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that defines a probability distribution over the next text token given a multimodal context. Let <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denote such a context, which may contain subsequences of text tokens <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> and/or speech representations <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>. For each position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the model predicts the distribution over the next text token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> conditioned on <math alttext=\"{\\bm{c}}_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{c}}_{\\leq i}</annotation></semantics></math>: <math alttext=\"P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transfer learning is used to alleviate this limitation, with <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> initialized from a pretrained text-only language model <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;which better approximates <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math>&#8212;in the hope of enabling general knowledge transfer even when fine-tuned on narrow speech data.\nIn practice, however, models trained this way under-perform on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>. One contributor to this under-performance is <em class=\"ltx_emph ltx_font_italic\">cross-modal misalignment</em>, i.e., inconsistent predictions across modalities, which we formalize as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another contributor to this under-performance on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> is <em class=\"ltx_emph ltx_font_italic\">forgetting</em>, which measures the loss of the original text behavior:</p>\n\n",
                "matched_terms": [
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This quantity measures how much the speech-adapted model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges from the text-based LLM <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on text inputs, indicating loss of text knowledge and reduced ability to transfer capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models in a pretraining setup, modeling general sequences without the data templates used in instruction-tuned models. This choice reflects our focus on broad-domain alignment, avoiding restriction to dialogue data and acknowledging the scarcity of speech instruction data. Our data therefore consist of multimodal sequences <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> composed of interleaved speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math> and text <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> inputs, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>)</cite>. For our training objective, we introduce a variable <math alttext=\"\\alpha\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0,1]</annotation></semantics></math> that interpolates between a standard maximum likelihood objective, and a cross-modal distillation objective, similar to that used by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the two natural English speech corpora LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib21\" title=\"\">2024</a>)</cite> (read speech) and the YODAS-EN subset of the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib18\" title=\"\">2024</a>)</cite> (conversational), both among the largest and most semantically diverse publicly available speech datasets. However, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they still lack domain coverage relative to text pretraining corpora. Since forgetting is driven by domain shift, we also synthesize a spoken version of a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-text-token subset of FineWeb-Edu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib28\" title=\"\">2024</a>)</cite>&#8212;a high-quality broad-domain text pretraining corpus&#8212;to study the impact of aligning text and speech training domains, following the approach of <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. We use the Kokoro-TTS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hexgrad/kokoro-82M\" title=\"\">https://github.com/hexgrad/kokoro-82M</a></span></span></span> with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> voice, which provides the highest quality generations<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md\" title=\"\">https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md</a></span></span></span>, to synthesize the data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To produce interleaved speech&#8211;text sequences, we segment each utterance into subsequences and interleave spans of random length at runtime: <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>&#8211;<math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> words for text segments and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> words for speech segments. For LibriHeavy and Emilia, word-level timestamps of the corresponding transcriptions are obtained using the forced aligner from <cite class=\"ltx_cite ltx_citemacro_citet\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib29\" title=\"\">2024</a>)</cite>. For synthesized speech, we use the built-in functionality of Kokoro TTS to get word-level timestamps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the standard design of speech-adapted LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib2\" title=\"\">2025</a>)</cite>, consisting of a speech encoder that extracts representations from waveforms, an adapter that maps them into the input space of the language model, and the language model itself. For these experiments we initialize <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> from the text LLM Qwen2.5-3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite> base model, and use the same text LLM as teacher <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, allowing us to measure how much original text capability is maintained when processing speech and how much is lost during speech training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "speech",
                    "text",
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
                "matched_terms": [
                    "misalignment",
                    "performance",
                    "forgetting",
                    "r2r2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results with <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math> and broad-domain (FineWeb-Edu-train) training data. In this setting, misalignment is essentially the quantity being optimized&#8212;distillation directly targets cross-modal consistency&#8212;and, since the evaluation distribution (FineWeb-Edu-test) matches the training distribution, the metric aligns with the objective. Accordingly, domain-matched distillation yields the lowest misalignment and the strongest speech-understanding performance.</p>\n\n",
                "matched_terms": [
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analyses highlight two central insights: (i) cross-modal knowledge distillation objective is more effective than maximum likelihood for mitigating misalignment and forgetting (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and (ii) better matching the domain of speech training data to that of text pretraining yields further gains when combined with cross-modal distillation. We use these insights to design a learning strategy to address\nthe text-speech understanding gap in the next section.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "misalignment",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Natural speech corpora are narrower in terms of domain coverage compared to text pretraining corpora (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Large-scale synthetic speech (i.e., same size as text pretraining corpora), while useful for improving the domain coverage, is both costly and lacking in the paralinguistic richness essential for natural spoken interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Debnath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib10\" title=\"\">2024</a>; Minixhofer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib26\" title=\"\">2025</a>)</cite>. It is therefore desirable to reduce reliance on large-scale synthetic speech data. To this end, we propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation. <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> is designed directly around our two key insights: distillation ensures robust alignment and mitigates forgetting, while active selection enables closer domain matching through a minimal, model-guided infusion of synthetic speech data rather than costly large-scale synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})</annotation></semantics></math> is the misalignment at cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, measured on a small probe set <math alttext=\"{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8834;</mo><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math> for which we pre-synthesize speech. Clusters with higher misalignment&#8212;where <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges most from <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;are therefore upweighted. The resulting importance weight is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample clusters in proportion to their importance rather than reweighting per-example losses, thereby avoiding the need to synthesize and train on the entire <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>. Given a fixed synthesis budget, we repeatedly draw a cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> according to <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>, select a text sequence <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> uniformly from <math alttext=\"{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math>, synthesize its speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m6\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>, and form an interleaved context <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m7\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math>. Each sample is added to the active dataset <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m8\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> until the budget is exhausted, after which <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> is used to continue training <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m10\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>. To prevent forgetting of Stage&#160;I training, we combine <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m11\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> with <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m12\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and minimize <math alttext=\"\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m13\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><mo>&#8746;</mo><msub><mi>&#120123;</mi><mtext>active</mtext></msub></mrow><mo>,</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we ask how well our approach preserves the text capabilities of the original text-based LLM backbone compared to the baselines. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>We use the DiVA model available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b\" title=\"\">https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b</a>\n. While <cite class=\"ltx_cite ltx_citemacro_citet\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite> report freezing a Llama-3-8B backbone, the released version on HuggingFace appears to be based on Llama-3.1-8B. Moreover, we found the released weights to differ from the Llama-3.1-8B checkpoint, which explains the differences in text performance reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</span></span></span> shows that, unlike other speech-adapted models, which exhibit substantial forgetting, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> maintains the closest performance to the original text LLM. This result highlights the effectiveness of the distillation objective in constraining the model to remain faithful to its teacher while learning to achieve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "speech",
                    "text",
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language model processes multimodal sequences of text and speech representations <math alttext=\"{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119919;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>k</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}</annotation></semantics></math> and outputs a probability distribution over a vocabulary <math alttext=\"{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120141;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbb{V}}_{t}</annotation></semantics></math> of text tokens. Subsequences of <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> may correspond either to adapter-output speech sequences <math alttext=\"{\\bm{Z}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119937;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{Z}}^{\\prime}</annotation></semantics></math> or to sequences of text embeddings <math alttext=\"{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119916;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>w</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}</annotation></semantics></math> obtained by applying an embedding function to text tokens <math alttext=\"{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119960;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><msub><mi>l</mi><mi>w</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})</annotation></semantics></math>, with <math alttext=\"w_{i}\\in{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi>&#120141;</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}\\in{\\mathbb{V}}_{t}</annotation></semantics></math>. <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> is processed by a stack of transformer decoder layers, and the output logits at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> are linearly predicted from the corresponding hidden representation, yielding a <math alttext=\"|{\\mathbb{V}}_{t}|\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#120141;</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|{\\mathbb{V}}_{t}|</annotation></semantics></math>-dimensional logit vector that is Softmax-normalized into the probability distribution <math alttext=\"P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119919;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})</annotation></semantics></math>. We initialize the language model from pretrained text language models from the Qwen2.5 family of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fit ordinary least-squares models with (i) average speech performance (%) as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">and (ii) average text performance (%) as</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "r2r2",
                    "predictor",
                    "speech",
                    "partial",
                    "text",
                    "misalignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Scaling laws of misalignment with training tokens (DD) for runs withÂ Î±>0\\alpha>0:\nM=E+Bâ€‹Dâˆ’Î²M=E+B\\,D^{-\\beta}. Reported are the LOOCV R2R^{2} and DD needed to be within 5% of EE.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m14\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"R^{2}_{\\text{LOOCV}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m17\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>LOOCV</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{LOOCV}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Tokens@5%<math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m18\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" colspan=\"6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_italic\">LibriHeavy + Emilia</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.25</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">45.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">4.94B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.50</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3.2e10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.75</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2.07B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.75</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.2e8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.85</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">6.00B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.00</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3806</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.91</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">47.58B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_italic\">FineWeb-Edu</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.00</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.96</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">268.4B</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "emilia",
            "scaling",
            "Î²beta",
            "32e10",
            "600b",
            "mebâ€‹dâˆ’Î²mebdbeta",
            "rloocv2r2textloocv",
            "Î±0alpha0",
            "Î±alpha",
            "reported",
            "runs",
            "training",
            "207b",
            "needed",
            "12e8",
            "494b",
            "4758b",
            "tokens",
            "libriheavy",
            "within",
            "tokens5ee",
            "2684b",
            "finewebedu",
            "r2r2",
            "loocv",
            "laws",
            "misalignment"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
            "<p class=\"ltx_p\">Our analyses highlight two central insights: (i) cross-modal knowledge distillation objective is more effective than maximum likelihood for mitigating misalignment and forgetting (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and (ii) better matching the domain of speech training data to that of text pretraining yields further gains when combined with cross-modal distillation. We use these insights to design a learning strategy to address\nthe text-speech understanding gap in the next section.</p>\n\n",
            "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "needed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We find that training on narrow-domain speech corpora with standard objectives used in prior work worsens both forgetting and broad-domain misalignment as the amount of training data increases. In contrast, using a cross-modal knowledge distillation objective&#8212;where the text-based LLM backbone serves as the teacher&#8212;improves alignment and mitigates forgetting, even when trained on narrow-domain data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the two natural English speech corpora LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib21\" title=\"\">2024</a>)</cite> (read speech) and the YODAS-EN subset of the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib18\" title=\"\">2024</a>)</cite> (conversational), both among the largest and most semantically diverse publicly available speech datasets. However, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they still lack domain coverage relative to text pretraining corpora. Since forgetting is driven by domain shift, we also synthesize a spoken version of a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-text-token subset of FineWeb-Edu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib28\" title=\"\">2024</a>)</cite>&#8212;a high-quality broad-domain text pretraining corpus&#8212;to study the impact of aligning text and speech training domains, following the approach of <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. We use the Kokoro-TTS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hexgrad/kokoro-82M\" title=\"\">https://github.com/hexgrad/kokoro-82M</a></span></span></span> with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> voice, which provides the highest quality generations<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md\" title=\"\">https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md</a></span></span></span>, to synthesize the data.</p>\n\n",
                "matched_terms": [
                    "emilia",
                    "training",
                    "finewebedu",
                    "libriheavy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To produce interleaved speech&#8211;text sequences, we segment each utterance into subsequences and interleave spans of random length at runtime: <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>&#8211;<math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> words for text segments and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> words for speech segments. For LibriHeavy and Emilia, word-level timestamps of the corresponding transcriptions are obtained using the forced aligner from <cite class=\"ltx_cite ltx_citemacro_citet\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib29\" title=\"\">2024</a>)</cite>. For synthesized speech, we use the built-in functionality of Kokoro TTS to get word-level timestamps.</p>\n\n",
                "matched_terms": [
                    "emilia",
                    "libriheavy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "within",
                    "finewebedu",
                    "training",
                    "tokens",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "emilia",
                    "libriheavy",
                    "Î±alpha",
                    "r2r2",
                    "training",
                    "loocv",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "emilia",
                    "libriheavy",
                    "scaling",
                    "training",
                    "misalignment",
                    "Î±0alpha0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "scaling",
                    "Î±0alpha0",
                    "finewebedu",
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results with <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math> and broad-domain (FineWeb-Edu-train) training data. In this setting, misalignment is essentially the quantity being optimized&#8212;distillation directly targets cross-modal consistency&#8212;and, since the evaluation distribution (FineWeb-Edu-test) matches the training distribution, the metric aligns with the objective. Accordingly, domain-matched distillation yields the lowest misalignment and the strongest speech-understanding performance.</p>\n\n",
                "matched_terms": [
                    "training",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply our method to the Qwen2.5 3B and 7B base LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>, yielding the <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B and <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B models. We follow the experimental setup of Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for architecture, data, and evaluation tasks. We use Emilia and LibriHeavy (<math alttext=\"141,612\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>141</mn><mo>,</mo><mn>612</mn></mrow><annotation encoding=\"application/x-tex\">141,612</annotation></semantics></math> hours) as our <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, and use a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-token FineWeb-Edu subset as our <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>.\nFor Stage&#160;II, we train a clustering model on <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> using balanced <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means with <math alttext=\"K=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">K=128</annotation></semantics></math> over <span class=\"ltx_text ltx_font_typewriter\">BAAI/bge-large-en-v1.5</span> embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span>, with a synthesis budget of <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> of <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We train SALAD models for <math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mn>24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math>B tokens during Stage&#160;I and additional <math alttext=\"1.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mn>1.9</mn><annotation encoding=\"application/x-tex\">1.9</annotation></semantics></math>B tokens during Stage&#160;II. Further training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS3\" title=\"A.3 Training Details: Closing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>, and additional ablations are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "emilia",
                    "libriheavy",
                    "finewebedu",
                    "reported",
                    "training",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models are trained with the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib24\" title=\"\">2019</a>)</cite> with a weight decay of <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. We adopt a warmup-stable-decay learning-rate schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(H&#228;gele et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib16\" title=\"\">2024</a>)</cite>, consisting of a linear warmup of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> steps followed by a linear decay to zero over the final <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math> of training. The peak learning rate is tuned separately for each model size, with distinct values for the language-model backbone and the adapter.\nWe use a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM. All models are trained with a batch size of approximately 1M tokens and a context window of 2048 tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "runs",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "needed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "r2r2",
                    "training",
                    "loocv",
                    "misalignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "tokens",
                    "training",
                    "Î±0alpha0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "emilia",
                    "libriheavy"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: SALAD outperforms most baselines and is competitive with the strongest speech-adapted LLMs and cascaded pipelines. â€œAcc.â€ denotes the accuracy of the speech-adapted LLM given speech input; â€œGapâ€ denotes the difference between the accuracy of the text-based LLM given text input and the â€œAcc.â€ column. Gap cells are color-coded from best (green) to worst (red) for each task, where lower is better.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" rowspan=\"2\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">StoryCloze</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">MMSU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">OBQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">HellaSwag</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">ARC-C</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">PIQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Avg.</td>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Random</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">25.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">25.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">25.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">25.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">33.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"15\"><span class=\"ltx_text ltx_font_italic\">Cascaded <span class=\"ltx_text ltx_font_bold\">toplines</span>: ASR (Whisper-v3-Large) + LLM</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ASR + Qwen2.5-3B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">82.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed0.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">58.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed3.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">76.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed4.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">69.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!95.0!GapRed1.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">77.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!92.0!GapRed4.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">78.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!99.0!GapRed0.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">73.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!99.0!GapRed2.5</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ASR + Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">84.2</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!97.0!GapRed0.8</td>\n<td class=\"ltx_td ltx_align_left\">67.1</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed3.7</td>\n<td class=\"ltx_td ltx_align_left\">84.0</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed5.0</td>\n<td class=\"ltx_td ltx_align_left\">74.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!94.0!GapRed2.0</td>\n<td class=\"ltx_td ltx_align_left\">86.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!98.0!GapRed1.9</td>\n<td class=\"ltx_td ltx_align_left\">79.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed0.0</td>\n<td class=\"ltx_td ltx_align_left\">79.4</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed2.2</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"15\"><span class=\"ltx_text ltx_font_italic\">End-to-end systems</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2-Audio-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">71.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!57.0!GapRed9.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">29.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!37.0!GapRed18.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">39.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!14.0!GapRed37.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">64.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!65.0!GapRed7.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">43.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!22.0!GapRed28.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">73.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!53.0!GapRed5.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">53.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!35.0!GapRed17.8</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DiVA-Llama3.1-8B</td>\n<td class=\"ltx_td ltx_align_left\">68.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!4.0!GapRed19.7</td>\n<td class=\"ltx_td ltx_align_left\">36.1</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!8.0!GapRed25.8</td>\n<td class=\"ltx_td ltx_align_left\">40.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed42.4</td>\n<td class=\"ltx_td ltx_align_left\">54.2</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed21.3</td>\n<td class=\"ltx_td ltx_align_left\">45.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed36.0</td>\n<td class=\"ltx_td ltx_align_left\">70.0</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed11.4</td>\n<td class=\"ltx_td ltx_align_left\">52.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed26.1</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GLM-4-Voice-9B</td>\n<td class=\"ltx_td ltx_align_left\">78.2</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed20.6</td>\n<td class=\"ltx_td ltx_align_left\">38.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed27.6</td>\n<td class=\"ltx_td ltx_align_left\">57.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!32.0!GapRed30.1</td>\n<td class=\"ltx_td ltx_align_left\">68.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!46.0!GapRed11.9</td>\n<td class=\"ltx_td ltx_align_left\">64.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!21.0!GapRed28.7</td>\n<td class=\"ltx_td ltx_align_left\">72.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!83.0!GapRed1.9</td>\n<td class=\"ltx_td ltx_align_left\">63.4</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!25.0!GapRed20.1</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-Omni-7B</td>\n<td class=\"ltx_td ltx_align_left\">80.1</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!77.0!GapRed4.9</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">61.0</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-9.8</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">85.5</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">3.5</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">68.4</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!63.0!GapRed8.3</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">87.1</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">1.3</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">78.0</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!83.0!GapRed1.9</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">76.7</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!88.0!GapRed5.0</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8195;Stage&#160;I</td>\n<td class=\"ltx_td ltx_align_left\">75.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!65.0!GapRed7.4</td>\n<td class=\"ltx_td ltx_align_left\">47.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!54.0!GapRed14.6</td>\n<td class=\"ltx_td ltx_align_left\">65.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!67.0!GapRed16.3</td>\n<td class=\"ltx_td ltx_align_left\">68.8</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!93.0!GapRed2.2</td>\n<td class=\"ltx_td ltx_align_left\">75.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!86.0!GapRed6.2</td>\n<td class=\"ltx_td ltx_align_left\">78.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!97.0!GapRed0.3</td>\n<td class=\"ltx_td ltx_align_left\">68.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!76.0!GapRed8.0</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8195;Stage&#160;II</td>\n<td class=\"ltx_td ltx_align_left\">75.8</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!66.0!GapRed7.1</td>\n<td class=\"ltx_td ltx_align_left\">52.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!76.0!GapRed<span class=\"ltx_text ltx_font_bold\">9.4</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">76.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed5.1</td>\n<td class=\"ltx_td ltx_align_left\">68.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!93.0!GapRed<span class=\"ltx_text ltx_font_bold\">2.3</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">79.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!98.0!GapRed1.9</td>\n<td class=\"ltx_td ltx_align_left\">78.1</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed0.5</td>\n<td class=\"ltx_td ltx_align_left\">72.0</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!90.0!GapRed<span class=\"ltx_text ltx_font_bold\">4.6</span>\n</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_nopad_r\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8195;Stage&#160;I</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">81.5</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!84.0!GapRed<span class=\"ltx_text ltx_font_bold\">3.5</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">55.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!51.0!GapRed15.5</td>\n<td class=\"ltx_td ltx_align_left\">69.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!59.0!GapRed19.3</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">74.2</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!92.0!GapRed2.5</td>\n<td class=\"ltx_td ltx_align_left\">82.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!86.0!GapRed6.1</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">80.3</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed<span class=\"ltx_text ltx_font_bold\">0.4</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">73.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!76.0!GapRed7.9</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#8195;Stage&#160;II</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">81.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!84.0!GapRed<span class=\"ltx_text ltx_font_bold\">3.5</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">57.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!60.0!GapRed13.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">75.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!73.0!GapRed13.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">74.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!91.0!GapRed2.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">84.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!91.0!GapRed4.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">80.3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed<span class=\"ltx_text ltx_font_bold\">0.4</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">75.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!83.0!GapRed6.2</td>\n<td class=\"ltx_td ltx_border_bb\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "each",
            "cellcolorgapgreen660gapred71",
            "outperforms",
            "cellcolorgapgreen990gapred01",
            "cellcolorgapgreen1000gapred38",
            "most",
            "llm",
            "cellcolorgapgreen830gapred19",
            "whisperv3large",
            "from",
            "accuracy",
            "cellcolorgapgreen760gapred79",
            "cellcolorgapgreen570gapred90",
            "cellcolorgapgreen00gapred276",
            "lower",
            "where",
            "given",
            "cellcolorgapgreen40gapred197",
            "acc",
            "cellcolorgapgreen250gapred201",
            "cellcolorgapgreen1000gapred13",
            "salad",
            "endtoend",
            "cellcolorgapgreen80gapred258",
            "cellcolorgapgreen770gapred49",
            "cascaded",
            "glm4voice9b",
            "column",
            "competitive",
            "cellcolorgapgreen840gapred35",
            "cellcolorgapgreen960gapred50",
            "cellcolorgapgreen970gapred03",
            "cellcolorgapgreen860gapred62",
            "denotes",
            "strongest",
            "cellcolorgapgreen970gapred08",
            "cellcolorgapgreen860gapred61",
            "cellcolorgapgreen930gapred23",
            "cellcolorgapgreen350gapred178",
            "difference",
            "cellcolorgapgreen670gapred163",
            "cellcolorgapgreen1000gapred37",
            "cellcolorgapgreen990gapred25",
            "cellcolorgapgreen1000gapred35",
            "cellcolorgapgreen630gapred83",
            "cells",
            "input",
            "text",
            "cellcolorgapgreen930gapred22",
            "avg",
            "cellcolorgapgreen960gapred49",
            "qwen2audio7b",
            "cellcolorgapgreen960gapred51",
            "cellcolorgapgreen1000gapred22",
            "cellcolorgapgreen530gapred54",
            "cellcolorgapgreen920gapred42",
            "qwen253b",
            "llms",
            "cellcolorgapgreen00gapred424",
            "â€œgapâ€",
            "cellcolorgapgreen00gapred114",
            "qwen257b",
            "cellcolorgapgreen960gapred05",
            "cellcolorgapgreen730gapred139",
            "cellcolorgapgreen1000gapred02",
            "better",
            "cellcolorgapgreen220gapred285",
            "cellcolorgapgreen320gapred301",
            "textbased",
            "cellcolorgapgreen910gapred27",
            "baselines",
            "obqa",
            "speechadapted",
            "cellcolorgapgreen140gapred371",
            "hellaswag",
            "stage",
            "piqa",
            "worst",
            "cellcolorgapgreen830gapred62",
            "colorcoded",
            "cellcolorgapgreen760gapred80",
            "between",
            "mmsu",
            "toplines",
            "cellcolorgapgreen760gapred94",
            "cellcolorgapgreen900gapred46",
            "â€œaccâ€",
            "cellcolorgapgreen370gapred187",
            "asr",
            "speech",
            "cellcolorgapgreen1000gapred00",
            "cellcolorgapgreen960gapred04",
            "red",
            "task",
            "cellcolorgapgreen210gapred287",
            "cellcolorgapgreen00gapred213",
            "cellcolorgapgreen00gapred261",
            "cellcolorgapgreen540gapred146",
            "qwen25omni7b",
            "arcc",
            "cellcolorgapgreen940gapred20",
            "divallama318b",
            "salad7b",
            "cellcolorgapgreen600gapred133",
            "cellcolorgapgreen460gapred119",
            "cellcolorgapgreen510gapred155",
            "cellcolorgapgreen910gapred44",
            "green",
            "cellcolorgapgreen00gapred206",
            "cellcolorgapgreen650gapred74",
            "systems",
            "cellcolorgapgreen1000gapred98",
            "cellcolorgapgreen980gapred19",
            "salad3b",
            "cellcolorgapgreen00gapred360",
            "cellcolorgapgreen590gapred193",
            "cellcolorgapgreen880gapred50",
            "pipelines",
            "cellcolorgapgreen920gapred25",
            "storycloze",
            "random",
            "best",
            "cellcolorgapgreen950gapred19",
            "cellcolorgapgreen650gapred79",
            "gap"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
            "<p class=\"ltx_p\">While Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> show meaningful Stage&#160;II gains&#8212;and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> corroborates them&#8212;the role of domain-gap targeting remains unclear. Is it necessary to explicitly target domain gaps, or would tuning on a small amount of target, general-domain data suffice to improve alignment and performance? How does this trade-off evolve as we increase the selected data?</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "cascaded",
                    "speechadapted",
                    "textbased",
                    "competitive",
                    "llm",
                    "from",
                    "salad",
                    "gap",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "cascaded",
                    "speechadapted",
                    "pipelines",
                    "textbased",
                    "systems",
                    "task",
                    "llm",
                    "gap",
                    "endtoend",
                    "asr",
                    "speech",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "textbased",
                    "given",
                    "llm",
                    "from",
                    "gap",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "llms",
                    "speechadapted",
                    "textbased",
                    "better",
                    "llm",
                    "gap",
                    "salad",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify forgetting and cross-modal misalignment as the statistical distances between the outputs of a speech-adapted LLM and its text-based LLM backbone on matched text&#8211;speech inputs from a broad-domain pretraining corpus, and show that these measures are highly predictive of the text&#8211;speech understanding gap on broad-domain benchmarks.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "llm",
                    "from",
                    "gap",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We find that training on narrow-domain speech corpora with standard objectives used in prior work worsens both forgetting and broad-domain misalignment as the amount of training data increases. In contrast, using a cross-modal knowledge distillation objective&#8212;where the text-based LLM backbone serves as the teacher&#8212;improves alignment and mitigates forgetting, even when trained on narrow-domain data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llm",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show that cross-modal distillation alone leaves residual misalignment when trained only on narrow-domain data. To further address this misalignment, we introduce an active learning algorithm that targets domain mismatches between natural speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a method that builds on insights from our analyses, apply it to 3B and 7B LLMs, and benchmark them against recent speech-adapted LLMs in the 3B&#8211;9B range on spoken versions of six broad-domain knowledge and reasoning benchmarks. Our models outperform most baselines in spoken language understanding and perform competitively with the strongest, while training on over an order of magnitude less data.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "llms",
                    "most",
                    "baselines",
                    "from",
                    "strongest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent end-to-end methods for speech-adapted LLMs typically generate text as an intermediate representation conditioned on speech inputs, and then generate speech conditioned on that text&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite>. In this work, we focus on generating intermediate text conditioned on speech input as our primary task since it directly reflects language capability, leaving the task of generating speech for future work.\nSpecifically,\nwe aim to build a speech-adapted language model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>, parameterized by weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that defines a probability distribution over the next text token given a multimodal context. Let <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denote such a context, which may contain subsequences of text tokens <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> and/or speech representations <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>. For each position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the model predicts the distribution over the next text token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> conditioned on <math alttext=\"{\\bm{c}}_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{c}}_{\\leq i}</annotation></semantics></math>: <math alttext=\"P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "llms",
                    "task",
                    "given",
                    "each",
                    "endtoend",
                    "speech",
                    "text",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We seek to train <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> so that its predictions match the distribution of <math alttext=\"({\\bm{w}},{\\bm{c}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>&#119960;</mi><mo>,</mo><mi>&#119940;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{w}},{\\bm{c}})</annotation></semantics></math> pairs drawn from the natural language distribution <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math>. Most prior works approximate <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math> by minimizing the negative log-likelihood on a speech dataset <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m5\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib38\" title=\"\">2023</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "most",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transfer learning is used to alleviate this limitation, with <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> initialized from a pretrained text-only language model <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;which better approximates <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math>&#8212;in the hope of enabling general knowledge transfer even when fine-tuned on narrow speech data.\nIn practice, however, models trained this way under-perform on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>. One contributor to this under-performance is <em class=\"ltx_emph ltx_font_italic\">cross-modal misalignment</em>, i.e., inconsistent predictions across modalities, which we formalize as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"D_{\\mathrm{KL}}(\\cdot|\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{\\mathrm{KL}}(\\cdot|\\cdot)</annotation></semantics></math> denotes the Kullback&#8211;Leibler divergence between two distributions, <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> denotes a text sequence, <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denotes a semantically equivalent multimodal context sequence, and <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> denotes the next text token. This quantity measures how differently the model predicts the next token when conditioned on text versus equivalent multimodal context.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "text",
                    "where",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This quantity measures how much the speech-adapted model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges from the text-based LLM <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on text inputs, indicating loss of text knowledge and reduced ability to transfer capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "llm",
                    "from",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "llms",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models in a pretraining setup, modeling general sequences without the data templates used in instruction-tuned models. This choice reflects our focus on broad-domain alignment, avoiding restriction to dialogue data and acknowledging the scarcity of speech instruction data. Our data therefore consist of multimodal sequences <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> composed of interleaved speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math> and text <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> inputs, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>)</cite>. For our training objective, we introduce a variable <math alttext=\"\\alpha\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0,1]</annotation></semantics></math> that interpolates between a standard maximum likelihood objective, and a cross-modal distillation objective, similar to that used by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the two natural English speech corpora LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib21\" title=\"\">2024</a>)</cite> (read speech) and the YODAS-EN subset of the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib18\" title=\"\">2024</a>)</cite> (conversational), both among the largest and most semantically diverse publicly available speech datasets. However, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they still lack domain coverage relative to text pretraining corpora. Since forgetting is driven by domain shift, we also synthesize a spoken version of a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-text-token subset of FineWeb-Edu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib28\" title=\"\">2024</a>)</cite>&#8212;a high-quality broad-domain text pretraining corpus&#8212;to study the impact of aligning text and speech training domains, following the approach of <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. We use the Kokoro-TTS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hexgrad/kokoro-82M\" title=\"\">https://github.com/hexgrad/kokoro-82M</a></span></span></span> with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> voice, which provides the highest quality generations<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md\" title=\"\">https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md</a></span></span></span>, to synthesize the data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "most",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To produce interleaved speech&#8211;text sequences, we segment each utterance into subsequences and interleave spans of random length at runtime: <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>&#8211;<math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> words for text segments and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> words for speech segments. For LibriHeavy and Emilia, word-level timestamps of the corresponding transcriptions are obtained using the forced aligner from <cite class=\"ltx_cite ltx_citemacro_citet\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib29\" title=\"\">2024</a>)</cite>. For synthesized speech, we use the built-in functionality of Kokoro TTS to get word-level timestamps.</p>\n\n",
                "matched_terms": [
                    "random",
                    "each",
                    "from",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate on broad-domain benchmarks of general knowledge, reasoning, and language understanding commonly used for LLMs, considering both their text and spoken versions: StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA (OBQA) from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite>. We adopt a few-shot prompting approach for evaluation across all tasks, with accuracy as the metric. For further details on the benchmarks, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS4\" title=\"A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "hellaswag",
                    "piqa",
                    "storycloze",
                    "from",
                    "mmsu",
                    "accuracy",
                    "obqa",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the standard design of speech-adapted LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib2\" title=\"\">2025</a>)</cite>, consisting of a speech encoder that extracts representations from waveforms, an adapter that maps them into the input space of the language model, and the language model itself. For these experiments we initialize <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> from the text LLM Qwen2.5-3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite> base model, and use the same text LLM as teacher <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, allowing us to measure how much original text capability is maintained when processing speech and how much is lost during speech training.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "llms",
                    "llm",
                    "speech",
                    "from",
                    "input",
                    "qwen253b",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While prior work has paid great attention to speech encoder and adapter design&#8212;typically to promote cross-modal alignment by making speech representations more text-like&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite>&#8212;we adopt a simple architecture: the lightweight Mimi speech tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite> as encoder and a <math alttext=\"122\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>122</mn><annotation encoding=\"application/x-tex\">122</annotation></semantics></math>M-parameter stack of transformer decoder layers as adapter. We make this choice because current representation alignment methods rely on large non-causal encoders and complex modules unsuited for low-latency streaming, which is essential for downstream conversational speech-adapted LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Accordingly, we use causal, streaming-friendly models with low-level, non&#8211;text-like representations as a &#8220;worst-case&#8221; input alignment scenario, expecting our findings to generalize to more aligned text-speech representations while being directly applicable to low-latency architectures. The encoder remains frozen during training, while the adapter and language model are optimized. For further details on the model architecture, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS1\" title=\"A.1 Model description &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "llms",
                    "speechadapted",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each",
                    "given"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "given",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
                "matched_terms": [
                    "better",
                    "denotes",
                    "most",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "difference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analyses highlight two central insights: (i) cross-modal knowledge distillation objective is more effective than maximum likelihood for mitigating misalignment and forgetting (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and (ii) better matching the domain of speech training data to that of text pretraining yields further gains when combined with cross-modal distillation. We use these insights to design a learning strategy to address\nthe text-speech understanding gap in the next section.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gap",
                    "text",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Natural speech corpora are narrower in terms of domain coverage compared to text pretraining corpora (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Large-scale synthetic speech (i.e., same size as text pretraining corpora), while useful for improving the domain coverage, is both costly and lacking in the paralinguistic richness essential for natural spoken interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Debnath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib10\" title=\"\">2024</a>; Minixhofer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib26\" title=\"\">2025</a>)</cite>. It is therefore desirable to reduce reliance on large-scale synthetic speech data. To this end, we propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation. <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> is designed directly around our two key insights: distillation ensures robust alignment and mitigates forgetting, while active selection enables closer domain matching through a minimal, model-guided infusion of synthetic speech data rather than costly large-scale synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "salad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stage",
                    "salad",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we aim to develop a sampling strategy for choosing which text samples to synthesize.\nWe draw inspiration from CRISP <cite class=\"ltx_cite ltx_citemacro_citep\">(Grangier et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib15\" title=\"\">2025</a>)</cite>, adopting a clustered importance-sampling strategy that derives a target-domain dataset from a broad-domain corpus by reweighting clusters. Concretely, let <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> be a large broad-domain text corpus. We partition it into <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> clusters <math alttext=\"{K(c)}_{c=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">{K(c)}_{c=1}^{K}</annotation></semantics></math> using sentence embeddings and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, and define</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute per-cluster importance weights as <math alttext=\"w(c)=\\tfrac{P_{\\text{target}}(c)}{P_{\\text{web}}(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mtext>target</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>P</mi><mtext>web</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">w(c)=\\tfrac{P_{\\text{target}}(c)}{P_{\\text{web}}(c)}</annotation></semantics></math>. In CRISP, <math alttext=\"P_{\\text{target}}(c)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>target</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{target}}(c)</annotation></semantics></math> comes from the distribution of a small target-domain dataset across the clusters. In our case, no such dataset exists, so we let the model itself define <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>. Specifically, we treat the divergence between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> within each cluster as a proxy for how much that cluster belongs to the &#8220;missing&#8221; domain. We define</p>\n\n",
                "matched_terms": [
                    "each",
                    "from",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})</annotation></semantics></math> is the misalignment at cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, measured on a small probe set <math alttext=\"{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8834;</mo><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math> for which we pre-synthesize speech. Clusters with higher misalignment&#8212;where <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges most from <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;are therefore upweighted. The resulting importance weight is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "most",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample clusters in proportion to their importance rather than reweighting per-example losses, thereby avoiding the need to synthesize and train on the entire <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>. Given a fixed synthesis budget, we repeatedly draw a cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> according to <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>, select a text sequence <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> uniformly from <math alttext=\"{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math>, synthesize its speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m6\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>, and form an interleaved context <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m7\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math>. Each sample is added to the active dataset <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m8\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> until the budget is exhausted, after which <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> is used to continue training <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m10\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>. To prevent forgetting of Stage&#160;I training, we combine <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m11\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> with <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m12\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and minimize <math alttext=\"\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m13\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><mo>&#8746;</mo><msub><mi>&#120123;</mi><mtext>active</mtext></msub></mrow><mo>,</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "given",
                    "each",
                    "from",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply our method to the Qwen2.5 3B and 7B base LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>, yielding the <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B and <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B models. We follow the experimental setup of Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for architecture, data, and evaluation tasks. We use Emilia and LibriHeavy (<math alttext=\"141,612\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>141</mn><mo>,</mo><mn>612</mn></mrow><annotation encoding=\"application/x-tex\">141,612</annotation></semantics></math> hours) as our <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, and use a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-token FineWeb-Edu subset as our <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>.\nFor Stage&#160;II, we train a clustering model on <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> using balanced <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means with <math alttext=\"K=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">K=128</annotation></semantics></math> over <span class=\"ltx_text ltx_font_typewriter\">BAAI/bge-large-en-v1.5</span> embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span>, with a synthesis budget of <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> of <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We train SALAD models for <math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mn>24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math>B tokens during Stage&#160;I and additional <math alttext=\"1.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mn>1.9</mn><annotation encoding=\"application/x-tex\">1.9</annotation></semantics></math>B tokens during Stage&#160;II. Further training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS3\" title=\"A.3 Training Details: Closing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>, and additional ablations are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "salad7b",
                    "salad3b",
                    "stage",
                    "salad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "cascaded",
                    "llms",
                    "textbased",
                    "difference",
                    "llm",
                    "given",
                    "speech",
                    "each",
                    "from",
                    "asr",
                    "between",
                    "input",
                    "accuracy",
                    "gap",
                    "salad",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stage",
                    "arcc",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we ask how well our approach preserves the text capabilities of the original text-based LLM backbone compared to the baselines. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>We use the DiVA model available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b\" title=\"\">https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b</a>\n. While <cite class=\"ltx_cite ltx_citemacro_citet\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite> report freezing a Llama-3-8B backbone, the released version on HuggingFace appears to be based on Llama-3.1-8B. Moreover, we found the released weights to differ from the Llama-3.1-8B checkpoint, which explains the differences in text performance reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</span></span></span> shows that, unlike other speech-adapted models, which exhibit substantial forgetting, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> maintains the closest performance to the original text LLM. This result highlights the effectiveness of the distillation objective in constraining the model to remain faithful to its teacher while learning to achieve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "llm",
                    "baselines",
                    "from",
                    "salad",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "llms",
                    "textbased",
                    "competitive",
                    "baselines",
                    "gap",
                    "salad",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder transforms <math alttext=\"l_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">l_{a}</annotation></semantics></math> speech audio frames <math alttext=\"{\\bm{a}}\\in{\\mathbb{R}}^{l_{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119938;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>l</mi><mi>a</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{a}}\\in{\\mathbb{R}}^{l_{a}}</annotation></semantics></math> into a sequence of <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>-dimensional latent speech representations <math alttext=\"{\\bm{Z}}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{Z}}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}</annotation></semantics></math> of length <math alttext=\"l_{s}&lt;l_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>l</mi><mi>s</mi></msub><mo>&lt;</mo><msub><mi>l</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">l_{s}&lt;l_{a}</annotation></semantics></math>. We use the Mimi speech tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>, a causal lightweight speech encoder. Mimi produces multi-codebook representations <math alttext=\"{\\bm{\\mathsfit{Z}}}\\in{\\mathbb{R}}^{l_{s}\\times q\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>q</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\mathsfit{Z}}}\\in{\\mathbb{R}}^{l_{s}\\times q\\times d_{s}}</annotation></semantics></math>, where <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> is the number of codebooks. We add the representations across codebooks, resulting in <math alttext=\"{\\bm{Z}}=\\sum_{i=1}^{q}{\\bm{\\mathsfit{Z}}}_{:,i,:}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msub><mi>&#119937;</mi><mrow><mo rspace=\"0em\">:</mo><mo>,</mo><mi>i</mi><mo>,</mo><mo>:</mo></mrow></msub></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{Z}}=\\sum_{i=1}^{q}{\\bm{\\mathsfit{Z}}}_{:,i,:}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}</annotation></semantics></math>. The speech encoder remains frozen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language model processes multimodal sequences of text and speech representations <math alttext=\"{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119919;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>k</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}</annotation></semantics></math> and outputs a probability distribution over a vocabulary <math alttext=\"{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120141;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbb{V}}_{t}</annotation></semantics></math> of text tokens. Subsequences of <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> may correspond either to adapter-output speech sequences <math alttext=\"{\\bm{Z}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119937;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{Z}}^{\\prime}</annotation></semantics></math> or to sequences of text embeddings <math alttext=\"{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119916;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>w</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}</annotation></semantics></math> obtained by applying an embedding function to text tokens <math alttext=\"{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119960;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><msub><mi>l</mi><mi>w</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})</annotation></semantics></math>, with <math alttext=\"w_{i}\\in{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi>&#120141;</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}\\in{\\mathbb{V}}_{t}</annotation></semantics></math>. <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> is processed by a stack of transformer decoder layers, and the output logits at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> are linearly predicted from the corresponding hidden representation, yielding a <math alttext=\"|{\\mathbb{V}}_{t}|\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#120141;</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|{\\mathbb{V}}_{t}|</annotation></semantics></math>-dimensional logit vector that is Softmax-normalized into the probability distribution <math alttext=\"P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119919;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})</annotation></semantics></math>. We initialize the language model from pretrained text language models from the Qwen2.5 family of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llms",
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models are trained with the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib24\" title=\"\">2019</a>)</cite> with a weight decay of <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. We adopt a warmup-stable-decay learning-rate schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(H&#228;gele et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib16\" title=\"\">2024</a>)</cite>, consisting of a linear warmup of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> steps followed by a linear decay to zero over the final <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math> of training. The peak learning rate is tuned separately for each model size, with distinct values for the language-model backbone and the adapter.\nWe use a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM. All models are trained with a batch size of approximately 1M tokens and a context window of 2048 tokens.</p>\n\n",
                "matched_terms": [
                    "each",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models follow the hyperparameters and setup in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>, except that SALAD-3B uses a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM; SALAD-7B uses <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-4}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-6}</annotation></semantics></math> for the LLM.</p>\n\n",
                "matched_terms": [
                    "salad7b",
                    "salad3b",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Stage&#160;I, each batch is sampled with probability <math alttext=\"2/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">2/3</annotation></semantics></math> from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"1/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1/3</annotation></semantics></math> from the SmolLM corpus, following the common practice of mixing in pretraining data to mitigate forgetting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. In Stage&#160;II, batches are drawn with equal probability from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math>, and the SmolLM corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "salad",
                    "stage",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "task",
                    "hellaswag",
                    "given",
                    "piqa",
                    "storycloze",
                    "each",
                    "from",
                    "mmsu",
                    "accuracy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n",
                "matched_terms": [
                    "random",
                    "task",
                    "mmsu",
                    "best",
                    "each",
                    "from",
                    "between",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "task",
                    "hellaswag",
                    "most",
                    "piqa",
                    "baselines",
                    "storycloze",
                    "each",
                    "mmsu",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generate the domain annotations in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> by annotating embedding clusters with an LLM (Claude 3.7 Sonnet). Below, we outline the annotation and validation procedures. For a clustering model with <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math> clusters, the judge validated the annotations with an accuracy of <math alttext=\"56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">56\\%</annotation></semantics></math> (random chance is <math alttext=\"1.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.6\\%</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "random",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each cluster, we sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> positives (closest to the centroid) and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> negatives (from the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> nearest neighbor clusters). An LLM is prompted with these examples to propose a short title, a descriptive label, and inclusion/exclusion criteria.</p>\n\n",
                "matched_terms": [
                    "each",
                    "llm",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a catalog of all cluster labels and ask the LLM to classify holdout texts into exactly one cluster. This produces multiclass accuracy estimates per cluster and overall. The pipeline supports resumability and incremental judging, enabling efficient large-scale evaluation.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "salad",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important question is whether the improvements from the active learning stage stem from its design, or merely from the extra training steps added after Stage&#160;I.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that Stage&#160;II yields consistent gains across tasks over Stage&#160;I for a given budget, with the exception of StoryCloze, where performance deteriorates slightly.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "given",
                    "storycloze",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "outperforms",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F7\" title=\"Figure 7 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> breaks down the accuracy improvements across MMSU categories from Stage&#160;I to Stage&#160;II, showing that the largest statistically significant gains occur in categories such as biology and chemistry. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the density of samples per cluster for the top-<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> clusters selected by the active learning algorithm. Using the LLM-assisted annotation procedure described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS5\" title=\"A.5 Cluster Annotation Pipeline &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>, we assign a domain label to each cluster and find that the most heavily sampled domain is <span class=\"ltx_text ltx_font_italic\">Molecular Biology</span>. These findings support our interpretation that Stage&#160;II boosts performance on these benchmarks by targeting more technical domains. Moreover, they indicate that <span class=\"ltx_text ltx_font_italic\">our active learning algorithm effectively identifies and addresses meaningful domain gaps</span>, leading to measurable performance improvements.</p>\n\n",
                "matched_terms": [
                    "most",
                    "stage",
                    "each",
                    "from",
                    "mmsu",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "from",
                    "mmsu"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Performance (Accuracy, %\\%) of the SALAD-3B model after Stage II training with active selection vs. random (uniform) selection.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MMSU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">OBQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ARC-C</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">PIQA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Uniform</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">75.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">49.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">68.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Active Sel.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">75.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">76.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">68.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">79.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "random",
            "arcc",
            "hellaswag",
            "stage",
            "salad3b",
            "model",
            "active",
            "after",
            "uniform",
            "piqa",
            "training",
            "sel",
            "mmsu",
            "accuracy",
            "obqa",
            "selection",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">While Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> show meaningful Stage&#160;II gains&#8212;and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> corroborates them&#8212;the role of domain-gap targeting remains unclear. Is it necessary to explicitly target domain gaps, or would tuning on a small amount of target, general-domain data suffice to improve alignment and performance? How does this trade-off evolve as we increase the selected data?</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "active",
                    "model",
                    "training",
                    "selection",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "active",
                    "selection",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate on broad-domain benchmarks of general knowledge, reasoning, and language understanding commonly used for LLMs, considering both their text and spoken versions: StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA (OBQA) from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite>. We adopt a few-shot prompting approach for evaluation across all tasks, with accuracy as the metric. For further details on the benchmarks, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS4\" title=\"A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "piqa",
                    "mmsu",
                    "accuracy",
                    "obqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the standard design of speech-adapted LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib2\" title=\"\">2025</a>)</cite>, consisting of a speech encoder that extracts representations from waveforms, an adapter that maps them into the input space of the language model, and the language model itself. For these experiments we initialize <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> from the text LLM Qwen2.5-3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite> base model, and use the same text LLM as teacher <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, allowing us to measure how much original text capability is maintained when processing speech and how much is lost during speech training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While prior work has paid great attention to speech encoder and adapter design&#8212;typically to promote cross-modal alignment by making speech representations more text-like&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite>&#8212;we adopt a simple architecture: the lightweight Mimi speech tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite> as encoder and a <math alttext=\"122\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>122</mn><annotation encoding=\"application/x-tex\">122</annotation></semantics></math>M-parameter stack of transformer decoder layers as adapter. We make this choice because current representation alignment methods rely on large non-causal encoders and complex modules unsuited for low-latency streaming, which is essential for downstream conversational speech-adapted LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Accordingly, we use causal, streaming-friendly models with low-level, non&#8211;text-like representations as a &#8220;worst-case&#8221; input alignment scenario, expecting our findings to generalize to more aligned text-speech representations while being directly applicable to low-latency architectures. The encoder remains frozen during training, while the adapter and language model are optimized. For further details on the model architecture, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS1\" title=\"A.1 Model description &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results with <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math> and broad-domain (FineWeb-Edu-train) training data. In this setting, misalignment is essentially the quantity being optimized&#8212;distillation directly targets cross-modal consistency&#8212;and, since the evaluation distribution (FineWeb-Edu-test) matches the training distribution, the metric aligns with the objective. Accordingly, domain-matched distillation yields the lowest misalignment and the strongest speech-understanding performance.</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Natural speech corpora are narrower in terms of domain coverage compared to text pretraining corpora (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Large-scale synthetic speech (i.e., same size as text pretraining corpora), while useful for improving the domain coverage, is both costly and lacking in the paralinguistic richness essential for natural spoken interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Debnath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib10\" title=\"\">2024</a>; Minixhofer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib26\" title=\"\">2025</a>)</cite>. It is therefore desirable to reduce reliance on large-scale synthetic speech data. To this end, we propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation. <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> is designed directly around our two key insights: distillation ensures robust alignment and mitigates forgetting, while active selection enables closer domain matching through a minimal, model-guided infusion of synthetic speech data rather than costly large-scale synthesis.</p>\n\n",
                "matched_terms": [
                    "active",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "selection",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample clusters in proportion to their importance rather than reweighting per-example losses, thereby avoiding the need to synthesize and train on the entire <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>. Given a fixed synthesis budget, we repeatedly draw a cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> according to <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>, select a text sequence <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> uniformly from <math alttext=\"{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math>, synthesize its speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m6\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>, and form an interleaved context <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m7\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math>. Each sample is added to the active dataset <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m8\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> until the budget is exhausted, after which <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> is used to continue training <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m10\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>. To prevent forgetting of Stage&#160;I training, we combine <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m11\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> with <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m12\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and minimize <math alttext=\"\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m13\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><mo>&#8746;</mo><msub><mi>&#120123;</mi><mtext>active</mtext></msub></mrow><mo>,</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "training",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply our method to the Qwen2.5 3B and 7B base LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>, yielding the <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B and <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B models. We follow the experimental setup of Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for architecture, data, and evaluation tasks. We use Emilia and LibriHeavy (<math alttext=\"141,612\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>141</mn><mo>,</mo><mn>612</mn></mrow><annotation encoding=\"application/x-tex\">141,612</annotation></semantics></math> hours) as our <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, and use a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-token FineWeb-Edu subset as our <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>.\nFor Stage&#160;II, we train a clustering model on <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> using balanced <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means with <math alttext=\"K=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">K=128</annotation></semantics></math> over <span class=\"ltx_text ltx_font_typewriter\">BAAI/bge-large-en-v1.5</span> embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span>, with a synthesis budget of <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> of <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We train SALAD models for <math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mn>24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math>B tokens during Stage&#160;I and additional <math alttext=\"1.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mn>1.9</mn><annotation encoding=\"application/x-tex\">1.9</annotation></semantics></math>B tokens during Stage&#160;II. Further training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS3\" title=\"A.3 Training Details: Closing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>, and additional ablations are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "salad3b",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
                "matched_terms": [
                    "salad3b",
                    "model",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we ask how well our approach preserves the text capabilities of the original text-based LLM backbone compared to the baselines. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>We use the DiVA model available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b\" title=\"\">https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b</a>\n. While <cite class=\"ltx_cite ltx_citemacro_citet\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite> report freezing a Llama-3-8B backbone, the released version on HuggingFace appears to be based on Llama-3.1-8B. Moreover, we found the released weights to differ from the Llama-3.1-8B checkpoint, which explains the differences in text performance reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</span></span></span> shows that, unlike other speech-adapted models, which exhibit substantial forgetting, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> maintains the closest performance to the original text LLM. This result highlights the effectiveness of the distillation objective in constraining the model to remain faithful to its teacher while learning to achieve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "active",
                    "selection",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models are trained with the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib24\" title=\"\">2019</a>)</cite> with a weight decay of <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. We adopt a warmup-stable-decay learning-rate schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(H&#228;gele et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib16\" title=\"\">2024</a>)</cite>, consisting of a linear warmup of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> steps followed by a linear decay to zero over the final <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math> of training. The peak learning rate is tuned separately for each model size, with distinct values for the language-model backbone and the adapter.\nWe use a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM. All models are trained with a batch size of approximately 1M tokens and a context window of 2048 tokens.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "model",
                    "piqa",
                    "mmsu",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n",
                "matched_terms": [
                    "random",
                    "model",
                    "performance",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "model",
                    "piqa",
                    "mmsu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generate the domain annotations in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> by annotating embedding clusters with an LLM (Claude 3.7 Sonnet). Below, we outline the annotation and validation procedures. For a clustering model with <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math> clusters, the judge validated the annotations with an accuracy of <math alttext=\"56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">56\\%</annotation></semantics></math> (random chance is <math alttext=\"1.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.6\\%</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "random",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "training",
                    "after",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "model",
                    "training",
                    "selection",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important question is whether the improvements from the active learning stage stem from its design, or merely from the extra training steps added after Stage&#160;I.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that Stage&#160;II yields consistent gains across tasks over Stage&#160;I for a given budget, with the exception of StoryCloze, where performance deteriorates slightly.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "after",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "uniform",
                    "selection",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F7\" title=\"Figure 7 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> breaks down the accuracy improvements across MMSU categories from Stage&#160;I to Stage&#160;II, showing that the largest statistically significant gains occur in categories such as biology and chemistry. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the density of samples per cluster for the top-<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> clusters selected by the active learning algorithm. Using the LLM-assisted annotation procedure described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS5\" title=\"A.5 Cluster Annotation Pipeline &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>, we assign a domain label to each cluster and find that the most heavily sampled domain is <span class=\"ltx_text ltx_font_italic\">Molecular Biology</span>. These findings support our interpretation that Stage&#160;II boosts performance on these benchmarks by targeting more technical domains. Moreover, they indicate that <span class=\"ltx_text ltx_font_italic\">our active learning algorithm effectively identifies and addresses meaningful domain gaps</span>, leading to measurable performance improvements.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "active",
                    "mmsu",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "performance",
                    "mmsu"
                ]
            }
        ]
    },
    "S4.T5": {
        "caption": "Table 5: SALAD best preserves the original text capabilities of the text-based LLM backbone after speech training compared to the baselines. â€œAcc.â€ denotes the accuracy of the speech-adapted LLM given text input; â€œGapâ€ denotes the difference between the text-based LLM given text input and the â€œAcc.â€ column. Gap cells are color-coded from best (green) to worst (red) for each task, where lower is better.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" rowspan=\"2\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">StoryCloze</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">MMSU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">OBQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">HellaSwag</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">ARC-C</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">PIQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gap</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2-Audio-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">81.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-0.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">46.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!90.0!GapRed1.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">73.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!74.0!GapRed3.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">69.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!78.0!GapRed2.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">68.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!81.0!GapRed3.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">76.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed2.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">69.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!81.0!GapRed2.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DiVA-Llama3.1-8B</td>\n<td class=\"ltx_td ltx_align_left\">80.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!56.0!GapRed7.4</td>\n<td class=\"ltx_td ltx_align_left\">60.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!91.0!GapRed1.0</td>\n<td class=\"ltx_td ltx_align_left\">82.0</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!84.0!GapRed1.3</td>\n<td class=\"ltx_td ltx_align_left\">67.8</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!23.0!GapRed7.7</td>\n<td class=\"ltx_td ltx_align_left\">81.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!94.0!GapRed0.2</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">80.8</span></td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!35.0!GapRed0.6</td>\n<td class=\"ltx_td ltx_align_left\">75.7</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!75.0!GapRed3.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GLM-4-Voice-9B</td>\n<td class=\"ltx_td ltx_align_left\">81.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed17.2</td>\n<td class=\"ltx_td ltx_align_left\">48.4</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed17.8</td>\n<td class=\"ltx_td ltx_align_left\">69.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed17.8</td>\n<td class=\"ltx_td ltx_align_left\">70.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed9.9</td>\n<td class=\"ltx_td ltx_align_left\">70.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed22.4</td>\n<td class=\"ltx_td ltx_align_left\">77.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-3.4</td>\n<td class=\"ltx_td ltx_align_left\">69.9</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!0.0!GapRed13.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-Omni-7B</td>\n<td class=\"ltx_td ltx_align_left\">80.5</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!73.0!GapRed4.5</td>\n<td class=\"ltx_td ltx_align_left\">66.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!72.0!GapRed4.5</td>\n<td class=\"ltx_td ltx_align_left\">87.3</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!82.0!GapRed1.7</td>\n<td class=\"ltx_td ltx_align_left\">69.8</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!32.0!GapRed6.9</td>\n<td class=\"ltx_td ltx_align_left\">87.6</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!92.0!GapRed0.8</td>\n<td class=\"ltx_td ltx_align_left\">78.8</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!27.0!GapRed1.1</td>\n<td class=\"ltx_td ltx_align_left\">78.4</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!73.0!GapRed3.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SALAD-3B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">82.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!98.0!GapRed0.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">62.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-0.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">83.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-1.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">70.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed0.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">83.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">-1.2</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">78.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!45.0!GapRed0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">76.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed-0.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">SALAD-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">84.9</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">-2.0</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">-0.8</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">90.1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!96.0!GapRed<span class=\"ltx_text ltx_font_bold\">-1.1</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">76.9</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">0.2</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">89.2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!98.0!GapRed-0.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">80.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!50.0!GapRed<span class=\"ltx_text ltx_font_bold\">-0.3</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">82.2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>GapGreen!100.0!GapRed<span class=\"ltx_text ltx_font_bold\">-0.9</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cellcolorgapgreen500gapred03",
            "each",
            "cellcolorgapgreen730gapred33",
            "llm",
            "cellcolorgapgreen840gapred13",
            "from",
            "accuracy",
            "capabilities",
            "cellcolorgapgreen910gapred10",
            "lower",
            "where",
            "cellcolorgapgreen1000gapred12",
            "cellcolorgapgreen00gapred224",
            "given",
            "acc",
            "salad",
            "cellcolorgapgreen350gapred06",
            "cellcolorgapgreen780gapred26",
            "cellcolorgapgreen230gapred77",
            "cellcolorgapgreen1000gapred05",
            "glm4voice9b",
            "column",
            "denotes",
            "cellcolorgapgreen820gapred17",
            "difference",
            "after",
            "preserves",
            "cellcolorgapgreen450gapred00",
            "cells",
            "input",
            "text",
            "cellcolorgapgreen920gapred08",
            "avg",
            "qwen2audio7b",
            "cellcolorgapgreen980gapred08",
            "cellcolorgapgreen900gapred13",
            "cellcolorgapgreen270gapred11",
            "â€œgapâ€",
            "cellcolorgapgreen740gapred33",
            "cellcolorgapgreen1000gapred02",
            "better",
            "cellcolorgapgreen1000gapred08",
            "textbased",
            "baselines",
            "cellcolorgapgreen1000gapred06",
            "cellcolorgapgreen730gapred45",
            "cellcolorgapgreen1000gapred09",
            "obqa",
            "speechadapted",
            "hellaswag",
            "cellcolorgapgreen1000gapred20",
            "cellcolorgapgreen960gapred11",
            "piqa",
            "worst",
            "colorcoded",
            "cellcolorgapgreen720gapred45",
            "between",
            "mmsu",
            "cellcolorgapgreen810gapred34",
            "â€œaccâ€",
            "speech",
            "cellcolorgapgreen810gapred22",
            "cellcolorgapgreen00gapred99",
            "red",
            "task",
            "qwen25omni7b",
            "arcc",
            "cellcolorgapgreen980gapred02",
            "cellcolorgapgreen560gapred74",
            "original",
            "divallama318b",
            "salad7b",
            "cellcolorgapgreen940gapred02",
            "green",
            "cellcolorgapgreen1000gapred34",
            "cellcolorgapgreen00gapred136",
            "salad3b",
            "cellcolorgapgreen320gapred69",
            "training",
            "cellcolorgapgreen1000gapred19",
            "cellcolorgapgreen750gapred30",
            "backbone",
            "cellcolorgapgreen00gapred178",
            "storycloze",
            "compared",
            "best",
            "cellcolorgapgreen00gapred172",
            "gap",
            "cellcolorgapgreen00gapred28"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Finally, we ask how well our approach preserves the text capabilities of the original text-based LLM backbone compared to the baselines. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>We use the DiVA model available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b\" title=\"\">https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b</a>\n. While <cite class=\"ltx_cite ltx_citemacro_citet\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite> report freezing a Llama-3-8B backbone, the released version on HuggingFace appears to be based on Llama-3.1-8B. Moreover, we found the released weights to differ from the Llama-3.1-8B checkpoint, which explains the differences in text performance reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</span></span></span> shows that, unlike other speech-adapted models, which exhibit substantial forgetting, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> maintains the closest performance to the original text LLM. This result highlights the effectiveness of the distillation objective in constraining the model to remain faithful to its teacher while learning to achieve cross-modal alignment.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "original",
                    "textbased",
                    "llm",
                    "from",
                    "salad",
                    "training",
                    "between",
                    "speech",
                    "capabilities",
                    "gap",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "original",
                    "textbased",
                    "task",
                    "compared",
                    "llm",
                    "gap",
                    "speech",
                    "capabilities",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "original",
                    "textbased",
                    "compared",
                    "given",
                    "llm",
                    "from",
                    "training",
                    "gap",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "llm",
                    "better",
                    "gap",
                    "salad",
                    "training",
                    "between",
                    "speech",
                    "capabilities",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify forgetting and cross-modal misalignment as the statistical distances between the outputs of a speech-adapted LLM and its text-based LLM backbone on matched text&#8211;speech inputs from a broad-domain pretraining corpus, and show that these measures are highly predictive of the text&#8211;speech understanding gap on broad-domain benchmarks.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "backbone",
                    "llm",
                    "from",
                    "gap",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We find that training on narrow-domain speech corpora with standard objectives used in prior work worsens both forgetting and broad-domain misalignment as the amount of training data increases. In contrast, using a cross-modal knowledge distillation objective&#8212;where the text-based LLM backbone serves as the teacher&#8212;improves alignment and mitigates forgetting, even when trained on narrow-domain data.</p>\n\n",
                "matched_terms": [
                    "backbone",
                    "textbased",
                    "llm",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show that cross-modal distillation alone leaves residual misalignment when trained only on narrow-domain data. To further address this misalignment, we introduce an active learning algorithm that targets domain mismatches between natural speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a method that builds on insights from our analyses, apply it to 3B and 7B LLMs, and benchmark them against recent speech-adapted LLMs in the 3B&#8211;9B range on spoken versions of six broad-domain knowledge and reasoning benchmarks. Our models outperform most baselines in spoken language understanding and perform competitively with the strongest, while training on over an order of magnitude less data.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "speechadapted",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent end-to-end methods for speech-adapted LLMs typically generate text as an intermediate representation conditioned on speech inputs, and then generate speech conditioned on that text&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite>. In this work, we focus on generating intermediate text conditioned on speech input as our primary task since it directly reflects language capability, leaving the task of generating speech for future work.\nSpecifically,\nwe aim to build a speech-adapted language model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>, parameterized by weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that defines a probability distribution over the next text token given a multimodal context. Let <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denote such a context, which may contain subsequences of text tokens <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> and/or speech representations <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>. For each position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the model predicts the distribution over the next text token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> conditioned on <math alttext=\"{\\bm{c}}_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{c}}_{\\leq i}</annotation></semantics></math>: <math alttext=\"P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "task",
                    "given",
                    "each",
                    "speech",
                    "text",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We seek to train <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> so that its predictions match the distribution of <math alttext=\"({\\bm{w}},{\\bm{c}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>&#119960;</mi><mo>,</mo><mi>&#119940;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{w}},{\\bm{c}})</annotation></semantics></math> pairs drawn from the natural language distribution <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math>. Most prior works approximate <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math> by minimizing the negative log-likelihood on a speech dataset <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m5\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib38\" title=\"\">2023</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transfer learning is used to alleviate this limitation, with <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> initialized from a pretrained text-only language model <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;which better approximates <math alttext=\"\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Q}</annotation></semantics></math>&#8212;in the hope of enabling general knowledge transfer even when fine-tuned on narrow speech data.\nIn practice, however, models trained this way under-perform on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>. One contributor to this under-performance is <em class=\"ltx_emph ltx_font_italic\">cross-modal misalignment</em>, i.e., inconsistent predictions across modalities, which we formalize as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"D_{\\mathrm{KL}}(\\cdot|\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{\\mathrm{KL}}(\\cdot|\\cdot)</annotation></semantics></math> denotes the Kullback&#8211;Leibler divergence between two distributions, <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> denotes a text sequence, <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denotes a semantically equivalent multimodal context sequence, and <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> denotes the next text token. This quantity measures how differently the model predicts the next token when conditioned on text versus equivalent multimodal context.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "text",
                    "where",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another contributor to this under-performance on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> is <em class=\"ltx_emph ltx_font_italic\">forgetting</em>, which measures the loss of the original text behavior:</p>\n\n",
                "matched_terms": [
                    "text",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This quantity measures how much the speech-adapted model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges from the text-based LLM <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on text inputs, indicating loss of text knowledge and reduced ability to transfer capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "llm",
                    "from",
                    "speech",
                    "capabilities",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "each",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models in a pretraining setup, modeling general sequences without the data templates used in instruction-tuned models. This choice reflects our focus on broad-domain alignment, avoiding restriction to dialogue data and acknowledging the scarcity of speech instruction data. Our data therefore consist of multimodal sequences <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> composed of interleaved speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math> and text <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> inputs, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>)</cite>. For our training objective, we introduce a variable <math alttext=\"\\alpha\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0,1]</annotation></semantics></math> that interpolates between a standard maximum likelihood objective, and a cross-modal distillation objective, similar to that used by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "training",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the two natural English speech corpora LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib21\" title=\"\">2024</a>)</cite> (read speech) and the YODAS-EN subset of the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib18\" title=\"\">2024</a>)</cite> (conversational), both among the largest and most semantically diverse publicly available speech datasets. However, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they still lack domain coverage relative to text pretraining corpora. Since forgetting is driven by domain shift, we also synthesize a spoken version of a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-text-token subset of FineWeb-Edu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib28\" title=\"\">2024</a>)</cite>&#8212;a high-quality broad-domain text pretraining corpus&#8212;to study the impact of aligning text and speech training domains, following the approach of <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. We use the Kokoro-TTS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hexgrad/kokoro-82M\" title=\"\">https://github.com/hexgrad/kokoro-82M</a></span></span></span> with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> voice, which provides the highest quality generations<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md\" title=\"\">https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md</a></span></span></span>, to synthesize the data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To produce interleaved speech&#8211;text sequences, we segment each utterance into subsequences and interleave spans of random length at runtime: <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>&#8211;<math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> words for text segments and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> words for speech segments. For LibriHeavy and Emilia, word-level timestamps of the corresponding transcriptions are obtained using the forced aligner from <cite class=\"ltx_cite ltx_citemacro_citet\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib29\" title=\"\">2024</a>)</cite>. For synthesized speech, we use the built-in functionality of Kokoro TTS to get word-level timestamps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate on broad-domain benchmarks of general knowledge, reasoning, and language understanding commonly used for LLMs, considering both their text and spoken versions: StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA (OBQA) from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite>. We adopt a few-shot prompting approach for evaluation across all tasks, with accuracy as the metric. For further details on the benchmarks, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS4\" title=\"A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "piqa",
                    "storycloze",
                    "from",
                    "mmsu",
                    "accuracy",
                    "obqa",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the standard design of speech-adapted LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib2\" title=\"\">2025</a>)</cite>, consisting of a speech encoder that extracts representations from waveforms, an adapter that maps them into the input space of the language model, and the language model itself. For these experiments we initialize <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> from the text LLM Qwen2.5-3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite> base model, and use the same text LLM as teacher <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, allowing us to measure how much original text capability is maintained when processing speech and how much is lost during speech training.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "original",
                    "llm",
                    "speech",
                    "from",
                    "training",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While prior work has paid great attention to speech encoder and adapter design&#8212;typically to promote cross-modal alignment by making speech representations more text-like&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite>&#8212;we adopt a simple architecture: the lightweight Mimi speech tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite> as encoder and a <math alttext=\"122\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>122</mn><annotation encoding=\"application/x-tex\">122</annotation></semantics></math>M-parameter stack of transformer decoder layers as adapter. We make this choice because current representation alignment methods rely on large non-causal encoders and complex modules unsuited for low-latency streaming, which is essential for downstream conversational speech-adapted LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Accordingly, we use causal, streaming-friendly models with low-level, non&#8211;text-like representations as a &#8220;worst-case&#8221; input alignment scenario, expecting our findings to generalize to more aligned text-speech representations while being directly applicable to low-latency architectures. The encoder remains frozen during training, while the adapter and language model are optimized. For further details on the model architecture, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS1\" title=\"A.1 Model description &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speechadapted",
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "each",
                    "training",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "given",
                    "each",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "compared",
                    "given",
                    "training",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
                "matched_terms": [
                    "better",
                    "denotes",
                    "where",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "difference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analyses highlight two central insights: (i) cross-modal knowledge distillation objective is more effective than maximum likelihood for mitigating misalignment and forgetting (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and (ii) better matching the domain of speech training data to that of text pretraining yields further gains when combined with cross-modal distillation. We use these insights to design a learning strategy to address\nthe text-speech understanding gap in the next section.</p>\n\n",
                "matched_terms": [
                    "better",
                    "gap",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Natural speech corpora are narrower in terms of domain coverage compared to text pretraining corpora (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Large-scale synthetic speech (i.e., same size as text pretraining corpora), while useful for improving the domain coverage, is both costly and lacking in the paralinguistic richness essential for natural spoken interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Debnath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib10\" title=\"\">2024</a>; Minixhofer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib26\" title=\"\">2025</a>)</cite>. It is therefore desirable to reduce reliance on large-scale synthetic speech data. To this end, we propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation. <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> is designed directly around our two key insights: distillation ensures robust alignment and mitigates forgetting, while active selection enables closer domain matching through a minimal, model-guided infusion of synthetic speech data rather than costly large-scale synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "salad",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "salad",
                    "training",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we aim to develop a sampling strategy for choosing which text samples to synthesize.\nWe draw inspiration from CRISP <cite class=\"ltx_cite ltx_citemacro_citep\">(Grangier et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib15\" title=\"\">2025</a>)</cite>, adopting a clustered importance-sampling strategy that derives a target-domain dataset from a broad-domain corpus by reweighting clusters. Concretely, let <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> be a large broad-domain text corpus. We partition it into <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> clusters <math alttext=\"{K(c)}_{c=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">{K(c)}_{c=1}^{K}</annotation></semantics></math> using sentence embeddings and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, and define</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute per-cluster importance weights as <math alttext=\"w(c)=\\tfrac{P_{\\text{target}}(c)}{P_{\\text{web}}(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mtext>target</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>P</mi><mtext>web</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">w(c)=\\tfrac{P_{\\text{target}}(c)}{P_{\\text{web}}(c)}</annotation></semantics></math>. In CRISP, <math alttext=\"P_{\\text{target}}(c)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>target</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{target}}(c)</annotation></semantics></math> comes from the distribution of a small target-domain dataset across the clusters. In our case, no such dataset exists, so we let the model itself define <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>. Specifically, we treat the divergence between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> within each cluster as a proxy for how much that cluster belongs to the &#8220;missing&#8221; domain. We define</p>\n\n",
                "matched_terms": [
                    "each",
                    "from",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M(c)=\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{probe}}^{(c)})</annotation></semantics></math> is the misalignment at cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, measured on a small probe set <math alttext=\"{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#120123;</mi><mtext>probe</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8834;</mo><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{probe}}^{(c)}\\subset{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math> for which we pre-synthesize speech. Clusters with higher misalignment&#8212;where <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges most from <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math>&#8212;are therefore upweighted. The resulting importance weight is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample clusters in proportion to their importance rather than reweighting per-example losses, thereby avoiding the need to synthesize and train on the entire <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>. Given a fixed synthesis budget, we repeatedly draw a cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> according to <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>, select a text sequence <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> uniformly from <math alttext=\"{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math>, synthesize its speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m6\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>, and form an interleaved context <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m7\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math>. Each sample is added to the active dataset <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m8\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> until the budget is exhausted, after which <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> is used to continue training <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m10\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>. To prevent forgetting of Stage&#160;I training, we combine <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m11\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> with <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m12\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and minimize <math alttext=\"\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m13\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><mo>&#8746;</mo><msub><mi>&#120123;</mi><mtext>active</mtext></msub></mrow><mo>,</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "given",
                    "after",
                    "from",
                    "each",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply our method to the Qwen2.5 3B and 7B base LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>, yielding the <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B and <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B models. We follow the experimental setup of Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for architecture, data, and evaluation tasks. We use Emilia and LibriHeavy (<math alttext=\"141,612\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>141</mn><mo>,</mo><mn>612</mn></mrow><annotation encoding=\"application/x-tex\">141,612</annotation></semantics></math> hours) as our <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, and use a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-token FineWeb-Edu subset as our <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>.\nFor Stage&#160;II, we train a clustering model on <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> using balanced <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means with <math alttext=\"K=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">K=128</annotation></semantics></math> over <span class=\"ltx_text ltx_font_typewriter\">BAAI/bge-large-en-v1.5</span> embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span>, with a synthesis budget of <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> of <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We train SALAD models for <math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mn>24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math>B tokens during Stage&#160;I and additional <math alttext=\"1.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mn>1.9</mn><annotation encoding=\"application/x-tex\">1.9</annotation></semantics></math>B tokens during Stage&#160;II. Further training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS3\" title=\"A.3 Training Details: Closing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>, and additional ablations are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "salad",
                    "salad7b",
                    "salad3b",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "original",
                    "textbased",
                    "difference",
                    "llm",
                    "given",
                    "speech",
                    "each",
                    "from",
                    "gap",
                    "between",
                    "input",
                    "accuracy",
                    "salad",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
                "matched_terms": [
                    "salad3b",
                    "compared",
                    "baselines",
                    "training",
                    "salad",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "compared",
                    "training",
                    "mmsu",
                    "speech",
                    "arcc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "speechadapted",
                    "textbased",
                    "compared",
                    "baselines",
                    "gap",
                    "salad",
                    "speech",
                    "capabilities",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder transforms <math alttext=\"l_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">l_{a}</annotation></semantics></math> speech audio frames <math alttext=\"{\\bm{a}}\\in{\\mathbb{R}}^{l_{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119938;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>l</mi><mi>a</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{a}}\\in{\\mathbb{R}}^{l_{a}}</annotation></semantics></math> into a sequence of <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>-dimensional latent speech representations <math alttext=\"{\\bm{Z}}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{Z}}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}</annotation></semantics></math> of length <math alttext=\"l_{s}&lt;l_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>l</mi><mi>s</mi></msub><mo>&lt;</mo><msub><mi>l</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">l_{s}&lt;l_{a}</annotation></semantics></math>. We use the Mimi speech tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>, a causal lightweight speech encoder. Mimi produces multi-codebook representations <math alttext=\"{\\bm{\\mathsfit{Z}}}\\in{\\mathbb{R}}^{l_{s}\\times q\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>q</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\mathsfit{Z}}}\\in{\\mathbb{R}}^{l_{s}\\times q\\times d_{s}}</annotation></semantics></math>, where <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> is the number of codebooks. We add the representations across codebooks, resulting in <math alttext=\"{\\bm{Z}}=\\sum_{i=1}^{q}{\\bm{\\mathsfit{Z}}}_{:,i,:}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#119937;</mi><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msub><mi>&#119937;</mi><mrow><mo rspace=\"0em\">:</mo><mo>,</mo><mi>i</mi><mo>,</mo><mo>:</mo></mrow></msub></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{Z}}=\\sum_{i=1}^{q}{\\bm{\\mathsfit{Z}}}_{:,i,:}\\in{\\mathbb{R}}^{l_{s}\\times d_{s}}</annotation></semantics></math>. The speech encoder remains frozen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language model processes multimodal sequences of text and speech representations <math alttext=\"{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119919;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>k</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}</annotation></semantics></math> and outputs a probability distribution over a vocabulary <math alttext=\"{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120141;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbb{V}}_{t}</annotation></semantics></math> of text tokens. Subsequences of <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> may correspond either to adapter-output speech sequences <math alttext=\"{\\bm{Z}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119937;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{Z}}^{\\prime}</annotation></semantics></math> or to sequences of text embeddings <math alttext=\"{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119916;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>w</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}</annotation></semantics></math> obtained by applying an embedding function to text tokens <math alttext=\"{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119960;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><msub><mi>l</mi><mi>w</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})</annotation></semantics></math>, with <math alttext=\"w_{i}\\in{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi>&#120141;</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}\\in{\\mathbb{V}}_{t}</annotation></semantics></math>. <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> is processed by a stack of transformer decoder layers, and the output logits at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> are linearly predicted from the corresponding hidden representation, yielding a <math alttext=\"|{\\mathbb{V}}_{t}|\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#120141;</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|{\\mathbb{V}}_{t}|</annotation></semantics></math>-dimensional logit vector that is Softmax-normalized into the probability distribution <math alttext=\"P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119919;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})</annotation></semantics></math>. We initialize the language model from pretrained text language models from the Qwen2.5 family of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models are trained with the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib24\" title=\"\">2019</a>)</cite> with a weight decay of <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. We adopt a warmup-stable-decay learning-rate schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(H&#228;gele et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib16\" title=\"\">2024</a>)</cite>, consisting of a linear warmup of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> steps followed by a linear decay to zero over the final <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math> of training. The peak learning rate is tuned separately for each model size, with distinct values for the language-model backbone and the adapter.\nWe use a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM. All models are trained with a batch size of approximately 1M tokens and a context window of 2048 tokens.</p>\n\n",
                "matched_terms": [
                    "each",
                    "llm",
                    "backbone",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models follow the hyperparameters and setup in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>, except that SALAD-3B uses a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM; SALAD-7B uses <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-4}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-6}</annotation></semantics></math> for the LLM.</p>\n\n",
                "matched_terms": [
                    "salad7b",
                    "salad3b",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Stage&#160;I, each batch is sampled with probability <math alttext=\"2/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">2/3</annotation></semantics></math> from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"1/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1/3</annotation></semantics></math> from the SmolLM corpus, following the common practice of mixing in pretraining data to mitigate forgetting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. In Stage&#160;II, batches are drawn with equal probability from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math>, and the SmolLM corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "after",
                    "from",
                    "salad",
                    "training",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "task",
                    "hellaswag",
                    "given",
                    "piqa",
                    "storycloze",
                    "each",
                    "from",
                    "mmsu",
                    "accuracy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n",
                "matched_terms": [
                    "task",
                    "mmsu",
                    "best",
                    "each",
                    "from",
                    "between",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "task",
                    "hellaswag",
                    "piqa",
                    "baselines",
                    "storycloze",
                    "each",
                    "mmsu",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generate the domain annotations in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> by annotating embedding clusters with an LLM (Claude 3.7 Sonnet). Below, we outline the annotation and validation procedures. For a clustering model with <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math> clusters, the judge validated the annotations with an accuracy of <math alttext=\"56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">56\\%</annotation></semantics></math> (random chance is <math alttext=\"1.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.6\\%</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each cluster, we sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> positives (closest to the centroid) and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> negatives (from the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> nearest neighbor clusters). An LLM is prompted with these examples to propose a short title, a descriptive label, and inclusion/exclusion criteria.</p>\n\n",
                "matched_terms": [
                    "each",
                    "llm",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a catalog of all cluster labels and ask the LLM to classify holdout texts into exactly one cluster. This produces multiclass accuracy estimates per cluster and overall. The pipeline supports resumability and incremental judging, enabling efficient large-scale evaluation.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "training",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "compared",
                    "from",
                    "training",
                    "speech",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "salad",
                    "training",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important question is whether the improvements from the active learning stage stem from its design, or merely from the extra training steps added after Stage&#160;I.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that Stage&#160;II yields consistent gains across tasks over Stage&#160;I for a given budget, with the exception of StoryCloze, where performance deteriorates slightly.</p>\n\n",
                "matched_terms": [
                    "given",
                    "after",
                    "storycloze",
                    "from",
                    "training",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F7\" title=\"Figure 7 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> breaks down the accuracy improvements across MMSU categories from Stage&#160;I to Stage&#160;II, showing that the largest statistically significant gains occur in categories such as biology and chemistry. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the density of samples per cluster for the top-<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> clusters selected by the active learning algorithm. Using the LLM-assisted annotation procedure described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS5\" title=\"A.5 Cluster Annotation Pipeline &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>, we assign a domain label to each cluster and find that the most heavily sampled domain is <span class=\"ltx_text ltx_font_italic\">Molecular Biology</span>. These findings support our interpretation that Stage&#160;II boosts performance on these benchmarks by targeting more technical domains. Moreover, they indicate that <span class=\"ltx_text ltx_font_italic\">our active learning algorithm effectively identifies and addresses meaningful domain gaps</span>, leading to measurable performance improvements.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "each",
                    "from",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "from",
                    "original",
                    "mmsu"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Evaluation task prompts. Placeholders are shown in monospace. Blue highlights denote text/audio inputs, and orange highlights denote continuations where likelihood is evaluated.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;story prefix&gt;</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMSU (continuation)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The following are multiple choice questions (with answers) about <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;topic&gt;</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMSU (multiple choice)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The following are multiple choice questions (with answers) about <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;topic&gt;</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">A. <span class=\"ltx_text ltx_font_typewriter\">&lt;A&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">B. <span class=\"ltx_text ltx_font_typewriter\">&lt;B&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">C. <span class=\"ltx_text ltx_font_typewriter\">&lt;C&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">D. <span class=\"ltx_text ltx_font_typewriter\">&lt;D&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpenBookQA (continuation)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpenBookQA (multiple choice)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">A. <span class=\"ltx_text ltx_font_typewriter\">&lt;A&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">B. <span class=\"ltx_text ltx_font_typewriter\">&lt;B&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">C. <span class=\"ltx_text ltx_font_typewriter\">&lt;C&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">D. <span class=\"ltx_text ltx_font_typewriter\">&lt;D&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HellaSwag</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;context&gt;</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ARC-Challenge (continuation)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ARC-Challenge (multiple choice)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Question: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">A. <span class=\"ltx_text ltx_font_typewriter\">&lt;A&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">B. <span class=\"ltx_text ltx_font_typewriter\">&lt;B&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">C. <span class=\"ltx_text ltx_font_typewriter\">&lt;C&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF0FF;\">D. <span class=\"ltx_text ltx_font_typewriter\">&lt;D&gt;</span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Answer: <span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PIQA</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#DCF0FF;\">&lt;question&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#FFEBD2;\">&lt;answer&gt;</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "topic",
            "arcchallenge",
            "evaluated",
            "answers",
            "prompts",
            "inputs",
            "story",
            "likelihood",
            "blue",
            "evaluation",
            "questions",
            "openbookqa",
            "hellaswag",
            "context",
            "piqa",
            "mmsu",
            "highlights",
            "about",
            "following",
            "choice",
            "denote",
            "multiple",
            "storycloze",
            "textaudio",
            "where",
            "prefix",
            "continuations",
            "task",
            "orange",
            "continuation",
            "question",
            "answer",
            "monospace",
            "placeholders"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "task",
                    "inputs",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "evaluated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent end-to-end methods for speech-adapted LLMs typically generate text as an intermediate representation conditioned on speech inputs, and then generate speech conditioned on that text&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite>. In this work, we focus on generating intermediate text conditioned on speech input as our primary task since it directly reflects language capability, leaving the task of generating speech for future work.\nSpecifically,\nwe aim to build a speech-adapted language model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>, parameterized by weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that defines a probability distribution over the next text token given a multimodal context. Let <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denote such a context, which may contain subsequences of text tokens <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> and/or speech representations <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>. For each position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the model predicts the distribution over the next text token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> conditioned on <math alttext=\"{\\bm{c}}_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{c}}_{\\leq i}</annotation></semantics></math>: <math alttext=\"P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "inputs",
                    "denote",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"D_{\\mathrm{KL}}(\\cdot|\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{\\mathrm{KL}}(\\cdot|\\cdot)</annotation></semantics></math> denotes the Kullback&#8211;Leibler divergence between two distributions, <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> denotes a text sequence, <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denotes a semantically equivalent multimodal context sequence, and <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> denotes the next text token. This quantity measures how differently the model predicts the next token when conditioned on text versus equivalent multimodal context.</p>\n\n",
                "matched_terms": [
                    "where",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models in a pretraining setup, modeling general sequences without the data templates used in instruction-tuned models. This choice reflects our focus on broad-domain alignment, avoiding restriction to dialogue data and acknowledging the scarcity of speech instruction data. Our data therefore consist of multimodal sequences <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> composed of interleaved speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math> and text <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> inputs, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>)</cite>. For our training objective, we introduce a variable <math alttext=\"\\alpha\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0,1]</annotation></semantics></math> that interpolates between a standard maximum likelihood objective, and a cross-modal distillation objective, similar to that used by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "choice",
                    "likelihood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate on broad-domain benchmarks of general knowledge, reasoning, and language understanding commonly used for LLMs, considering both their text and spoken versions: StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA (OBQA) from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite>. We adopt a few-shot prompting approach for evaluation across all tasks, with accuracy as the metric. For further details on the benchmarks, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS4\" title=\"A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "openbookqa",
                    "arcchallenge",
                    "hellaswag",
                    "piqa",
                    "evaluation",
                    "storycloze",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "openbookqa",
                    "questions",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "openbookqa",
                    "arcchallenge",
                    "task",
                    "hellaswag",
                    "context",
                    "piqa",
                    "evaluation",
                    "storycloze",
                    "answer",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "openbookqa",
                    "arcchallenge",
                    "task",
                    "hellaswag",
                    "context",
                    "piqa",
                    "storycloze",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "choice",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important question is whether the improvements from the active learning stage stem from its design, or merely from the extra training steps added after Stage&#160;I.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that Stage&#160;II yields consistent gains across tasks over Stage&#160;I for a given budget, with the exception of StoryCloze, where performance deteriorates slightly.</p>\n\n",
                "matched_terms": [
                    "question",
                    "storycloze",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "questions",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "mmsu"
                ]
            }
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Type-II Analysis of Covariance (ANCOVA) results controlling for Î±\\alpha (fixed effects) and logâ¡(tokens)\\log(\\text{tokens}). Left: dependent variable is average speech performance (%). Right: dependent variable is average text performance (%). Reported are degrees of freedom (df), sum of squares (SS), mean squares (MS), F-statistics (FF), and corresponding pp-values (pp). Partial R2R^{2} values quantify the unique variance explained by each factor.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"6\">\n<span class=\"ltx_text ltx_font_bold\">Speech</span> (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"6\">\n<span class=\"ltx_text ltx_font_bold\">Text</span> (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Term</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">df</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">SS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">MS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m13\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m14\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></td>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\">Term</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">df</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">SS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">MS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m15\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m16\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m17\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.789</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.789</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">11.51</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.0025</td>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\">forgetting</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.2588</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.2588</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">7.945</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.0097</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m18\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (FE)</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">4</th>\n<td class=\"ltx_td ltx_align_right\">2.255</td>\n<td class=\"ltx_td ltx_align_right\">0.5638</td>\n<td class=\"ltx_td ltx_align_right\">2.327</td>\n<td class=\"ltx_td ltx_align_right\">0.0867</td>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">\n<math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m19\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (FE)</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">4</th>\n<td class=\"ltx_td ltx_align_right\">0.4121</td>\n<td class=\"ltx_td ltx_align_right\">0.1030</td>\n<td class=\"ltx_td ltx_align_right\">3.163</td>\n<td class=\"ltx_td ltx_align_right\">0.0329</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"\\log(\\text{tokens})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m20\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>tokens</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{tokens})</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">1</th>\n<td class=\"ltx_td ltx_align_right\">38.39</td>\n<td class=\"ltx_td ltx_align_right\">38.39</td>\n<td class=\"ltx_td ltx_align_right\">158.4</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"8.5\\times 10^{-12}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m21\" intent=\":literal\"><semantics><mrow><mn>8.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>12</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">8.5\\times 10^{-12}</annotation></semantics></math></td>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\"><math alttext=\"\\log(\\text{tokens})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m22\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>tokens</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{tokens})</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">1</th>\n<td class=\"ltx_td ltx_align_right\">0.0661</td>\n<td class=\"ltx_td ltx_align_right\">0.0661</td>\n<td class=\"ltx_td ltx_align_right\">2.031</td>\n<td class=\"ltx_td ltx_align_right\">0.168</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Residual</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">23</th>\n<td class=\"ltx_td ltx_align_right\">5.573</td>\n<td class=\"ltx_td ltx_align_right\">0.2423</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Residual</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">23</th>\n<td class=\"ltx_td ltx_align_right\">0.749</td>\n<td class=\"ltx_td ltx_align_right\">0.0326</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n</tr>\n</tbody>\n<tfoot class=\"ltx_tfoot\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" colspan=\"6\">Partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m23\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"\\log(\\text{misalignment})=0.33,\\ \\alpha=0.29,\\ \\log(\\text{tokens})=0.87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m24\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0.33</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><mi>&#945;</mi><mo>=</mo><mn>0.29</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>tokens</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0.87</mn></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})=0.33,\\ \\alpha=0.29,\\ \\log(\\text{tokens})=0.87</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" colspan=\"6\">Partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m25\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: forgetting<math alttext=\"=0.26,\\ \\alpha=0.35,\\ \\log(\\text{tokens})=0.08\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m26\" intent=\":literal\"><semantics><mrow><mrow><mi/><mo>=</mo><mn>0.26</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><mi>&#945;</mi><mo>=</mo><mn>0.35</mn></mrow><mo rspace=\"0.667em\">,</mo><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>tokens</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0.08</mn></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">=0.26,\\ \\alpha=0.35,\\ \\log(\\text{tokens})=0.08</annotation></semantics></math>\n</th>\n</tr>\n</tfoot>\n</table>\n\n",
        "informative_terms_identified": [
            "alpha029",
            "forgetting026Î±035logâ¡tokens008026",
            "left",
            "85Ã—10âˆ’1285times",
            "controlling",
            "each",
            "fstatistics",
            "partial",
            "logâ¡misalignment033Î±029logâ¡tokens087logtextmisalignment033",
            "freedom",
            "Î±alpha",
            "corresponding",
            "residual",
            "covariance",
            "average",
            "variable",
            "reported",
            "logtexttokens008",
            "explained",
            "text",
            "performance",
            "dependent",
            "ppvalues",
            "squares",
            "typeii",
            "analysis",
            "term",
            "mean",
            "sum",
            "results",
            "speech",
            "ancova",
            "factor",
            "variance",
            "right",
            "values",
            "degrees",
            "logâ¡misalignmentlogtextmisalignment",
            "fixed",
            "logtexttokens087",
            "unique",
            "r2r2",
            "forgetting",
            "quantify",
            "alpha035",
            "logâ¡tokenslogtexttokens",
            "effects"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We report Type-II ANCOVA in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T7\" title=\"Table 7 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>: for each term, we compare the full model against the reduced model with that term removed (while keeping the other terms), yielding <math alttext=\"SS_{\\text{term}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p2.m1\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>S</mi><mtext>term</mtext></msub></mrow><annotation encoding=\"application/x-tex\">SS_{\\text{term}}</annotation></semantics></math>, <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p2.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math>, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>.\nWe also report partial <math alttext=\"R^{2}=SS_{\\text{term}}/(SS_{\\text{term}}+SS_{\\text{resid}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>S</mi><mtext>term</mtext></msub></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>S</mi><mtext>term</mtext></msub></mrow><mo>+</mo><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>S</mi><mtext>resid</mtext></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">R^{2}=SS_{\\text{term}}/(SS_{\\text{term}}+SS_{\\text{resid}})</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "term",
                    "forgetting",
                    "performance",
                    "speech",
                    "text",
                    "dependent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work has proposed several strategies to reduce this gap.\nA common direction is cross-modal alignment, achieved either by optimizing the fusion between speech encoders and text LLMs to promote modality-agnostic representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib12\" title=\"\">2025</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib32\" title=\"\">2025</a>)</cite> or by explicitly training for consistent outputs across modalities given equivalent inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fathullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib14\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib33\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>. Another direction is data-driven methods, which synthesize large-scale speech from text corpora to narrow the distribution gap between speech and text training domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. While these methods show performance improvements, they focused on narrow-domain benchmarks and several did not evaluate performance relative to the original text-based LLMs. When evaluated on broader benchmarks, these approaches showed substantial drops compared to their text-based LLM backbones&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>. Despite these efforts, the text-speech understanding gap persists.\nMore recent methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite> demonstrated notable progress, but remain irreproducible due to missing training details and their reliance on massive proprietary speech datasets spanning millions of hours&#8212;equivalent to over <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>B text tokens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Estimated from the average duration of text tokens (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>320 ms) in our data.</span></span></span>, i.e., a budget comparable to full pretraining budgets in the text domain. Given the scarcity of publicly available speech and parallel speech&#8211;text data relative to text data, more sample-efficient methods are needed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "average",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we seek to understand the text&#8211;speech understanding gap in greater depth to better guide the design of efficient remedies.\nBridging the text&#8211;speech understanding gap requires a speech-adapted LLM to retain the knowledge of its text-based counterpart (i.e., avoid forgetting) and produce consistent outputs for equivalent speech and text inputs (i.e., avoid cross-modal misalignment). Forgetting refers to the loss of pretrained text capabilities during adaptation to speech, a well-documented effect of domain shift between pretraining and fine-tuning in LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nCross-modal misalignment is observed when semantically equivalent speech and text inputs give divergent outputs.\nWe quantify forgetting and cross-modal misalignment, and study these measures under different training objectives and data regimes. The gained insights lead us to propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation, a sample-efficient method to address the performance gap. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "forgetting",
                    "quantify",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify forgetting and cross-modal misalignment as the statistical distances between the outputs of a speech-adapted LLM and its text-based LLM backbone on matched text&#8211;speech inputs from a broad-domain pretraining corpus, and show that these measures are highly predictive of the text&#8211;speech understanding gap on broad-domain benchmarks.</p>\n\n",
                "matched_terms": [
                    "quantify",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We find that training on narrow-domain speech corpora with standard objectives used in prior work worsens both forgetting and broad-domain misalignment as the amount of training data increases. In contrast, using a cross-modal knowledge distillation objective&#8212;where the text-based LLM backbone serves as the teacher&#8212;improves alignment and mitigates forgetting, even when trained on narrow-domain data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show that cross-modal distillation alone leaves residual misalignment when trained only on narrow-domain data. To further address this misalignment, we introduce an active learning algorithm that targets domain mismatches between natural speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "residual",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent end-to-end methods for speech-adapted LLMs typically generate text as an intermediate representation conditioned on speech inputs, and then generate speech conditioned on that text&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>; KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib23\" title=\"\">2025</a>)</cite>. In this work, we focus on generating intermediate text conditioned on speech input as our primary task since it directly reflects language capability, leaving the task of generating speech for future work.\nSpecifically,\nwe aim to build a speech-adapted language model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>, parameterized by weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that defines a probability distribution over the next text token given a multimodal context. Let <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> denote such a context, which may contain subsequences of text tokens <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> and/or speech representations <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>. For each position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the model predicts the distribution over the next text token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> conditioned on <math alttext=\"{\\bm{c}}_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{c}}_{\\leq i}</annotation></semantics></math>: <math alttext=\"P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119940;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\theta}(w_{i+1}\\mid{\\bm{c}}_{\\leq i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another contributor to this under-performance on spoken language understanding tasks relative to <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> is <em class=\"ltx_emph ltx_font_italic\">forgetting</em>, which measures the loss of the original text behavior:</p>\n\n",
                "matched_terms": [
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This quantity measures how much the speech-adapted model <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> diverges from the text-based LLM <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p9.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on text inputs, indicating loss of text knowledge and reduced ability to transfer capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we study how cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) in speech-adapted LLMs affect downstream language understanding performance, and how different design decisions impact these two metrics. Specifically, we train multiple models while varying training objectives, datasets, and training budgets. Then, for each trained model, we measure cross-modal alignment, forgetting, and the performance on broad-domain language understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "forgetting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models in a pretraining setup, modeling general sequences without the data templates used in instruction-tuned models. This choice reflects our focus on broad-domain alignment, avoiding restriction to dialogue data and acknowledging the scarcity of speech instruction data. Our data therefore consist of multimodal sequences <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math> composed of interleaved speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math> and text <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> inputs, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>)</cite>. For our training objective, we introduce a variable <math alttext=\"\\alpha\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0,1]</annotation></semantics></math> that interpolates between a standard maximum likelihood objective, and a cross-modal distillation objective, similar to that used by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "variable",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the two natural English speech corpora LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib21\" title=\"\">2024</a>)</cite> (read speech) and the YODAS-EN subset of the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib18\" title=\"\">2024</a>)</cite> (conversational), both among the largest and most semantically diverse publicly available speech datasets. However, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they still lack domain coverage relative to text pretraining corpora. Since forgetting is driven by domain shift, we also synthesize a spoken version of a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-text-token subset of FineWeb-Edu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib28\" title=\"\">2024</a>)</cite>&#8212;a high-quality broad-domain text pretraining corpus&#8212;to study the impact of aligning text and speech training domains, following the approach of <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. We use the Kokoro-TTS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hexgrad/kokoro-82M\" title=\"\">https://github.com/hexgrad/kokoro-82M</a></span></span></span> with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> voice, which provides the highest quality generations<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md\" title=\"\">https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md</a></span></span></span>, to synthesize the data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To produce interleaved speech&#8211;text sequences, we segment each utterance into subsequences and interleave spans of random length at runtime: <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>&#8211;<math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> words for text segments and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> words for speech segments. For LibriHeavy and Emilia, word-level timestamps of the corresponding transcriptions are obtained using the forced aligner from <cite class=\"ltx_cite ltx_citemacro_citet\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib29\" title=\"\">2024</a>)</cite>. For synthesized speech, we use the built-in functionality of Kokoro TTS to get word-level timestamps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the standard design of speech-adapted LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib2\" title=\"\">2025</a>)</cite>, consisting of a speech encoder that extracts representations from waveforms, an adapter that maps them into the input space of the language model, and the language model itself. For these experiments we initialize <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> from the text LLM Qwen2.5-3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite> base model, and use the same text LLM as teacher <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, allowing us to measure how much original text capability is maintained when processing speech and how much is lost during speech training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "average",
                    "forgetting",
                    "each",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows scatter plots with ordinary least squares fits for models trained on narrow-domain speech data (LibriHeavy + Emilia): (i) average speech performance (%) vs. <math alttext=\"\\log(\\text{misalignment})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>misalignment</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{misalignment})</annotation></semantics></math>, and (ii) average text performance (%) vs. forgetting. The solid line indicates the fitted regression; the shaded band is the <math alttext=\"95\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>95</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">95\\%</annotation></semantics></math> confidence interval for the mean prediction. Each panel also reports the Leave-One-Out Cross-Validation (LOOCV) <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of the univariate fit. The results show that speech performance declines with increasing misalignment (LOOCV <math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), and text performance declines with increasing forgetting (LOOCV <math alttext=\"R^{2}=0.74\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.74</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.74</annotation></semantics></math>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports both univariate and partial <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> (i.e., variance explained when adding the other factor). Misalignment uniquely explains a large share of speech variance given forgetting (partial <math alttext=\"R^{2}\\approx 0.56\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.56</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.56</annotation></semantics></math>), while forgetting uniquely explains text variance given misalignment (partial <math alttext=\"R^{2}\\approx 0.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>&#8776;</mo><mn>0.32</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}\\approx 0.32</annotation></semantics></math>). These patterns hold when controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, with unique cross-validated <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> of <math alttext=\"\\sim 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.34</annotation></semantics></math> for speech (misalignment) and <math alttext=\"\\sim 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.23</annotation></semantics></math> for text (forgetting). For more details on the least squares analysis, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS6\" title=\"A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "controlling",
                    "each",
                    "partial",
                    "Î±alpha",
                    "average",
                    "explained",
                    "text",
                    "performance",
                    "squares",
                    "analysis",
                    "mean",
                    "results",
                    "speech",
                    "factor",
                    "variance",
                    "logâ¡misalignmentlogtextmisalignment",
                    "unique",
                    "r2r2",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports scaling curves for cross-modal misalignment, forgetting, and average speech performance. Training with NLL (i.e., <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>) on narrow-domain speech data (LibriHeavy + Emilia) leads to increasing cross-modal misalignment with scale. Given the strong relation between misalignment and speech performance shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F3\" title=\"Figure 3 &#8227; 3.4 Architecture &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, NLL training on narrow-domain data also yields the weakest results.\nNLL training leads to greater forgetting of the pretrained text behavior compared to models trained with nonzero <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values. This greater forgetting, however, has limited impact on the average text performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS7\" title=\"A.7 Effect of Distillation on Text Performance &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "values",
                    "average",
                    "forgetting",
                    "results",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Higher values of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> yield better alignment, and for <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math>, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale. Misalignment is well described by a typical log-linear neural scaling law&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib22\" title=\"\">2020</a>; Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib20\" title=\"\">2022</a>)</cite>. We fit scaling laws of misalignment with respect to the training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> of the form <math alttext=\"M=E+B\\,D^{-\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>E</mi><mo>+</mo><mrow><mi>B</mi><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi>D</mi><mrow><mo>&#8722;</mo><mi>&#946;</mi></mrow></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M=E+B\\,D^{-\\beta}</annotation></semantics></math>, where <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> denotes the irreducible misalignment and <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> capture scaling efficiency.\nThe fitted laws are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, along with LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and the estimated number of tokens required to reach within 5% of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m9\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>. The irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m10\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> decreases with <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m11\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and for all <math alttext=\"0&lt;\\alpha&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m12\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>&#945;</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0&lt;\\alpha&lt;1</annotation></semantics></math>, misalignment saturates early in training. Distillation is therefore the most scalable approach with respect to misalignment.\nAlthough forgetting increases slightly with scale regardless of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px2.p2.m13\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, this effect has limited impact on performance.</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "values",
                    "r2r2",
                    "forgetting",
                    "reported",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that training on broad-domain data (FineWeb-Edu) with <math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math> reduces misalignment relative to narrow-domain data, although misalignment still grows with scale. This indicates that domain matching alone does not resolve cross-modal misalignment. Counterintuitively, forgetting is slightly worse than with narrow-domain training, but the difference is small. Speech performance is not tied to misalignment, with the model outperforming others despite higher misalignment. However, while <math alttext=\"\\alpha&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha&gt;0</annotation></semantics></math> yields consistent improvements with scale, NLL training on broad-domain data shows no meaningful scaling gains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "forgetting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results with <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math> and broad-domain (FineWeb-Edu-train) training data. In this setting, misalignment is essentially the quantity being optimized&#8212;distillation directly targets cross-modal consistency&#8212;and, since the evaluation distribution (FineWeb-Edu-test) matches the training distribution, the metric aligns with the objective. Accordingly, domain-matched distillation yields the lowest misalignment and the strongest speech-understanding performance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analyses highlight two central insights: (i) cross-modal knowledge distillation objective is more effective than maximum likelihood for mitigating misalignment and forgetting (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and (ii) better matching the domain of speech training data to that of text pretraining yields further gains when combined with cross-modal distillation. We use these insights to design a learning strategy to address\nthe text-speech understanding gap in the next section.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Natural speech corpora are narrower in terms of domain coverage compared to text pretraining corpora (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Large-scale synthetic speech (i.e., same size as text pretraining corpora), while useful for improving the domain coverage, is both costly and lacking in the paralinguistic richness essential for natural spoken interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Debnath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib10\" title=\"\">2024</a>; Minixhofer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib26\" title=\"\">2025</a>)</cite>. It is therefore desirable to reduce reliance on large-scale synthetic speech data. To this end, we propose <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>: <span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation. <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> is designed directly around our two key insights: distillation ensures robust alignment and mitigates forgetting, while active selection enables closer domain matching through a minimal, model-guided infusion of synthetic speech data rather than costly large-scale synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We structure <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> as a two-stage process. <span class=\"ltx_text ltx_font_bold\">Stage&#160;I (Distillation on Natural Speech)</span> trains <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> on natural speech by minimizing <math alttext=\"\\mathcal{L}_{\\text{DIST}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}</annotation></semantics></math> between <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math> and <math alttext=\"Q_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\phi}</annotation></semantics></math> on <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E5\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), leveraging the strong scaling behavior of distillation until alignment plateaus at the irreducible misalignment <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>, which, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.T2\" title=\"Table 2 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, occurs within a practical training budget. <span class=\"ltx_text ltx_font_bold\">Stage&#160;II (Active Selection for Domain Expansion)</span> then addresses the residual misalignment by introducing a small but strategically chosen amount of synthetic speech through <em class=\"ltx_emph ltx_font_italic\">active selection</em>, guided by the model&#8217;s own misalignment signals. This targeted augmentation complements Stage&#160;I by expanding domain coverage while keeping reliance on synthetic data minimal, consistent with our emphasis on sample efficiency and the use of natural speech.</p>\n\n",
                "matched_terms": [
                    "residual",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample clusters in proportion to their importance rather than reweighting per-example losses, thereby avoiding the need to synthesize and train on the entire <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>. Given a fixed synthesis budget, we repeatedly draw a cluster <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> according to <math alttext=\"P_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">P_{\\text{target}}</annotation></semantics></math>, select a text sequence <math alttext=\"{\\bm{w}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m4\" intent=\":literal\"><semantics><mi>&#119960;</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math> uniformly from <math alttext=\"{\\mathbb{D}}_{\\text{web}}^{(c)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>&#120123;</mi><mtext>web</mtext><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}^{(c)}</annotation></semantics></math>, synthesize its speech <math alttext=\"{\\bm{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m6\" intent=\":literal\"><semantics><mi>&#119938;</mi><annotation encoding=\"application/x-tex\">{\\bm{a}}</annotation></semantics></math>, and form an interleaved context <math alttext=\"{\\bm{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m7\" intent=\":literal\"><semantics><mi>&#119940;</mi><annotation encoding=\"application/x-tex\">{\\bm{c}}</annotation></semantics></math>. Each sample is added to the active dataset <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m8\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> until the budget is exhausted, after which <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> is used to continue training <math alttext=\"P_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m10\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">P_{\\theta}</annotation></semantics></math>. To prevent forgetting of Stage&#160;I training, we combine <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m11\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math> with <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m12\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and minimize <math alttext=\"\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m13\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DIST</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><mo>&#8746;</mo><msub><mi>&#120123;</mi><mtext>active</mtext></msub></mrow><mo>,</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DIST}}({\\mathbb{D}}_{\\text{speech}}\\cup{\\mathbb{D}}_{\\text{active}},\\theta)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "forgetting",
                    "each",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "each",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we ask how well our approach preserves the text capabilities of the original text-based LLM backbone compared to the baselines. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>We use the DiVA model available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b\" title=\"\">https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b</a>\n. While <cite class=\"ltx_cite ltx_citemacro_citet\">Held et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite> report freezing a Llama-3-8B backbone, the released version on HuggingFace appears to be based on Llama-3.1-8B. Moreover, we found the released weights to differ from the Llama-3.1-8B checkpoint, which explains the differences in text performance reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</span></span></span> shows that, unlike other speech-adapted models, which exhibit substantial forgetting, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> maintains the closest performance to the original text LLM. This result highlights the effectiveness of the distillation objective in constraining the model to remain faithful to its teacher while learning to achieve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "text",
                    "forgetting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "forgetting",
                    "results",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"{\\bm{Z}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>&#119937;</mi><annotation encoding=\"application/x-tex\">{\\bm{Z}}</annotation></semantics></math> encodes low-level phonetic and acoustic information. The role of the adapter is to transform <math alttext=\"{\\bm{Z}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#119937;</mi><annotation encoding=\"application/x-tex\">{\\bm{Z}}</annotation></semantics></math> into <math alttext=\"{\\bm{Z}}^{\\prime}\\in{\\mathbb{R}}^{l_{s}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119937;</mi><mo>&#8242;</mo></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{Z}}^{\\prime}\\in{\\mathbb{R}}^{l_{s}\\times d}</annotation></semantics></math>, a higher-level, more text-like representation. We implement the adapter as a stack of decoder-only transformer layers, preserving causality and streamability. If <math alttext=\"d_{s}\\neq d\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>&#8800;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d_{s}\\neq d</annotation></semantics></math>, a linear projection is applied at the output to obtain <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>-dimensional representations. We evaluated several adapter sizes and found performance to saturate at around <math alttext=\"122\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mn>122</mn><annotation encoding=\"application/x-tex\">122</annotation></semantics></math>M parameters; this configuration was therefore adopted for all reported experiments. The adapter consists of <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> Llama-style decoder layers with residual dimension 960, MLP dimension <math alttext=\"2560\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mn>2560</mn><annotation encoding=\"application/x-tex\">2560</annotation></semantics></math>, and <math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math> attention heads with 5 KV heads.</p>\n\n",
                "matched_terms": [
                    "residual",
                    "reported",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language model processes multimodal sequences of text and speech representations <math alttext=\"{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119919;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>k</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{H}}\\in{\\mathbb{R}}^{k\\times d}</annotation></semantics></math> and outputs a probability distribution over a vocabulary <math alttext=\"{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120141;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbb{V}}_{t}</annotation></semantics></math> of text tokens. Subsequences of <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> may correspond either to adapter-output speech sequences <math alttext=\"{\\bm{Z}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119937;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{Z}}^{\\prime}</annotation></semantics></math> or to sequences of text embeddings <math alttext=\"{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119916;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>l</mi><mi>w</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{E}}\\in{\\mathbb{R}}^{l_{w}\\times d}</annotation></semantics></math> obtained by applying an embedding function to text tokens <math alttext=\"{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119960;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><msub><mi>l</mi><mi>w</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}=(w_{1},\\ldots,w_{l_{w}})</annotation></semantics></math>, with <math alttext=\"w_{i}\\in{\\mathbb{V}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi>&#120141;</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}\\in{\\mathbb{V}}_{t}</annotation></semantics></math>. <math alttext=\"{\\bm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mi>&#119919;</mi><annotation encoding=\"application/x-tex\">{\\bm{H}}</annotation></semantics></math> is processed by a stack of transformer decoder layers, and the output logits at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> are linearly predicted from the corresponding hidden representation, yielding a <math alttext=\"|{\\mathbb{V}}_{t}|\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#120141;</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|{\\mathbb{V}}_{t}|</annotation></semantics></math>-dimensional logit vector that is Softmax-normalized into the probability distribution <math alttext=\"P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>&#119919;</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(w_{i+1}\\mid{\\bm{H}}_{\\leq i})</annotation></semantics></math>. We initialize the language model from pretrained text language models from the Qwen2.5 family of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models are trained with the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib24\" title=\"\">2019</a>)</cite> with a weight decay of <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. We adopt a warmup-stable-decay learning-rate schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(H&#228;gele et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib16\" title=\"\">2024</a>)</cite>, consisting of a linear warmup of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> steps followed by a linear decay to zero over the final <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math> of training. The peak learning rate is tuned separately for each model size, with distinct values for the language-model backbone and the adapter.\nWe use a learning rate of <math alttext=\"10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-3}</annotation></semantics></math> for the adapter and <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> for the LLM. All models are trained with a batch size of approximately 1M tokens and a context window of 2048 tokens.</p>\n\n",
                "matched_terms": [
                    "each",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Stage&#160;I, each batch is sampled with probability <math alttext=\"2/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">2/3</annotation></semantics></math> from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"1/3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1/3</annotation></semantics></math> from the SmolLM corpus, following the common practice of mixing in pretraining data to mitigate forgetting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;thune et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib3\" title=\"\">2025</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>. In Stage&#160;II, batches are drawn with equal probability from <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, <math alttext=\"{\\mathbb{D}}_{\\text{active}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>active</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{active}}</annotation></semantics></math>, and the SmolLM corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "forgetting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n",
                "matched_terms": [
                    "each",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "each",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fit ordinary least-squares models with (i) average speech performance (%) as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">and (ii) average text performance (%) as</p>\n\n",
                "matched_terms": [
                    "average",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\gamma_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#947;</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">\\gamma_{\\alpha}</annotation></semantics></math> are fixed effects for <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "effects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both analyses indicate that, after controlling for <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and training budget, misalignment is strongly associated with speech performance, and forgetting is strongly associated with text performance. To quantify out-of-sample explanatory power of the focal predictor beyond controls, we also compute the partial LOOCV <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math>: <math alttext=\"R^{2}_{\\text{cv,full}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,full</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,full}}</annotation></semantics></math> versus <math alttext=\"R^{2}_{\\text{cv,controls}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>R</mi><mtext>cv,controls</mtext><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">R^{2}_{\\text{cv,controls}}</annotation></semantics></math>.\nWe obtain <math alttext=\"\\approx 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.34</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.34</annotation></semantics></math> for misalignment (speech) and <math alttext=\"\\approx 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.23</annotation></semantics></math> for forgetting (text), consistent with the ANCOVA results.</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "controlling",
                    "forgetting",
                    "r2r2",
                    "quantify",
                    "results",
                    "speech",
                    "partial",
                    "ancova",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F5\" title=\"Figure 5 &#8227; A.6 Ordinary Least Squares Analysis &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows how the average text performance varies with the choice of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.E4\" title=\"In 3.1 Objective &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, where <math alttext=\"\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>. The models are trained on data drawn from <math alttext=\"{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{D}}\\in\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math> with training budgets ranging from <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m5\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens. Overall, average text performance is less sensitive to these changes compared to average speech performance. Nonetheless, we observe a consistent, but small, improvement when training with the distillation objective (<math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math>) relative to the NLL objective (<math alttext=\"\\alpha=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "Î±alpha",
                    "average",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate these questions, we vary <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, which controls selectivity, and sweep the Stage&#160;II synthetic budget from <math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math> to <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math> of the natural-speech dataset size (LibriHeavy + Emilia). We evaluate <math alttext=\"\\gamma\\in\\{0,5,10,30\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma\\in\\{0,5,10,30\\}</annotation></semantics></math>, where <math alttext=\"\\gamma=0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0</annotation></semantics></math> corresponds to uniform sampling from the target domain. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F6\" title=\"Figure 6 &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports average performance across spoken tasks as a function of budget. Active selection with <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> consistently outperforms all other settings across budgets, <em class=\"ltx_emph ltx_font_italic\">underscoring the importance of targeting domain gaps</em>. By contrast, over-focusing on gaps (<math alttext=\"\\gamma&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma&gt;5</annotation></semantics></math>) yields gains only at very small budgets (<math alttext=\"0.1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px2.p2.m8\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.1\\%</annotation></semantics></math>) and falls behind even uniform sampling at larger budgets, suggesting that while selective targeting is beneficial, <span class=\"ltx_text ltx_font_italic\">maintaining exploration and diversity is equally crucial</span>.</p>\n\n",
                "matched_terms": [
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F7\" title=\"Figure 7 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> breaks down the accuracy improvements across MMSU categories from Stage&#160;I to Stage&#160;II, showing that the largest statistically significant gains occur in categories such as biology and chemistry. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the density of samples per cluster for the top-<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> clusters selected by the active learning algorithm. Using the LLM-assisted annotation procedure described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS5\" title=\"A.5 Cluster Annotation Pipeline &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>, we assign a domain label to each cluster and find that the most heavily sampled domain is <span class=\"ltx_text ltx_font_italic\">Molecular Biology</span>. These findings support our interpretation that Stage&#160;II boosts performance on these benchmarks by targeting more technical domains. Moreover, they indicate that <span class=\"ltx_text ltx_font_italic\">our active learning algorithm effectively identifies and addresses meaningful domain gaps</span>, leading to measurable performance improvements.</p>\n\n",
                "matched_terms": [
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "performance"
                ]
            }
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Results with mismatched StageÂ II and evaluation speakers (VoiceBench speaker).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">MMSU</span>\n<span class=\"ltx_p\">(VoiceBench)</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">OBQA</span>\n<span class=\"ltx_p\">(VoiceBench)</span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Gap (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Gap (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B Stg. I</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">47.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">14.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">65.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">16.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B Stg. II</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">52.4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">9.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">74.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">7.5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "obqa",
            "stage",
            "salad3b",
            "model",
            "stg",
            "speaker",
            "evaluation",
            "acc",
            "gap",
            "speakers",
            "results",
            "mmsu",
            "voicebench",
            "mismatched"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Both our synthetic data from Stage&#160;II and the evaluations are generated with the same speaker, selected as the highest-quality voice available in our text-to-speech system. A natural concern is that the observed gains from Stage&#160;II might not generalize to other speakers. To address this concern, we evaluated on the original VoiceBench samples from MMSU and OpenBookQA&#8212;the tasks showing the largest improvements in Stage&#160;II&#8212;which use a different synthesizer and a speaker of the opposite gender from the one used in our Stage&#160;II training.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T8\" title=\"Table 8 &#8227; Do the gains of Stage II generalize across speakers? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> reports the results. While performance decreases slightly, a large fraction of the Stage&#160;II gain is preserved, indicating that <span class=\"ltx_text ltx_font_italic\">the improvements are robust and not tied to a single speaker&#8217;s characteristics or synthesizers</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts&#8212;and even cascaded pipelines&#8212;on language understanding tasks. We term this shortfall the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>&#8212;<span class=\"ltx_text ltx_font_bold\">S</span>ample-efficient <span class=\"ltx_text ltx_font_bold\">A</span>lignment with <span class=\"ltx_text ltx_font_bold\">L</span>earning through <span class=\"ltx_text ltx_font_bold\">A</span>ctive selection and cross-modal <span class=\"ltx_text ltx_font_bold\">D</span>istillation&#8212;which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m11\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B LLMs, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.</p>\n\n",
                "matched_terms": [
                    "gap",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities in general knowledge and reasoning, often surpassing specialized systems across a wide range of tasks. This success has motivated efforts to extend LLMs to the speech domain, opening new possibilities for spoken natural interaction. A straightforward approach to extend LLMs to the speech domain is the cascaded pipeline, where automatic speech recognition (ASR) models map speech to text and the LLM is applied to the transcribed text. While effective in preserving text capabilities, cascaded pipelines largely remove speaker and paralinguistic cues essential for natural spoken interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(Maimon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib25\" title=\"\">2025</a>)</cite>. To address this limitation, recent work has explored end-to-end approaches, adapting text-based LLMs to directly process speech inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib31\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib34\" title=\"\">2024</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib13\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib27\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib11\" title=\"\">2024</a>)</cite>. Despite their promise, speech-adapted LLMs struggle with the core requirement of language understanding, consistently under-performing text-based LLMs and even cascaded systems on language understanding tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib8\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>; Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib9\" title=\"\">2025</a>)</cite>.\nWe refer to this shortfall as the <em class=\"ltx_emph ltx_font_italic\">text&#8211;speech understanding gap</em>: the performance drop when a speech-adapted LLM performs a language understanding task in the speech domain compared to the original LLM performing the same task in the text domain.\nClosing this gap is a crucial step toward building AI systems capable of truly natural spoken interaction.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate on broad-domain benchmarks of general knowledge, reasoning, and language understanding commonly used for LLMs, considering both their text and spoken versions: StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA (OBQA) from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite>. We adopt a few-shot prompting approach for evaluation across all tasks, with accuracy as the metric. For further details on the benchmarks, see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS4\" title=\"A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "voicebench",
                    "evaluation",
                    "obqa",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our models by selecting <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from <math alttext=\"\\{0,0.25,0.5,0.75,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,0.25,0.5,0.75,1\\}</annotation></semantics></math>, selecting training data <math alttext=\"{\\mathbb{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mi>&#120123;</mi><annotation encoding=\"application/x-tex\">{\\mathbb{D}}</annotation></semantics></math> from <math alttext=\"\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext>Emilia+LibriHeavy</mtext><mo>,</mo><mtext>FineWeb-Edu</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\text{Emilia+LibriHeavy},\\text{FineWeb-Edu}\\}</annotation></semantics></math>, and adjusting training budget <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> within the range (<math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>B&#8211;<math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math>B tokens). Each model is trained using the hyperparameters described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS2\" title=\"A.2 Training Details: Analyzing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.\nFor each trained model, we measure cross-modal misalignment (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E2\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and forgetting (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.E3\" title=\"In 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) on a test subset of our text&#8211;speech version of FineWeb-Edu, and calculate the average accuracy on the broad-domain benchmarks. We present the results below.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3.F4\" title=\"Figure 4 &#8227; How does the training objective interact with cross-modal alignment and forgetting? &#8227; 3.5 Results &#8227; 3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results with <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1</annotation></semantics></math> and broad-domain (FineWeb-Edu-train) training data. In this setting, misalignment is essentially the quantity being optimized&#8212;distillation directly targets cross-modal consistency&#8212;and, since the evaluation distribution (FineWeb-Edu-test) matches the training distribution, the metric aligns with the objective. Accordingly, domain-matched distillation yields the lowest misalignment and the strongest speech-understanding performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply our method to the Qwen2.5 3B and 7B base LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib30\" title=\"\">2025</a>)</cite>, yielding the <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B and <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-7B models. We follow the experimental setup of Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for architecture, data, and evaluation tasks. We use Emilia and LibriHeavy (<math alttext=\"141,612\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>141</mn><mo>,</mo><mn>612</mn></mrow><annotation encoding=\"application/x-tex\">141,612</annotation></semantics></math> hours) as our <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math>, and use a <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>B-token FineWeb-Edu subset as our <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math>.\nFor Stage&#160;II, we train a clustering model on <math alttext=\"{\\mathbb{D}}_{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>web</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{web}}</annotation></semantics></math> using balanced <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means with <math alttext=\"K=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">K=128</annotation></semantics></math> over <span class=\"ltx_text ltx_font_typewriter\">BAAI/bge-large-en-v1.5</span> embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span>, with a synthesis budget of <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> of <math alttext=\"{\\mathbb{D}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#120123;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\mathbb{D}}_{\\text{speech}}</annotation></semantics></math> and <math alttext=\"\\gamma=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=5</annotation></semantics></math> (Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.E8\" title=\"In 4.1 Method &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We train SALAD models for <math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mn>24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math>B tokens during Stage&#160;I and additional <math alttext=\"1.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mn>1.9</mn><annotation encoding=\"application/x-tex\">1.9</annotation></semantics></math>B tokens during Stage&#160;II. Further training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS3\" title=\"A.3 Training Details: Closing the Text-Speech Gap &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>, and additional ablations are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "salad3b",
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models against the following speech-adapted LLMs from the literature: Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib6\" title=\"\">2024</a>)</cite>, DiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib19\" title=\"\">2025</a>)</cite>, GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib37\" title=\"\">2025</a>)</cite>, and Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib35\" title=\"\">2025</a>)</cite>. We also evaluate against a cascaded pipeline that pairs Whisper-Large-v3 ASR with the Qwen2.5 LLMs used as the backbones of Qwen2.5-Omni and our <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models. The cascade pipeline serves as our topline reference for spoken language understanding. For each model, we report the per-task accuracy as well as the text&#8211;speech gap, defined as the difference between the performance of a speech-adapted LLM given speech input and the performance of the original text-based LLM given the corresponding text input.</p>\n\n",
                "matched_terms": [
                    "gap",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T3\" title=\"Table 3 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the performance of our approach compared to existing baselines and the cascaded pipeline topline. Overall, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves performance competitive with the strongest model, Qwen2.5-Omni, while using over an order of magnitude less speech data (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#footnote2\" title=\"footnote 2 &#8227; Figure 1 &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). In particular, <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>-3B outperforms all larger end-to-end baselines except Qwen2.5-Omni, showing that strong text&#8211;speech alignment can be achieved with much smaller models when combined with our training strategy. Relative to cascaded toplines, the strongest <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> models are competitive, underperforming them only slightly while retaining the advantages of end-to-end modeling.</p>\n\n",
                "matched_terms": [
                    "salad3b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next ask whether targeting the misaligned domains is responsible for the performance gains we observe.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Results &#8227; 4 Closing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the effect of Stage&#160;II training when using our active data selection strategy compared to uniform sampling.\nActive data selection shows greater gains in MMSU, OpenBookQA, and ARC-C. These tasks involve scientific questions and more technical terminology than the others, making them more likely to fall outside the natural speech distribution on which the model is trained in Stage&#160;I (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S2.F2\" title=\"Figure 2 &#8227; 2 Preliminaries &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For more analyses on the effect of active selection see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "model",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied the text&#8211;speech understanding gap, identifying forgetting of text capabilities and cross-modal misalignment as the two factors behind the underperformance of speech-adapted LLMs compared to their text-based counterparts. Building on this analysis, we introduced <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span>, a sample-efficient approach that combines cross-modal distillation with active data selection to target these challenges directly. Our experiments on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>B and <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B models demonstrated that <span class=\"ltx_text ltx_font_smallcaps\">SALAD</span> achieves competitive performance with strong open-weight baselines, while using over an order of magnitude less data. These results suggest that carefully designed objectives and data selection strategies can substantially reduce reliance on costly, massive synthetic data or proprietary speech resources, paving the way for data-efficient methods to close the text&#8211;speech understanding gap.</p>\n\n",
                "matched_terms": [
                    "gap",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Stage&#160;II training of SALAD models, we resume from the last checkpoint before learning-rate decay and continue for 1.9B tokens with a linear decay of the learning rate. An exception is the experiments in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F8\" title=\"Figure 8 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS8\" title=\"A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a>), where training was resumed from the checkpoint after decay and continued for 950M tokens with a constant learning rate fixed to the post-decay value. These runs were conducted before we identified the setup that yielded stronger results for SALAD models.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib17\" title=\"\">2023</a>)</cite>, MMSU and OpenBookQA from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib5\" title=\"\">2024</a>)</cite>, HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib36\" title=\"\">2019</a>)</cite>, ARC-Challenge <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib7\" title=\"\">2018</a>)</cite>, and PIQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib4\" title=\"\">2020</a>)</cite> for evaluation. In all cases, we synthesize spoken versions from the corresponding text data using the Kokoro TTS tokenizer with the <span class=\"ltx_text ltx_font_typewriter\">af-heart</span> speaker&#8212;including for benchmarks such as StoryCloze and VoiceBench&#8212;so as to maintain control over prompt formats, as described below. All are multiple-choice benchmarks. For each task, the model estimates the normalized log probability of each answer given the context, and accuracy is computed by checking whether the highest-scoring option matches the gold answer.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "voicebench",
                    "model",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.T6\" title=\"Table 6 &#8227; A.4 Evaluation Protocol &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the prompt templates used for each task. In the VoiceBench versions of MMSU and OpenBookQA, the model receives the list of answer options as part of the prompt context. However, we observed that smaller models, as well as models from the literature (e.g., DiVA), performed close to random under this setup. This behavior is consistent with prior findings that small language models often struggle with multi-choice formats when answer options are embedded in the input prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Allal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#bib.bib1\" title=\"\">2025</a>)</cite>. To address this limitation, we synthesized our own versions of MMSU and OpenBookQA with controlled prompt formats, including a continuation variant in which the model predicts the correct answer directly. For each model, we report performance under the prompt format that yields the best results. We also observed differences between predicting the answer as the full option or just the letter label of that option. We also evaluate both variants.</p>\n\n",
                "matched_terms": [
                    "voicebench",
                    "model",
                    "results",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since most of our baselines are instruction-tuned models, we adopt when needed each model&#8217;s native chat template: the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Answer:</span>&#8221; segment is assigned to the assistant role, while the remaining context is presented as user input. To mitigate performance differences due to prompting, we further condition models on the task format using few-shot demonstrations. For each task, we include as many demonstrations as can fit in the audio context window of <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p3.m1\" intent=\":literal\"><semantics><mn>2048</mn><annotation encoding=\"application/x-tex\">2048</annotation></semantics></math> tokens, up to a maximum of five. The number of shots used per task is: StoryCloze (5), MMSU (4), OpenBookQA (5), HellaSwag (3), ARC-Challenge (1), and PIQA (5). For each model, we report results with the best-performing prompt format.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SALAD makes use of an active selection algorithm in Stage&#160;II to identify and cover gaps between natural speech and broad-domain text datasets. We analyze the role of this stage and study the impact of its hyperparameters on the overall performance. For these experiments, we apply Stage&#160;II training to the model trained with <math alttext=\"\\alpha=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.0</annotation></semantics></math> in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#S3\" title=\"3 Analyzing the Text-Speech Gap &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. During Stage&#160;II, the model is trained for additional 950M tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F7\" title=\"Figure 7 &#8227; How important is targeting domain gaps as more synthetic data is allowed? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> breaks down the accuracy improvements across MMSU categories from Stage&#160;I to Stage&#160;II, showing that the largest statistically significant gains occur in categories such as biology and chemistry. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.F9\" title=\"Figure 9 &#8227; Does the active learning stage identify meaningful domain gaps? &#8227; A.8 Active Selection Analyses &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the density of samples per cluster for the top-<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS8.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> clusters selected by the active learning algorithm. Using the LLM-assisted annotation procedure described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13632v1#A1.SS5\" title=\"A.5 Cluster Annotation Pipeline &#8227; Appendix A Appendix &#8227; Closing the Gap Between Text and Speech Understanding in LLMs\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>, we assign a domain label to each cluster and find that the most heavily sampled domain is <span class=\"ltx_text ltx_font_italic\">Molecular Biology</span>. These findings support our interpretation that Stage&#160;II boosts performance on these benchmarks by targeting more technical domains. Moreover, they indicate that <span class=\"ltx_text ltx_font_italic\">our active learning algorithm effectively identifies and addresses meaningful domain gaps</span>, leading to measurable performance improvements.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "mmsu"
                ]
            }
        ]
    }
}