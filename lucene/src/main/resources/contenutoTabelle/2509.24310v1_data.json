{
    "S4.T1": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE I: Details of datasets used in terms of division and durations",
        "body": "Corpus\nSubset\nDuration (hours)\n\n\nLibriSpeech [48]\n\ntrain-clean-100\n100.6\n\n\ntrain-clean-360\n363.6\n\n\ndev-clean\n5.4\n\n\n\ntest-clean\n5.4\n\n\nASRU Mandarin [47]\n\ntrain\n444.8\n\n\ndev\n29.3\n\n\ntest\n25.9\n\n\nASRU [47]\n\ntrain\n193.0\n\n\ndev\n21.3\n\n\ntest\n20.4\n\n\nSEAME [46]\n\ntrain\n96.6\n\n\nvalid\n4.9\n\n\ndevman\\text{dev}_{\\texttt{man}}\n7.5\n\n\ndevsge\\text{dev}_{\\texttt{sge}}\n3.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Corpus</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Subset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Duration (hours)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib48\" title=\"\">48</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">train-clean-100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">100.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">train-clean-360</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">363.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">dev-clean</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">5.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">test-clean</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">5.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">ASRU Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">444.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">dev</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">29.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">25.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">ASRU&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">193.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">dev</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">21.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">20.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"4\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">SEAME&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">96.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">valid</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">4.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">7.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">3.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "subset",
            "librispeech",
            "testclean",
            "datasets",
            "hours",
            "asru",
            "used",
            "details",
            "test",
            "mandarin",
            "division",
            "duration",
            "devmantextdevtextttman",
            "train",
            "corpus",
            "valid",
            "terms",
            "dev",
            "trainclean360",
            "devclean",
            "seame",
            "devsgetextdevtextttsge",
            "trainclean100",
            "durations"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "valid",
                    "used",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "test",
                    "used",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "devmantextdevtextttman",
                    "test",
                    "corpus",
                    "seame",
                    "devsgetextdevtextttsge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "trainclean360",
                    "asru",
                    "used",
                    "trainclean100"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "corpus",
                    "seame",
                    "datasets",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "seame",
                    "asru",
                    "datasets",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "librispeech",
                    "datasets",
                    "asru",
                    "seame",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "librispeech",
                    "datasets",
                    "asru",
                    "seame",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "test",
                    "used",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "test",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "test",
                    "mandarin",
                    "librispeech",
                    "testclean",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "test",
                    "mandarin",
                    "seame",
                    "datasets",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "datasets",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "test",
                    "seame",
                    "datasets",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "devmantextdevtextttman",
                    "seame",
                    "datasets",
                    "used",
                    "devsgetextdevtextttsge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "librispeech",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "datasets",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "used",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "valid",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "used",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "mandarin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "mandarin",
                    "hours"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE II: Utterance-level duration ratios, dataset-level token distribution (English words and Mandarin characters) ratios, and CMI of the ASRU and SEAME test sets. CMI values in brackets exclude monolingual utterances",
        "body": "Subset\nDuration ratio (%)\nToken ratio (%)\nCMI\n\n\nMan\nEng\nCS\nMan\nEng\n\n\nASRU test\n0\n0\n100\n89\n11\n11.1\n\n\nSEAME devman\\text{SEAME dev}_{\\texttt{man}}\n14\n7\n79\n74\n26\n16.2 (23.9)\n\n\nSEAME devsge\\text{SEAME dev}_{\\texttt{sge}}\n6\n41\n53\n37\n63\n12.3 (29.8)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Subset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding:0.5pt 4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Duration ratio (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Token ratio (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 4.3pt;\"><span class=\"ltx_text ltx_font_bold\">CMI</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.3pt;\">Man</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.3pt;\">Eng</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.3pt;\">CS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.3pt;\">Man</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.3pt;\">Eng</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">ASRU test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">11.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\"><math alttext=\"\\text{SEAME dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><msub><mtext>SEAME dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{SEAME dev}_{\\texttt{man}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.3pt;\">16.2 (23.9)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.3pt;\"><math alttext=\"\\text{SEAME dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><msub><mtext>SEAME dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{SEAME dev}_{\\texttt{sge}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.3pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.3pt;\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.3pt;\">53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.3pt;\">37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.3pt;\">63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.3pt;\">12.3 (29.8)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sets",
            "subset",
            "eng",
            "datasetlevel",
            "asru",
            "words",
            "man",
            "test",
            "exclude",
            "english",
            "mandarin",
            "devtextttman",
            "devtextttsge",
            "duration",
            "utterances",
            "distribution",
            "ratios",
            "characters",
            "values",
            "brackets",
            "monolingual",
            "token",
            "ratio",
            "cmi",
            "utterancelevel",
            "devsgetextseame",
            "seame",
            "devmantextseame"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "mandarin",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "test",
                    "sets",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "utterances",
                    "test",
                    "monolingual",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "utterances",
                    "mandarin",
                    "english",
                    "characters",
                    "values",
                    "monolingual",
                    "seame",
                    "asru",
                    "words",
                    "cmi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "sets",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "test",
                    "values",
                    "token",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "test",
                    "monolingual",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "test",
                    "monolingual",
                    "mandarin",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "test",
                    "mandarin",
                    "english",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "english",
                    "seame",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "test",
                    "sets",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "cmi",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "utterances",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "cmi",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "cmi",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "cmi",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "asru"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE III: Performance evaluation of state-of-the-art approaches on test data from the ASRU 2019 challenge and SEAME dataset in terms of MER (%). “p​tpt”, “a​d​a​p​t.adapt.”, “f​tft” denote pre-training, adaptation tuning, and full-parameter fine-tuning, respectively. Results marked with “*” are taken from the original publication",
        "body": "Method\n#Train.\np​tpt\nASRU\nSEAME\n\n\nParams.\ntest ↓\\downarrow\n\ndevman\\text{dev}_{\\texttt{man}} ↓\\downarrow\n\n\ndevsge\\text{dev}_{\\texttt{sge}} ↓\\downarrow\n\n\n\nTransformer [57]\n\n29.8 M\n✗\n13.1\n17.7\n24.5\n\n\nConformer [4]\n\n48.3 M\n✗\n12.8\n16.6\n23.3\n\n\n+ LPB [19]\n\n79.9 M\n✗\n11.8\n16.3\n22.9\n\n\n+ ILB [20]\n\n79.9 M\n✗\n11.8\n16.4\n23.2\n\n\n+ LAL [21]\n\n48.3 M\n✗\n11.7\n16.4\n23.3\n\n\n+ LLM GER [58, 21]\n\n48.3 M\n✗\n11.0\n15.7\n22.0\n\n\nBranchformer [59]\n\n39.0 M\n✗\n11.9\n16.4\n23.2\n\n\nE-Branchformer [60]\n\n39.9 M\n✗\n11.8\n16.4\n23.2\n\n\n+ CAMEL [39]\n\n55.3 M\n✗\n11.4\n16.1\n22.8\n\n\nConExtBiMamba* [53]\n\n54.6 M\n✗\n11.5\n16.6\n23.4\n\n\nWhisper-small [9]\n\n-\n✓\n24.9\n72.3\n53.8\n\n\n+ a​d​a​p​t.adapt. w/ LoRA [54]\n\n4.4 M\n✓\n10.2\n13.5\n18.7\n\n\n+ a​d​a​p​t.adapt. w/ Bi-LoRA\n4.4 M\n✓\n10.0\n13.8\n19.3\n\n\n+ f​tft encoder-only\n87.0 M\n✓\n9.8\n14.6\n19.5\n\n\n+ f​tft decoder-only\n153.6 M\n✓\n10.3\n17.4\n25.8\n\n\n+ f​tft full model\n240.6 M\n✓\n8.8\n13.2\n18.4\n\n\n+ w/ SPT* [61]\n\n240.7 M\n✓\n12.8\n13.1\n18.7\n\n\n+ w/ FA-LID-LB [21]\n\n240.6 M\n✓\n8.8\n13.3\n18.3\n\n\n+ w/ FA-LID-UB [21]\n\n240.6 M\n✓\n8.7\n13.2\n18.3\n\n\n+ w/ CTC-LID [62]\n\n240.6 M\n✓\n8.9\n13.7\n18.8\n\n\n+ w/ LAL [21]\n\n240.6 M\n✓\n8.7\n13.3\n18.3",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">#Train.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.1pt;\"><math alttext=\"pt\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">pt</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">ASRU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">SEAME</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">Params.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">test <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_bold\">dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_bold\">dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib57\" title=\"\">57</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">29.8 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">13.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">17.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">24.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">48.3 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">12.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ LPB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">79.9 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">22.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ ILB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">79.9 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ LAL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">48.3 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ LLM GER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib58\" title=\"\">58</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">48.3 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">15.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">22.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">Branchformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib59\" title=\"\">59</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">39.0 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">E-Branchformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib60\" title=\"\">60</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">39.9 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ CAMEL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">55.3 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">22.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">ConExtBiMamba*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">54.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">11.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">16.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">23.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">24.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">53.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"adapt.\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m13\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">adapt.</annotation></semantics></math> w/ LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">4.4 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">10.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"adapt.\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m14\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">adapt.</annotation></semantics></math> w/ Bi-LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">4.4 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">19.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m15\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> encoder-only</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">87.0 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">9.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">14.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">19.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m16\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> decoder-only</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">153.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">10.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">17.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">25.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m17\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> full model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">8.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ w/ SPT*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib61\" title=\"\">61</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.7 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">12.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ w/ FA-LID-LB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">8.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ w/ FA-LID-UB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">8.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ w/ CTC-LID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib62\" title=\"\">62</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">8.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">18.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ w/ LAL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">240.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">8.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.1pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.1pt;\">18.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "llm",
            "f​tft",
            "encoderonly",
            "challenge",
            "p​tpt",
            "publication",
            "↓downarrow",
            "ger",
            "spt",
            "camel",
            "conformer",
            "asru",
            "transformer",
            "stateoftheart",
            "finetuning",
            "whispersmall",
            "ilb",
            "adaptation",
            "lpb",
            "test",
            "lal",
            "“a​d​a​p​tadapt”",
            "from",
            "bilora",
            "denote",
            "iii",
            "falidub",
            "original",
            "results",
            "lora",
            "mer",
            "devmantextdevtextttman",
            "model",
            "params",
            "train",
            "evaluation",
            "dataset",
            "branchformer",
            "conextbimamba",
            "tuning",
            "“p​tpt”",
            "performance",
            "falidlb",
            "approaches",
            "terms",
            "“f​tft”",
            "a​d​a​p​tadapt",
            "full",
            "fullparameter",
            "ebranchformer",
            "marked",
            "method",
            "seame",
            "ctclid",
            "pretraining",
            "data",
            "respectively",
            "decoderonly",
            "devsgetextdevtextttsge",
            "taken"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
            "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
            "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
            "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
            "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "method",
                    "data",
                    "performance",
                    "stateoftheart"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "adaptation",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "from",
                    "data",
                    "original",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "model",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "from",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-specific processing within the ASR decoder typically involves language-specific self-attention computations. This is achieved by using separate token embedding layers for two languages. We define <math alttext=\"\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><mi mathvariant=\"normal\">_</mi><mo>,</mo><mi>n</mi></mrow><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)</annotation></semantics></math> as the token embedding sequence comprising language-specific token embeddings. The language-specific token embeddings <math alttext=\"\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})</annotation></semantics></math> are fed into their respective self-attention module. Here, we use <math alttext=\"n^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime}</annotation></semantics></math> and <math alttext=\"n^{\\prime\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime\\prime}</annotation></semantics></math> to denote the indices within <math alttext=\"\\mathbf{Y}^{mat}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}</annotation></semantics></math>, respectively. The above can be illustrated as</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "method",
                    "data",
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "challenge",
                    "from",
                    "data",
                    "performance",
                    "finetuning",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "model",
                    "from",
                    "method",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "finetuning",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "data",
                    "challenge",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "data",
                    "challenge",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "original",
                    "llm",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "method",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "test",
                    "challenge",
                    "dataset",
                    "seame",
                    "asru",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "devmantextdevtextttman",
                    "test",
                    "dataset",
                    "from",
                    "seame",
                    "data",
                    "devsgetextdevtextttsge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "challenge",
                    "dataset",
                    "from",
                    "asru",
                    "pretraining",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "seame",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "test",
                    "seame",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "model",
                    "branchformer",
                    "conextbimamba",
                    "from",
                    "conformer",
                    "seame",
                    "asru",
                    "pretraining",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "respectively",
                    "model",
                    "from",
                    "bilora",
                    "asru",
                    "seame",
                    "finetuning",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "performance",
                    "mer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "from",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "test",
                    "ebranchformer",
                    "camel",
                    "from",
                    "seame",
                    "asru",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "adaptation",
                    "model",
                    "test",
                    "lal",
                    "lora",
                    "method",
                    "bilora",
                    "asru",
                    "seame",
                    "pretraining",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "test",
                    "performance",
                    "from",
                    "asru",
                    "pretraining",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "mer",
                    "test",
                    "from",
                    "seame",
                    "asru",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "performance",
                    "seame",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "dataset",
                    "from",
                    "seame",
                    "asru",
                    "data",
                    "original",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "finetuning",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "model",
                    "test",
                    "performance",
                    "seame",
                    "asru",
                    "data",
                    "original",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "seame",
                    "asru",
                    "data",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "from",
                    "performance",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "from",
                    "seame",
                    "asru",
                    "data",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "seame",
                    "asru",
                    "data",
                    "performance",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "whispersmall",
                    "model",
                    "test",
                    "performance",
                    "from",
                    "method",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "evaluation",
                    "asru",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "data",
                    "results",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "model",
                    "from",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "from",
                    "method",
                    "asru",
                    "data",
                    "original",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "dataset",
                    "method",
                    "asru",
                    "data",
                    "original",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "data",
                    "performance",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "llm",
                    "model",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "data",
                    "performance",
                    "approaches"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE IV: Evaluation of multilingual ASR models pre-trained on monolingual and code-switching data, with results reported before and after fine-tuning on code-switching data. Results marked with “*” are taken from the original publication",
        "body": "Method\nMonolingual\nCS\n\n\nASRU Man.↓\\downarrow\n\nLS-test↓\\downarrow\n\nASRU test↓\\downarrow\n\n\n\nConformer-L Mix [4]\n\n4.8\n5.7\n8.8\n\n\nConformer-L Mono [4]\n\n3.7\n4.5\n33.9\n\n\n+ f​tft ASRU\n19.9\n87.0\n11.2\n\n\nBi-encoder* [12]\n\n3.3\n9.9\n9.8\n\n\n+ f​tft ASRU* [12]\n\n5.4\n28.0\n9.3\n\n\nWhisper-small [9]\n\n19.0\n3.4\n24.9\n\n\n+ f​tft ASRU\n15.8\n13.3\n8.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Monolingual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 5.7pt;\"><span class=\"ltx_text ltx_font_bold\">CS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">ASRU Man.<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">LS-test<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">ASRU test<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">Conformer-L Mix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">4.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">5.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">8.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">Conformer-L Mono&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">3.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">4.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">33.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">19.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">87.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">11.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">Bi-encoder*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">3.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">9.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">9.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">28.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.7pt;\">9.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">19.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">3.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.7pt;\">24.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.7pt;\">15.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.7pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.7pt;\">8.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "f​tft",
            "publication",
            "conformerl",
            "asru",
            "pretrained",
            "finetuning",
            "lstest↓downarrow",
            "codeswitching",
            "whispersmall",
            "mix",
            "mono",
            "from",
            "original",
            "multilingual",
            "results",
            "before",
            "evaluation",
            "test↓downarrow",
            "monolingual",
            "asr",
            "reported",
            "biencoder",
            "models",
            "marked",
            "method",
            "man↓downarrow",
            "data",
            "after",
            "taken"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
            "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "from",
                    "method",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "monolingual",
                    "data",
                    "multilingual",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "from",
                    "pretrained",
                    "data",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "codeswitching",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "from",
                    "pretrained",
                    "data",
                    "original",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}_{\\mathrm{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>asr</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{asr}}</annotation></semantics></math> denotes the ASR loss, <math alttext=\"\\mathcal{L}_{\\mathrm{lang}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>lang</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{lang}}</annotation></semantics></math> represents the loss from the auxiliary language-aware task, and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is a multi-task learning parameter.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another important line of research focuses on incorporating language-specific processing within CS-ASR models. Language-specific encoders process the acoustic feature vectors <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> separately into hidden representations <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{t}\\in\\mathbb{R}^{D}\\mid t=1,\\ldots,T^{\\prime})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>T</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{t}\\in\\mathbb{R}^{D}\\mid t=1,\\ldots,T^{\\prime})</annotation></semantics></math> before performing a weighted sum of them. The weights <math alttext=\"\\mathbf{A}^{mat/emb}=(a_{t}\\in[0,1]\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119808;</mi><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}^{mat/emb}=(a_{t}\\in[0,1]\\mid t=1,\\ldots,T)</annotation></semantics></math> are frame-level language posteriors computed by a built-in LID module. These are computed via</p>\n\n",
                "matched_terms": [
                    "models",
                    "before"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "from",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "biencoder",
                    "before",
                    "monolingual",
                    "method",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "monolingual",
                    "from",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "monolingual",
                    "from",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "method",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "codeswitching",
                    "finetuning",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "data",
                    "multilingual",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "from",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs present a promising solution for this task due to their inherent multilingual and translation capabilities. However, unconstrained generation may lead to unnatural or linguistically implausible switches. To address this, linguistic constraints such as the ECT can be incorporated to guide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib45\" title=\"\">45</a>]</cite>. ECT constrains code-switching to occur only at points where the grammatical structures of both languages align. When implemented in prompt-based generation, ECT can be simplified since LLMs already internalize linguistic regularities to maintain syntactic compatibility and naturalness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "original",
                    "codeswitching",
                    "from",
                    "monolingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "monolingual",
                    "from",
                    "method",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "data",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "codeswitching",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "monolingual",
                    "from",
                    "asru",
                    "multilingual",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "monolingual",
                    "asru",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "monolingual",
                    "asru",
                    "data",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "mix",
                    "models",
                    "monolingual",
                    "conformerl",
                    "from",
                    "asru",
                    "pretrained",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "biencoder",
                    "models",
                    "from",
                    "asru",
                    "pretrained",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "before",
                    "from",
                    "data",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "codeswitching"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reported",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "from",
                    "method",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "models",
                    "asru",
                    "data",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "before",
                    "models",
                    "from",
                    "asru",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "models",
                    "monolingual",
                    "method",
                    "asru",
                    "pretrained",
                    "multilingual",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "models",
                    "whispersmall",
                    "from",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asru",
                    "data",
                    "original",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "finetuning",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "original",
                    "results",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "original",
                    "data",
                    "from",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "results",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "whispersmall",
                    "from",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "from",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "results",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "whispersmall",
                    "models",
                    "from",
                    "asru",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "asru",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "before",
                    "from",
                    "method",
                    "finetuning",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "results",
                    "asru",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "results",
                    "data",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "asru",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "method",
                    "asru",
                    "data",
                    "original",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "method",
                    "asru",
                    "data",
                    "original",
                    "finetuning",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "data",
                    "finetuning",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "models",
                    "codeswitching",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "data"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE V: Comparison of fine-tuning the Whisper-small model on real, synthesized speech, and data augmented with synthesized speech. Here, the target texts and prompt samples are sourced from the original ASRU dataset. Real and synthesized data are denoted as “Real” and “Syn.”, respectively",
        "body": "Method\nTTS\nASRU\nSEAME\n\n\nfine-tune\ntest↓\\downarrow\n\n\ndevman\\text{dev}_{\\texttt{man}}↓\\downarrow\n\n\ndevsge\\text{dev}_{\\texttt{sge}}↓\\downarrow\n\n\n\nWhisper-small\n-\n24.9\n72.3\n53.8\n\n\n+ f​tft Syn. w/ Real text\n✗\n11.0\n26.3\n36.1\n\n\n+ f​tft Syn. w/ Real text\n✓\n10.5\n15.3\n20.9\n\n\n+ f​tft Real data\n-\n8.8\n13.2\n18.4\n\n\n+ Syn. w/ Real text\n✓\n9.3\n13.0\n18.3\n\n\n+ 2×\\times Syn. w/ Real text\n✓\n9.3\n12.9\n18.0\n\n\n+ 3×\\times Syn. w/ Real text\n✓\n9.4\n12.7\n17.7",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">ASRU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">SEAME</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">fine-tune</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">test<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">Whisper-small</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">24.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">53.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">11.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">26.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">36.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m7\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">10.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">15.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">20.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m8\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Real data</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.1pt;\"><span class=\"ltx_text ltx_font_bold\">8.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">13.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.1pt;\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">+ Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">9.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">13.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">18.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">+ 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m9\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">9.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">12.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.1pt;\">18.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">+ 3<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.1pt;\">9.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.1pt;\">12.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.1pt;\">17.7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "real",
            "f​tft",
            "text",
            "devmantextdevtextttman↓downarrow",
            "devsgetextdevtextttsge↓downarrow",
            "target",
            "asru",
            "finetuning",
            "finetune",
            "denoted",
            "speech",
            "whispersmall",
            "texts",
            "sourced",
            "synthesized",
            "tts",
            "here",
            "from",
            "original",
            "model",
            "3×times",
            "2×times",
            "test↓downarrow",
            "dataset",
            "samples",
            "“syn”",
            "prompt",
            "method",
            "“real”",
            "seame",
            "augmented",
            "data",
            "respectively",
            "comparison",
            "syn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
            "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
            "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
            "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "tts",
                    "from",
                    "method",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "tts",
                    "from",
                    "data",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "tts",
                    "from",
                    "data",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "from",
                    "denoted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-specific processing within the ASR decoder typically involves language-specific self-attention computations. This is achieved by using separate token embedding layers for two languages. We define <math alttext=\"\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><mi mathvariant=\"normal\">_</mi><mo>,</mo><mi>n</mi></mrow><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)</annotation></semantics></math> as the token embedding sequence comprising language-specific token embeddings. The language-specific token embeddings <math alttext=\"\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})</annotation></semantics></math> are fed into their respective self-attention module. Here, we use <math alttext=\"n^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime}</annotation></semantics></math> and <math alttext=\"n^{\\prime\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime\\prime}</annotation></semantics></math> to denote the indices within <math alttext=\"\\mathbf{Y}^{mat}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}</annotation></semantics></math>, respectively. The above can be illustrated as</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "method",
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "from",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "tts",
                    "synthesized",
                    "from",
                    "method",
                    "target",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "text",
                    "tts",
                    "from",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "tts",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we propose to integrate a simplified version of ECT (SECT) into the prompt design for LLM-based code-switching text generation. Beyond leveraging the inherent syntactic knowledge of LLM, SECT offers improved computational efficiency and avoids imposing overly rigid constraints compared to ECT.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "text",
                    "from",
                    "target",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "text",
                    "from",
                    "method",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "from",
                    "seame",
                    "data",
                    "denoted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "from",
                    "dataset",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "dataset",
                    "seame",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "data",
                    "respectively",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "respectively",
                    "model",
                    "from",
                    "seame",
                    "asru",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "model",
                    "tts",
                    "from",
                    "target",
                    "asru",
                    "seame",
                    "data",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "respectively",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "real",
                    "text",
                    "from",
                    "method",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "finetuning",
                    "comparison",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "method",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "from",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "from",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "seame",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "real",
                    "model",
                    "synthesized",
                    "tts",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "respectively",
                    "model",
                    "text",
                    "synthesized",
                    "seame",
                    "asru",
                    "augmented",
                    "data",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "synthesized",
                    "from",
                    "seame",
                    "asru",
                    "data",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whispersmall",
                    "texts",
                    "model",
                    "synthesized",
                    "from",
                    "target",
                    "seame",
                    "data",
                    "respectively",
                    "finetune"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "tts",
                    "dataset",
                    "from",
                    "target",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "model",
                    "text",
                    "tts",
                    "dataset",
                    "from",
                    "target",
                    "asru",
                    "seame",
                    "data",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "prompt",
                    "model",
                    "text",
                    "tts",
                    "dataset",
                    "from",
                    "seame",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "seame",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whispersmall",
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "from",
                    "method",
                    "finetuning",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison",
                    "real",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "real",
                    "text",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "texts",
                    "model",
                    "text",
                    "tts",
                    "from",
                    "target",
                    "asru",
                    "data",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "text",
                    "from",
                    "method",
                    "target",
                    "asru",
                    "data",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "tts",
                    "from",
                    "data"
                ]
            }
        ]
    },
    "S5.T6": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE VI: Comparison of textual and acoustic impacts on synthesized data for fine-tuning the Whisper-small model. Text and Prompt denote the source of synthesized speech and the prompt speech, respectively. TTS f​tft denotes the data type used for fine-tuning the TTS model",
        "body": "Mismatch\nTarget\nReference\n\nTTS f​tft\n\nSEAME\nASRU\n\n\n\ndevman\\text{dev}_{\\texttt{man}}↓\\downarrow\n\n\ndevsge\\text{dev}_{\\texttt{sge}}↓\\downarrow\n\ntest↓\\downarrow\n\n\n\n-\nSEAME\nSEAME\nSEAME\n15.3\n20.9\n-\n\n\nAccent\nSEAME\nASRU\nSEAME\n16.1\n22.1\n-\n\n\nAccent\nSEAME\nLibriSpeech\nSEAME\n17.4\n25.5\n-\n\n\nText\nASRU\nSEAME\nSEAME\n35.1\n68.2\n-\n\n\n-\nASRU\nASRU\nASRU\n-\n-\n10.1\n\n\nAccent\nASRU\nSEAME\nASRU\n-\n-\n10.9\n\n\nText\nSEAME\nASRU\nASRU\n-\n-\n16.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Mismatch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Target</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Reference</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\">TTS</span> <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m3\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">SEAME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">ASRU</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m4\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">\n<math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m6\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">test<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">15.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">20.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">16.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">22.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">LibriSpeech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">17.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">25.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">35.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">68.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.8pt;\">10.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.8pt;\">10.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">SEAME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.8pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.8pt;\">16.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "f​tft",
            "impacts",
            "source",
            "text",
            "accent",
            "librispeech",
            "devmantextdevtextttman↓downarrow",
            "devsgetextdevtextttsge↓downarrow",
            "target",
            "asru",
            "finetuning",
            "used",
            "speech",
            "whispersmall",
            "synthesized",
            "tts",
            "reference",
            "denote",
            "textual",
            "model",
            "test↓downarrow",
            "denotes",
            "mismatch",
            "prompt",
            "seame",
            "acoustic",
            "data",
            "respectively",
            "comparison"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "model",
                    "text",
                    "tts",
                    "accent",
                    "data",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "accent",
                    "acoustic",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "accent",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "text",
                    "tts",
                    "acoustic",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "model",
                    "text",
                    "tts",
                    "accent",
                    "data",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each token probability is conditioned on the acoustic features and the previously generated tokens. This formulation corresponds to the attention-based sequence-to-sequence model, in which decoding is performed autoregressively. The predicted sequence <math alttext=\"\\widehat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\widehat{Y}</annotation></semantics></math> is obtained by maximizing the above conditional probability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-specific processing within the ASR decoder typically involves language-specific self-attention computations. This is achieved by using separate token embedding layers for two languages. We define <math alttext=\"\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><mi mathvariant=\"normal\">_</mi><mo>,</mo><mi>n</mi></mrow><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)</annotation></semantics></math> as the token embedding sequence comprising language-specific token embeddings. The language-specific token embeddings <math alttext=\"\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})</annotation></semantics></math> are fed into their respective self-attention module. Here, we use <math alttext=\"n^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime}</annotation></semantics></math> and <math alttext=\"n^{\\prime\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime\\prime}</annotation></semantics></math> to denote the indices within <math alttext=\"\\mathbf{Y}^{mat}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}</annotation></semantics></math>, respectively. The above can be illustrated as</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "finetuning",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "tts",
                    "synthesized",
                    "reference",
                    "target",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "prompt",
                    "text",
                    "tts",
                    "accent",
                    "reference",
                    "target",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we propose to integrate a simplified version of ECT (SECT) into the prompt design for LLM-based code-switching text generation. Beyond leveraging the inherent syntactic knowledge of LLM, SECT offers improved computational efficiency and avoids imposing overly rigid constraints compared to ECT.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "model",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "seame",
                    "asru",
                    "data",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "used",
                    "librispeech",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "seame",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "data",
                    "respectively",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech",
                    "seame",
                    "asru",
                    "data",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "respectively",
                    "model",
                    "librispeech",
                    "seame",
                    "asru",
                    "finetuning",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "model",
                    "tts",
                    "librispeech",
                    "target",
                    "asru",
                    "seame",
                    "data",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "respectively",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "finetuning",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "model",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "used",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "librispeech",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "seame",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "text",
                    "tts",
                    "target",
                    "asru",
                    "seame",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "synthesized",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "model",
                    "text",
                    "synthesized",
                    "seame",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "synthesized",
                    "seame",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whispersmall",
                    "model",
                    "accent",
                    "synthesized",
                    "reference",
                    "target",
                    "seame",
                    "data",
                    "respectively",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "model",
                    "text",
                    "synthesized",
                    "tts",
                    "target",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "mismatch",
                    "prompt",
                    "model",
                    "text",
                    "accent",
                    "tts",
                    "librispeech",
                    "target",
                    "asru",
                    "seame",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "whispersmall",
                    "textual",
                    "prompt",
                    "model",
                    "text",
                    "accent",
                    "tts",
                    "seame",
                    "asru",
                    "acoustic",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "model",
                    "text",
                    "accent",
                    "seame",
                    "asru",
                    "acoustic",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whispersmall",
                    "model",
                    "text",
                    "synthesized",
                    "finetuning",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "data",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "tts",
                    "reference",
                    "target",
                    "asru",
                    "data",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "source",
                    "text",
                    "target",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "synthesized",
                    "tts",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "finetuning",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "synthesized",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "model",
                    "text",
                    "tts",
                    "accent",
                    "data"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE VII: Comparison of code-switching text synthesis with different LLMs and prompts, performance is evaluated with code-switching samples synthesized via 1500 ASRU Mandarin text samples, where the fine-tuned model is tested on the ASRU test set. The CMI of the ASRU training set is 17.0",
        "body": "Method\nASR\nGPT-4o\nCMI\n\n\nWER↓\\downarrow\nscore↑\\uparrow\n\ng​tgt: 17.0\n\n\nDeepSeek-R1 w/ baseline prompt\n22.9\n6.8\n32.2\n\n\nDeepSeek-R1 w/ EZSwitch prompt [33]\n\n22.2\n6.0\n24.6\n\n\nDeepSeek-R1 w/ SECT prompt (ours)\n19.2\n7.0\n17.3\n\n\nw/o in-context learning examples\n19.6\n7.0\n15.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">ASR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">GPT-4o</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">CMI</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">score<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">\n<math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math>: 17.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">DeepSeek-R1 w/ baseline prompt</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">22.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">6.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">32.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">DeepSeek-R1 w/ EZSwitch prompt&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">22.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">24.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">DeepSeek-R1 w/ SECT prompt (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">19.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">17.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">w/o in-context learning examples</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">19.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">15.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "text",
            "via",
            "incontext",
            "ours",
            "g​tgt",
            "sect",
            "asru",
            "gpt4o",
            "ezswitch",
            "codeswitching",
            "evaluated",
            "test",
            "llms",
            "synthesized",
            "mandarin",
            "learning",
            "where",
            "score↑uparrow",
            "tested",
            "baseline",
            "synthesis",
            "model",
            "examples",
            "prompts",
            "cmi",
            "performance",
            "samples",
            "wer↓downarrow",
            "set",
            "asr",
            "prompt",
            "finetuned",
            "different",
            "vii",
            "method",
            "deepseekr1",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "text",
                    "llms",
                    "via",
                    "learning",
                    "different",
                    "method",
                    "performance",
                    "sect",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "synthesis",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "text",
                    "codeswitching",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "text",
                    "llms",
                    "mandarin",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "text",
                    "llms",
                    "learning",
                    "different",
                    "comparison",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "via",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each token probability is conditioned on the acoustic features and the previously generated tokens. This formulation corresponds to the attention-based sequence-to-sequence model, in which decoding is performed autoregressively. The predicted sequence <math alttext=\"\\widehat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\widehat{Y}</annotation></semantics></math> is obtained by maximizing the above conditional probability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the above approach rarely makes explicit use of language information. To mitigate this limitation, recent studies have explored enriching the model via multi-task learning with auxiliary language-aware tasks, such as LD or LID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>. These tasks are typically performed at the frame or token level to achieve fine-grained language change detection. The CS-ASR model is then optimized with a multi-task objective, formulated as the weighted sum of the ASR loss and the auxiliary language-aware loss as</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model",
                    "via",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}_{\\mathrm{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>asr</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{asr}}</annotation></semantics></math> denotes the ASR loss, <math alttext=\"\\mathcal{L}_{\\mathrm{lang}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>lang</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{lang}}</annotation></semantics></math> represents the loss from the auxiliary language-aware task, and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is a multi-task learning parameter.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "learning",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "learning",
                    "method",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "text",
                    "where",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "synthesized",
                    "method",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "synthesis",
                    "mandarin",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "text",
                    "via",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "codeswitching",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "text",
                    "codeswitching",
                    "via",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs present a promising solution for this task due to their inherent multilingual and translation capabilities. However, unconstrained generation may lead to unnatural or linguistically implausible switches. To address this, linguistic constraints such as the ECT can be incorporated to guide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib45\" title=\"\">45</a>]</cite>. ECT constrains code-switching to occur only at points where the grammatical structures of both languages align. When implemented in prompt-based generation, ECT can be simplified since LLMs already internalize linguistic regularities to maintain syntactic compatibility and naturalness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "llms",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we propose to integrate a simplified version of ECT (SECT) into the prompt design for LLM-based code-switching text generation. Beyond leveraging the inherent syntactic knowledge of LLM, SECT offers improved computational efficiency and avoids imposing overly rigid constraints compared to ECT.</p>\n\n",
                "matched_terms": [
                    "text",
                    "codeswitching",
                    "sect",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "model",
                    "text",
                    "mandarin",
                    "incontext",
                    "via",
                    "learning",
                    "examples",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "training",
                    "text",
                    "llms",
                    "method",
                    "sect",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "evaluated",
                    "training",
                    "test",
                    "asru",
                    "performance",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "set",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "asru",
                    "performance",
                    "set",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "text",
                    "mandarin",
                    "asru",
                    "cmi",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "test",
                    "where",
                    "asru",
                    "cmi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "finetuned",
                    "mandarin",
                    "learning",
                    "asru",
                    "prompts",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "synthesis",
                    "model",
                    "finetuned",
                    "mandarin",
                    "different",
                    "asru",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "text",
                    "llms",
                    "via",
                    "examples",
                    "deepseekr1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mandarin",
                    "evaluated",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "text",
                    "llms",
                    "method",
                    "gpt4o",
                    "cmi",
                    "performance",
                    "samples",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "performance",
                    "different",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "baseline",
                    "asru",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "test",
                    "learning",
                    "via",
                    "asru",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "method",
                    "asru",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "test",
                    "mandarin",
                    "asru",
                    "performance",
                    "set",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "mandarin",
                    "asru",
                    "performance",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "performance",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "synthesis",
                    "text",
                    "different",
                    "asru",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "synthesis",
                    "model",
                    "synthesized",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "test",
                    "finetuned",
                    "text",
                    "synthesized",
                    "asru",
                    "performance",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "training",
                    "synthesized",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "synthesized",
                    "prompts",
                    "cmi",
                    "performance",
                    "set",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "finetuned",
                    "text",
                    "synthesized",
                    "where",
                    "baseline",
                    "prompts",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "finetuned",
                    "text",
                    "different",
                    "asru",
                    "prompts",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "model",
                    "text",
                    "asru",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "text",
                    "different",
                    "asru",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "model",
                    "test",
                    "text",
                    "synthesized",
                    "via",
                    "method",
                    "gpt4o",
                    "cmi",
                    "performance",
                    "samples",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "prompt",
                    "text",
                    "incontext",
                    "learning",
                    "examples",
                    "baseline",
                    "asru",
                    "gpt4o",
                    "cmi",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "text",
                    "asru",
                    "prompts",
                    "samples",
                    "set",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "text",
                    "via",
                    "method",
                    "asru",
                    "prompts",
                    "sect",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "training",
                    "text",
                    "llms",
                    "synthesized",
                    "mandarin",
                    "method",
                    "asru",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "text",
                    "synthesized",
                    "mandarin",
                    "where",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "text",
                    "synthesized",
                    "mandarin",
                    "deepseekr1",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "training",
                    "text",
                    "llms",
                    "learning",
                    "different",
                    "where",
                    "performance",
                    "sect"
                ]
            }
        ]
    },
    "S5.T8": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE VIII: Prompts used for code-switching text generation and evaluation",
        "body": "Method\nPrompt\n\n\n\n\nbaseline\n\n\nYou are a bilingual English–Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a natural code-switching sentence.\n\n\n\n\n\n\n\n<Input Sentence>\n\n\n\n\nEZSwitch\n\n\nYou are a bilingual English-Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a code-mixed sentence with Romanized English and Mandarin with specific keywords that should appear.\n\n\n\n\n\n\n\n<Input Sentence>\n\n\n\n\n\n\n\nWords wanted: <List of Words>\n\n\n\n\nSECT (ours)\n\n\nYou are a bilingual English–Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a natural code-switching sentence. Please strictly follow the rules below:\n1. Preserve the full meaning of the original sentence.\n2. At most two English words or short phrases may substitute for Mandarin words in each sentence.\n3. The code-switching output must be natural and fluent, consistent with the daily speech habits of Mandarin–English bilinguals.\nThere are three examples:\nExample 1. Mandarin: … Code-switching: …\nExample 2. Mandarin: … Code-switching: …\nExample 3. Mandarin: … Code-switching: …\nPlease output the code-switching sentence directly without extra text or explanation.:\n\n\n\n\n\n\n\n<Input Sentence>\n\n\n\n\nGPT Eval\n\n\nYou are an expert evaluator of code-switched text quality. You will evaluate Mandarin-English code-switched sentences based on the following criteria:\n1. Naturalness: How natural and fluent does the code-switched sentence sound?\n2. Grammar: Is the sentence grammatically correct in both languages?\n3. Semantic preservation: Does the code-switched version preserve the meaning of the original Mandarin?\n4. Code-switching appropriateness: Are the language switches natural and appropriate?\nPlease provide a single score from 1 to 10 (where 10 is excellent and 1 is very poor).\nReturn ONLY the numerical score as a single digit, nothing else.\n\n\n\n\n\n\n\nOriginal Mandarin: <Original Mandarin>\n\n\n\n\n\n\n\nCode-Switching text: <Code-Switching text>",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding:0.5pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prompt</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">baseline</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are a bilingual English&#8211;Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a natural code-switching sentence.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_typewriter\">&lt;Input Sentence&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">EZSwitch</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are a bilingual English-Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a code-mixed sentence with Romanized English and Mandarin with specific keywords that should appear.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_typewriter\">&lt;Input Sentence&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Words wanted: <span class=\"ltx_text ltx_font_typewriter\">&lt;List of Words&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">SECT (ours)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are a bilingual English&#8211;Mandarin code-switching expert. Your task is to translate the given Mandarin sentence into a natural code-switching sentence. Please strictly follow the rules below:</span>\n<span class=\"ltx_p\">1. Preserve the full meaning of the original sentence.</span>\n<span class=\"ltx_p\">2. At most two English words or short phrases may substitute for Mandarin words in each sentence.</span>\n<span class=\"ltx_p\">3. The code-switching output must be natural and fluent, consistent with the daily speech habits of Mandarin&#8211;English bilinguals.</span>\n<span class=\"ltx_p\">There are three examples:</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Example 1. Mandarin: &#8230; Code-switching: &#8230;</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Example 2. Mandarin: &#8230; Code-switching: &#8230;</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Example 3. Mandarin: &#8230; Code-switching: &#8230;</span></span>\n<span class=\"ltx_p\">Please output the code-switching sentence directly without extra text or explanation.:</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_typewriter\">&lt;Input Sentence&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">GPT Eval</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are an expert evaluator of code-switched text quality. You will evaluate Mandarin-English code-switched sentences based on the following criteria:</span>\n<span class=\"ltx_p\">1. Naturalness: How natural and fluent does the code-switched sentence sound?</span>\n<span class=\"ltx_p\">2. Grammar: Is the sentence grammatically correct in both languages?</span>\n<span class=\"ltx_p\">3. Semantic preservation: Does the code-switched version preserve the meaning of the original Mandarin?</span>\n<span class=\"ltx_p\">4. Code-switching appropriateness: Are the language switches natural and appropriate?</span>\n<span class=\"ltx_p\">Please provide a single score from 1 to 10 (where 10 is excellent and 1 is very poor).\nReturn ONLY the numerical score as a single digit, nothing else.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Original Mandarin: <span class=\"ltx_text ltx_font_typewriter\">&lt;Original Mandarin&gt;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding:0.5pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Code-Switching text: <span class=\"ltx_text ltx_font_typewriter\">&lt;Code-Switching text&gt;</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "meaning",
            "list",
            "codemixed",
            "excellent",
            "englishmandarin",
            "appear",
            "digit",
            "wanted",
            "gpt",
            "mandarin",
            "sentences",
            "else",
            "translate",
            "evaluation",
            "there",
            "semantic",
            "how",
            "method",
            "short",
            "sect",
            "based",
            "appropriateness",
            "strictly",
            "evaluate",
            "very",
            "mandarin–english",
            "follow",
            "appropriate",
            "given",
            "following",
            "original",
            "language",
            "directly",
            "examples",
            "specific",
            "prompts",
            "expert",
            "only",
            "sound",
            "two",
            "english–mandarin",
            "numerical",
            "preserve",
            "languages",
            "bilingual",
            "text",
            "sentence",
            "phrases",
            "output",
            "input",
            "single",
            "please",
            "score",
            "will",
            "speech",
            "codeswitching",
            "each",
            "grammatically",
            "most",
            "preservation",
            "where",
            "you",
            "daily",
            "baseline",
            "from",
            "nothing",
            "provide",
            "generation",
            "version",
            "keywords",
            "does",
            "into",
            "prompt",
            "full",
            "poor",
            "your",
            "habits",
            "eval",
            "criteria",
            "grammar",
            "task",
            "quality",
            "evaluator",
            "viii",
            "ours",
            "words",
            "used",
            "ezswitch",
            "three",
            "substitute",
            "example",
            "consistent",
            "rules",
            "english",
            "both",
            "romanized",
            "extra",
            "mandarinenglish",
            "return",
            "codeswitched",
            "bilinguals",
            "correct",
            "without",
            "switches",
            "explanation",
            "below",
            "natural",
            "must",
            "fluent"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
            "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "codeswitching",
                    "quality",
                    "text",
                    "from",
                    "specific",
                    "method",
                    "both",
                    "languages",
                    "used",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "language",
                    "codeswitched",
                    "sentence",
                    "two",
                    "single",
                    "both",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "codeswitching",
                    "text",
                    "two",
                    "from",
                    "both",
                    "languages",
                    "bilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "language",
                    "from",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "codeswitching",
                    "sentences",
                    "task",
                    "text",
                    "mandarin",
                    "english",
                    "from",
                    "both",
                    "languages",
                    "original",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "codeswitching",
                    "text",
                    "from",
                    "both",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "given",
                    "input",
                    "where",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each token probability is conditioned on the acoustic features and the previously generated tokens. This formulation corresponds to the attention-based sequence-to-sequence model, in which decoding is performed autoregressively. The predicted sequence <math alttext=\"\\widehat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\widehat{Y}</annotation></semantics></math> is obtained by maximizing the above conditional probability.</p>\n\n",
                "matched_terms": [
                    "each",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}_{\\mathrm{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>asr</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{asr}}</annotation></semantics></math> denotes the ASR loss, <math alttext=\"\\mathcal{L}_{\\mathrm{lang}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>lang</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{lang}}</annotation></semantics></math> represents the loss from the auxiliary language-aware task, and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is a multi-task learning parameter.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another important line of research focuses on incorporating language-specific processing within CS-ASR models. Language-specific encoders process the acoustic feature vectors <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> separately into hidden representations <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{t}\\in\\mathbb{R}^{D}\\mid t=1,\\ldots,T^{\\prime})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>T</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{t}\\in\\mathbb{R}^{D}\\mid t=1,\\ldots,T^{\\prime})</annotation></semantics></math> before performing a weighted sum of them. The weights <math alttext=\"\\mathbf{A}^{mat/emb}=(a_{t}\\in[0,1]\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119808;</mi><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}^{mat/emb}=(a_{t}\\in[0,1]\\mid t=1,\\ldots,T)</annotation></semantics></math> are frame-level language posteriors computed by a built-in LID module. These are computed via</p>\n\n",
                "matched_terms": [
                    "language",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "each",
                    "language",
                    "output",
                    "input",
                    "where",
                    "from",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-specific processing within the ASR decoder typically involves language-specific self-attention computations. This is achieved by using separate token embedding layers for two languages. We define <math alttext=\"\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><mi mathvariant=\"normal\">_</mi><mo>,</mo><mi>n</mi></mrow><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mix}=(\\mathbf{y}_{\\_,n}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)</annotation></semantics></math> as the token embedding sequence comprising language-specific token embeddings. The language-specific token embeddings <math alttext=\"\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}=(\\mathbf{y}_{n^{\\prime},n}^{mat}\\in\\mathbb{R}^{D}\\mid n^{\\prime}=1,\\ldots,N^{mat})</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>,</mo><mi>n</mi></mrow><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>N</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}=(\\mathbf{y}_{n^{\\prime\\prime},n}^{emb}\\in\\mathbb{R}^{D}\\mid n^{\\prime\\prime}=1,\\ldots,N^{emb})</annotation></semantics></math> are fed into their respective self-attention module. Here, we use <math alttext=\"n^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime}</annotation></semantics></math> and <math alttext=\"n^{\\prime\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msup><mi>n</mi><mo>&#8242;&#8242;</mo></msup><annotation encoding=\"application/x-tex\">n^{\\prime\\prime}</annotation></semantics></math> to denote the indices within <math alttext=\"\\mathbf{Y}^{mat}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{mat}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}^{emb}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{emb}</annotation></semantics></math>, respectively. The above can be illustrated as</p>\n\n",
                "matched_terms": [
                    "languages",
                    "two",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathrm{SelfAtten}^{mat}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m8\" intent=\":literal\"><semantics><mrow><msup><mi>SelfAtten</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{SelfAtten}^{mat}(\\cdot)</annotation></semantics></math> and <math alttext=\"\\mathrm{SelfAtten}^{emb}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><msup><mi>SelfAtten</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{SelfAtten}^{emb}(\\cdot)</annotation></semantics></math> denote the computations within the self-attention module for matrix and embedded languages. The embeddings <math alttext=\"\\mathbf{Y}_{o}^{mat}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>o</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{o}^{mat}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{o}^{emb}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>o</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{o}^{emb}</annotation></semantics></math> represent the outputs of the respective self-attention modules. They are subsequently combined back into a unified <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m12\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>-length embedding sequence <math alttext=\"\\mathbf{Y}_{o}^{mix}=(\\mathbf{y}_{\\_,n,o}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p2.m13\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119832;</mi><mi>o</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119858;</mi><mrow><mi mathvariant=\"normal\">_</mi><mo>,</mo><mi>n</mi><mo>,</mo><mi>o</mi></mrow><mrow><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>/</mo><mi>e</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{o}^{mix}=(\\mathbf{y}_{\\_,n,o}^{mat/emb}\\in\\mathbb{R}^{D}\\mid n=1,\\ldots,N)</annotation></semantics></math> before the cross-attention computations.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "into",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "language",
                    "each",
                    "codeswitching",
                    "example",
                    "task",
                    "two",
                    "single",
                    "method",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "from",
                    "languages",
                    "bilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "appropriate",
                    "language",
                    "codeswitching",
                    "each",
                    "example",
                    "text",
                    "where",
                    "from",
                    "languages",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "text",
                    "input",
                    "from",
                    "method",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "english",
                    "mandarin",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "prompt",
                    "evaluate",
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "language",
                    "task",
                    "text",
                    "two",
                    "single",
                    "natural",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "language",
                    "directly",
                    "text",
                    "from",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs present a promising solution for this task due to their inherent multilingual and translation capabilities. However, unconstrained generation may lead to unnatural or linguistically implausible switches. To address this, linguistic constraints such as the ECT can be incorporated to guide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib45\" title=\"\">45</a>]</cite>. ECT constrains code-switching to occur only at points where the grammatical structures of both languages align. When implemented in prompt-based generation, ECT can be simplified since LLMs already internalize linguistic regularities to maintain syntactic compatibility and naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "codeswitching",
                    "generation",
                    "task",
                    "where",
                    "both",
                    "switches",
                    "languages",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we propose to integrate a simplified version of ECT (SECT) into the prompt design for LLM-based code-switching text generation. Beyond leveraging the inherent syntactic knowledge of LLM, SECT offers improved computational efficiency and avoids imposing overly rigid constraints compared to ECT.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "generation",
                    "prompt",
                    "text",
                    "version",
                    "into",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sentence",
                    "output",
                    "input",
                    "words",
                    "englishmandarin",
                    "appropriate",
                    "codeswitching",
                    "each",
                    "three",
                    "example",
                    "rules",
                    "mandarin",
                    "from",
                    "sentences",
                    "original",
                    "language",
                    "examples",
                    "into",
                    "prompt",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "each",
                    "generation",
                    "language",
                    "directly",
                    "prompt",
                    "text",
                    "from",
                    "method",
                    "into",
                    "used",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "two",
                    "english–mandarin",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "two",
                    "from",
                    "into",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "evaluate",
                    "from",
                    "only",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "codeswitching",
                    "text",
                    "mandarin",
                    "english",
                    "two",
                    "words",
                    "languages",
                    "sentences",
                    "bilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "each",
                    "language",
                    "codeswitching",
                    "where",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "each",
                    "three",
                    "language",
                    "mandarin",
                    "two",
                    "from",
                    "prompts",
                    "following",
                    "used",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "mandarin",
                    "english",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "generation",
                    "prompt",
                    "text",
                    "examples",
                    "into",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "how",
                    "from",
                    "provide",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "language",
                    "two",
                    "baseline",
                    "switches",
                    "languages",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "directly",
                    "from",
                    "both",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "without",
                    "single",
                    "method",
                    "languages",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "directly",
                    "mandarin",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "language",
                    "codeswitched",
                    "mandarin",
                    "english",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the SEAME dataset presents more complex linguistic patterns, where the matrix and embedded languages alternate dynamically across utterances. The absence of English-to-Mandarin translation ability in Whisper may limit its ability to generalize to the code-switching scenarios in SEAME data, especially when English serves as the matrix language. This hypothesis aligns with the statement in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.SS3\" title=\"III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III-C</span></a>, suggesting that the translation capacity, especially in the direction matching the matrix-to-embedded language, can assist in improving CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "language",
                    "english",
                    "where",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "each",
                    "prompt",
                    "evaluate",
                    "text",
                    "only",
                    "from",
                    "both",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "three",
                    "text",
                    "two",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "original",
                    "language",
                    "from",
                    "consistent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> indicate that model-centric approaches are more beneficial for code-switching data with lower language confusion, where textual patterns are more easily learned.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "language",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "language",
                    "how",
                    "from",
                    "prompts",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "text",
                    "where",
                    "from",
                    "baseline",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "example",
                    "consistent",
                    "text",
                    "from",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "language",
                    "prompt",
                    "codeswitching",
                    "text",
                    "from",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "codeswitching",
                    "text",
                    "specific",
                    "only",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "generation",
                    "text",
                    "preservation",
                    "from",
                    "method",
                    "used",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "three",
                    "prompt",
                    "text",
                    "without",
                    "examples",
                    "baseline",
                    "does",
                    "natural",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "text",
                    "from",
                    "into",
                    "prompts",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "text",
                    "only",
                    "two",
                    "from",
                    "method",
                    "prompts",
                    "original",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "generation",
                    "text",
                    "mandarin",
                    "method",
                    "into",
                    "original",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "codeswitching",
                    "consistent",
                    "text",
                    "mandarin",
                    "where",
                    "short",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codeswitching",
                    "generation",
                    "quality",
                    "text",
                    "mandarin",
                    "given",
                    "input",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "language",
                    "codeswitching",
                    "text",
                    "where",
                    "from",
                    "both",
                    "into",
                    "sect"
                ]
            }
        ]
    },
    "S5.T9": {
        "source_file": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives",
        "caption": "TABLE IX: Comparison between fine-tuning the Whisper-small model on ASRU data with different data augmentation methods. Text synthesis is performed via LLM guided by the proposed SECT prompts, and TTS is performed using target texts sourced from “Text source” and reference prompts sourced from ASRU training set. Real and synthesized data are denoted as “Real” and “Syn.”, respectively",
        "body": "Method\nText\nData Dur.\nASRU\n\n\nsource\n(hours)\ntest↓\\downarrow\n\n\n\nWhisper-small\n-\n-\n24.9\n\n\n+ f​tft ASRU\n-\n193\n8.8\n\n\n+ f​tft ASRU + ASRU Mandarin\n-\n193 + 100\n8.9\n\n\n+ f​tft ASRU + ASRU Mandarin\n-\n193 + 500\n9.2\n\n\n+ f​tft Syn. w/ Real text\nASRU\n180\n10.5\n\n\n+ f​tft ASRU + Syn. w/ Real text\n193 + 180\n9.3\n\n\n+ f​tft Syn. text w/ Qwen-chat\n\n352\n18.4\n\n\n+ f​tft Syn. text w/ DeepSeek-R1\nASRU\n509\n13.8\n\n\n+ f​tft ASRU + Syn. text w/ DeepSeek-R1\nMandarin\n193 + 100\n8.5\n\n\n+ f​tft ASRU + Syn. text w/ DeepSeek-R1\n\n193 + 247\n8.3\n\n\n+ f​tft ASRU + Syn. text w/ DeepSeek-R1\n\n193 + 509\n8.4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">Data Dur.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">ASRU</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">(hours)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">test<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">Whisper-small</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">24.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m2\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">193</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">8.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m3\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + ASRU Mandarin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 100</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">8.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m4\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + ASRU Mandarin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 500</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">9.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m5\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 3.1pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">180</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">10.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + Syn. w/ Real text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 180</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">9.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m7\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Syn. text w/ Qwen-chat</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">352</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.1pt;\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m8\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> Syn. text w/ DeepSeek-R1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">ASRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">509</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">13.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + Syn. text w/ DeepSeek-R1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">Mandarin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 100</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\">8.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m10\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + Syn. text w/ DeepSeek-R1</td>\n<td class=\"ltx_td ltx_border_r\" style=\"padding:0.5pt 3.1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 247</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.1pt;\"><span class=\"ltx_text ltx_font_bold\">8.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">+ <math alttext=\"ft\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m11\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">ft</annotation></semantics></math> ASRU + Syn. text w/ DeepSeek-R1</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.1pt;\">193 + 509</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.1pt;\">8.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "llm",
            "real",
            "training",
            "f​tft",
            "source",
            "text",
            "via",
            "proposed",
            "target",
            "asru",
            "“text",
            "hours",
            "finetuning",
            "guided",
            "denoted",
            "whispersmall",
            "texts",
            "sourced",
            "tts",
            "synthesized",
            "mandarin",
            "from",
            "between",
            "reference",
            "methods",
            "performed",
            "qwenchat",
            "augmentation",
            "dur",
            "synthesis",
            "model",
            "test↓downarrow",
            "prompts",
            "“syn”",
            "set",
            "different",
            "comparison",
            "source”",
            "“real”",
            "method",
            "deepseekr1",
            "data",
            "respectively",
            "sect",
            "syn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This SECT-based augmentation strategy is compared with the TTS-based data augmentation using only original text data as the target text introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.SS3\" title=\"V-C TTS with original text and speaker variety &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a>, and the results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>. The two augmentation strategies differ in the source of their target texts. The TTS-based method draws target texts from the intra-sentence ASRU training set, while the SECT-based method obtains code-switching target texts by translating Mandarin-only ASRU training texts via an LLM guided by SECT prompts.</p>\n\n",
            "<p class=\"ltx_p\">Results in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T9\" title=\"TABLE IX &#8227; V-F TTS with text generated via SECT-prompted LLM &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a> show that augmenting the training data with synthesized speech based on existing text patterns leads to degraded ASR performance on the ASRU dataset. This suggests that simply reusing the original training text during TTS fails to introduce sufficient linguistic diversity, limiting the benefit of data augmentation. In contrast, LLM-based text generation introduces greater variety into the synthesized speech&#8211;text pairs. Therefore, it exhibits higher performance than fine-tuning on the original dataset. Results also indicate that additionally including pure Mandarin data in fine-tuning degrades the performance. This finding further supports the effectiveness of the proposed SECT-guided text generation method using LLMs for code-switching data augmentation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech&#8211;text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "tts",
                    "via",
                    "different",
                    "proposed",
                    "from",
                    "method",
                    "methods",
                    "data",
                    "sect",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching involves the alternation between two or more languages within spontaneous multilingual speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib1\" title=\"\">1</a>]</cite>. Intra-sentence code-switching occurs when the language changes within a single sentence, while inter-sentential code-switching involves language alternation at sentence boundaries and is commonly treated as multilingual&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>. Despite significant advancements in monolingual speech processing tasks, such as automatic speech recognition&#160;(ASR) and text-to-speech synthesis&#160;(TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, code-switching speech processing remains challenging due to both the complex linguistic characteristics inherent to code-switched speech and the scarcity of annotated data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthesis",
                    "tts",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inherent challenges in code-switching speech processing can be broadly attributed to two primary factors, which we define as accent bias and language confusion. Speakers may exhibit native-like fluency in multiple languages and seamlessly alternate between them during spontaneous conversation. However, accent, particularly common among L2 speakers, can blur the phonetic boundaries between the two languages. We define this phenomenon as accent bias, which complicates both acoustic modeling and the mapping from acoustic features to token embeddings. In the meantime, code-switching introduces language confusion within the matrix language, the dominant or structural language in a code-switching utterance. This confusion is primarily associated with text and arises from the insertion of elements from an embedded language that disrupts the expected syntactic and phonological patterns during language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib8\" title=\"\">8</a>]</cite>. Moreover, in naturally bilingual communities, the matrix and embedded languages may alternate, resulting in increasingly ambiguous boundaries between them. This linguistic complexity presents additional challenges. As a result, even large-scale pre-trained multilingual speech models, such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>, exhibit degraded performance on code-switching data, despite demonstrating high performance in both the matrix and embedded languages individually.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "from",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accent bias has typically been mitigated from a data-centric perspective through adaptation with accented speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib11\" title=\"\">11</a>]</cite>. Existing studies have attempted to address the language confusion in code-switching speech recognition&#160;(CS-ASR) by integrating language discriminative information. One line of research has sought to factorize the recognition process into separate monolingual components&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>]</cite>. To effectively coordinate the language-specific processing, an additional divide-and-conquer strategy, such as the mixture-of-experts (MoE) framework or factorization methods, is often required&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib16\" title=\"\">16</a>]</cite>. Another important stream of work has pursued a unified multi-task paradigm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. Such approaches involve multi-tasking learning with an auxiliary language identification&#160;(LID) or diarization (LD) module to enrich the unified model with frame- or token-level language information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior research has explored data augmentation strategies to mitigate data scarcity for low-resource and code-switching ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib25\" title=\"\">25</a>]</cite>. Traditional data augmentation methods for ASR include speed perturbation and SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib27\" title=\"\">27</a>]</cite>. These approaches expand training data by altering the acoustic properties of the original speech signal, such as modifying the speaking rate or applying spectrogram masking. Voice conversion (VC) and TTS provide another solution by introducing speaker variability or generating speech signals from text data. Pre-trained VC and TTS systems typically underperform for low-resource languages due to limited tokenizer exposure and insufficient linguistic coverage&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>. In contrast, code-switching scenarios involving high-resource language pairs, such as English and Mandarin, moderately circumvent this limitation, as both languages are well-represented in large-scale TTS training corpora&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib29\" title=\"\">29</a>]</cite>. Consequently, synthesizing code-switching speech using TTS and VC systems pre-trained on high-resource constituent languages offers a practical and effective data augmentation strategy to improve the performance of CS-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>. Existing research has also explored synthesizing code-switching text to increase textual diversity for TTS-based data augmentation in ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>]</cite>. Earlier approaches typically relied on rule-based frameworks or generative models such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib31\" title=\"\">31</a>]</cite>. More recently, LLMs have been adopted for this task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite> and have proven effective in generating code-switching text data that more aligns with real-world code-switching sentences.</p>\n\n",
                "matched_terms": [
                    "training",
                    "text",
                    "mandarin",
                    "tts",
                    "from",
                    "methods",
                    "data",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate CS-ASR from both model- and data-centric perspectives. On the model side, we conduct a comprehensive review and comparison of recent algorithmic approaches, including language-specific processing and multi-task learning with auxiliary language-aware objectives. On the data side, TTS-based data augmentation is first explored to generate paired speech&#8211;text code-switching data. We next disentangle the effects of language confusion and accent bias and analyze their impact on CS-ASR by synthesizing speech with varied textual characteristics and accents. To enhance textual diversity, we introduce a prompting strategy that simplifies the equivalence constraint theory (SECT) to guide LLMs in generating linguistically valid code-switching text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib34\" title=\"\">34</a>]</cite>. The generated text can be further leveraged with TTS to create diverse and effective training data for CS-ASR. By comparing model- and data-centric approaches, we summarize practical strategies for different code-switching scenarios, offering insight for advancing CS-ASR research and applications.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "tts",
                    "different",
                    "sect",
                    "from",
                    "data",
                    "comparison",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing works have demonstrated that CS-ASR can be achieved via a general ASR system with a unified vocabulary comprising all multilingual tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. Let the input speech signal be represented as a sequence of acoustic feature vectors <math alttext=\"\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>F</mi></msup><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=(\\mathbf{x}_{t}\\in\\mathbb{R}^{F}\\mid t=1,\\ldots,T)</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the feature dimension and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the sequence length. The corresponding tokenized text sequence is defined as <math alttext=\"Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{n}\\in\\mathcal{V}\\mid n=1,\\ldots,N)</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8746;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}^{(L_{1})}\\cup\\mathcal{V}^{(L_{2})}</annotation></semantics></math> is the vocabulary comprising vocabularies of languages <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math>, and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the token sequence length. The speech recognition process is achieved by computing the conditional probability distribution of the token sequence given the acoustic features</p>\n\n",
                "matched_terms": [
                    "text",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each token probability is conditioned on the acoustic features and the previously generated tokens. This formulation corresponds to the attention-based sequence-to-sequence model, in which decoding is performed autoregressively. The predicted sequence <math alttext=\"\\widehat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\widehat{Y}</annotation></semantics></math> is obtained by maximizing the above conditional probability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the above approach rarely makes explicit use of language information. To mitigate this limitation, recent studies have explored enriching the model via multi-task learning with auxiliary language-aware tasks, such as LD or LID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>. These tasks are typically performed at the frame or token level to achieve fine-grained language change detection. The CS-ASR model is then optimized with a multi-task objective, formulated as the weighted sum of the ASR loss and the auxiliary language-aware loss as</p>\n\n",
                "matched_terms": [
                    "model",
                    "via",
                    "performed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"mat\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">mat</annotation></semantics></math> and <math alttext=\"emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">emb</annotation></semantics></math> denote the matrix and embedded language, respectively, and <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> is element-wise product. We use <math alttext=\"\\mathrm{MatEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>MatEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MatEncoder}\\left(\\cdot\\right)</annotation></semantics></math> and <math alttext=\"\\mathrm{EmbEncoder}\\left(\\cdot\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>EmbEncoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{EmbEncoder}\\left(\\cdot\\right)</annotation></semantics></math> to represent the computations within language-specific encoders. The mixed hidden representations are denoted as <math alttext=\"\\mathbf{H}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{mix}</annotation></semantics></math> and are fed into the decoder layers. In this manner, the system approximates monolingual ASR at the frame level to reduce language confusion. Ideally, when the input is monolingual, the corresponding encoder should receive a weight close to one, while the other encoder is assigned a weight close to zero. This mechanism ensures that each <math alttext=\"\\mathbf{h}_{t}^{mix}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>t</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}^{mix}</annotation></semantics></math> corresponding to one language closely resembles the output of the corresponding monolingual encoder, which suppresses interference from the non-target language.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "from",
                    "denoted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The auxiliary language-aware task is typically assigned a relatively small weight (i.e., high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value) compared to the ASR task during training, indicating its minor role in guiding the optimization. By contrast, language-specific modules contribute symmetrically across languages. Each module processes features in the same manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib15\" title=\"\">15</a>]</cite>, regardless of the relative proportions of matrix and embedded languages in the training data. In the training stage, language-specific modules may require pre-training on monolingual data to learn language-discriminative information before being applied to code-switching data, whereas multi-task learning can be performed solely on code-switching data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib21\" title=\"\">21</a>]</cite>. During inference, the auxiliary language branch is typically inactive, the decoding process is thus identical to that of a unified model. In contrast, language-specific modules require two passes through the encoder or decoder, introducing additional computational overhead and higher decoding latency. For example, the bi-encoder method exhibits real-time factors (RTF) and latency more than twice those of a model with a single encoder. These differences highlight that auxiliary tasks can enhance CS-ASR with minimal overhead, while language-specific modules deliver more explicit language discrimination at the expense of efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "method",
                    "performed",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching, particularly intra-sentential code-switching, occurs far less frequently than monolingual speech. Moreover, annotating code-switching data requires bilingual expertise, which further limits the availability of resources. As a result, code-switching ASR suffers from data scarcity, even though constituents can be individually high-resource languages.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, existing research has leveraged resources from the constituent languages and explored methods for synthesizing code-switching text and speech. Several studies have shown that monolingual data, particularly from the matrix language, can support the development of CS-ASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib39\" title=\"\">39</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib38\" title=\"\">38</a>]</cite> investigated building CS-ASR models using only monolingual data. These approaches typically employ language-specific connectionist temporal classification (CTC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib37\" title=\"\">37</a>]</cite>, where decoding is first performed independently for each language and then combined through joint decoding. In this process, tokens in the matrix-language sequence are substituted with the corresponding embedded-language tokens when appropriate. However, these works also indicate that subsequent fine-tuning on code-switching data provides substantial performance gains. Moreover, an excessive reliance on monolingual data can result in training imbalance and degrade performance on code-switching speech. Therefore, augmenting code-switching data is crucial for effectively addressing this scarcity.</p>\n\n",
                "matched_terms": [
                    "training",
                    "text",
                    "from",
                    "methods",
                    "performed",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in TTS models have enabled the use of synthesized speech for augmenting ASR training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib24\" title=\"\">24</a>]</cite>, particularly mitigating the data scarcity in low-resource scenarios. In this work, we investigate the effectiveness of TTS-based data augmentation for improving CS-ASR with a CosyVoice2 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F2\" title=\"Figure 2 &#8227; III-B Code-switching speech synthesis &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. CosyVoice2 is an LLM-based TTS method, comprising a speech tokenizer, an LLM, and a conditional flow matching (CFM) module&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib41\" title=\"\">41</a>]</cite>. The speech tokenizer is developed by integrating finite scalar quantization into the encoder module of an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib43\" title=\"\">43</a>]</cite>, converting continuous speech signals into discrete speech tokens. The LLM then serves as a text&#8211;speech language model, generating speech tokens autoregressively with the input text tokens. Finally, the conditional flow matching (CFM) module decodes the speech tokens into a Mel spectrogram, conditioned on the reference speech, the speaker embeddings, and the LLM-generated tokens from the target text.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "model",
                    "training",
                    "text",
                    "tts",
                    "synthesized",
                    "from",
                    "reference",
                    "method",
                    "target",
                    "data",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt LLM-based TTS rather than traditional TTS for code-switching speech synthesis. This is because the LLM is pre-trained with machine translation tasks and thus inherently exhibits cross-lingual capability, which is advantageous in code-switching scenarios. In addition, the model is pretrained on massive English and Mandarin data. We only update the LLM module within CosyVoice2 during fine-tuning on the target code-switching data since the speech tokenizer and flow matching modules are not open-sourced.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "synthesis",
                    "model",
                    "tts",
                    "mandarin",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the effects of text style and accent bias on CS-ASR, we simulate diverse code-switching speech&#8211;text pairs via TTS. The synthetic speech is generated using varying combinations of target and reference text and speech domains, such that it inherits the acoustic characteristics and speaker identity from the reference (i.e., prompt) speech while adhering to the textual pattern of the target text. This design allows us to isolate and evaluate the individual and combined effects of stylistic and accentual variation on ASR robustness.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tts",
                    "via",
                    "from",
                    "reference",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CS-ASR is inherently a cross-lingual task, as it involves dynamic alternation between two or more languages within a single utterance or conversation. A natural solution to this challenge is to develop a multilingual ASR model equipped with text translation capabilities across language pairs. However, such an approach typically demands large-scale supervised multilingual and parallel data, which are often unavailable, particularly for spontaneous speech commonly observed in code-switching scenarios. Consequently, the applicability of translation-augmented multilingual ASR models remains limited in real-world settings due to severe data scarcity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "between",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative strategy focuses on directly optimizing ASR models with code-switching data. However, this approach is often limited by data scarcity. Although TTS can alleviate the shortage of speech resources, generating diverse and natural code-switching text remains a major challenge. This difficulty arises from the wide variability in code-switching patterns, which may involve arbitrary combinations of syntactic structures, lexical substitutions, and language change points. Consequently, synthesizing high-quality code-switching text is crucial, as it forms the basis for constructing paired speech&#8211;text data via TTS.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tts",
                    "via",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we propose to integrate a simplified version of ECT (SECT) into the prompt design for LLM-based code-switching text generation. Beyond leveraging the inherent syntactic knowledge of LLM, SECT offers improved computational efficiency and avoids imposing overly rigid constraints compared to ECT.</p>\n\n",
                "matched_terms": [
                    "text",
                    "llm",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, monolingual sentences are translated from a high-resource matrix language (Mandarin) into their English-Mandarin code-switching counterparts via the prompt-guided LLM. This process is governed by SECT, which is formulated as three rules. The first rule specifies the matrix language and prevents the LLM from making substantial modifications to the original sentence. The second rule introduces a constraint to control the degree of code mixing, which can be adjusted depending on the desired characteristics of the target code-switching text. Finally, the third rule ensures that the generated code-switching sentences adhere to the syntactic structure of the matrix language while embedding content words from the embedded language at linguistically appropriate switch points. To further guide the model, we adopt an in-context learning strategy by providing three illustrative examples within the prompt. Each example consists of an input and output, being a monolingual sentence and its code-switching counterpart.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "model",
                    "text",
                    "mandarin",
                    "via",
                    "from",
                    "target",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with existing LLM-based approaches for code-switching text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib30\" title=\"\">30</a>]</cite>, SECT offers a more convenient solution by exploiting the LLM&#8217;s intrinsic knowledge and eliminating the need for feeding parallel matrix&#8211;embedded language text into each prompt. By leveraging the generative capability of LLMs under SECT constraints, the proposed method synthesizes high-quality code-switching text directly from monolingual data, which can then be used for downstream ASR training.</p>\n\n",
                "matched_terms": [
                    "training",
                    "text",
                    "proposed",
                    "from",
                    "method",
                    "data",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated CS-ASR performance using two widely studied English&#8211;Mandarin code-switching speech corpora: the ASRU 2019 Code-Switching Challenge dataset and the SEAME dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib47\" title=\"\">47</a>]</cite>. The ASRU data comprises four subsets: a Mandarin-only training set, an intra-sentence code-switching training set, two intra-sentence code-switching development sets (the dev1 set is used), and an intra-sentence code-switching test set.</p>\n\n",
                "matched_terms": [
                    "data",
                    "set",
                    "training",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SEAME dataset is a corpus of spontaneous conversational speech recorded in Singapore and Malaysia, featuring monolingual and code-switching utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib46\" title=\"\">46</a>]</cite>. Following the partitioning protocol described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib17\" title=\"\">17</a>]</cite>, we divided the dataset into a 101.5-hour training set and two test sets, denoted as <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math>. A 4.9-hour validation set was further extracted from the training data to monitor training progress.</p>\n\n",
                "matched_terms": [
                    "training",
                    "from",
                    "data",
                    "denoted",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech dataset and the Mandarin-only training set from the ASRU 2019 Code-Switching Challenge are also used to evaluate monolingual ASR performance and to pre-train multilingual models from scratch. To ensure balanced training, only the train-clean-100 and train-clean-360 dataset of LibriSpeech are used during multilingual pre-training.</p>\n\n",
                "matched_terms": [
                    "from",
                    "set",
                    "training",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two code-switching datasets differ primarily in accent and syntactic structure. Since the ASRU dataset is recorded in mainland China, data within this corpus is characterized by a Chinese accent and contains Mandarin-centric text, with a grammatical structure primarily framed in Mandarin and embedded English segments. Sentences within the ASRU training and development sets, on average, contains 1.6 English words and 8.6 Chinese characters. As opposed to the ASRU data, the SEAME dataset is recorded in Singapore and Malaysia and features speakers with South-East Asian accents. We also compute the code mixing index&#160;(CMI) as defined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib49\" title=\"\">49</a>]</cite>, to measure the degree of language mixing of code-switching text data. The CMI values of the ASRU and SEAME training sets are 17.0 and 13.6, respectively, while the CMI of SEAME increases to 24.2 when monolingual utterances are excluded. Therefore, SEAME data contains more frequent code-switching than the ASRU dataset, largely due to the bilingual education systems and language policies in these regions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib50\" title=\"\">50</a>]</cite>. The higher frequency and fluidity of language switching suggest that SEAME poses greater challenges for code-switching ASR, particularly as the matrix and embedded languages can alternate across sentences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "text",
                    "mandarin",
                    "asru",
                    "data",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed statistics for all datasets are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T1\" title=\"TABLE I &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S4.T2\" title=\"TABLE II &#8227; IV-A Datasets &#8227; IV Datasets and experiments &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> reports the duration ratios of each language based on utterance-level annotations for code-switching test sets. CMI values of the ASRU and SEAME training data are 17.0 and 13.6, respectively, where that of SEAME training data is 24.2 after excluding monolingual utterances.</p>\n\n",
                "matched_terms": [
                    "data",
                    "respectively",
                    "training",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with models trained from scratch, such as Conformer, were conducted using ESPNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib35\" title=\"\">35</a>]</cite>. For training on the SEAME and ASRU datasets, we adopted the hyperparameter settings from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite> for Conformer and from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib53\" title=\"\">53</a>]</cite> for Branchformer and ConExtBiMamba. Pre-training on monolingual datasets followed the hyperparameters specified in the LibriSpeech recipe provided by ESPNet. The resulting model is referred to as Conformer-L, reflecting its larger hidden dimension of 512, in contrast to the 256-dimensional hidden size used in the Conformer models trained solely on SEAME or ASRU data. This setup is also employed for Conformer-L Mix, which is pre-trained on a mixture of code-switching and monolingual datasets. Inference was performed using the average of the weights from the ten best-performing models on the development sets, with a beam size of five.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "from",
                    "asru",
                    "performed",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments with Whisper models were conducted using Whisper-small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib9\" title=\"\">9</a>]</cite>. Fine-tuning employed the AdamW optimizer, with the learning rate linearly warmed up from 0 to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> over 10,000 update steps, followed by a linear decay schedule. Training used a batch size of eight with gradient accumulation over two steps. For inference, we adopted greedy decoding with model weights averaged from the three best-performing checkpoints on the development set. Language prompts <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>zh<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>en<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> were applied to the ASRU and SEAME datasets, respectively. LoRA with a rank of 24 was integrated into the feed-forward networks of Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib54\" title=\"\">54</a>]</cite>. Following the Bi-encoder mechanism&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib12\" title=\"\">12</a>]</cite>, which is described in equations&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E4\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)&#8211;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.E6\" title=\"In II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), Bi-LoRA was pre-trained separately on LibriSpeech and ASRU Mandarin, and subsequently fine-tuned on either ASRU or SEAME, with each LoRA configured to a rank of 12.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "respectively",
                    "model",
                    "training",
                    "mandarin",
                    "from",
                    "asru",
                    "prompts",
                    "finetuning",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis was performed using the open-source TTS model CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib7\" title=\"\">7</a>]</cite>, which was fine-tuned on the target domain data before synthesis. Speech samples from different datasets as prompt speech were used to simulate various accents: LibriSpeech for American English, ASRU for Mandarin spoken in mainland China, and SEAME for Southeast Asian-accented English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "model",
                    "mandarin",
                    "tts",
                    "different",
                    "from",
                    "target",
                    "asru",
                    "performed",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-switching text generation was performed via Qwen 1.5-32B-chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib55\" title=\"\">55</a>]</cite>, and Deepseek-R1 API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib56\" title=\"\">56</a>]</cite>, respectively. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S3.F3\" title=\"Figure 3 &#8227; III-C Code-switching text generation &#8227; III Data-centric analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we employed a prompt containing several examples of converting monolingual sentences into their code-switching counterparts. The LLMs then generated code-switching sentences by referencing the prompt and applying the transformation to the target monolingual inputs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "via",
                    "deepseekr1",
                    "target",
                    "performed",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the CS-ASR models using the mixed error rate (MER), which combines word error rate (WER) for English and character error rate (CER) for Mandarin. English and Mandarin ASR performance was reported in WER and CER, respectively.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "mandarin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR performance, we adopt an LLM-as-a-judge method to evaluate the quality of texts generated by LLMs. Specifically, GPT-4o assigns a score from 1 to 10 to each sample with reference to the criteria described in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The final score is computed as the average across all text samples. We also evaluate the similarity in language confusion between real text and LLM-generated text by comparing their CMI.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "real",
                    "text",
                    "from",
                    "reference",
                    "between",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comparison of existing model-centric approaches to provide insight into how different algorithmic strategies are designed for CS-ASR. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> presents a comparison between models trained from scratch and those fine-tuned on Whisper-small models. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> illustrates the impact of pre-training and fine-tuning strategies on CS-ASR performance.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "different",
                    "from",
                    "between",
                    "finetuning",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggest that algorithmic strategies, such as adopting more advanced model architectures or incorporating language-aware designs, yield significant performance gains over the baseline on the ASRU dataset. In contrast, these strategies lead to only moderate improvements on the SEAME dataset. These can be attributed to the characteristics of the two corpora. Compared to SEAME, the ASRU dataset contains fewer language switches and thus exhibits a lower degree of language confusion, such as less frequent code-switching and discourse markers. As a result, its syntactic and lexical characteristics are more closely aligned with the matrix language, resulting in less language confusion. In addition to more complex textual patterns, SEAME data is characterized by Southeast Asian accents. The above makes the two languages less distinguishable and introduces greater challenges for code-switching ASR models. Consequently, language-specific processing or joint optimization with language-aware tasks provides limited improvement on SEAME.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is useful to note that CAMEL, which is built upon E-Branchformer, achieves the highest performance on SEAME and ASRU test sets among all models trained from scratch by integrating both language-specific modules and multi-task learning with LD. Instead of directly enriching the encoder with language-discriminative information via multi-task learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#bib.bib19\" title=\"\">19</a>]</cite>, CAMEL incorporates language information within the ASR decoder. Specifically, the key and values matrices within the LD decoder are extracted before being used to perform cross-attention with the query matrix associated with the token embedding sequence. This suggests that both strategies are beneficial to CS-ASR and highlights the potential of combining them in a complementary and well-coordinated manner to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "via",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S2.SS2\" title=\"II-B Language-specific Modules &#8227; II Model-centric methods &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a>, the language-specific process aims to achieve monolingual ASR due to the limited multilingual capacity of the ASR model. Nevertheless, for large-scale multilingual pre-trained models such as Whisper, the necessity of language-specific processing is substantially diminished. This is supported by the results of comparing adaptation strategies for Whisper-small using LoRA and Bi-LoRA. Specifically, although the bi-LoRA method involves separate pre-training of LoRA modules on the respective languages, it fails to consistently yield higher performance compared to a single LoRA on the SEAME and ASRU test sets. Compared to the integration of language-specific modules, joint optimization with auxiliary tasks, such as LAL and FA-LID, proves more effective in improving CS-ASR performance for large-scale pre-trained multilingual models. However, the use of language-specific modules remains a more flexible and modular solution, as it can be deployed as adapters without updating the base model. In contrast, auxiliary-task-based methods often require updating model parameters, making them relatively heavier for practical deployment, although the auxiliary branch typically introduces only a small increase in model parameters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "method",
                    "methods",
                    "asru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explored the impact of pre-training strategies on CS-ASR performance and presented the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T4\" title=\"TABLE IV &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>. Training a Conformer-L model directly on the mix of monolingual data and code-switching data obtains higher performance than sequentially pre-training on monolingual data and fine-tuning on code-switching data. This suggests that CS-ASR systems benefit from both cross-lingual and monolingual knowledge during training. While incorporating code-switching data in the training data moderately degrades monolingual ASR performance, it results in higher overall performance than both the pre-training and the fine-tuning strategies. The pre-trained Whisper-small model demonstrates an inherent ability to handle code-switching due to its large-scale multilingual pre-training. Fine-tuning the Whisper-small model on the ASRU leads to higher performance on both in-domain ASRU Mandarin and code-switching test data, while significantly degrading the performance on the LibriSpeech test-clean set.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "training",
                    "mandarin",
                    "from",
                    "asru",
                    "data",
                    "finetuning",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One interesting observation about the performance of Whisper models in CS-ASR tasks is the gap between the ASRU and SEAME datasets. The Whisper-small model reaches a Mixed Error Rate (MER) of 24.9% on the ASRU test set but performs significantly worse on SEAME, with MERs above 60% on SEAME test sets. This may be attributed to the translation ability of Whisper models from Mandarin to English. Since Mandarin consistently serves as the matrix language in ASRU and English as the embedded one, this translation ability may help the model recognize the code-switched segments more accurately.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "mandarin",
                    "from",
                    "between",
                    "asru",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the impact of TTS-based data augmentation on CS-ASR performance for both the SEAME and ASRU datasets, using only original text data as the target text for TTS synthesis. The corresponding results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. To introduce speaker variation beyond what is seen by the original text data, each target text was paired with a prompt audio randomly sampled from a different speaker within the same dataset. This strategy ensures that the prompt and target did not originate from the same speaker.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "text",
                    "tts",
                    "different",
                    "from",
                    "target",
                    "asru",
                    "data",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning a CS-ASR model on synthesized data outperforms the zero-shot setup, it yields moderately lower performance compared to fine-tuning on real speech. Additionally, fine-tuning the TTS model on target code-switching data prior to speech synthesis provides substantial benefits for downstream ASR performance.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "real",
                    "model",
                    "tts",
                    "synthesized",
                    "target",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results indicate that augmenting training with synthesized speech and original text improves performance on SEAME but degrades performance on ASRU. When augmented with synthesized data three times the size of the original training set, the fine-tuned model achieves the best WER of 12.7% and 17.7% on the two SEAME test sets, respectively. However, this data augmentation strategy leads to performance degradation for ASRU. This discrepancy can be attributed to differences in textual complexity between the SEAME and ASRU datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "synthesized",
                    "between",
                    "asru",
                    "data",
                    "respectively",
                    "set",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above is consistent with our finding in the previous section that the ASRU data exhibits less language confusion compared to the SEAME data. Therefore, a model fine-tuned on ASRU data is able to learn its structural patterns effectively from the original training data, and additional synthesized ASRU data introduces redundancy or noise rather than beneficial linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "synthesized",
                    "from",
                    "asru",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> set has a CMI closer to the SEAME training data, the performance gap between <math alttext=\"\\text{dev}_{\\texttt{man}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">man</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{man}}</annotation></semantics></math> and <math alttext=\"\\text{dev}_{\\texttt{sge}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mtext>dev</mtext><mtext class=\"ltx_mathvariant_monospace\">sge</mtext></msub><annotation encoding=\"application/x-tex\">\\text{dev}_{\\texttt{sge}}</annotation></semantics></math> presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> suggests that the model performs worse on data with a higher degree of code mixing. To further examine how accent and language confusion in code-switching data affect ASR performance, we synthesized speech using reference prompts and target texts from multiple datasets, respectively. The resulting synthetic data were then used to fine-tune the Whisper-small model, and performance was compared across conditions.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "texts",
                    "model",
                    "training",
                    "synthesized",
                    "from",
                    "between",
                    "reference",
                    "target",
                    "prompts",
                    "data",
                    "respectively",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a baseline, we fine-tuned the model using in-domain synthesized speech, where the prompts, target texts, and TTS model all originate from the same dataset. As expected, pairing text with a prompt from the same domain yields the highest ASR performance when using synthesized speech for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "tts",
                    "from",
                    "target",
                    "prompts",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To simulate accent mismatch, we paired text samples from the target dataset with prompt speech samples from a different dataset while keeping the TTS model consistent with the target dataset. For example, using a TTS model fine-tuned on SEAME data to synthesize speech for SEAME text with LibriSpeech or ASRU prompts results in an accent mismatch. Conversely, the textual mismatch, primarily associated with syntactic and lexical characteristics, was simulated by pairing out-of-domain text with an in-domain prompt and TTS model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "tts",
                    "different",
                    "from",
                    "target",
                    "asru",
                    "prompts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key insight from our study is that the textual characteristics of code-switching data has a significantly greater impact on CS-ASR performance than accent variation. Specifically, fine-tuning the CS-ASR model on data with textual mismatch results in substantially greater performance degradation compared to fine-tuning on data with accent mismatch. This performance gap can be attributed to differences in textual complexity. The SEAME dataset contains more fluent and frequent language switching within utterances, leading to greater language confusion than the structurally simpler ASRU dataset. When SEAME-style TTS model and prompt, which encourage frequent language alternations, are paired with ASRU text, the mismatch causes the model to learn inconsistent switching patterns, ultimately impairing generalization. It is also worth noting that the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparison of model-centric methods &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> indicate that encoder-only fine-tuning yields higher performance than decoder-only fine-tuning for the Whisper-small model. This further supports the above conclusion by demonstrating that the performance gap cannot be attributed to Whisper models having fully mastered acoustic modeling on code-switching data.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "text",
                    "tts",
                    "from",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above observation holds consistently across datasets, experimental results show that fine-tuning on ASRU- or LibriSpeech-accented SEAME text leads to only a moderate performance drop on SEAME data. This asymmetry suggests that the rich and complex switching patterns in SEAME provide sufficient linguistic diversity for the model to generalize effectively, even when the acoustic characteristics like accent differ. These findings underpin the conclusion that the syntactic and lexical characteristics of the training data play a more critical role than accent in determining CS-ASR performance. Moreover, the above implies that a CS-ASR model may not be robust against various levels of language confusion, such as different frequencies of code-switching or the presence of discourse markers. Consequently, this implies that training purely on code-switching data with fixed or specific textual patterns may not provide a generalizable or scalable solution for robust CS-ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "different",
                    "asru",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of the proposed SECT-guided text generation method on 1,500 synthesized text samples in terms of WER for ASR performance, GPT-4o scores, and CMI. ASR performance was evaluated by fine-tuning a Whisper-small model on speech synthesized from the SECT-generated texts before test. GPT-4o scores were obtained via an LLM-as-a-judge protocol, used together with CMI to assess the naturalness, grammatical correctness, and semantic preservation of the generated text samples.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "via",
                    "proposed",
                    "from",
                    "method",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison between SECT-guided prompts and other prompting strategies is presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T7\" title=\"TABLE VII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>, and the prompting strategies and LLM-as-a-judge evaluation are introduced in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T8\" title=\"TABLE VIII &#8227; V-E Evaluation of the proposed SECT prompt &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>. The results demonstrate that the SECT-guided prompting strategy achieves the best performance in terms of both WER and GPT-4o scores, and yields a CMI closer to that of the real ASRU code-switching text.</p>\n\n",
                "matched_terms": [
                    "real",
                    "text",
                    "between",
                    "asru",
                    "prompts",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, we conducted ablation studies by comparing three prompting strategies: the baseline prompt, the SECT prompt without in-context learning, and the SECT prompt with in-context learning. Results confirm that the effectiveness of the SECT-based prompting strategies since it consistently outperforms the baseline regardless of whether in-context examples are included. Although incorporating in-context learning examples does not improve the GPT-4o scores, it leads to lower WER and a CMI value that more closely aligns with the real ASRU data. This implies that while ECT provides beneficial structural constraints for generating linguistically valid code-switching text, it may also impose rigid switching patterns. Incorporating in-context examples helps mitigate such rigidity, leading to more natural code-switching behavior.</p>\n\n",
                "matched_terms": [
                    "real",
                    "text",
                    "asru",
                    "data",
                    "sect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then explored incorporating the SECT-prompted LLM into TTS-based data augmentation for code-switching ASR. Specifically, code-switching texts generated by the SECT-prompted LLM were used as the target text to a TTS model in order to synthesize code-switching speech samples. Speech samples from the ASRU training set were used as reference prompts. The resulting synthetic speech&#8211;text pairs were then used for data augmentation in ASR training.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "texts",
                    "model",
                    "training",
                    "text",
                    "tts",
                    "from",
                    "reference",
                    "target",
                    "asru",
                    "prompts",
                    "data",
                    "set",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also investigated the impact of varying the amount of synthesized speech&#8211;text pairs on CS-ASR performance. Results show that increasing the amount of pure Mandarin data during Whisper fine-tuning leads to performance degradation. This finding is consistent with the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24310v1#S5.T5\" title=\"TABLE V &#8227; V-B Impact of Pre-training &#8227; V Results and Analysis &#8227; Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, where data augmentation introduces limited code-switching patterns and results in lower performance. In contrast, incorporating synthesized code-switching speech&#8211;text pairs into fine-tuning yields moderate improvements. Notably, the highest ASR performance is observed when the fine-tuning data includes a balanced amount of real and synthetic code-switching data, suggesting that carefully curated synthetic data can effectively complement limited real-world code-switching resources. This also implies that the synthesized text data still falls short of real-world text in terms of naturalness and linguistic richness.</p>\n\n",
                "matched_terms": [
                    "real",
                    "text",
                    "mandarin",
                    "synthesized",
                    "data",
                    "finetuning",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the high-performance DeepSeek-R1 API, we explored the use of Qwen-chat-32B, an open-sourced LLM. However, Qwen-chat-32B exhibits significantly lower performance than the DeepSeek-R1 API in terms of the quality of the generated code-switching text. With the same amount of Mandarin input sentences, the Qwen-chat-32B model successfully generates code-switching sentences corresponding to 352 hours of synthesized speech data, significantly less than 509 hours of speech data synthesized with texts generated by the DeepSeek-R1 API. This observation is reasonable, given that the DeepSeek-R1 API model has a substantially larger parameter count compared to Qwen-chat-32B. It also suggests that API-accessible models remain a strong choice for data generation, even in specialized scenarios such as code-switching.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "texts",
                    "model",
                    "text",
                    "synthesized",
                    "mandarin",
                    "hours",
                    "deepseekr1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigated CS-ASR from both model- and data-centric perspectives, focusing on language confusion, accent bias, and data scarcity. On the model side, we compared language-specific processing and multi-task learning, providing insights into their trade-offs across training and inference. On the data side, we explored TTS-based augmentation and showed that syntactic and lexical characteristics have a greater impact on CS-ASR performance than accent. To further mitigate data scarcity, we proposed SECT, a prompting strategy that guides LLMs to generate linguistically valid code-switching text. Combined with TTS, SECT produces diverse training data and outperforms existing prompting strategies. Comparing model- and data-centric approaches, we found that algorithmic methods are more effective in low-confusion scenarios with clear matrix&#8211;embedded language structures, while TTS-based augmentation is beneficial across different levels of confusion. Simple TTS augmentation with speaker variation improves performance in highly confused settings, whereas creating new code-switching text is essential for low-confusion data where textual diversity is limited.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "tts",
                    "different",
                    "proposed",
                    "from",
                    "methods",
                    "data",
                    "sect",
                    "augmentation"
                ]
            }
        ]
    }
}