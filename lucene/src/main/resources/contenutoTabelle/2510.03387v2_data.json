{
    "S3.T1": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 1: Real audio sources with 200 samples per source for all tasks.  (VSP is video sharing platform. SR is sampling rate.)",
        "body": "Source\nSR(kHz)\nDescription\nPublic Split\n\n\nMandarin Podcast 1\n48\nPodcast in Mandarin\nyes\n\n\nArabic Speech Corpus [9]\n\n48\nArabic speech\nyes\n\n\nEnglish Podcast\n44.1\nPodcast in English\nyes\n\n\nConference\n48\nConference speakers\nyes\n\n\nFleurs German [10]\n\n16\nGerman research data\nyes\n\n\nHigh Quality Podcasts\n44.1, 48\nHigh quality podcasts\nyes\n\n\nJapanese Shortwave\n12\nJapanese shortwave radio\nyes\n\n\nVSP Documentary\n44.1\nDocumentary audio\nyes\n\n\nVSP Phone Call\n44.1\nPhone call audio\nyes\n\n\nVSP Semi-professional\n44.1\nSemi-professional quality\nyes\n\n\nRadio Drama\n44.1\nRadio drama\nno\n\n\nMandarin Podcast 2\n44.1\nPodcast in Mandarin\nno\n\n\nDigitized Cassette\n32\nDigitized cassettes\nno\n\n\nDipco [11]\n\n16\nDinner party recording\nno\n\n\nFleurs English [10]\n\n16\nEnglish research data\nno\n\n\nLibrivox [12]\n\n22.05\nAudiobook recordings\nno\n\n\nOld Radio\n22.05\nOld radio recordings\nno\n\n\nPhone Home\n8\nUnscripted phone call\nno\n\n\nRussian Audiobook\n44.1\nRussian audiobook\nno\n\n\nVSP Home Mic\n44.1\nHome microphone audio\nno\n\n\nVSP Professional\n44.1\nProfessional quality\nno",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SR(kHz)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Description</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Public Split</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Mandarin Podcast 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">48</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Podcast in Mandarin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">48</td>\n<td class=\"ltx_td ltx_align_left\">Arabic speech</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English Podcast</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Podcast in English</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Conference</td>\n<td class=\"ltx_td ltx_align_center\">48</td>\n<td class=\"ltx_td ltx_align_left\">Conference speakers</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fleurs German <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib10\" title=\"\">10</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_left\">German research data</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">High Quality Podcasts</td>\n<td class=\"ltx_td ltx_align_center\">44.1, 48</td>\n<td class=\"ltx_td ltx_align_left\">High quality podcasts</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Japanese Shortwave</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_left\">Japanese shortwave radio</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Documentary</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Documentary audio</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Phone Call</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Phone call audio</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Semi-professional</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Semi-professional quality</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Radio Drama</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Radio drama</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Mandarin Podcast 2</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Podcast in Mandarin</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Digitized Cassette</td>\n<td class=\"ltx_td ltx_align_center\">32</td>\n<td class=\"ltx_td ltx_align_left\">Digitized cassettes</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Dipco <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib11\" title=\"\">11</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_left\">Dinner party recording</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fleurs English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib10\" title=\"\">10</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_left\">English research data</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Librivox <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.05</td>\n<td class=\"ltx_td ltx_align_left\">Audiobook recordings</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Old Radio</td>\n<td class=\"ltx_td ltx_align_center\">22.05</td>\n<td class=\"ltx_td ltx_align_left\">Old radio recordings</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phone Home</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_left\">Unscripted phone call</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Russian Audiobook</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Russian audiobook</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Home Mic</td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_left\">Home microphone audio</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">VSP Professional</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">44.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Professional quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">no</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sources",
            "real",
            "quality",
            "semiprofessional",
            "source",
            "tasks",
            "public",
            "rate",
            "vsp",
            "yes",
            "german",
            "video",
            "split",
            "home",
            "mic",
            "unscripted",
            "sharing",
            "speech",
            "high",
            "documentary",
            "conference",
            "all",
            "mandarin",
            "srkhz",
            "english",
            "russian",
            "podcast",
            "platform",
            "dinner",
            "old",
            "digitized",
            "japanese",
            "call",
            "microphone",
            "recordings",
            "phone",
            "description",
            "podcasts",
            "arabic",
            "corpus",
            "librivox",
            "drama",
            "audiobook",
            "samples",
            "speakers",
            "professional",
            "shortwave",
            "dipco",
            "sampling",
            "cassettes",
            "research",
            "cassette",
            "radio",
            "party",
            "fleurs",
            "recording",
            "data",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
            "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sources",
                    "real",
                    "tasks",
                    "research",
                    "split",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "public",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sources",
                    "real",
                    "research",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Blind Evaluation Protocol</span>: We designed and deployed a fully blind evaluation framework where participants had no access to evaluation data, ensuring an unbiased assessment of model generalization to unseen audio sources and manipulations. During Round 1 (70 days) of the competition, we received more than 700 submissions from 12 teams.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sources",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Against Processing and Laundering</span>: We introduced benchmark tasks specifically focused on detection of synthetic audio subjected to common post-processing (e.g., compression, resampling) and laundering intended to bypass detection.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "samples",
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rapid developments in generative AI have led to the creation of\nAI systems capable of creating realistic synthetic speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib1\" title=\"\">1</a>]</cite>. In response to this, a number of forensic systems have been developed to detect synthetic audio, and several datasets and challenges have been designed to further research in this area.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "research",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "research",
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "source",
                    "research",
                    "speakers",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "public",
                    "research",
                    "split",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "data",
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the evaluation step, we sequestered the participants&#8217; models from the internet to prevent exfiltration of the competition data or their potential use of externally hosted services. To level the field, all submissions were executed on the same compute resources of an NVIDIA T4 GPU with 16GB VRAM, 32GB RAM and 8 CPUs. Additionally, all submissions were limited to 10,000 seconds, and participating teams were limited to five submissions per day.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "data",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "real",
                    "source",
                    "public",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "real",
                    "all",
                    "public",
                    "split",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "real",
                    "quality",
                    "german",
                    "video",
                    "sharing",
                    "speech",
                    "high",
                    "english",
                    "mandarin",
                    "russian",
                    "digitized",
                    "japanese",
                    "phone",
                    "podcasts",
                    "arabic",
                    "cassettes",
                    "radio",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "source",
                    "sampling",
                    "public",
                    "rate",
                    "split",
                    "data",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "real",
                    "phone",
                    "public",
                    "data",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "real",
                    "source",
                    "all",
                    "recording",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "real",
                    "tasks",
                    "rate",
                    "split",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "real",
                    "source",
                    "all",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shortwave",
                    "japanese",
                    "real",
                    "quality",
                    "source",
                    "phone",
                    "russian",
                    "arabic",
                    "corpus",
                    "radio",
                    "recording",
                    "home",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both tables, the upper rows represent models and sources from the public split, while the lower rows correspond to those exclusive to the private split. Because participants could iteratively improve their algorithms through repeated submissions, we would expect performance on models from the public split to be higher. We saw some evidence of this in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> but the difference was not drastic suggesting that either the algorithms do generalize or the participants had private split models in their training sets. On the reals, we did not see any clear performance difference between private and public splits.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "public",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "phone",
                    "all",
                    "research",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "microphone",
                    "all",
                    "rate",
                    "recording",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "high",
                    "sources",
                    "real",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "data",
                    "real",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "all",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sources",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 2: Machine-generated audio with 200 samples / source for Task 1. Same models were used for Task 2 and 3. (Closed sources are in bold.)",
        "body": "Source\nSR (kHz)\nVoice Cloning\nPublic Split\n\n\n\nCartesia [14]\n\n44.1\nYes\nyes\n\n\n\nElevenlabs [15]\n\n44.1\nYes\nyes\n\n\nFish [16]\n\n44.1\nYes\nyes\n\n\nHierspeech [17]\n\n48\nYes\nyes\n\n\nKokoro [18]\n\n24\nNo\nyes\n\n\nParler [19]\n\n44.1\nYes\nyes\n\n\nStyle [20]\n\n24\nYes\nyes\n\n\nEdge [21]\n\n24\nNo\nno\n\n\nF5 [22]\n\n24\nYes\nno\n\n\nMetavoice [23]\n\n48\nYes\nno\n\n\n\nOpenAI [24]\n\n24\nNo\nno\n\n\nSeamless [25]\n\n16\nNo\nno\n\n\nZonos [26]\n\n44.1\nYes\nno",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Source</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SR (kHz)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Voice Cloning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Public Split</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Cartesia</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">44.1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text ltx_font_bold\">Elevenlabs</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib15\" title=\"\">15</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">44.1</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fish <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib16\" title=\"\">16</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">44.1</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hierspeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib17\" title=\"\">17</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">48</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Kokoro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">24</th>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Parler <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib19\" title=\"\">19</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">44.1</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Style <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib20\" title=\"\">20</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">24</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">24</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">F5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib22\" title=\"\">22</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">24</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metavoice <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib23\" title=\"\">23</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">48</th>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text ltx_font_bold\">OpenAI</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib24\" title=\"\">24</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">24</th>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Seamless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib25\" title=\"\">25</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">44.1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">no</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "machinegenerated",
            "sources",
            "task",
            "source",
            "closed",
            "public",
            "hierspeech",
            "yes",
            "split",
            "same",
            "used",
            "openai",
            "voice",
            "metavoice",
            "cloning",
            "kokoro",
            "zonos",
            "cartesia",
            "khz",
            "parler",
            "edge",
            "bold",
            "samples",
            "models",
            "elevenlabs",
            "fish",
            "style",
            "seamless",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "models",
                    "split",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "public",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "sources",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Blind Evaluation Protocol</span>: We designed and deployed a fully blind evaluation framework where participants had no access to evaluation data, ensuring an unbiased assessment of model generalization to unseen audio sources and manipulations. During Round 1 (70 days) of the competition, we received more than 700 submissions from 12 teams.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "machinegenerated",
                    "sources",
                    "models",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "voice",
                    "models",
                    "public",
                    "split",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "models",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "same",
                    "audio",
                    "used",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the evaluation step, we sequestered the participants&#8217; models from the internet to prevent exfiltration of the competition data or their potential use of externally hosted services. To level the field, all submissions were executed on the same compute resources of an NVIDIA T4 GPU with 16GB VRAM, 32GB RAM and 8 CPUs. Additionally, all submissions were limited to 10,000 seconds, and participating teams were limited to five submissions per day.</p>\n\n",
                "matched_terms": [
                    "same",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "task",
                    "source",
                    "public",
                    "split",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "public",
                    "split",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "source",
                    "public",
                    "split",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "task",
                    "public",
                    "same",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "task",
                    "source",
                    "same",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "same",
                    "audio",
                    "split",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sources",
                    "task",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hierspeech",
                    "edge",
                    "zonos",
                    "elevenlabs",
                    "seamless",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both tables, the upper rows represent models and sources from the public split, while the lower rows correspond to those exclusive to the private split. Because participants could iteratively improve their algorithms through repeated submissions, we would expect performance on models from the public split to be higher. We saw some evidence of this in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> but the difference was not drastic suggesting that either the algorithms do generalize or the participants had private split models in their training sets. On the reals, we did not see any clear performance difference between private and public splits.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sources",
                    "public",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "used",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "used",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sources",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "same",
                    "models",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sources",
                    "used",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "same",
                    "used"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 3: Post-processing used on generated audio in Task 2.",
        "body": "Augmentation\nDescription\n\n\n\n\nAAC 16k\nAAC compr. with a 16kbps\n\n\nMP3-AAC 16k\nChained MP3 and AAC compr., 16kbps\n\n\nOpus 16k\nOpus compr. with 16kbps\n\n\nResample Up\nResample up to 48 kHz\n\n\nTime Stretch\nSped up while maintaining pitch\n\n\nEncodec\nNeural codec\n\n\nMP3-AAC-MP3 16k\nChained MP3, AAC, MP3 compr., 16kbps\n\n\nPhone Audio\nG.722 compr., 16kbps, sampled at 8 kHz\n\n\nSemanticodec\nNeural codec\n\n\nUnaugmented\nOriginal generated audio\n\n\nFocalcodec\nNeural codec\n\n\nMP3 VBR\nMP3 compr. with variable bit rate\n\n\nPitch Shift\nShift the pitch up and down\n\n\nSnac\nNeural codec\n\n\nVorbis 16k\nVorbis compr. with 16kbps\n\n\nMP3 16k\nMP3 compr. with 16kbps\n\n\nNoise\nAdd Gaussian noise\n\n\nResample Down\nResample down to 16 kHz\n\n\nSpeech Filter\nBand-pass filter from 507000 Hz",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Augmentation</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Description</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AAC 16k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AAC compr. with a 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MP3-AAC 16k</td>\n<td class=\"ltx_td ltx_align_left\">Chained MP3 and AAC compr., 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Opus 16k</td>\n<td class=\"ltx_td ltx_align_left\">Opus compr. with 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Resample Up</td>\n<td class=\"ltx_td ltx_align_left\">Resample up to 48 kHz</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Time Stretch</td>\n<td class=\"ltx_td ltx_align_left\">Sped up while maintaining pitch</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Encodec</td>\n<td class=\"ltx_td ltx_align_left\">Neural codec</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MP3-AAC-MP3 16k</td>\n<td class=\"ltx_td ltx_align_left\">Chained MP3, AAC, MP3 compr., 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phone Audio</td>\n<td class=\"ltx_td ltx_align_left\">G.722 compr., 16kbps, sampled at 8 kHz</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Semanticodec</td>\n<td class=\"ltx_td ltx_align_left\">Neural codec</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Unaugmented</td>\n<td class=\"ltx_td ltx_align_left\">Original generated audio</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Focalcodec</td>\n<td class=\"ltx_td ltx_align_left\">Neural codec</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MP3 VBR</td>\n<td class=\"ltx_td ltx_align_left\">MP3 compr. with variable bit rate</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Pitch Shift</td>\n<td class=\"ltx_td ltx_align_left\">Shift the pitch up and down</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Snac</td>\n<td class=\"ltx_td ltx_align_left\">Neural codec</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Vorbis 16k</td>\n<td class=\"ltx_td ltx_align_left\">Vorbis compr. with 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MP3 16k</td>\n<td class=\"ltx_td ltx_align_left\">MP3 compr. with 16kbps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Noise</td>\n<td class=\"ltx_td ltx_align_left\">Add Gaussian noise</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Resample Down</td>\n<td class=\"ltx_td ltx_align_left\">Resample down to 16 kHz</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Speech Filter</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Band-pass filter from 50&#8211;7000 Hz</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "16kbps",
            "opus",
            "task",
            "rate",
            "g722",
            "focalcodec",
            "snac",
            "used",
            "16k",
            "chained",
            "speech",
            "semanticodec",
            "bit",
            "add",
            "shift",
            "mp3aacmp3",
            "gaussian",
            "neural",
            "507000",
            "maintaining",
            "from",
            "filter",
            "khz",
            "postprocessing",
            "pitch",
            "encodec",
            "original",
            "augmentation",
            "noise",
            "phone",
            "description",
            "sampled",
            "vorbis",
            "compr",
            "mp3",
            "codec",
            "mp3aac",
            "stretch",
            "aac",
            "down",
            "time",
            "sped",
            "bandpass",
            "vbr",
            "while",
            "variable",
            "generated",
            "unaugmented",
            "audio",
            "resample"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generated",
                    "postprocessing",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "postprocessing",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Blind Evaluation Protocol</span>: We designed and deployed a fully blind evaluation framework where participants had no access to evaluation data, ensuring an unbiased assessment of model generalization to unseen audio sources and manipulations. During Round 1 (70 days) of the competition, we received more than 700 submissions from 12 teams.</p>\n\n",
                "matched_terms": [
                    "from",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Against Processing and Laundering</span>: We introduced benchmark tasks specifically focused on detection of synthetic audio subjected to common post-processing (e.g., compression, resampling) and laundering intended to bypass detection.</p>\n\n",
                "matched_terms": [
                    "postprocessing",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rapid developments in generative AI have led to the creation of\nAI systems capable of creating realistic synthetic speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib1\" title=\"\">1</a>]</cite>. In response to this, a number of forensic systems have been developed to detect synthetic audio, and several datasets and challenges have been designed to further research in this area.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generated",
                    "time",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "postprocessing",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "from",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "task",
                    "from",
                    "generated",
                    "postprocessing",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "task",
                    "from",
                    "while",
                    "generated",
                    "postprocessing",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "while",
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "while",
                    "generated",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "original",
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "audio",
                    "phone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "from",
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "while",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "rate",
                    "maintaining",
                    "from",
                    "while",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "time",
                    "from",
                    "while",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "phone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both tables, the upper rows represent models and sources from the public split, while the lower rows correspond to those exclusive to the private split. Because participants could iteratively improve their algorithms through repeated submissions, we would expect performance on models from the public split to be higher. We saw some evidence of this in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> but the difference was not drastic suggesting that either the algorithms do generalize or the participants had private split models in their training sets. On the reals, we did not see any clear performance difference between private and public splits.</p>\n\n",
                "matched_terms": [
                    "from",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "while",
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "opus",
                    "noise",
                    "aac",
                    "task",
                    "gaussian",
                    "phone",
                    "from",
                    "while",
                    "mp3",
                    "postprocessing",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "task",
                    "generated",
                    "neural",
                    "codec",
                    "from",
                    "original",
                    "encodec",
                    "postprocessing",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "rate",
                    "from",
                    "while",
                    "generated",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Task 2, the updated scores for all augmentation techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F4\" title=\"Figure 4 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, The updated scores for Task 3 for all laundering techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F5\" title=\"Figure 5 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "from",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "noise",
                    "used",
                    "shift"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 4: Laundering applied to generated audio in Task 3.",
        "body": "Laundering Technique\nDescription\n\n\n\n\nCar\nReal car background noise\n\n\nReverb\nAdded reverberation\n\n\nOver Air\nPlayed back over the air\n\n\nCar-Reverb Over Air\nAdded car noise, applied reverb, and recorded over air",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Laundering Technique</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Description</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Car</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Real car background noise</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Reverb</td>\n<td class=\"ltx_td ltx_align_left\">Added reverberation</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Over Air</td>\n<td class=\"ltx_td ltx_align_left\">Played back over the air</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Car-Reverb Over Air</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Added car noise, applied reverb, and recorded over air</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reverb",
            "real",
            "task",
            "over",
            "carreverb",
            "played",
            "recorded",
            "applied",
            "reverberation",
            "technique",
            "back",
            "noise",
            "description",
            "added",
            "laundering",
            "air",
            "car",
            "generated",
            "audio",
            "background"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real",
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real",
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Against Processing and Laundering</span>: We introduced benchmark tasks specifically focused on detection of synthetic audio subjected to common post-processing (e.g., compression, resampling) and laundering intended to bypass detection.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "laundering",
                    "generated",
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "real",
                    "task",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "real",
                    "task",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "over",
                    "generated",
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "applied",
                    "real",
                    "task",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "over",
                    "real",
                    "task",
                    "laundering",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "over",
                    "real",
                    "task",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "reverb",
                    "noise",
                    "applied",
                    "over",
                    "task",
                    "car",
                    "technique",
                    "played",
                    "laundering",
                    "air",
                    "generated",
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "real",
                    "audio",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, The updated scores for Task 3 for all laundering techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F5\" title=\"Figure 5 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "real",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "real",
                    "over",
                    "audio"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 5: Details on the top five team",
        "body": "Code\nName\nInstitution\n\n\n\n\nISP\nISPL\nPolitecnico di Milano\n\n\nVIP\nViper-Purdue\nPurdue University\n\n\nJAI\nJAIST-HIS\nJapan Advanced Institute of Science and Tech,\n\n\nANO\nAnon_Peking\nBeijing/University of Chinese Academy of Sciences\n\n\nDMF\nDMF\nHangzhou Dianzi University",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Code</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Name</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Institution</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ISP</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ISPL</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Politecnico di Milano</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VIP</td>\n<td class=\"ltx_td ltx_align_left\">Viper-Purdue</td>\n<td class=\"ltx_td ltx_align_left\">Purdue University</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">JAI</td>\n<td class=\"ltx_td ltx_align_left\">JAIST-HIS</td>\n<td class=\"ltx_td ltx_align_left\">Japan Advanced Institute of Science and Tech,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ANO</td>\n<td class=\"ltx_td ltx_align_left\">Anon_Peking</td>\n<td class=\"ltx_td ltx_align_left\">Beijing/University of Chinese Academy of Sciences</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">DMF</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">DMF</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Hangzhou Dianzi University</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "jai",
            "academy",
            "dmf",
            "name",
            "details",
            "jaisthis",
            "top",
            "five",
            "ano",
            "ispl",
            "university",
            "anonpeking",
            "hangzhou",
            "milano",
            "japan",
            "tech",
            "vip",
            "sciences",
            "chinese",
            "viperpurdue",
            "purdue",
            "code",
            "isp",
            "institution",
            "science",
            "politecnico",
            "beijinguniversity",
            "advanced",
            "dianzi",
            "institute",
            "team"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "advanced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "details",
                    "top"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "team",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "ano",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "top",
                    "university",
                    "ispl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "team",
                    "ispl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "team",
                    "ispl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "five",
                    "ano",
                    "ispl",
                    "dmf"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 6: True Positive Rate (TPR), True Negative Rate (TNR), Balanced Accuracy (BAC) for detection of generated audio.",
        "body": "Team\nTask 1\nTask 2\nTask 3\n\n\n\nTPR\nTNR\nBAC\nTPR\nTNR\nBAC\nTPR\nTNR\nBAC\n\n\n\n\nISP\n0.79\n0.95\n0.87\n0.67\n0.93\n0.80\n0.39\n0.92\n0.66\n\n\nVIP\n0.74\n0.80\n0.77\n0.44\n0.90\n0.67\n0.60\n0.50\n0.55\n\n\nJAI\n0.46\n0.90\n0.68\n0.67\n0.77\n0.72\n0.55\n0.76\n0.65\n\n\nANO\n0.77\n0.71\n0.74\n0.84\n0.71\n0.78\n0.41\n0.72\n0.57\n\n\nDMF\n0.86\n0.49\n0.67\n0.92\n0.23\n0.58\n0.81\n0.21\n0.51",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Team</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Task 1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Task 2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Task 3</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TPR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TNR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BAC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TPR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TNR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BAC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TPR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TNR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BAC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">ISP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VIP</th>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center\">0.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">JAI</th>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center\">0.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ANO</th>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.74</td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">DMF</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.51</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detection",
            "jai",
            "task",
            "bac",
            "rate",
            "tpr",
            "dmf",
            "audio",
            "tnr",
            "balanced",
            "ano",
            "true",
            "vip",
            "accuracy",
            "negative",
            "isp",
            "positive",
            "generated",
            "team"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "detection",
                    "balanced",
                    "generated",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Against Processing and Laundering</span>: We introduced benchmark tasks specifically focused on detection of synthetic audio subjected to common post-processing (e.g., compression, resampling) and laundering intended to bypass detection.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "task",
                    "balanced",
                    "generated",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "generated",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bac",
                    "tnr",
                    "balanced",
                    "positive",
                    "true",
                    "negative",
                    "tpr",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generated",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generated",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "bac",
                    "generated",
                    "team",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "audio",
                    "bac",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bac",
                    "tnr",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "task",
                    "bac",
                    "vip",
                    "negative",
                    "accuracy",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "task",
                    "bac",
                    "ano",
                    "generated",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "task",
                    "bac",
                    "rate",
                    "vip",
                    "generated",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "balanced",
                    "task",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "team",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "positive",
                    "true",
                    "dmf",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "detection",
                    "jai",
                    "ano"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 7: Balanced accuracy conditioned on the generation model in Task 1. TNR from Table 6 is used in the calculation.",
        "body": "Model\nISP\nVIP\nJAI\nANO\nDMF\nPublic Split\n\n\nElevenlabs\n0.97\n0.88\n0.82\n0.58\n0.71\nyes\n\n\nFish\n0.94\n0.79\n0.81\n0.91\n0.62\nyes\n\n\nHierspeech\n0.76\n0.62\n0.62\n0.86\n0.63\nyes\n\n\nKokoro\n0.98\n0.90\n0.84\n0.80\n0.73\nyes\n\n\nParler\n0.97\n0.74\n0.80\n0.48\n0.66\nyes\n\n\nSeamless\n0.93\n0.90\n0.86\n0.94\n0.75\nyes\n\n\nStyle\n0.82\n0.71\n0.46\n0.46\n0.75\nyes\n\n\nCartesia\n0.91\n0.84\n0.81\n0.67\n0.72\nno\n\n\nEdge\n0.68\n0.83\n0.85\n0.73\n0.73\nno\n\n\nF5\n0.90\n0.67\n0.72\n0.50\n0.56\nno\n\n\nMetavoice\n0.88\n0.80\n0.76\n0.56\n0.71\nno\n\n\nOpenAI\n0.86\n0.85\n0.75\n0.92\n0.72\nno\n\n\nZonos\n0.71\n0.48\n0.56\n0.45\n0.52\nno\n\n\nAverage Bal. Acc\n0.87\n0.77\n0.74\n0.68\n0.67",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ISP</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">VIP</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">JAI</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ANO</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DMF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Public Split</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Elevenlabs</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.97</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.88</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.82</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.58</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fish</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.94</span></td>\n<td class=\"ltx_td ltx_align_right\">0.79</td>\n<td class=\"ltx_td ltx_align_right\">0.81</td>\n<td class=\"ltx_td ltx_align_right\">0.91</td>\n<td class=\"ltx_td ltx_align_right\">0.62</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hierspeech</td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.62</td>\n<td class=\"ltx_td ltx_align_right\">0.62</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kokoro</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">0.90</td>\n<td class=\"ltx_td ltx_align_right\">0.84</td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Parler</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.97</span></td>\n<td class=\"ltx_td ltx_align_right\">0.74</td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n<td class=\"ltx_td ltx_align_right\">0.48</td>\n<td class=\"ltx_td ltx_align_right\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Seamless</td>\n<td class=\"ltx_td ltx_align_right\">0.93</td>\n<td class=\"ltx_td ltx_align_right\">0.90</td>\n<td class=\"ltx_td ltx_align_right\">0.86</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.94</span></td>\n<td class=\"ltx_td ltx_align_right\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Style</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_right\">0.46</td>\n<td class=\"ltx_td ltx_align_right\">0.46</td>\n<td class=\"ltx_td ltx_align_right\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Cartesia</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.91</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.84</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.81</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Edge</td>\n<td class=\"ltx_td ltx_align_right\">0.68</td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">F5</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.72</td>\n<td class=\"ltx_td ltx_align_right\">0.50</td>\n<td class=\"ltx_td ltx_align_right\">0.56</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Metavoice</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.56</td>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OpenAI</td>\n<td class=\"ltx_td ltx_align_right\">0.86</td>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n<td class=\"ltx_td ltx_align_right\">0.75</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_right\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Zonos</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.71</span></td>\n<td class=\"ltx_td ltx_align_right\">0.48</td>\n<td class=\"ltx_td ltx_align_right\">0.56</td>\n<td class=\"ltx_td ltx_align_right\">0.45</td>\n<td class=\"ltx_td ltx_align_right\">0.52</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Average Bal. Acc</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.77</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "jai",
            "task",
            "public",
            "conditioned",
            "hierspeech",
            "yes",
            "split",
            "dmf",
            "used",
            "openai",
            "tnr",
            "metavoice",
            "balanced",
            "ano",
            "kokoro",
            "zonos",
            "calculation",
            "from",
            "cartesia",
            "generation",
            "acc",
            "model",
            "parler",
            "vip",
            "edge",
            "average",
            "accuracy",
            "isp",
            "bal",
            "elevenlabs",
            "fish",
            "style",
            "seamless"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
            "<p class=\"ltx_p\">In both tables, the upper rows represent models and sources from the public split, while the lower rows correspond to those exclusive to the private split. Because participants could iteratively improve their algorithms through repeated submissions, we would expect performance on models from the public split to be higher. We saw some evidence of this in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> but the difference was not drastic suggesting that either the algorithms do generalize or the participants had private split models in their training sets. On the reals, we did not see any clear performance difference between private and public splits.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "public"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Blind Evaluation Protocol</span>: We designed and deployed a fully blind evaluation framework where participants had no access to evaluation data, ensuring an unbiased assessment of model generalization to unseen audio sources and manipulations. During Round 1 (70 days) of the competition, we received more than 700 submissions from 12 teams.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "balanced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "public",
                    "split",
                    "used",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "balanced",
                    "from",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tnr",
                    "public",
                    "conditioned",
                    "balanced",
                    "split",
                    "average",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "public",
                    "split",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "average",
                    "public",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "public",
                    "edge",
                    "from",
                    "cartesia",
                    "split",
                    "elevenlabs",
                    "average",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "public",
                    "from",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "average",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tnr",
                    "balanced",
                    "from",
                    "split",
                    "dmf",
                    "accuracy",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "hierspeech",
                    "edge",
                    "zonos",
                    "from",
                    "elevenlabs",
                    "seamless",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "conditioned",
                    "task",
                    "tnr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "task",
                    "vip",
                    "from",
                    "used",
                    "accuracy",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "jai",
                    "task",
                    "ano",
                    "from",
                    "used",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "task",
                    "conditioned",
                    "vip",
                    "from",
                    "used",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "balanced",
                    "task",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Task 2, the updated scores for all augmentation techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F4\" title=\"Figure 4 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, The updated scores for Task 3 for all laundering techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F5\" title=\"Figure 5 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "from",
                    "balanced",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "from",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "model",
                    "ano",
                    "dmf",
                    "used"
                ]
            }
        ]
    },
    "S5.T8": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 8: Balanced accuracy conditioned on the real source in Task 1 TPR from Table 6 was used in the calculation.",
        "body": "Source\nISP\nVIP\nJAI\nANO\nDMF\nPublic Split\n\n\nMandarin Podcast 1\n0.90\n0.87\n0.89\n0.68\n0.61\nyes\n\n\nFleurs German\n0.90\n0.86\n0.69\n0.72\n0.83\nyes\n\n\nVSP Semi-professional\n0.90\n0.80\n0.89\n0.71\n0.65\nyes\n\n\nVSP Phone Call\n0.86\n0.78\n0.84\n0.73\n0.64\nyes\n\n\nVSP Documentary\n0.87\n0.84\n0.81\n0.66\n0.69\nyes\n\n\nArabic Speech Corpus\n0.62\n0.38\n0.59\n0.68\n0.43\nyes\n\n\nHigh Quality Podcasts\n0.85\n0.74\n0.71\n0.71\n0.52\nyes\n\n\nJapanese Shortwave\n0.90\n0.65\n0.84\n0.68\n0.92\nyes\n\n\nConference\n0.88\n0.79\n0.63\n0.63\n0.48\nyes\n\n\nEnglish Podcast\n0.89\n0.78\n0.78\n0.73\n0.48\nyes\n\n\nFleurs English\n0.90\n0.84\n0.56\n0.73\n0.89\nno\n\n\nDipco\n0.88\n0.87\n0.54\n0.41\n0.90\nno\n\n\nDigitized Cassette\n0.90\n0.87\n0.89\n0.73\n0.87\nno\n\n\nLibrivox\n0.86\n0.83\n0.86\n0.73\n0.65\nno\n\n\nOld Radio\n0.90\n0.76\n0.65\n0.72\n0.51\nno\n\n\nPhone Home\n0.89\n0.69\n0.69\n0.73\n0.83\nno\n\n\nRussian Audiobook\n0.89\n0.56\n0.48\n0.46\n0.53\nno\n\n\nMandarin Podcast 2\n0.90\n0.87\n0.77\n0.73\n0.91\nno\n\n\nVSP Home Mic\n0.88\n0.83\n0.88\n0.73\n0.59\nno\n\n\nRadio Drama\n0.89\n0.82\n0.78\n0.73\n0.54\nno\n\n\nVSP Professional\n0.89\n0.76\n0.79\n0.73\n0.67\nno\n\n\nAverage Bal. Acc.\n0.87\n0.77\n0.74\n0.68\n0.67",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Source</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ISP</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">VIP</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">JAI</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ANO</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DMF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Public Split</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Mandarin Podcast 1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.87</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.89</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fleurs German</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">0.86</td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.72</td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Semi-professional</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n<td class=\"ltx_td ltx_align_right\">0.89</td>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_right\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Phone Call</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.84</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.64</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Documentary</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_right\">0.84</td>\n<td class=\"ltx_td ltx_align_right\">0.81</td>\n<td class=\"ltx_td ltx_align_right\">0.66</td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Arabic Speech Corpus</td>\n<td class=\"ltx_td ltx_align_right\">0.62</td>\n<td class=\"ltx_td ltx_align_right\">0.38</td>\n<td class=\"ltx_td ltx_align_right\">0.59</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.68</span></td>\n<td class=\"ltx_td ltx_align_right\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">High Quality Podcasts</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_align_right\">0.74</td>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_right\">0.52</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Japanese Shortwave</td>\n<td class=\"ltx_td ltx_align_right\">0.90</td>\n<td class=\"ltx_td ltx_align_right\">0.65</td>\n<td class=\"ltx_td ltx_align_right\">0.84</td>\n<td class=\"ltx_td ltx_align_right\">0.68</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Conference</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_right\">0.79</td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_right\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English Podcast</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">yes</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Fleurs English</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.84</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.56</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.73</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Dipco</td>\n<td class=\"ltx_td ltx_align_right\">0.88</td>\n<td class=\"ltx_td ltx_align_right\">0.87</td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n<td class=\"ltx_td ltx_align_right\">0.41</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Digitized Cassette</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">0.87</td>\n<td class=\"ltx_td ltx_align_right\">0.89</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Librivox</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Old Radio</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.65</td>\n<td class=\"ltx_td ltx_align_right\">0.72</td>\n<td class=\"ltx_td ltx_align_right\">0.51</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phone Home</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Russian Audiobook</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.56</td>\n<td class=\"ltx_td ltx_align_right\">0.48</td>\n<td class=\"ltx_td ltx_align_right\">0.46</td>\n<td class=\"ltx_td ltx_align_right\">0.53</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Mandarin Podcast 2</td>\n<td class=\"ltx_td ltx_align_right\">0.90</td>\n<td class=\"ltx_td ltx_align_right\">0.87</td>\n<td class=\"ltx_td ltx_align_right\">0.77</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Home Mic</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Radio Drama</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.82</td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VSP Professional</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.79</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">no</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Average Bal. Acc.</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.77</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "jai",
            "real",
            "task",
            "semiprofessional",
            "source",
            "quality",
            "public",
            "conditioned",
            "vsp",
            "yes",
            "tpr",
            "german",
            "split",
            "dmf",
            "home",
            "mic",
            "used",
            "speech",
            "high",
            "documentary",
            "conference",
            "mandarin",
            "balanced",
            "ano",
            "english",
            "russian",
            "calculation",
            "from",
            "podcast",
            "old",
            "digitized",
            "call",
            "japanese",
            "acc",
            "phone",
            "vip",
            "podcasts",
            "arabic",
            "corpus",
            "librivox",
            "drama",
            "average",
            "audiobook",
            "accuracy",
            "professional",
            "isp",
            "shortwave",
            "dipco",
            "bal",
            "cassette",
            "radio",
            "fleurs"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "split",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "public"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "balanced",
                    "real",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "public",
                    "split",
                    "used",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "from",
                    "balanced",
                    "task",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "average",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task",
                    "source",
                    "public",
                    "conditioned",
                    "balanced",
                    "tpr",
                    "split",
                    "average",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "public",
                    "real",
                    "task",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "real",
                    "source",
                    "public",
                    "split",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "digitized",
                    "high",
                    "japanese",
                    "real",
                    "quality",
                    "phone",
                    "english",
                    "mandarin",
                    "russian",
                    "podcasts",
                    "german",
                    "arabic",
                    "from",
                    "radio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "source",
                    "public",
                    "from",
                    "split",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "real",
                    "task",
                    "phone",
                    "public",
                    "from",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "average",
                    "real",
                    "task",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task",
                    "balanced",
                    "tpr",
                    "from",
                    "split",
                    "dmf",
                    "accuracy",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "from",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shortwave",
                    "japanese",
                    "real",
                    "quality",
                    "source",
                    "phone",
                    "russian",
                    "vip",
                    "arabic",
                    "corpus",
                    "radio",
                    "home",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both tables, the upper rows represent models and sources from the public split, while the lower rows correspond to those exclusive to the private split. Because participants could iteratively improve their algorithms through repeated submissions, we would expect performance on models from the public split to be higher. We saw some evidence of this in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> but the difference was not drastic suggesting that either the algorithms do generalize or the participants had private split models in their training sets. On the reals, we did not see any clear performance difference between private and public splits.</p>\n\n",
                "matched_terms": [
                    "from",
                    "public",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "conditioned",
                    "real",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task",
                    "phone",
                    "vip",
                    "from",
                    "used",
                    "accuracy",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "task",
                    "ano",
                    "from",
                    "used",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "task",
                    "conditioned",
                    "vip",
                    "from",
                    "used",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "high",
                    "real",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "real",
                    "task",
                    "balanced",
                    "from",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Task 2, the updated scores for all augmentation techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F4\" title=\"Figure 4 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, The updated scores for Task 3 for all laundering techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F5\" title=\"Figure 5 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "from",
                    "real",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "jai",
                    "ano",
                    "dmf",
                    "used"
                ]
            }
        ]
    },
    "S5.T9": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 9: Balanced accuracy conditioned on the augmentation type applied to generated data in Task 2.",
        "body": "Augmentation\nISP\nVIP\nJAI\nANO\nDMF\n\n\n\n\nAAC 16k\n0.77\n0.72\n0.82\n0.63\n0.59\n\n\nEncodec\n0.92\n0.66\n0.85\n0.80\n0.58\n\n\nFocalcodec\n0.95\n0.63\n0.83\n0.88\n0.57\n\n\nMP3AACmp3 16k\n0.67\n0.79\n0.83\n0.63\n0.59\n\n\nMP3AAC 16k\n0.73\n0.78\n0.82\n0.61\n0.59\n\n\nMP3 16k\n0.83\n0.78\n0.78\n0.60\n0.58\n\n\nMP3 VBR\n0.84\n0.74\n0.76\n0.67\n0.58\n\n\nNoise\n0.70\n0.45\n0.54\n0.60\n0.58\n\n\nOpus 16k\n0.69\n0.73\n0.83\n0.73\n0.61\n\n\nPhone audio\n0.71\n0.61\n0.79\n0.64\n0.56\n\n\nPitch shift\n0.85\n0.70\n0.77\n0.88\n0.60\n\n\nResample down\n0.77\n0.63\n0.76\n0.68\n0.57\n\n\nResample up\n0.82\n0.61\n0.76\n0.68\n0.57\n\n\nSemanticodec\n0.83\n0.67\n0.85\n0.89\n0.58\n\n\nSnac\n0.78\n0.65\n0.81\n0.87\n0.60\n\n\nSpeech filter\n0.76\n0.61\n0.67\n0.66\n0.45\n\n\nTime stretch\n0.85\n0.69\n0.78\n0.89\n0.61\n\n\nVorbis 16k\n0.86\n0.67\n0.75\n0.67\n0.57\n\n\nAverage Bal. Acc.\n0.80\n0.67\n0.78\n0.72\n0.58",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Augmentation</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ISP</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">VIP</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">JAI</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ANO</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DMF</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AAC 16k</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.77</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.63</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Encodec</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_right\">0.66</td>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Focalcodec</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.95</span></td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_right\">0.88</td>\n<td class=\"ltx_td ltx_align_right\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MP3&#8211;AAC&#8211;mp3 16k</th>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.79</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.83</span></td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_right\">0.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MP3&#8211;AAC 16k</th>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n<td class=\"ltx_td ltx_align_right\">0.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MP3 16k</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.83</span></td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.60</td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MP3 VBR</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n<td class=\"ltx_td ltx_align_right\">0.74</td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Noise</th>\n<td class=\"ltx_td ltx_align_right\">0.70</td>\n<td class=\"ltx_td ltx_align_right\">0.45</td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n<td class=\"ltx_td ltx_align_right\">0.60</td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Opus 16k</th>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.83</span></td>\n<td class=\"ltx_td ltx_align_right\">0.73</td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Phone audio</th>\n<td class=\"ltx_td ltx_align_right\">0.71</td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.79</span></td>\n<td class=\"ltx_td ltx_align_right\">0.64</td>\n<td class=\"ltx_td ltx_align_right\">0.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Pitch shift</th>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n<td class=\"ltx_td ltx_align_right\">0.70</td>\n<td class=\"ltx_td ltx_align_right\">0.77</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_right\">0.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Resample down</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_right\">0.63</td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.68</td>\n<td class=\"ltx_td ltx_align_right\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Resample up</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n<td class=\"ltx_td ltx_align_right\">0.76</td>\n<td class=\"ltx_td ltx_align_right\">0.68</td>\n<td class=\"ltx_td ltx_align_right\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Semanticodec</th>\n<td class=\"ltx_td ltx_align_right\">0.83</td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Snac</th>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\">0.65</td>\n<td class=\"ltx_td ltx_align_right\">0.81</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_right\">0.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Speech filter</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.66</td>\n<td class=\"ltx_td ltx_align_right\">0.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Time stretch</th>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.78</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_right\">0.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vorbis 16k</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.75</td>\n<td class=\"ltx_td ltx_align_right\">0.67</td>\n<td class=\"ltx_td ltx_align_right\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Average Bal. Acc.</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.78</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.58</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "opus",
            "jai",
            "type",
            "task",
            "mp3aacmp3",
            "conditioned",
            "focalcodec",
            "snac",
            "dmf",
            "16k",
            "speech",
            "applied",
            "semanticodec",
            "mp3aac",
            "shift",
            "balanced",
            "ano",
            "filter",
            "encodec",
            "pitch",
            "augmentation",
            "noise",
            "acc",
            "phone",
            "vorbis",
            "vip",
            "mp3",
            "average",
            "stretch",
            "accuracy",
            "isp",
            "aac",
            "down",
            "time",
            "generated",
            "bal",
            "vbr",
            "data",
            "audio",
            "resample"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in synthetic audio generation that are driven by increasingly sophisticated text-to-speech (TTS) models and speech manipulation techniques pose significant challenges to the authenticity and trustworthiness of audio content. As synthetic speech becomes more realistic and widely accessible, malicious actors are increasingly able to create convincing forgeries that can undermine public trust, enable fraud and impersonation, and threaten security-sensitive applications.\nAs a result, forensic detection methods must evolve to keep pace with the growing threat of synthetic audio, particularly in scenarios involving post-processing, compression, or intentional laundering designed to evade forensic analysis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "balanced",
                    "data",
                    "generated",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Blind Evaluation Protocol</span>: We designed and deployed a fully blind evaluation framework where participants had no access to evaluation data, ensuring an unbiased assessment of model generalization to unseen audio sources and manipulations. During Round 1 (70 days) of the competition, we received more than 700 submissions from 12 teams.</p>\n\n",
                "matched_terms": [
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source-Balanced Dataset Design</span>: We constructed a balanced evaluation set to fairly assess model performance across a diverse range of human and machine-generated audio comprising 13 TTS generative models and 21 real sources for a total of 90 hours and 22,700 audio samples.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rapid developments in generative AI have led to the creation of\nAI systems capable of creating realistic synthetic speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib1\" title=\"\">1</a>]</cite>. In response to this, a number of forensic systems have been developed to detect synthetic audio, and several datasets and challenges have been designed to further research in this area.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generated",
                    "time",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Identification Competitions.</span>\nA range of challenges and competitions have emerged in response to the growing prevalence of AI-generated or &#8220;deepfake\" audio. The ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib6\" title=\"\">6</a>]</cite> were among the earliest of these challenges to arise. These challenges primarily focused on speaker verification, i.e. determining whether speech was produced by a known speaker or by using a synthetic audio system to spoof that speaker, as opposed to detecting synthetic audio from an unknown source.\nSeveral synthetic audio detectors &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite> were also developed during the DARPA Semantic Forensics (SemaFor) program, however, evaluations within this program were limited to program performers and were not accessible to the wider research community. Finally, in 2022, the IEEE Signal Processing Cup hosted a competition that tasked participants with synthetic audio attribution&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib8\" title=\"\">8</a>]</cite>, but its primary audience was undergraduate students, limiting its impact on the broader research field.\nIn contrast, the SAFE competition was open to the full research community and pursued two main objectives: (1) to catalyze the development of robust synthetic audio detection systems capable of generalizing across unknown speakers, and (2) to identify persistent challenges and performance gaps in current detection methodologies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "data",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "balanced",
                    "generated",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "applied",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "applied",
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "average",
                    "data",
                    "time",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "task",
                    "balanced",
                    "conditioned",
                    "average",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "average",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sources spanned many common ways audio is consumed, including face-to-face speech, phone calls, radio, podcasts, and audio from video sharing platforms. They also contained speech from multiple languages, including Arabic, English, German, Japanese, Mandarin, and Russian. Audio quality varied from high quality podcasts and radio dramas to telephone audio and digitized cassettes. The diversity of sourcing and audio quality forced participants to build detectors that are robust to a wide distribution of real data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "audio",
                    "phone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "average",
                    "generated",
                    "data",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "opus",
                    "applied",
                    "noise",
                    "aac",
                    "task",
                    "down",
                    "shift",
                    "phone",
                    "average",
                    "vorbis",
                    "data",
                    "mp3",
                    "filter",
                    "generated",
                    "pitch",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "applied",
                    "task",
                    "average",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "task",
                    "balanced",
                    "data",
                    "dmf",
                    "generated",
                    "accuracy",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "conditioned",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "time",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "phone",
                    "vip",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "opus",
                    "noise",
                    "aac",
                    "task",
                    "phone",
                    "vip",
                    "mp3",
                    "accuracy",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "task",
                    "generated",
                    "ano",
                    "encodec",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "applied",
                    "task",
                    "conditioned",
                    "vip",
                    "generated",
                    "audio",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "task",
                    "balanced",
                    "data",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Task 2, the updated scores for all augmentation techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F4\" title=\"Figure 4 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "time",
                    "balanced",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "generated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "noise",
                    "jai",
                    "shift",
                    "ano",
                    "dmf",
                    "data"
                ]
            }
        ]
    },
    "S6.T10": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 10: Balanced accuracy conditioned on the laundering type applied to generated data in Task 3.",
        "body": "Laundering Technique\nISP\nVIP\nJAI\nANO\nDMF\n\n\n\n\nCar\n0.67\n0.50\n0.53\n0.62\n0.57\n\n\nPlayed\n0.62\n0.54\n0.54\n0.69\n0.54\n\n\nReverb\n0.75\n0.46\n0.70\n0.59\n0.58\n\n\nPlayed + Reverb + Car\n0.58\n0.69\n0.49\n0.72\n0.35\n\n\nAverage Bal. Acc.\n0.66\n0.55\n0.57\n0.65\n0.51",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Laundering Technique</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ISP</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">VIP</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">JAI</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ANO</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DMF</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Car</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.67</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.50</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.53</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.62</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Played</th>\n<td class=\"ltx_td ltx_align_right\">0.62</td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n<td class=\"ltx_td ltx_align_right\">0.54</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Reverb</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.75</span></td>\n<td class=\"ltx_td ltx_align_right\">0.46</td>\n<td class=\"ltx_td ltx_align_right\">0.70</td>\n<td class=\"ltx_td ltx_align_right\">0.59</td>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Played + Reverb + Car</th>\n<td class=\"ltx_td ltx_align_right\">0.58</td>\n<td class=\"ltx_td ltx_align_right\">0.69</td>\n<td class=\"ltx_td ltx_align_right\">0.49</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.72</span></td>\n<td class=\"ltx_td ltx_align_right\">0.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Average Bal. Acc.</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.66</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.55</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.57</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.51</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reverb",
            "jai",
            "type",
            "task",
            "conditioned",
            "played",
            "dmf",
            "applied",
            "balanced",
            "ano",
            "technique",
            "acc",
            "vip",
            "laundering",
            "average",
            "accuracy",
            "isp",
            "generated",
            "car",
            "bal",
            "data"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3 Results.</span>\nThis task presented the most difficult challenge. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S6.T10\" title=\"Table 10 &#8227; 6 Conclusion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows BAC conditioned on the laundering method applied only to the generated samples. Here, we used our existing benchmark detectors to select a set of operations that maximally decreased the detection rate. These include adding real-life car background noise, applying reverb, and recording sound played over the air. While all laundering techniques were effective on their own, the combination of all three was the most difficult to detect. Even on its own, recording synthetic audio being played over the air resulted in significant degradation. ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> on unprocessed samples in Task 1 to <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m2\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p8.m4\" intent=\":literal\"><semantics><mn>.50</mn><annotation encoding=\"application/x-tex\">.50</annotation></semantics></math> This effect likely occurred due to reintroducing the artifacts of the complete recording pipeline, including the computer analog-to-digital converter, speaker amplifier, acoustic environment, microphone amplifier, and digital-to-analog, etc. While this laundering technique is not scalable, it proved highly effective. These observations point to a potential and easily exploitable vulnerability in the detection of synthetic audio using current methods.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at <a class=\"ltx_ref ltx_href\" href=\"https://stresearch.github.io/SAFE/\" title=\"\">https://stresearch.github.io/SAFE/</a>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these growing threats, we created the SAFE (Synthetic Audio Forensic Evaluation) Challenge. SAFE emphasizes critical research areas that include robustness across diverse data sources (including both generated and real audio), resilience to emerging laundering techniques, and computational efficiency for practical deployment considerations. SAFE aims to provide a rigorous, blind evaluation framework that targets three key areas of forensic analysis: detection of 1) raw synthetic speech, 2) compressed and resampled synthetic audio, and 3) audio subjected to laundering attacks intended to obfuscate synthetic origins. By benchmarking performance across a diverse, balanced set of real and synthetic sources under controlled computational conditions, SAFE aimed to evaluate not only detection accuracy, but also generalization across unseen sources and resilience to realistic adversarial conditions.\nThis competition makes the following key contributions:</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "data",
                    "laundering",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "data",
                    "accuracy",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Calibrating the difficulty of the competition was guided by several key principles. First, the tasks were intentionally structured to increase in difficulty, ensuring a progression from basic detection to more challenging, real-world adversarial scenarios. To achieve this difficulty gradient, we chose task objectives and underlying datasets that represent semantically different, and increasingly more complicated and adversarial, problem spaces. The first task consisted of raw synthetically generated audio; the second task consisted of synthetically generated audio compressed in differing ways, adding simulated complexity of real-world conditions; and the third task consisted of an adversarial approach that attempted to fool participant detectors by adding various background noise, which we refer to as laundering attacks. Second, we designed the difficulty of the first task to be balanced on two baseline audio detectors developed under the DARPA SemaFor program&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib7\" title=\"\">7</a>]</cite>. Setting the balanced accuracy of both models to <math alttext=\".83\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mn>.83</mn><annotation encoding=\"application/x-tex\">.83</annotation></semantics></math> ensured that the first task was challenging enough to allow for meaningful improvements in detection model design but not so difficult that participants could not make progress given the limited feedback.</p>\n\n",
                "matched_terms": [
                    "task",
                    "balanced",
                    "laundering",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1. Detection of synthetic audio</span> focused purely on the detection of generated voice audio from popular TTS models. No modifications or post-processing were applied to the generated audio or the real sources. Information about the 21 real audio sources and 13 synthetic models are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "applied",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2. Detection of processed synthetic audio</span> tested how detection performance is affected by common forms of post-processing, such as audio compression codecs and resampling. The post-processing only applied to generated audio, while the real data from Task 1 remained the same. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> lists the 19 different post-processing methods used.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "applied",
                    "task",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 3. Detection of laundered synthetic audio</span> tested detection performance on laundered audio. In this task,\nthe generated audio files were laundered to purposefully evade detection, while the real dataset remained unaltered.\nFour different laundering methods shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> were applied.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "applied",
                    "task",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "average",
                    "generated",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main metric for the competition was balanced accuracy (BAC) defined as an average of the true positive (TPR) and the true negative rates (TNR).\nWe provided limited feedback on any given submission&#8217;s performance. The competition maintained both a public and a private leaderboard, and participants only had access to the public leaderboard while the competition was active. The public leaderboard showed these metrics on the entire public split as well as conditioned on every real and generated source. We anonymized the names of sources on the public leaderboard to allow for true black-box testing. The same evaluation criteria applies to each task, and the final evaluation was based on balanced accuracy over the private set.</p>\n\n",
                "matched_terms": [
                    "task",
                    "balanced",
                    "conditioned",
                    "average",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The real audio for the SAFE Challenge contained <math alttext=\"21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21</mn><annotation encoding=\"application/x-tex\">21</annotation></semantics></math> diverse sources of audio. All audio was kept in its original format (codec and sampling rate). The real audio portion of the dataset contained 200 samples per source for a total of 4,200 audio sources and 18.25 hours of data. The average length per source was .86 hours, and the average clip length was 15.64 seconds. This data source summary, along with additional metrics and descriptions, are listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Tasks &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Almost half (10 out 21 sources) were included in the public split.</p>\n\n",
                "matched_terms": [
                    "average",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generators:</span> The machine generated audio for the SAFE Challenge was built from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math> high-performing text-to-speech models. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for more details. Only 7 out of 13 models were included in the public split. The models were a mix of proprietary (Cartesia, Edge, ElevenLabs, and OpenAI) and open source. We created 200 samples per generated source for a total of 2,600 audio sources and 8.8 hours of data. The average length per source was .68 hours, and the average clip length was 12.24 seconds. The the sample rate of clips and whether or not the model supports voice cloning is displayed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Evaluation Criteria &#8227; 3 Competition Overview &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Where available, we saved the audio as uncompressed WAVE files at the native sampling rate of the model.</p>\n\n",
                "matched_terms": [
                    "average",
                    "generated",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "task",
                    "data",
                    "average",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Laundering:</span> In Task <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>, we applied four different laundering techniques shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T4\" title=\"Table 4 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> to the generated audio,\nwhile keeping the real audio the same.\nThe first technique added real-life car background noise to the generated audio clips. The second technique played the audio clips over the air and recorded them. The third technique added reverberation to the generated audio. The final technique combined the previous techniques by first adding the car background noise, adding reverberation, then playing that back over the air while recording. For each source, we created 50 samples, totaling 200 samples across the 4 augmentations. Across all 13 sources, 2,600 total samples were generated, corresponding to 8.8 hours of audio. The average clip length was 12.2 seconds.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "task",
                    "car",
                    "technique",
                    "played",
                    "laundering",
                    "average",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "task",
                    "balanced",
                    "data",
                    "laundering",
                    "dmf",
                    "generated",
                    "accuracy",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 1 Results.</span> The performance of the top 5 teams for Task 1 is displayed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T7\" title=\"Table 7 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which shows BAC conditioned on the specific generator model. This was computed by averaging TPR conditioned on the generator and TNR on all real data from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Additionally, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T8\" title=\"Table 8 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows BAC conditioned on the source of real data computed in an analogous manner.\nTop submissions performed surprisingly well across a large collection of recent generators and diverse real sources. Performing this well is impressive considering that none of the details of the competition were unknown apriori. Even during the competition the specifics of the sources and models were not shared. The following tables show Task 1 performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "conditioned",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated audio from newer models (such as Zonos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib26\" title=\"\">26</a>]</cite> and Edge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib21\" title=\"\">21</a>]</cite> with BAC <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> by ISP team) were the hardest to detect, while popular models that have been available for some time (such as ElevenLabs and Seamless with ISP BAC of <math alttext=\".97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>.97</mn><annotation encoding=\"application/x-tex\">.97</annotation></semantics></math> and <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>) were detected more easily. Interestingly, older but more obscure models (such as Hierspeech with BAC <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math>) were also difficult.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For real audio, some of the rare non-English audio, such as the Arabic Speech Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib9\" title=\"\">9</a>]</cite> (ISP had BAC of <math alttext=\".62\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mn>.62</mn><annotation encoding=\"application/x-tex\">.62</annotation></semantics></math>), Japanese shortwave radio, and Russian audio books (VIP had BAC of <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math> and <math alttext=\".56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mn>.56</mn><annotation encoding=\"application/x-tex\">.56</annotation></semantics></math>), formed the hardest challenges to detect, potentially due to the dominance of other large resource languages in training. Poor recording quality in older audio, such as the Phone Home source (VIP had BAC of <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>), also gave the detectors more difficulty.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task 2 Results.</span>\nIn this task, the real audio remained unmodified, while several operations were applied to the generated audio. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S5.T9\" title=\"Table 9 &#8227; 5.1 Task Specific Discussion &#8227; 5 Round 1 Results &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows BAC conditioned on specific operation type again with TNR computed over the real data.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "type",
                    "task",
                    "conditioned",
                    "data",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results expose several consistent failure modes across all participants. Adding Gaussian noise with a signal-to-noise ratio of 15-40dB degraded the performance of the detectors most consistently across the board (ISP BAC dropped from <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> in Task 1 to <math alttext=\".70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m2\" intent=\":literal\"><semantics><mn>.70</mn><annotation encoding=\"application/x-tex\">.70</annotation></semantics></math> while VIP dropped from <math alttext=\".77\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m3\" intent=\":literal\"><semantics><mn>.77</mn><annotation encoding=\"application/x-tex\">.77</annotation></semantics></math> to <math alttext=\".45\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m4\" intent=\":literal\"><semantics><mn>.45</mn><annotation encoding=\"application/x-tex\">.45</annotation></semantics></math>). Speech-specific processing and codecs, such as Opus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>]</cite>, Phone Audio, and Speech filtering, also caused a significant negative effect on detection accuracy. For example, ISP BAC dropped to <math alttext=\".69\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m5\" intent=\":literal\"><semantics><mn>.69</mn><annotation encoding=\"application/x-tex\">.69</annotation></semantics></math>, <math alttext=\".71\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m6\" intent=\":literal\"><semantics><mn>.71</mn><annotation encoding=\"application/x-tex\">.71</annotation></semantics></math> and <math alttext=\".76\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p6.m7\" intent=\":literal\"><semantics><mn>.76</mn><annotation encoding=\"application/x-tex\">.76</annotation></semantics></math> respectively. Lastly, chaining multiple operations together, such as converting between MP3 and AAC codecs, degraded performance significantly as well. This shows that further research should be conducted to make detectors more robust against these post-processing operations. Furthermore, such techniques could be used to intentionally avoid detection by forensic systems, as our Task&#160;3 resutls show.</p>\n\n",
                "matched_terms": [
                    "vip",
                    "accuracy",
                    "task",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "task",
                    "ano",
                    "generated",
                    "isp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we provided an overview of the Synthetic Audio Forensics Evaluation (SAFE) Challenge that tested the robustness of synthetic audio detectors against a diversity of real sources, unknown audio generation models, and benign and malicious audio processing operations. We ran and detailed a unique audio forensic competition where participants were not given training data, had limited knowledge of how data was created, and were provided with limited performance feedback. Under these difficult conditions, several submissions achieved high accuracy with limited degradation under benign audio processing. However, targeted laundering still resulted in significant reduction in detection performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "accuracy",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "task",
                    "balanced",
                    "data",
                    "generated",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, The updated scores for Task 3 for all laundering techniques from Round 2 are located in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F5\" title=\"Figure 5 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "laundering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "balanced",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was also some miscalibration among the algorithms with some favoring lower false positive rates over higher true positive rates, as seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F7\" title=\"Figure 7 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. However, DMF argued that in real applications not detecting generated audio is usually more costly than a false alarm.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "data",
                    "jai",
                    "ano"
                ]
            }
        ]
    },
    "A1.T11": {
        "source_file": "Synthetic Audio Forensics Evaluation (SAFE) Challenge",
        "caption": "Table 11: Training datasets used by each team.",
        "body": "Dataset\nANO\nDMF\nJAI\nISPL\nISSF\n\n\nASVspoof2021 [33]\n\nX\n\n\n\n\n\n\nADD 2023 Track 1.2 Test R2 [34]\n\nX\n\n\n\n\n\n\nCtrSVDD 2024 [35]\n\nX\n\n\n\n\n\n\nEmilia dataset [36]\n\nX\n\n\n\n\n\n\nCosyVoice2 [37]\n\nX\n\n\n\n\n\n\nASVspoof2019 [2]\n\n\nX\n\nX\nX\n\n\nCodecfake [38]\n\n\nX\n\n\nX\n\n\nCFAD [39]\n\n\nX\n\n\n\n\n\nWaveFake [40]\n\n\nX\n\n\n\n\n\nIn-the-Wild [41]\n\n\nX\n\nX\n\n\n\nJMAD dataset [42]\n\n\n\nX\n\n\n\n\nFake-or-Real [43]\n\n\n\n\nX\n\n\n\nMLAAD [44]\n\n\n\n\nX\nX\n\n\nDiffSSD [4]\n\n\n\n\nX\n\n\n\nLibriSpeech [45]\n\n\n\n\nX\n\n\n\nLJSpeech [46]\n\n\n\n\nX\n\n\n\nVCTK [47]\n\n\n\n\nX\n\n\n\nMozilla CommonVoice [48]\n\n\n\n\nX\n\n\n\nSpoofCeleb [49]\n\n\n\n\n\nX\n\n\nM-AILABS [50]\n\n\n\n\n\nX\n\n\nFamous Figures (in-house dataset)\n\n\n\n\nX",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">ANO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">DMF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">JAI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">ISPL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">ISSF</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">ASVspoof2021 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib33\" title=\"\">33</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">ADD 2023 Track 1.2 Test R2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib34\" title=\"\">34</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">CtrSVDD 2024 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib35\" title=\"\">35</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Emilia dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib37\" title=\"\">37</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Codecfake <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib38\" title=\"\">38</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">CFAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib39\" title=\"\">39</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">WaveFake <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib40\" title=\"\">40</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">In-the-Wild <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib41\" title=\"\">41</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">JMAD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib42\" title=\"\">42</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Fake-or-Real <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib43\" title=\"\">43</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">MLAAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib44\" title=\"\">44</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">DiffSSD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib45\" title=\"\">45</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">LJSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">VCTK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mozilla CommonVoice <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib48\" title=\"\">48</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SpoofCeleb <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">M-AILABS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib50\" title=\"\">50</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Famous Figures (in-house dataset)</td>\n<td class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">X</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "jai",
            "figures",
            "training",
            "librispeech",
            "mlaad",
            "issf",
            "datasets",
            "dmf",
            "mailabs",
            "used",
            "vctk",
            "track",
            "each",
            "add",
            "spoofceleb",
            "test",
            "ctrsvdd",
            "asvspoof2021",
            "ano",
            "ispl",
            "asvspoof2019",
            "codecfake",
            "diffssd",
            "jmad",
            "mozilla",
            "inthewild",
            "fakeorreal",
            "dataset",
            "ljspeech",
            "emilia",
            "famous",
            "cfad",
            "cosyvoice2",
            "inhouse",
            "commonvoice",
            "team",
            "wavefake"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The two main factors that could affect team performance are the training data and model architectures used. The SAFE dataset was curated specifically to be varied and encourage the participants to build models that generalize well. The participating teams tended to use different training data from each other, however, ASVspoof2019 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite> was used by three different teams. Two teams used datasets curated in-house with one team relying only on that dataset. ISPL trained on the most datasets, providing anecdotal evidence that a wider set of training data boosts performance. ISSF used a systematic iterative approach to their dataset generation, starting with a single source ASVspoof2019 and increasing the number of sources they trained on while tracking performance. They chose the added datasets to specifically augment identified weaknesses in the previous version of their training dataset. The training datasets used by the teams who presented at IH&amp;MMSEC 2025 are in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.T11\" title=\"Table 11 &#8227; A.1 Training Data Used By Teams &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Audio Datasets.</span>\nHigh-quality datasets containing both real and synthetic audio are essential for advancing research on synthetic audio detection and for benchmarking the performance of detection systems. To keep pace with the rapidly evolving landscape of generative audio technology, several synthetic audio datasets have been developed. Among the earliest are those created for the ASVspoof challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib2\" title=\"\">2</a>]</cite>, which focus on spoofed audio for automatic speaker verification. Salvi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib3\" title=\"\">3</a>]</cite> introduced the TIMIT-TTS dataset to provide realistic synthetic speech tracks aligned with deepfake videos for audio and multimodal forensic analysis, using a pipeline that integrates text-to-speech synthesis with dynamic time warping. More recently, Bhagtani et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib4\" title=\"\">4</a>]</cite> released the DiffSSD dataset to fill a critical gap by offering synthetic audio generated using diffusion-based methods&#8212;an increasingly prevalent class of generative models.</p>\n\n",
                "matched_terms": [
                    "diffssd",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SAFE Challenge aimed to mobilize the research community to advance the state of the art in audio voice forensics and drive innovation in detecting synthetic and manipulated audio artifacts. The challenge focused on several critical aspects including generalizability across diverse audio sources and newly emerging synthesis models, robustness to benign post-processing and targeted laundering methods. The competition was intentionally designed to mimic real-world setting. Participants did not have access to the dataset and were not aware of the specific sources or generative models used in data creation. We provided only minimal feedback in the form of top-level accuracy across a subset of anonymized sources in the public split.</p>\n\n",
                "matched_terms": [
                    "used",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further simulate real world settings, we required models to make a binary decision of either \"real\" or \"generated\" in their submission file. Therefore, the evaluation was performed at a confidence threshold chosen by each participant in each test sample. However, to facilitate further analysis, the submission file also had to contain a decision score and an average inference time for each test data point input file.</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each task, the dataset consisted of a public and a private split. The public split was a subset of the private split. The public split consisted of 10 out of 21 real sources and 7 out of 13 TTS generator sources. The private split contained all the data. We made this choice to encourage participants to develop detectors that did not overfit to the public dataset.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-processing:</span> For Task <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> of the SAFE Challenge, we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different augmentations, mostly consisting of common compression schemes, to the generated dataset, while the real data remains the same. We chose these augmentations to mimic processes that could reasonably have been applied to audio on the internet. For each model, we randomly sampled <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips from those generated for Task <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and we applied <math alttext=\"18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>18</mn><annotation encoding=\"application/x-tex\">18</annotation></semantics></math> different compressions to these <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> clips for a total of <math alttext=\"380\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>380</mn><annotation encoding=\"application/x-tex\">380</annotation></semantics></math> samples per model, including the unaugmented clips. In total, the augmented data contained 16.8 hours of audio with an average clip length of 12.25 seconds. The augmentations included well-known compression algorithms such as AAC, MP3, Opus, and Vorbis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib28\" title=\"\">28</a>]</cite>. The augmentations also included resampling up and down, neural codecs, a speech filter, pitch shift, Gaussian noise, speeding up the audio, and a compression meant to mimic phone audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T3\" title=\"Table 3 &#8227; 4.1 Sourcing Real Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> contains a list of augmentations applied to the audio files, which were applied to the public and private splits.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows Round 1 competition results for the top five performing teams<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>ranked by balanced accuracy on the private split for the Task 1 at the end of Round 1</span></span></span>. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T5\" title=\"Table 5 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the team names and their institution. Generally, we observed an expected trend of balanced accuracy decreasing as tasks became more difficult. From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Machine Generated Audio &#8227; 4 Dataset Description &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that detecting unprocessed generated audio (Task 1) followed the expected outcome of being the easiest task, with top performer (ISP) achieving a balanced accuracy (BAC) of&#160;<math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, True Positve Rate (TPR) of <math alttext=\".79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>.79</mn><annotation encoding=\"application/x-tex\">.79</annotation></semantics></math> and True Negative Rate (TNR) of <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>. However, when we applied common compression codecs and resampling, the TPR dropped to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> while maintaining a TNR of <math alttext=\".93\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>.93</mn><annotation encoding=\"application/x-tex\">.93</annotation></semantics></math>. The laundering methods had the most drastic effect in detector performance with TPR dropping to <math alttext=\".39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>.39</mn><annotation encoding=\"application/x-tex\">.39</annotation></semantics></math>. Recall, that for Tasks 2 and 3, the real data remained unchanged from Task 1, only the generated data was modified. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#S0.F1\" title=\"Figure 1 &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the same trend of decreasing performance over tasks with full ROC\ncurves per team (each dot corresponds to the team&#8217;s chosen decision threshold).\nThe full ROC curves provide insights into whether each detector was miscalibrated. For example, DMF in Task 1 operated at a highly unbalanced threshold corresponding to <math alttext=\".67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mn>.67</mn><annotation encoding=\"application/x-tex\">.67</annotation></semantics></math> BAC. Reducing FPR would improve BAC to <math alttext=\".7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><mn>.7</mn><annotation encoding=\"application/x-tex\">.7</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "dmf",
                    "each",
                    "team"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-encoding the audio using some of the latest neural codecs (such as Encodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib32\" title=\"\">32</a>]</cite> and Focal codec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib30\" title=\"\">30</a>]</cite>) did not significantly affect performance. (ISP BAC was <math alttext=\".92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m1\" intent=\":literal\"><semantics><mn>.92</mn><annotation encoding=\"application/x-tex\">.92</annotation></semantics></math> and <math alttext=\".95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m2\" intent=\":literal\"><semantics><mn>.95</mn><annotation encoding=\"application/x-tex\">.95</annotation></semantics></math>). These methods&#8217; similarity to decoders used in the synthetic generation models likely caused this effect. In some cases, post-processing actually improved detector performance overall (ANO and JAI BAC improved from <math alttext=\".68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m3\" intent=\":literal\"><semantics><mn>.68</mn><annotation encoding=\"application/x-tex\">.68</annotation></semantics></math> and <math alttext=\".74\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m4\" intent=\":literal\"><semantics><mn>.74</mn><annotation encoding=\"application/x-tex\">.74</annotation></semantics></math> in Task 1 to <math alttext=\".72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m5\" intent=\":literal\"><semantics><mn>.72</mn><annotation encoding=\"application/x-tex\">.72</annotation></semantics></math> and <math alttext=\".78\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p7.m6\" intent=\":literal\"><semantics><mn>.78</mn><annotation encoding=\"application/x-tex\">.78</annotation></semantics></math> in Task 2), potentially making the generated audio appear closer to other well known generators than the original pre-processed versions.</p>\n\n",
                "matched_terms": [
                    "jai",
                    "used",
                    "ano"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results from Round 2 follow the same trends as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, with balanced accuracy decreasing across subsequent tasks. The teams remain the same as Round <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> with the addition of ISSF from University of Michigan. The top performer for Tasks 1-3 was still ISPL with a balanced accuracies of <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math>, <math alttext=\".80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>.80</mn><annotation encoding=\"application/x-tex\">.80</annotation></semantics></math> and <math alttext=\".66\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mn>.66</mn><annotation encoding=\"application/x-tex\">.66</annotation></semantics></math>. The second best performing models in Tasks 2 and 3 differed from ISPL by only a few percentage points. The updated scores for Task 1 on the real and generated data from Round 2 are located in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F2\" title=\"Figure 2 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F3\" title=\"Figure 3 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "figures",
                    "ispl",
                    "issf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All teams exhibited significant improvements in their balanced accuracy over time, as depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#A1.F6\" title=\"Figure 6 &#8227; Appendix A Round 2 Discussion &#8227; Synthetic Audio Forensics Evaluation (SAFE) Challenge\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The highest performing team, ISPL, improved from a balanced accuracy of <math alttext=\".52\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mn>.52</mn><annotation encoding=\"application/x-tex\">.52</annotation></semantics></math> to <math alttext=\".87\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m2\" intent=\":literal\"><semantics><mn>.87</mn><annotation encoding=\"application/x-tex\">.87</annotation></semantics></math> between their initial and final submissions. Other teams had similar levels of improvement between their first and last submissions and exhibited improvements deep into Round 2. In every case, the highest initial balanced accuracy was below <math alttext=\".65\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m3\" intent=\":literal\"><semantics><mn>.65</mn><annotation encoding=\"application/x-tex\">.65</annotation></semantics></math>. This highlights one weakness of state of the art models currently: difficulty generalizing. For real world applications, there may not be ground truth available to use to retrain or recalibrate the models.</p>\n\n",
                "matched_terms": [
                    "team",
                    "ispl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There was some overlap between some of the performers in their choice of model architecture.\nThree of the five performers who presented at IH&amp;MMSEC, ANO, JAI and ISSF used Aasist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib51\" title=\"\">51</a>]</cite> as the base model for their detectors which is a GAN-based speech detection system. ANO combined Aasist with a sample weight learning module to help combat issues with distribution shift. JAI combined RawNet2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03387v2#bib.bib52\" title=\"\">52</a>]</cite> with Aasist and self-supervised-learning techniques. ISSF used a self-supervised-learning front-end for feature extraction and an Aasist backend for classification. DMF, like JAI, used RawNet2 as part of their detector pipeline. DMF tested various combinations of label noise learning (LNL), inter-sample distillation (ISD) and SSI (self-supervised initialization) as the input into the RawNet2 framework.\nISPL, the highest performer, used a mixture-of-experts approach (MOE) for their detector. Specifically, they used a mixture of implicitly localized experts (MILE) where each expert has a different model architecture, but the training data is the same across experts. ISPL settled on using three experts using LCNN + MelSpec, Resnet + MelSpec and ResNet + LogSpec.</p>\n\n",
                "matched_terms": [
                    "each",
                    "jai",
                    "training",
                    "ano",
                    "ispl",
                    "issf",
                    "dmf",
                    "used"
                ]
            }
        ]
    }
}