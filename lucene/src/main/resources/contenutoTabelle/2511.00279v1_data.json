{
    "S2.T1": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 1: Detailed architectural configuration for LongCat-ViT.",
        "body": "Patch Size\nPos Embed\nHidden Size\nIntermediate Size\nNum Layers\nNum Heads\nParameters (M)\n\n\n14\n2D-RoPE\n1280\n5184\n32\n16\n637",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Patch Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pos Embed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Hidden Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Intermediate Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Num Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Num Heads</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Parameters (M)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2D-RoPE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1280</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">5184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">637</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "detailed",
            "parameters",
            "size",
            "hidden",
            "2drope",
            "longcatvit",
            "pos",
            "num",
            "architectural",
            "configuration",
            "layers",
            "heads",
            "patch",
            "embed",
            "intermediate"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Design</span>\nLongCat-ViT is a Transformer-based encoder that retains the core structure of the conventional Vision Transformer while integrating several key enhancements: a unified patchification module for native image and video inputs, 2D rotary position embeddings (2D-RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite>, SwiGLU activation function, RMSNorm layer, a LayerScale module, and Query-Key normalization. Together, these refinements yield a more robust and efficient architecture compared to conventional designs.\nTo improve computational efficiency in video frame encoding during real-time interaction while maintaining model performance, we adopt a relatively lightweight model configuration.\nThe detailed hyper-parameters of the model are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Vision Encoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn line with common practice, a two-layer multilayer perceptron (MLP) with pre-normalization is employed as the vision-language projector to align visual and textual representations.\nMoreover, a 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> pixel-unshuffle operation is applied along the spatial dimension to mitigate the quadratic computational complexity associated with high-resolution inputs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "size",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "architectural",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Encoder</span>\n\nTo optimize response latency and accommodate speech inputs of arbitrary duration, the audio encoder is designed using a streaming architecture.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio encoder takes 80-dimensional Fbank features as input. The architecture incorporates a Pre-FFN module that reduces the audio sequence length by a factor of eight through frame splicing downsampling technique, with each frame representing an 80ms time window.\nThe core process is performed by a streaming encoder that maintains a Transformer-like structure while incorporating several key modifications: (1) a Pre-Norm configuration for enhanced training stability, and (2) replacing standard self-attention modules with FSMN layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib15\" title=\"\">2018</a>)</cite> to enable efficient feature processing within constrained context windows.\nTo balance latency and performance, we implement a hybrid approach where only the final six layers incorporate a one-frame look-ahead mechanism, while maintaining strict causality in the preceding layers.\nThe architecture concludes with a post-FFN module for additional feature refinement.\nThe audio encoder is trained under supervised learning using speech recognition data using the CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib16\" title=\"\">2006</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "configuration",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "parameters",
                    "longcatvit"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 2: Computation distribution per micro-batch across different modalities during the SFT stage.",
        "body": "Module\nComputational Cost (TFLOPs)\n\n\nmin\nmax\nmean\nstd\n\n\nAudio Encoder\n0.01\n109.96\n3.29\n7.94\n\n\nVision Encoder\n0.08\n400.37\n89.85\n61.02\n\n\nLLM Decoder\n1920.57\n4667.74\n3531.32\n1111.11",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Module</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">Computational Cost (TFLOPs)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">min</td>\n<td class=\"ltx_td ltx_align_center\">max</td>\n<td class=\"ltx_td ltx_align_center\">mean</td>\n<td class=\"ltx_td ltx_align_center\">std</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio Encoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">109.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Vision Encoder</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">400.37</td>\n<td class=\"ltx_td ltx_align_center\">89.85</td>\n<td class=\"ltx_td ltx_align_center\">61.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">LLM Decoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1920.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4667.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3531.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1111.11</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "cost",
            "sft",
            "stage",
            "tflops",
            "different",
            "module",
            "min",
            "llm",
            "distribution",
            "max",
            "audio",
            "std",
            "encoder",
            "computational",
            "computation",
            "decoder",
            "during",
            "microbatch",
            "vision",
            "modalities",
            "across",
            "mean"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities",
                    "computational",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we attempt to address the aforementioned challenges.\nTo overcome the first challenge, we carefully design a multi-stage large-scale pretraining pipeline. Based on an early-stage text pretraining foundation model, we progressively incorporate audio and visual data into the large-scale pretraining process. By adopting a balanced multimodal data mixture and effective ealy-fusion strategy, the model achieves deeply integrated comprehension across modalities while maintaining strong unimodal performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "during",
                    "llm",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large-Scale with Real-time Audio-Visual Interaction:</span> By leveraging an efficient LLM backbone, carefully designed lightweight modality encoders and decoder, and a chunk-wise audio-visual feature interleaving mechanism, LongCat-Flash-Omni achieves low-latency, high-quality audio-visual processing and streaming speech generation. It supports a context window of up to 128K tokens, enabling advanced capabilities in long-term memory, multi-turn dialogue, and temporal reasoning across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "decoder",
                    "llm",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effective Early-Fusion Training:</span> The model adopts an innovative multi-stage pretraining pipeline that progressively incorporates text, audio, and visual modalities under a balanced data strategy and early-fusion training paradigm, ensuring strong omni-modal performance without degradation in any single modality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "llm",
                    "audio",
                    "modalities",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the reminder of this section, we first introduce each structural component of LongCat-Flash-Omni, including the vision encoder that supports inputs with arbitrary aspect ratios and native-resolution feature encoding, the audio encoder, decoder, and tokenizer, and the LLM backbone. Then we elaborate on the video processing strategy and the architectural components that enable low-latency, real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "llm",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vision encoder serves as a critical component in multimodal language models. To effectively encode visual inputs such as images and videos, LongCat-Flash-Omni incorporates a well-designed Vision Transformer (ViT), referred to as LongCat-ViT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib10\" title=\"\">2025</a>)</cite>. LongCat-ViT achieves high performance across multimodal tasks, natively supports inputs with various resolutions and aspect ratios, while providing unified encoding capabilities for both image and video data.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "vision",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Design</span>\nLongCat-ViT is a Transformer-based encoder that retains the core structure of the conventional Vision Transformer while integrating several key enhancements: a unified patchification module for native image and video inputs, 2D rotary position embeddings (2D-RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite>, SwiGLU activation function, RMSNorm layer, a LayerScale module, and Query-Key normalization. Together, these refinements yield a more robust and efficient architecture compared to conventional designs.\nTo improve computational efficiency in video frame encoding during real-time interaction while maintaining model performance, we adopt a relatively lightweight model configuration.\nThe detailed hyper-parameters of the model are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Vision Encoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn line with common practice, a two-layer multilayer perceptron (MLP) with pre-normalization is employed as the vision-language projector to align visual and textual representations.\nMoreover, a 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> pixel-unshuffle operation is applied along the spatial dimension to mitigate the quadratic computational complexity associated with high-resolution inputs.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "module",
                    "during",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Vision-Language Pretraining</span>\nLongCat-ViT adopts a progressive training scheme that integrates two complementary adaptation strategies: (1) progressive resolution adaptation, which leverages curriculum learning by transitioning from fixed low-resolution (e.g., 224) pretraining to native-resolution fine-tuning; and (2) progressive visual modality adaptation, which postpones the incorporation of video data until the final training stage to reduce computational overhead. To further facilitate convergence in the early phases, feature distillation from a frozen pretrained vision model is introduced as an auxiliary objective, and the weight of this objective gradually reduces in later stages. The model is trained from scratch on a total of 14.6 billion samples during the contrastive pretraining phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "computational",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We input audio in different formats to the LLM backbone of LongCat-Flash-Omni across training phases. Specifically, during pre-training stages 1-4, an <span class=\"ltx_text ltx_font_italic\">audio tokenizer</span> is employed to convert raw speech into four-codebook discrete tokens, enabling consistent next-token prediction and improved training efficiency.\nHowever, we observe that such discretization hinders the model&#8217;s ability to capture fine-grained acoustic details. Therefore, from pre-training stage 5 (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>) we incorporate an <span class=\"ltx_text ltx_font_italic\">audio encoder</span> to convert raw speech into continuous audio features before input to the LLM.\nFor speech generation, to align with the inherent next token prediction paradigm, the LLM consistently outputs the four-codebook discrete tokens, which are subsequently converted back into a waveform using an <span class=\"ltx_text ltx_font_italic\">audio decoder</span>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "stage",
                    "decoder",
                    "different",
                    "during",
                    "llm",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Encoder</span>\n\nTo optimize response latency and accommodate speech inputs of arbitrary duration, the audio encoder is designed using a streaming architecture.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio encoder takes 80-dimensional Fbank features as input. The architecture incorporates a Pre-FFN module that reduces the audio sequence length by a factor of eight through frame splicing downsampling technique, with each frame representing an 80ms time window.\nThe core process is performed by a streaming encoder that maintains a Transformer-like structure while incorporating several key modifications: (1) a Pre-Norm configuration for enhanced training stability, and (2) replacing standard self-attention modules with FSMN layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib15\" title=\"\">2018</a>)</cite> to enable efficient feature processing within constrained context windows.\nTo balance latency and performance, we implement a hybrid approach where only the final six layers incorporate a one-frame look-ahead mechanism, while maintaining strict causality in the preceding layers.\nThe architecture concludes with a post-FFN module for additional feature refinement.\nThe audio encoder is trained under supervised learning using speech recognition data using the CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib16\" title=\"\">2006</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "llm",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic Video Frame Sampling</span> We adopt a default sampling rate of 2 frames per second (2 FPS), while also enable dynamic adjustments according to video duration.\nDuring training, shorter videos are sampled at a higher frame rate to capture denser temporal information, ensuring at least 16 frames for effective information utilization. In contrast, for excessively long videos, frames will be sampled uniformly based on a maximum frame number constraint. This frame cap further facilitates training by regulating memory consumption and preserving computational efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Audio-Visual Feature Interleaving</span> Unlike offline audio-visual understanding tasks, where audio and visual features can be concatenated at the sequence level, real-time audio-visual interaction requires features from the audio and video streams to be prefilled into the LLM backbone as early as possible to minimize response latency once the user query is received.\nTo achieve this, we design a temporally-synchronized, chunk-wise audio-visual feature interleaving mechanism. Audio-visual feature chunks are structured as &#8220;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-start-token&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#8230;&#161;&#8212;audio-end-token&#8212;&#191;&#8221;, where the timestamp is represented in textual form, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS4.SSS1\" title=\"2.4.1 Video Strategy &#8227; 2.4 Video Strategy and Streaming Audio-Visual Interaction &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.4.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "during",
                    "cost",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To broaden semantic coverage and avoid overfitting to dominant patterns, we significantly enhance data diversity and balance. We cluster image-text pairs based on joint image and text embeddings and resample from each cluster to ensure representation of long-tail content, while simultaneously removing low-quality samples frequently concentrated in specific clusters. Experimental results confirm that preserving diversity at this stage is more beneficial than overly strict quality filtering. Furthermore, to address the pronounced long-tail distribution inherent in multimodal datasets, we adopt a concept-based resampling strategy inspired by MetaCLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib21\" title=\"\">2023</a>)</cite>. By expanding the vocabulary with a 200K-scale Chinese lexicon and resampling for broader concept coverage, we achieve a more balanced distribution across semantic categories.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "across",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "across",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most fundamental challenges in training omni-modal models lies in the significant heterogeneity of data distributions across modalities. Each modality exhibits distinct structural properties and representational characteristics.\nText, for instance, is a highly compressed and abstract symbolic representation of human knowledge, and LLMs have demonstrated remarkable success in modeling large-scale text sequences. Speech, on the other hand, is the acoustic manifestation of human thoughts and concepts, a sequential signal like text, but enriched with paralinguistic information such as speaker timbre, emotion, prosody, and accent. However, its semantic density is much lower than that of text: while a typical speech tokenizer operating at 12.5 Hz must generate roughly 12.5 tokens per second, humans only speak about 3-4 text tokens per second. This mismatch makes sequence modeling for speech inherently more challenging than for text.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by these observations, we adopt a curriculum-inspired, progressive training strategy that gradually transitions from simpler to more complex sequence modeling tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F5\" title=\"Figure 5 &#8227; 3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nWe begin with large-scale text pretraining (<span class=\"ltx_text ltx_font_bold\">Stage-0</span>), leveraging the maturity and stability of LLMs as a strong initialization for subsequent multimodal learning. Building on this foundation, we introduce speech data, which is structurally closer to text, to align acoustic representations with the language model&#8217;s feature space and effectively integrate paralinguistic information (<span class=\"ltx_text ltx_font_bold\">Stage-1</span>). Once speech-text alignment is established, we incorporate large-scale image-caption pairs and vision-language interleaved corpora (<span class=\"ltx_text ltx_font_bold\">Stage-2</span>) for vision-language alignment, enriching the model&#8217;s visual knowledge. We then introduce the most complex video data to enable spatial-temporal reasoning (<span class=\"ltx_text ltx_font_bold\">Stage-3</span>), meanwhile integrating higher-quality and more diverse image datasets to enhance vision comprehension.\nTo further support long-context reasoning and multi-turn interactions, we extend the model&#8217;s context window from 8K to 128K tokens (<span class=\"ltx_text ltx_font_bold\">Stage-4</span>).\nFinally, to mitigate information loss in audio inputs represented by discrete speech tokens, we introduce an audio encoder alignment stage (<span class=\"ltx_text ltx_font_bold\">Stage-5</span>) that enables the model to directly process continuous audio features, thereby enhancing fidelity in downstream speech tasks.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "decoder",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This training stage uses approximately 5.1 trillion tokens,\nwith the ratio of text tokens and audio tokens is 2:1. A slight learning rate decay is applied.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "decoder",
                    "module",
                    "during",
                    "llm",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of the model is highly dependent on the quality and composition of the training data. Therefore, the domain mixture plays a crucial role in determining the final performance. Vision data, in particular, exhibits greater heterogeneity in content distribution, learning difficulty, and scale, thereby requiring more careful control over data composition.\n</p>\n\n",
                "matched_terms": [
                    "vision",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To augment the model&#8217;s proficiency in capturing extended sequential relationships across various modalities, we gradually extend the context length to 32K and then 128K tokens, enabling the model to process and reason over more complex tasks such as long-term memory modeling and multi-turn real-time interaction.\nWe scale the context length from 8K to 32K tokens using 100B training tokens and adjust RoPE&#8217;s base frequency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite> from 1M to 5M to preserve positional encoding quality. The context length is then further expanded to 128K tokens with an additional 20B tokens of training, requiring a proportional increase of the RoPE&#8217;s base frequency to 10M to maintain stable attention across the extended sequence length.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For textual and speech modalities, we utilize the foundation language model&#8217;s high-quality text corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, alongside carefully curated speech data from our multilingual audio repository (over 10 million hours of processed Chinese/English recordings). Notably, we maintain the same 2:1:1 text-to-vision-to-speech ratio as established in the pre-training phase, ensuring modality balance during context extension. This comprehensive approach ensures consistent performance across all modalities while scaling to extended contexts.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "modalities",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "module",
                    "llm",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, inputs are formatted as follows: &#8220;Task Prompt + Speech Input + LLM Response&#8221;, where the loss is computed exclusively on the &#8220;LLM Response&#8221; segment.\nTo improve task generalization, we design multiple prompt variations for each task and randomly select prompts during training.\n</p>\n\n",
                "matched_terms": [
                    "during",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training stage is a critical phase that transforms the pre-trained foundation model into a task-adaptive, human-aligned system with strong instruction-following, multimodal reasoning, and interactive capabilities. While the pre-training phase enhances multimodal perception and world knowledge, post-training focuses on refining response alignment, controllability, and fidelity through a combination of supervised and preference-based optimization.\nThis stage consists of two components: (1) Supervised Fine-Tuning (SFT), which equips the model with multimodal instruction-following, reasoning, and spoken interaction capabilities through high-quality and diverse instruction data, and (2) Reinforcement Learning (RL), which further enhances the model&#8217;s behavioral alignment, coherence, and consistency through Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite>.\nTogether, these procedures ensure that the final model not only demonstrates robust omni-modal understanding and reasoning capabilities but also generates responses that are semantically accurate, perceptually natural, and contextually coherent across both offline and online interaction scenarios.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "across",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage focuses on two complementary objectives. First, it enhances the model&#8217;s multimodal instruction-following and reasoning capabilities by leveraging large-scale, high-quality, and diverse instruction data. Second, it strengthens the model&#8217;s multimodal interaction abilities by utilizing curated interaction data, including speech-to-speech conversational data and audio-visual interaction data.\nIn the following parts, we describe how we collect high-quality and diverse instruction data, then elaborate on the construction of multimodal interaction datasets, and finally present the SFT training recipes that unify and optimize these datasets.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "audio",
                    "encoder",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Speech Interaction Data</span> We construct a large-scale voice dialogue dataset using a two-stage approach. (1) Text-based dataset adaptation: we filter out contents like formulas, code, markdown and rewrite responses with an LLM to produce speech-friendly conversational language. (2) Voice-oriented dialogue generation: we prompt an LLM to generate diverse topics and create new multi-turn dialogues.\nTo enhance expressiveness and naturalness, professional voice actors record dialogues across a range of emotions, speaking styles, and major Chinese dialects (e.g., Sichuan, Beijing, Northeast). These recordings are then used to fine-tune a specialized TTS engine, ensuring consistent tone, high fidelity, and natural prosody. All dialogues are ultimately synthesized into high-quality speech using this engine.</p>\n\n",
                "matched_terms": [
                    "across",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "stage",
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During SFT, we freeze the audio encoder while updating all other modules. The frozen audio encoder retains robust acoustic representations learned from large-scale audio pre-training (i.e., pre-training stage-5 as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>), whereas the unfrozen layers allow the model to learn fine-grained multimodal alignment and instruction-following behavior. Empirically, we find this selective fine-tuning strategy stabilizes convergence and avoids catastrophic forgetting of low-level auditory features. Apart from multi-modal SFT data, we also incorporate pure text SFT data used in developing LongCat-Flash, which covers domains ranging from reasoning, mathematics, coding, tool use, and general-purpose dialogue.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "encoder",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the AdamW optimizer with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, and a weight decay of 0.1. The learning rate follows a linear warm-up over the first 4% of steps, followed by cosine decay from a peak value of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to zero. The SFT stage consists of a single training phase conducted for one epoch. The training is performed with a batch size of 1024.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the model&#8217;s multimodal capabilities and improve human alignment, we employ the Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite> during the reinforcement learning stage. The LongCat-Flash-Omni model supports multimodal inputs and enables parallel streaming outputs of both text and speech. However, most existing DPO variants are designed for text-only outputs or optimize text and speech separately. We argue that such decoupled optimization is suboptimal for maintaining coherence between textual and speech responses.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we extend the DPO to jointly optimize text and speech outputs to improve both alignment and stability across modalities. Specifically, since the LongCat-Flash-Omni includes one text head and multiple audio heads, we modify the DPO objective to optimize all heads simultaneously. The overall loss is defined as:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "during",
                    "modalities",
                    "across",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train for one epoch with a batch size of 256, sampling each batch to achieve a well-balanced mix of modalities. The learning rate follows a cosine decay schedule, with a warm-up fraction of 0.03, gradually decreasing from <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math> to 0. To balance preference learning between the text head and the speech head, we set their loss weight ratio by setting <math alttext=\"\\alpha:\\beta=1:1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#946;</mi><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha:\\beta=1:1</annotation></semantics></math>. To mitigate drift from the SFT model, we incorporate a Kullback-Leibler (KL) divergence regularizer with a weighting factor of 0.1.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "vision",
                    "audio",
                    "encoder",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "across",
                    "decoder",
                    "different",
                    "llm",
                    "distribution",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach is to decouple multimodal models across different parallel dimensions. For example, DistTrain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib38\" title=\"\">2025</a>)</cite> mitigates model and data heterogeneity by separating model partitioning strategies and sorting data; PipeWeaver&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib39\" title=\"\">2025</a>)</cite> reduces PP bubbles caused by heterogeneity through fine-grained data partitioning and dynamic pipeline scheduling; and Optimus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib40\" title=\"\">2025</a>)</cite> separates the parallelization strategies for encoders and LLMs, scheduling encoder computations into the LLM&#8217;s idle periods to eliminate bubbles caused by model heterogeneity.\nBuilding on the Optimus approach, we develop modality-decoupled parallelism (MDP), a simple yet effective multimodal training strategy. The core idea is to completely decouple the modality encoders and the LLM backbone at the distributed level, enabling independent scheduling and more efficient utilization of computational resources.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "across",
                    "different",
                    "llm",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we co-locate the modality encoders and the LLM decoder. The modality encoders utilize Hybrid Sharding Data Parallelism (HSDP)<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib41\" title=\"\">2023</a>)</cite> to reduce static memory, and full activation recomputation is employed to reduce activation memory usage. The LLM decoder adopts a combined distributed strategy including pipeline parallelism (PP), ZeRO-1 data parallelism (DP), context parallelism (CP), and expert parallelism (EP).\nTo simplify data mapping between modality encoders and the LLM decoder, we introduce an <span class=\"ltx_text ltx_font_italic\">InnerDP</span> parallelism strategy, which further partitions modality data across all microbatches.\nThe DP rank of modality encoders corresponds one-to-one with the DP rank of the LLM decoder, and the size of the InnerDP dimension is the product of the PP and CP of the LLM decoder (i.e., <math alttext=\"d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}</annotation></semantics></math>, <math alttext=\"d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}</annotation></semantics></math>). As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F8\" title=\"Figure 8 &#8227; 5.1.1 Modality-Decoupled Parallelism &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the MDP execution timeline consists of four phases:</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "llm",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Forward</span>: The BalanceData module first distributes the modality data from the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks. Then, the modality encoders on each rank compute the corresponding vision and audio embeddings. Finally, the ModalityBridge module aggregates these embeddings at the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank, which are then passed as input to the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "module",
                    "llm",
                    "audio",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Decoder Forward and Backward</span>: In this phase, modality embeddings are partitioned on the CP rank and fed into the LLM decoder for its forward and backward passes. The gradients of the vision and audio embeddings are then returned to the backward phase for the modality encoders. This design ensures the training efficiency of the LLM decoder, and also enables isolated optimization for modality encoders.</p>\n\n",
                "matched_terms": [
                    "vision",
                    "audio",
                    "decoder",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Backward</span>: The ModalityBridge module redistributes the modality embedding gradients from the LLM decoder to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks, and then the backward pass of the modality encoders is executed.</p>\n\n",
                "matched_terms": [
                    "module",
                    "encoder",
                    "decoder",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MDP, the ModalityBridge serves as the communication layer between the multimodal encoders and the LLM decoder.\nIt is responsible for transforming data organization formats to resolve discrepancies arising from the different parallelism strategies employed by the modality encoders and the LLM decoder. This process specifically involves handling modality embeddings during the forward phase and gradients during the backward phase.\nHowever, when processing longer context length, significant memory pressure emerges as the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank must read all micro-batches and perform gather and scatter operations on the modality embeddings and their gradients.\nTo address this challenge, we adopt chunk-based processing within ModalityBridge, which effectively alleviates the memory bottleneck while maintaining bitwise numerical consistency.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F9\" title=\"Figure 9 &#8227; 5.1.2 ModalityBridge &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this module comprises three core components:</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "different",
                    "module",
                    "during",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding Indexing</span>: Provides micro-batch-level embedding retrieval and gradient backpropagation during the LLM forward and backward phases.</p>\n\n",
                "matched_terms": [
                    "during",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these three components, we accomplish the transformation of data formats across different stages. In the forward phase, the transformation is performed according to the two-stage chunking approach, while in the backward phase, <span class=\"ltx_text ltx_font_italic\">Embedding Redistribution</span> executes the reverse process: gradients are first gathered via CP gather and then scattered to the corresponding <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks according to the global offset information for backward computation. Meanwhile, the chunking scheme reduces the peak memory usage to <math alttext=\"1/num\\_chunk\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">1/num\\_chunk</annotation></semantics></math> of the original, significantly alleviating memory pressure while ensuring bitwise numerical alignment.</p>\n\n",
                "matched_terms": [
                    "computation",
                    "across",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overarching strategy is twofold: first, select a distributed configuration by profiling core operator efficiency; second, meet the configuration&#8217;s memory budget via targeted memory optimizations. On the communication side, the shortcut architecture enables overlap between EP communication and computation within each micro-batch. In CPU-bound regimes, we employ fused operators to reduce kernel launch overhead and improve end-to-end throughput.</p>\n\n",
                "matched_terms": [
                    "computation",
                    "microbatch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimized Grouped GEMM</span>\nOur optimization strategies for Grouped GEMM achieve approximately 75% MFU in the evaluated scenarios.\n(1) <span class=\"ltx_text ltx_font_bold\">Dynamic SwapAB:</span> The WGMMA instruction is evaluated with the <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> dimension varying from 8 to 256, while the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> dimension is fixed at 64.\nWe design an optimized kernel that automatically performs SwapAB operations to maximize WGMMA performance.\n(2) <span class=\"ltx_text ltx_font_bold\">Configurable SM Usage:</span> The Grouped GEMM kernel allows configurable SM counts to avoid resource contention when overlapping with dispatch or combined communication operations.\n(3) <span class=\"ltx_text ltx_font_bold\">Fine-Tuning:</span> <span class=\"ltx_text ltx_font_italic\">TileShape</span>, <span class=\"ltx_text ltx_font_italic\">PipelineStage</span>, and <span class=\"ltx_text ltx_font_italic\">ClusterShape</span> are carefully tuned for the target workload to maximize hardware utilization and prevent register spilling.\n(4) <span class=\"ltx_text ltx_font_bold\">Scheduling and Pipeline Strategy:</span> A swizzled block mapping schedule is adopted to significantly improve L2 cache hit rates.\nIn addition, a ping-pong mechanism is introduced during the backward pass to overlap WGMMA computation with the loading and storing of weight gradients.</p>\n\n",
                "matched_terms": [
                    "during",
                    "computation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a decoupled multimodal inference framework that separates modality-specific encoders/decoders and the LLM for optimized deployment. Each module is deployed on dedicated hardware and accelerators tailored to its computational characteristics, mitigating cross-modal resource contention. The optimizations of LLM deployment follow LongCat-Flash, including Prefill-Decode (PD) Disaggregation and Single Batch Overlap for ScMoE to improve inference efficiency.\nCompared with conventional hybrid deployment, this separation achieves lower latency and higher throughput despite minor communication overhead.</p>\n\n",
                "matched_terms": [
                    "module",
                    "computational",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a highly efficient asynchronous streaming pipeline for model serving. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6.F11\" title=\"Figure 11 &#8227; 6.2 Asynchronous Streaming Pipeline &#8227; 6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, it consists of four sequentially linked and concurrently executed stages: VAD &amp; Frame Sampling, Audio-Visual Encoding, LLM Prefilling &amp; Decoding, and Audio Decoding. Each module supports incremental inference of streaming input and adaptive batching strategies, facilitating concurrent scheduling to reduce latency.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audio",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> A voice activity detection (VAD) module is deployed to detect whether the user is speaking in real-time. When the user is speaking, densely sampled frames as well as the audio will be used as model input, facilitating a deeper analysis of the video content. Otherwise, only sparsely sampled frames are used to track the high-level overview of the video stream.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, each request data packet from the user side consists of 1 second of audio and 2 corresponding video frames. Using the streaming prefill strategy, each request packet can be immediately prefilled, eliminating the need to wait until the user&#8217;s turn to finish before initiating computation. By overlapping the VAD endpoint detection (<math alttext=\"600\\sim 700ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>600</mn><mo>&#8764;</mo><mrow><mn>700</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">600\\sim 700ms</annotation></semantics></math>) and the prefill process, users can receive the model response within <math alttext=\"100ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">100ms</annotation></semantics></math> after the endpoint detection. This asynchronous pipeline enables real-time omni-modal interaction.</p>\n\n",
                "matched_terms": [
                    "computation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities",
                    "across"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 3: Nave memory usage breakdown by component.",
        "body": "Component\nStatic (ZeRO-1, GB)\nDynamic (GB)\n\n\nLLM\n16.8\n103.24\n\n\nNCCL\n\n12.93\n\n\nDeepEP\n\n0.45\n\n\nModality Encoders\n3.6\n\n\n\nTotal\n\n137.02GB (peak at non-pp0 stage)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Component</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Static (ZeRO-1, GB)</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dynamic (GB)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">LLM</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">16.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\">103.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">NCCL</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">12.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">DeepEP</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">0.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Modality Encoders</td>\n<td class=\"ltx_td ltx_align_right\">3.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_bold\">137.02&#8201;GB</span> (peak at non-pp0 stage)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "static",
            "peak",
            "usage",
            "stage",
            "breakdown",
            "encoders",
            "deepep",
            "nonpp0",
            "dynamic",
            "total",
            "zero1",
            "llm",
            "nave",
            "memory",
            "component",
            "nccl",
            "modality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Without optimization, the required memory for our target configuration is approximately 137&#160;GB per device (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.T3\" title=\"Table 3 &#8227; 5.3 Memory Optimization Strategies &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nGiven 80&#160;GB devices and accounting for EP imbalance peaks, we constrain the theoretical memory footprint to approximately 72&#160;GB.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "component",
                    "memory",
                    "usage",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large-Scale with Real-time Audio-Visual Interaction:</span> By leveraging an efficient LLM backbone, carefully designed lightweight modality encoders and decoder, and a chunk-wise audio-visual feature interleaving mechanism, LongCat-Flash-Omni achieves low-latency, high-quality audio-visual processing and streaming speech generation. It supports a context window of up to 128K tokens, enabling advanced capabilities in long-term memory, multi-turn dialogue, and temporal reasoning across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "encoders",
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the reminder of this section, we first introduce each structural component of LongCat-Flash-Omni, including the vision encoder that supports inputs with arbitrary aspect ratios and native-resolution feature encoding, the audio encoder, decoder, and tokenizer, and the LLM backbone. Then we elaborate on the video processing strategy and the architectural components that enable low-latency, real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "component",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Vision-Language Pretraining</span>\nLongCat-ViT adopts a progressive training scheme that integrates two complementary adaptation strategies: (1) progressive resolution adaptation, which leverages curriculum learning by transitioning from fixed low-resolution (e.g., 224) pretraining to native-resolution fine-tuning; and (2) progressive visual modality adaptation, which postpones the incorporation of video data until the final training stage to reduce computational overhead. To further facilitate convergence in the early phases, feature distillation from a frozen pretrained vision model is introduced as an auxiliary objective, and the weight of this objective gradually reduces in later stages. The model is trained from scratch on a total of 14.6 billion samples during the contrastive pretraining phase.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "total",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We input audio in different formats to the LLM backbone of LongCat-Flash-Omni across training phases. Specifically, during pre-training stages 1-4, an <span class=\"ltx_text ltx_font_italic\">audio tokenizer</span> is employed to convert raw speech into four-codebook discrete tokens, enabling consistent next-token prediction and improved training efficiency.\nHowever, we observe that such discretization hinders the model&#8217;s ability to capture fine-grained acoustic details. Therefore, from pre-training stage 5 (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>) we incorporate an <span class=\"ltx_text ltx_font_italic\">audio encoder</span> to convert raw speech into continuous audio features before input to the LLM.\nFor speech generation, to align with the inherent next token prediction paradigm, the LLM consistently outputs the four-codebook discrete tokens, which are subsequently converted back into a waveform using an <span class=\"ltx_text ltx_font_italic\">audio decoder</span>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic Video Frame Sampling</span> We adopt a default sampling rate of 2 frames per second (2 FPS), while also enable dynamic adjustments according to video duration.\nDuring training, shorter videos are sampled at a higher frame rate to capture denser temporal information, ensuring at least 16 frames for effective information utilization. In contrast, for excessively long videos, frames will be sampled uniformly based on a maximum frame number constraint. This frame cap further facilitates training by regulating memory consumption and preserving computational efficiency.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "dynamic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "total",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pre-training stage, the model undergoes a multimodal annealing phase, where the model&#8217;s training continues with a curated higher-quality data under an annealed learning rate to achieve superior performance.\nWe further incorporate video data, including video captioning and QA datasets, as well as a broader range of image-related data such as OCR, grounding, GUI, multi-image, and STEM datasets. We maintain the stage-2 data ratio to ensure cross-modal stability, using a text:vision:speech token ratio of 2:1:1, consuming 0.33 trillion tokens in total.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dynamic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the AdamW optimizer with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, and a weight decay of 0.1. The learning rate follows a linear warm-up over the first 4% of steps, followed by cosine decay from a peak value of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to zero. The SFT stage consists of a single training phase conducted for one epoch. The training is performed with a batch size of 1024.</p>\n\n",
                "matched_terms": [
                    "peak",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "usage",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "static",
                    "dynamic",
                    "total",
                    "llm",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach is to decouple multimodal models across different parallel dimensions. For example, DistTrain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib38\" title=\"\">2025</a>)</cite> mitigates model and data heterogeneity by separating model partitioning strategies and sorting data; PipeWeaver&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib39\" title=\"\">2025</a>)</cite> reduces PP bubbles caused by heterogeneity through fine-grained data partitioning and dynamic pipeline scheduling; and Optimus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib40\" title=\"\">2025</a>)</cite> separates the parallelization strategies for encoders and LLMs, scheduling encoder computations into the LLM&#8217;s idle periods to eliminate bubbles caused by model heterogeneity.\nBuilding on the Optimus approach, we develop modality-decoupled parallelism (MDP), a simple yet effective multimodal training strategy. The core idea is to completely decouple the modality encoders and the LLM backbone at the distributed level, enabling independent scheduling and more efficient utilization of computational resources.</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "llm",
                    "dynamic",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we co-locate the modality encoders and the LLM decoder. The modality encoders utilize Hybrid Sharding Data Parallelism (HSDP)<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib41\" title=\"\">2023</a>)</cite> to reduce static memory, and full activation recomputation is employed to reduce activation memory usage. The LLM decoder adopts a combined distributed strategy including pipeline parallelism (PP), ZeRO-1 data parallelism (DP), context parallelism (CP), and expert parallelism (EP).\nTo simplify data mapping between modality encoders and the LLM decoder, we introduce an <span class=\"ltx_text ltx_font_italic\">InnerDP</span> parallelism strategy, which further partitions modality data across all microbatches.\nThe DP rank of modality encoders corresponds one-to-one with the DP rank of the LLM decoder, and the size of the InnerDP dimension is the product of the PP and CP of the LLM decoder (i.e., <math alttext=\"d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}</annotation></semantics></math>, <math alttext=\"d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}</annotation></semantics></math>). As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F8\" title=\"Figure 8 &#8227; 5.1.1 Modality-Decoupled Parallelism &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the MDP execution timeline consists of four phases:</p>\n\n",
                "matched_terms": [
                    "static",
                    "usage",
                    "encoders",
                    "zero1",
                    "llm",
                    "memory",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Forward</span>: The BalanceData module first distributes the modality data from the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks. Then, the modality encoders on each rank compute the corresponding vision and audio embeddings. Finally, the ModalityBridge module aggregates these embeddings at the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank, which are then passed as input to the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Decoder Forward and Backward</span>: In this phase, modality embeddings are partitioned on the CP rank and fed into the LLM decoder for its forward and backward passes. The gradients of the vision and audio embeddings are then returned to the backward phase for the modality encoders. This design ensures the training efficiency of the LLM decoder, and also enables isolated optimization for modality encoders.</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Backward</span>: The ModalityBridge module redistributes the modality embedding gradients from the LLM decoder to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks, and then the backward pass of the modality encoders is executed.</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MDP, the ModalityBridge serves as the communication layer between the multimodal encoders and the LLM decoder.\nIt is responsible for transforming data organization formats to resolve discrepancies arising from the different parallelism strategies employed by the modality encoders and the LLM decoder. This process specifically involves handling modality embeddings during the forward phase and gradients during the backward phase.\nHowever, when processing longer context length, significant memory pressure emerges as the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank must read all micro-batches and perform gather and scatter operations on the modality embeddings and their gradients.\nTo address this challenge, we adopt chunk-based processing within ModalityBridge, which effectively alleviates the memory bottleneck while maintaining bitwise numerical consistency.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F9\" title=\"Figure 9 &#8227; 5.1.2 ModalityBridge &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this module comprises three core components:</p>\n\n",
                "matched_terms": [
                    "memory",
                    "encoders",
                    "llm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CP partitioning:</span> The aggregated data is split along the <math alttext=\"hidden\\_size\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">hidden\\_size</annotation></semantics></math> dimension to ensure uniform embedding distribution and balanced memory usage, followed by scattering to the respective CP ranks to further reduce per-device memory consumption.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During each chunking iteration, the overall memory footprint exhibits a rise-then-fall pattern, significantly lowering the peak memory usage.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "peak",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these three components, we accomplish the transformation of data formats across different stages. In the forward phase, the transformation is performed according to the two-stage chunking approach, while in the backward phase, <span class=\"ltx_text ltx_font_italic\">Embedding Redistribution</span> executes the reverse process: gradients are first gathered via CP gather and then scattered to the corresponding <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks according to the global offset information for backward computation. Meanwhile, the chunking scheme reduces the peak memory usage to <math alttext=\"1/num\\_chunk\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">1/num\\_chunk</annotation></semantics></math> of the original, significantly alleviating memory pressure while ensuring bitwise numerical alignment.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "peak",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimized Grouped GEMM</span>\nOur optimization strategies for Grouped GEMM achieve approximately 75% MFU in the evaluated scenarios.\n(1) <span class=\"ltx_text ltx_font_bold\">Dynamic SwapAB:</span> The WGMMA instruction is evaluated with the <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> dimension varying from 8 to 256, while the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> dimension is fixed at 64.\nWe design an optimized kernel that automatically performs SwapAB operations to maximize WGMMA performance.\n(2) <span class=\"ltx_text ltx_font_bold\">Configurable SM Usage:</span> The Grouped GEMM kernel allows configurable SM counts to avoid resource contention when overlapping with dispatch or combined communication operations.\n(3) <span class=\"ltx_text ltx_font_bold\">Fine-Tuning:</span> <span class=\"ltx_text ltx_font_italic\">TileShape</span>, <span class=\"ltx_text ltx_font_italic\">PipelineStage</span>, and <span class=\"ltx_text ltx_font_italic\">ClusterShape</span> are carefully tuned for the target workload to maximize hardware utilization and prevent register spilling.\n(4) <span class=\"ltx_text ltx_font_bold\">Scheduling and Pipeline Strategy:</span> A swizzled block mapping schedule is adopted to significantly improve L2 cache hit rates.\nIn addition, a ping-pong mechanism is introduced during the backward pass to overlap WGMMA computation with the loading and storing of weight gradients.</p>\n\n",
                "matched_terms": [
                    "dynamic",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combine the techniques in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Memory Optimization Strategies &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: (i) a V-shaped PP schedule to bound concurrently alive activation micro-batches <cite class=\"ltx_cite ltx_citemacro_citep\">(Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib42\" title=\"\">2024</a>)</cite>; (ii) selective recomputation for low-FLOP, high-activation operators such as SwiGLU and LayerNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>; (iii) memory-efficient permute, which moves aggregation of routing probabilities and hidden states into SwiGLU to reduce MoE unpermute memory <cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib44\" title=\"\">2019</a>)</cite>; (iv) fine-grained SM budgeting for NCCL communicators to reduce NCCL memory; and (v) Hybrid-Sharding Data Parallel (HSDP) for the modality encoder to further reduce its static footprint <cite class=\"ltx_cite ltx_citemacro_citep\">(Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nTo accommodate potential EP load imbalance, we implement <em class=\"ltx_emph ltx_font_italic\">dynamic expert recomputation</em>: when any rank is assigned too many tokens, we dynamically recompute its down-projection, freeing all MoE memory except the down-projection inputs on trigger. This prevents training crashes from expert imbalance while incurring only modest overhead.</p>\n\n",
                "matched_terms": [
                    "static",
                    "dynamic",
                    "memory",
                    "nccl",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "breakdown"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "memory",
                    "dynamic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We address major challenges in building such a system: cross-modal heterogeneity, unified offline and streaming capabilities, and low-latency interaction. Through a carefully designed multi-stage early-fusion pretraining pipeline, LongCat-Flash-Omni achieves deeply integrated representations that enable synergistic multimodal reasoning while preserving unimodality strength. Our introduction of human-in-the-loop data construction and a 128K-token context window further enhances multi-turn dialogue, temporal reasoning, and memory capabilities in dynamic interactive scenarios.\nOn the architectural side, the adoption of the ScMoE backbone with zero-computation experts, together with lightweight modality encoders and decoder, allows the model to support real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "encoders",
                    "dynamic",
                    "modality"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 4: Memory footprint under different optimization settings.",
        "body": "Static",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Static</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "footprint",
            "static",
            "settings",
            "under",
            "optimization",
            "different",
            "memory"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We combine the techniques in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Memory Optimization Strategies &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: (i) a V-shaped PP schedule to bound concurrently alive activation micro-batches <cite class=\"ltx_cite ltx_citemacro_citep\">(Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib42\" title=\"\">2024</a>)</cite>; (ii) selective recomputation for low-FLOP, high-activation operators such as SwiGLU and LayerNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>; (iii) memory-efficient permute, which moves aggregation of routing probabilities and hidden states into SwiGLU to reduce MoE unpermute memory <cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib44\" title=\"\">2019</a>)</cite>; (iv) fine-grained SM budgeting for NCCL communicators to reduce NCCL memory; and (v) Hybrid-Sharding Data Parallel (HSDP) for the modality encoder to further reduce its static footprint <cite class=\"ltx_cite ltx_citemacro_citep\">(Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nTo accommodate potential EP load imbalance, we implement <em class=\"ltx_emph ltx_font_italic\">dynamic expert recomputation</em>: when any rank is assigned too many tokens, we dynamically recompute its down-projection, freeing all MoE memory except the down-projection inputs on trigger. This prevents training crashes from expert imbalance while incurring only modest overhead.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "optimization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance long-term memory and contextual reasoning in extended dialogue settings, we construct a long-context multi-turn video interaction dataset. In this dataset, dialogue sequences are reordered so that certain queries appear at temporal positions distant from their corresponding visual segments, encouraging the model to retain and retrieve information over extended temporal spans. After reordering, all QA sequences are refined with a strong multimodal language model to resolve anaphoric references, correct temporal inconsistencies, and improve contextual coherence, resulting in natural and logically consistent multi-turn conversations.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "settings",
                    "optimization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "static",
                    "memory",
                    "optimization",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we co-locate the modality encoders and the LLM decoder. The modality encoders utilize Hybrid Sharding Data Parallelism (HSDP)<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib41\" title=\"\">2023</a>)</cite> to reduce static memory, and full activation recomputation is employed to reduce activation memory usage. The LLM decoder adopts a combined distributed strategy including pipeline parallelism (PP), ZeRO-1 data parallelism (DP), context parallelism (CP), and expert parallelism (EP).\nTo simplify data mapping between modality encoders and the LLM decoder, we introduce an <span class=\"ltx_text ltx_font_italic\">InnerDP</span> parallelism strategy, which further partitions modality data across all microbatches.\nThe DP rank of modality encoders corresponds one-to-one with the DP rank of the LLM decoder, and the size of the InnerDP dimension is the product of the PP and CP of the LLM decoder (i.e., <math alttext=\"d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{inner\\_dp}=d_{lm\\_cp}\\times d_{lm\\_pp}</annotation></semantics></math>, <math alttext=\"d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">d_{world\\_size}=d_{dp}\\times d_{inner\\_dp}</annotation></semantics></math>). As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F8\" title=\"Figure 8 &#8227; 5.1.1 Modality-Decoupled Parallelism &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the MDP execution timeline consists of four phases:</p>\n\n",
                "matched_terms": [
                    "static",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MDP, the ModalityBridge serves as the communication layer between the multimodal encoders and the LLM decoder.\nIt is responsible for transforming data organization formats to resolve discrepancies arising from the different parallelism strategies employed by the modality encoders and the LLM decoder. This process specifically involves handling modality embeddings during the forward phase and gradients during the backward phase.\nHowever, when processing longer context length, significant memory pressure emerges as the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank must read all micro-batches and perform gather and scatter operations on the modality embeddings and their gradients.\nTo address this challenge, we adopt chunk-based processing within ModalityBridge, which effectively alleviates the memory bottleneck while maintaining bitwise numerical consistency.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F9\" title=\"Figure 9 &#8227; 5.1.2 ModalityBridge &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this module comprises three core components:</p>\n\n",
                "matched_terms": [
                    "memory",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During each chunking iteration, the overall memory footprint exhibits a rise-then-fall pattern, significantly lowering the peak memory usage.</p>\n\n",
                "matched_terms": [
                    "footprint",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these three components, we accomplish the transformation of data formats across different stages. In the forward phase, the transformation is performed according to the two-stage chunking approach, while in the backward phase, <span class=\"ltx_text ltx_font_italic\">Embedding Redistribution</span> executes the reverse process: gradients are first gathered via CP gather and then scattered to the corresponding <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks according to the global offset information for backward computation. Meanwhile, the chunking scheme reduces the peak memory usage to <math alttext=\"1/num\\_chunk\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">1/num\\_chunk</annotation></semantics></math> of the original, significantly alleviating memory pressure while ensuring bitwise numerical alignment.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without optimization, the required memory for our target configuration is approximately 137&#160;GB per device (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.T3\" title=\"Table 3 &#8227; 5.3 Memory Optimization Strategies &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nGiven 80&#160;GB devices and accounting for EP imbalance peaks, we constrain the theoretical memory footprint to approximately 72&#160;GB.</p>\n\n",
                "matched_terms": [
                    "footprint",
                    "memory",
                    "optimization"
                ]
            }
        ]
    },
    "S7.T5": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 5: Evaluation of image understanding. Values marked with * are sourced from public reports. As GPT-4o does not support image grounding, we do not report its results on RefCOCO and ScreenSpot-v2.",
        "body": "Qwen2.5-VL\n\n\n72B-Instruct",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen2.5-VL</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">72B-Instruct</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "72binstruct",
            "evaluation",
            "results",
            "gpt4o",
            "image",
            "marked",
            "refcoco",
            "reports",
            "qwen25vl",
            "from",
            "public",
            "its",
            "report",
            "understanding",
            "does",
            "screenspotv2",
            "sourced",
            "grounding",
            "values",
            "support",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "its",
                    "understanding",
                    "results",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "does",
                    "understanding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the second challenge of balancing offline multimodal understanding with real-time audio-visual interaction, we introduce a human-in-the-loop strategy to construct high-quality interaction data, with careful consideration for long-term memory and multi-turn dialogue handling. In addition, we derive vision-speech question-answering data from existing vision-text corpora, enabling natural speech output and facilitating the transfer of strong offline multimodal understanding capabilities into interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "results",
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the model architecture of LongCat-Flash-Omni. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3\" title=\"3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4\" title=\"4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> describe the pretraining and post-training datasets and pipelines, respectively. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5\" title=\"5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6\" title=\"6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> introduce training infrastructures and inference deployment. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7\" title=\"7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the experimental results. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S8\" title=\"8 Conclusion &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> draws a conclusion of this report.</p>\n\n",
                "matched_terms": [
                    "results",
                    "report",
                    "reports"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "support",
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Native Resolution Encoding</span>\nConventional ViT models widely adopted in the community (e.g., CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib12\" title=\"\">2021</a>)</cite>, SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib13\" title=\"\">2023</a>)</cite>) typically resize input images to a fixed resolution, which can result in substantial information loss, particularly for images with extreme aspect ratios or high native resolutions.\nTo mitigate this limitation, LongCat-ViT encodes visual inputs at their native resolutions, circumventing the limitations of fixed-resolution ViT models. This preserves the spatial and contextual information inherent in visual data, thereby enhancing the model&#8217;s capability to comprehend and reason over complex visual inputs.\nFor each image or video frame, if the number of patches falls within a predefined range (576-5832 during training), only minimal resizing is applied to ensure both dimensions are divisible by 112. Otherwise, the image is rescaled to fit within this range while preserving its aspect ratio.</p>\n\n",
                "matched_terms": [
                    "image",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "from",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "support",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Audio-Visual Feature Interleaving</span> Unlike offline audio-visual understanding tasks, where audio and visual features can be concatenated at the sequence level, real-time audio-visual interaction requires features from the audio and video streams to be prefilled into the LLM backbone as early as possible to minimize response latency once the user query is received.\nTo achieve this, we design a temporally-synchronized, chunk-wise audio-visual feature interleaving mechanism. Audio-visual feature chunks are structured as &#8220;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-start-token&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#8230;&#161;&#8212;audio-end-token&#8212;&#191;&#8221;, where the timestamp is represented in textual form, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS4.SSS1\" title=\"2.4.1 Video Strategy &#8227; 2.4 Video Strategy and Streaming Audio-Visual Interaction &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.4.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Caption Data</span>\nHigh-quality image captions are crucial for aligning visual representations with the language model&#8217;s knowledge space. To this end, we build a large-scale image-caption dataset by first applying a multi-stage cleaning pipeline across text, image, and image-text pair levels. Low-quality samples, such as those with extremely short text, abnormal resolutions, or poor image quality, are removed using elaborately-refined heuristic rules. At the pair level, we further filter samples using a SigLIP similarity threshold, discarding those below a strict minimum to preserve downstream diversity. We then improve the dataset through re-captioning to generate dense, fine-grained captions with multiple open-source vision-language models while incorporating world knowledge from original annotations. Additional filtering, including mixed-language detection, repetition removal, and truncation handling is applied to help mitigate hallucinations and ensure the final image-text pairs are accurate, informative, and contextually rich.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To broaden semantic coverage and avoid overfitting to dominant patterns, we significantly enhance data diversity and balance. We cluster image-text pairs based on joint image and text embeddings and resample from each cluster to ensure representation of long-tail content, while simultaneously removing low-quality samples frequently concentrated in specific clusters. Experimental results confirm that preserving diversity at this stage is more beneficial than overly strict quality filtering. Furthermore, to address the pronounced long-tail distribution inherent in multimodal datasets, we adopt a concept-based resampling strategy inspired by MetaCLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib21\" title=\"\">2023</a>)</cite>. By expanding the vocabulary with a 200K-scale Chinese lexicon and resampling for broader concept coverage, we achieve a more balanced distribution across semantic categories.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Image-Text Data</span>\nInterleaved image-text data provides broader visual-textual coverage and improves few-shot performance, but it is often noisy and has uneven quality. To address this, we construct a high-quality dataset through a two-stage pipeline of filtering and diversity sampling from open-source data. In the filtering stage, we remove samples with noisy tokens, sensitive content, and overly complex samples, and discard corrupted or low-resolution images. We then improve image-text alignment by pairing each image with its most relevant text segment by SigLIP similarity scores, and discard pairs with low semantic alignment.\nTo sample a diverse and evenly distributed subset, we apply density-based pruning and semantic clustering similar to&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Abbas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib22\" title=\"\">2024</a>)</cite>. Samples are scored by knowledge content, quality, and similarity, then evenly sampled based on the cluster they belong to and their distances to the cluster centroid.\nThis process reduces the raw dataset by approximately 74%, while maintaining diversity and quality for multimodal pretraining.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data</span>\nWe curate richly annotated training samples encompassing various content types, including PDFs, papers and web pages. These efforts further enrich our document parsing dataset, enabling the model to learn both content extraction and structural understanding, and resulting in more robust and accurate document parsing.\nWe further include diverse OCR datasets covering scene text, structured documents, handwriting, and mathematical expressions.\nWe also synthesize multi-page and region-level OCR samples from scene and document OCR data, with each sample comprising 2 to 6 pages.\nFor OCR-related VQA, we incorporate a diverse range of datasets for visual question answering, covering domains of text-centric VQA, document VQA, table VQA, and chart VQA.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "grounding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To strengthen the model&#8217;s foundational capabilities in scientific reasoning and problem-solving, we construct a large-scale, multimodal STEM (Science, Technology, Engineering, and Mathematics) dataset.\nThe collected data are meticulously processed and structured into both multiple-choice and open-ended generative question-answer formats.\nWe implement a rigorous multi-stage filtering pipeline to ensure factual accuracy, eliminate ambiguity, and standardize formatting.\nThis effort culminates in a high-fidelity pre-training dataset comprising 15 million image-text pairs with substantial diversity in both subject matter and academic level, and covering a wide range of disciplines from K12 education to advanced university studies.\nThis foundational dataset is essential for enabling the model to achieve deep conceptual understanding and robust reasoning capabilities across a broad spectrum of scientific and technical domains.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s fine-grained image understanding, we constructed a taxonomy with various coarse-grained and fine-grained capabilities, such as &#8220;emotion recognition&#8221;, &#8220;vehicle identification&#8221;, and &#8220;time calculation with clocks&#8221;.\nBased on this taxonomy, we employed diverse strategies to collect multi-image question answering data, including open-source datasets with careful data selection and augmentation, and synthesizing images using dedicated tools.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our video dataset is primarily sourced from a broad range of publicly available corpora, covering diverse task types such as video classification, temporal grounding, detection, captioning, and question answering. We design a comprehensive data processing pipeline centered on rigorous quality filtering and targeted enhancement, resulting in a refined, high-quality dataset tailored for large-scale pre-training.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "grounding",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporally grounded video QA data</span> We construct a high-quality temporally grounded dataset by applying rule-based transformations to convert annotations from tasks such as temporal action detection, segmentation, video summarization, and temporal sentence grounding into question-answer (QA) pairs. Data complexity is further enriched through a proprietary model that generates more challenging and in-depth QA pairs.</p>\n\n",
                "matched_terms": [
                    "from",
                    "grounding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by these observations, we adopt a curriculum-inspired, progressive training strategy that gradually transitions from simpler to more complex sequence modeling tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F5\" title=\"Figure 5 &#8227; 3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nWe begin with large-scale text pretraining (<span class=\"ltx_text ltx_font_bold\">Stage-0</span>), leveraging the maturity and stability of LLMs as a strong initialization for subsequent multimodal learning. Building on this foundation, we introduce speech data, which is structurally closer to text, to align acoustic representations with the language model&#8217;s feature space and effectively integrate paralinguistic information (<span class=\"ltx_text ltx_font_bold\">Stage-1</span>). Once speech-text alignment is established, we incorporate large-scale image-caption pairs and vision-language interleaved corpora (<span class=\"ltx_text ltx_font_bold\">Stage-2</span>) for vision-language alignment, enriching the model&#8217;s visual knowledge. We then introduce the most complex video data to enable spatial-temporal reasoning (<span class=\"ltx_text ltx_font_bold\">Stage-3</span>), meanwhile integrating higher-quality and more diverse image datasets to enhance vision comprehension.\nTo further support long-context reasoning and multi-turn interactions, we extend the model&#8217;s context window from 8K to 128K tokens (<span class=\"ltx_text ltx_font_bold\">Stage-4</span>).\nFinally, to mitigate information loss in audio inputs represented by discrete speech tokens, we introduce an audio encoder alignment stage (<span class=\"ltx_text ltx_font_bold\">Stage-5</span>) that enables the model to directly process continuous audio features, thereby enhancing fidelity in downstream speech tasks.</p>\n\n",
                "matched_terms": [
                    "support",
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a perplexity (PPL)-gap-based signal to automatically guide data sampling allocation. The corpus is first segmented into distinct subsets according to semantics and tasks. During training, we monitor PPL convergence for each subset: if a subset&#8217;s convergence lags behind the expected reference level, its sampling weight will be dynamically increased.\nFor each subset, we build a corresponding validation set and use an off-the-shelf vision-language model to compute its per-sample PPL as the expected reference level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib32\" title=\"\">2023</a>; Michel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib33\" title=\"\">2022</a>)</cite>.\nTo prevent high-value samples from being diluted by aggregate statistics, samples for which the PPL convergence is consistent with downstream performance will be isolated and relabeled into independent data subsets.\nWith the original data partitions largely preserved while the high-value samples organized more finely, this approach significantly enhances data efficiency and downstream task performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training stage is a critical phase that transforms the pre-trained foundation model into a task-adaptive, human-aligned system with strong instruction-following, multimodal reasoning, and interactive capabilities. While the pre-training phase enhances multimodal perception and world knowledge, post-training focuses on refining response alignment, controllability, and fidelity through a combination of supervised and preference-based optimization.\nThis stage consists of two components: (1) Supervised Fine-Tuning (SFT), which equips the model with multimodal instruction-following, reasoning, and spoken interaction capabilities through high-quality and diverse instruction data, and (2) Reinforcement Learning (RL), which further enhances the model&#8217;s behavioral alignment, coherence, and consistency through Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite>.\nTogether, these procedures ensure that the final model not only demonstrates robust omni-modal understanding and reasoning capabilities but also generates responses that are semantically accurate, perceptually natural, and contextually coherent across both offline and online interaction scenarios.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image-Text SFT Data</span>\nWe curate a high-quality and diverse image-text SFT dataset, covering a wide spectrum of vision-language tasks, including: (1) Fundamental skills: image captioning and visual question answering, provided in both single-image and multi-image settings to capture diverse contextual dependencies. (2) Specialized skills: document and chart comprehension, OCR, visual grounding, agentic task execution, and STEM-related visual reasoning.\n</p>\n\n",
                "matched_terms": [
                    "image",
                    "grounding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-Text SFT Data</span>\nWe construct a large-scale video data pool of approximately 3 million videos sourced from diverse proprietary and open-source datasets, covering tasks such as general video understanding, classification, reasoning, grounding, temporal localization, segmentation, and highlight detection. From each source, representative subsets are annotated with capability tags using multimodal language models, following a predefined taxonomy encompassing visual perception, temporal and causal reasoning, and domain-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "grounding",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "from",
                    "its",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "its",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-Driven Automation</span> To comprehensively cover diverse interaction scenarios, we first establish an ability taxonomy encompassing six major dimensions: memorization, understanding, analysis, creation, application, and entertainment. Guided by this taxonomy, we collect a mixture of public and in-house videos as the source data to ensure balanced coverage across abilities. For each video, we first perform scene segmentation using PySceneDetect <cite class=\"ltx_cite ltx_citemacro_citep\">(Breakthrough, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib35\" title=\"\">2025</a>)</cite> to obtain a sequence of clips. For every clip, a powerful multimodal language model generates multiple rounds of progressively deep and context-aware QA pairs. In each round, subsequent queries build upon preceding conversations with logical progression, referential dependency, or anaphoric linkage to earlier turns. Such conversations encourage the model to reason over extended dialogue contexts, maintain entity and reference consistency in a natural, human-like audio-visual interaction setting. We then apply an automatic verification pipeline, employing an LLM-as-a-judge framework to evaluate and discard low-quality or inconsistent QA pairs. The remaining qualified samples are then composed into multi-turn dialogue, enabling the model to learn both intra-scene conversational continuity and inter-scene contextual transitions, thereby better reflecting real-world audio-visual interactions.</p>\n\n",
                "matched_terms": [
                    "public",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training infra contains multiple compute-communication overlaps, mainly including shortcut-based EP overlap and point-to-point (P2P) overlap. We tune the number of streaming multiprocessors (SMs) assigned to communication and compute kernels for each scenario to maximize hardware utilization, and we use distinct P2P groups/streams so that inter-stage pipeline-parallel (PP) communication does not interfere across stages.</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combine the techniques in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Memory Optimization Strategies &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: (i) a V-shaped PP schedule to bound concurrently alive activation micro-batches <cite class=\"ltx_cite ltx_citemacro_citep\">(Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib42\" title=\"\">2024</a>)</cite>; (ii) selective recomputation for low-FLOP, high-activation operators such as SwiGLU and LayerNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>; (iii) memory-efficient permute, which moves aggregation of routing probabilities and hidden states into SwiGLU to reduce MoE unpermute memory <cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib44\" title=\"\">2019</a>)</cite>; (iv) fine-grained SM budgeting for NCCL communicators to reduce NCCL memory; and (v) Hybrid-Sharding Data Parallel (HSDP) for the modality encoder to further reduce its static footprint <cite class=\"ltx_cite ltx_citemacro_citep\">(Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nTo accommodate potential EP load imbalance, we implement <em class=\"ltx_emph ltx_font_italic\">dynamic expert recomputation</em>: when any rank is assigned too many tokens, we dynamically recompute its down-projection, freeing all MoE memory except the down-projection inputs on trigger. This prevents training crashes from expert imbalance while incurring only modest overhead.</p>\n\n",
                "matched_terms": [
                    "from",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For features where bit alignment is not feasible, we systematically analyze all sources of numerical deviation and mitigate their impact by benchmarking against the gold reference implementation during verification. For example, we identified and aligned the accumulation order in DeepEP and All2All EP strategies, thereby achieving bit-level consistency in loss values and verifying implementation correctness.</p>\n\n",
                "matched_terms": [
                    "values",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "marked",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4o",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation of image understanding across six key dimensions, leveraging a diverse suite of benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Grounding &amp; Counting</span>: RefCOCO, RefCOCO+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kazemzadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib61\" title=\"\">2014</a>)</cite> (averaged in RefCOCO-avg), and CountBench<cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib62\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "refcoco",
                    "grounding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate audio-visual understanding on VideoMME under the no-subtitle protocol (w/o sub), and report results for two conditions: with audio (&#8220;VideoMME w/ audio&#8221;) and without audio (&#8220;VideoMME w/o audio&#8221;).\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "report",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "report",
                    "results",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "understanding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate cross-modal understanding using publicly available benchmarks, including OmniBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib118\" title=\"\">2024c</a>)</cite>, WorldSense&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib119\" title=\"\">2025</a>)</cite>, and DailyOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib120\" title=\"\">2025</a>)</cite>. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.\nTo address the limited quality and coverage of existing benchmarks, we further introduce a new benchmark, UNO-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib121\" title=\"\">2025</a>)</cite>, comprising 1,880 human-crafted questions spanning 44 task types, with 98% of the questions requiring cross-modal reasoning.\nWe constructed this benchmark by manually annotating our in-house dataset. This approach prevents data contamination and ensures the benchmark is highly representative of real-world application scenarios.\nIn addition to conventional multiple-choice questions, the evaluation includes innovative multi-step open-ended questions, providing a more realistic and discriminative assessment of complex reasoning abilities.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data construction process involves 10 trained professional conversationalists who conducted multi-turn dialogue sessions (approximately three minutes each) with each model under evaluation. These interactions are carried out via real-time audio-visual interfaces on the official app or web platforms provided by the respective model developers.\nA total of 200 dialogue sessions have been collected for each model, covering common real-world video call scenarios across four categories: problem solving, entertainment, self-improvement, and emotional support (50 samples per category).\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "from",
                    "understanding",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "report",
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "image",
                    "its",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, LongCat-Flash-Omni lays a strong foundation for the continued evolution of omni-modal intelligence. Future work will focus on expanding the diversity and scale of training data, integrating an adaptive thinking mode, refining streaming and generation capabilities, and exploring richer forms of embodied and interactive intelligence. We believe that the release of LongCat-Flash-Omni will not only accelerate research on multimodal understanding and generation but also inspire new applications and paradigms for building human-centered, AGI-oriented systems.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "not"
                ]
            }
        ]
    },
    "S7.T6": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 6: Evaluation of video understanding. Values marked with * are sourced from public reports.",
        "body": "Qwen2.5-VL\n\n\n72B-Instruct",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen2.5-VL</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">72B-Instruct</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "public",
            "reports",
            "evaluation",
            "understanding",
            "72binstruct",
            "sourced",
            "values",
            "qwen25vl",
            "marked",
            "from",
            "video"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the second challenge of balancing offline multimodal understanding with real-time audio-visual interaction, we introduce a human-in-the-loop strategy to construct high-quality interaction data, with careful consideration for long-term memory and multi-turn dialogue handling. In addition, we derive vision-speech question-answering data from existing vision-text corpora, enabling natural speech output and facilitating the transfer of strong offline multimodal understanding capabilities into interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Vision-Language Pretraining</span>\nLongCat-ViT adopts a progressive training scheme that integrates two complementary adaptation strategies: (1) progressive resolution adaptation, which leverages curriculum learning by transitioning from fixed low-resolution (e.g., 224) pretraining to native-resolution fine-tuning; and (2) progressive visual modality adaptation, which postpones the incorporation of video data until the final training stage to reduce computational overhead. To further facilitate convergence in the early phases, feature distillation from a frozen pretrained vision model is introduced as an auxiliary objective, and the weight of this objective gradually reduces in later stages. The model is trained from scratch on a total of 14.6 billion samples during the contrastive pretraining phase.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Efficient video processing remains a significant challenge due to the substantial variability in video properties, including durations ranging from a few seconds to several hours and resolutions spanning a wide spectrum. To address these challenges, we adopt a series of strategies to effectively balance between model performance and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Audio-Visual Feature Interleaving</span> Unlike offline audio-visual understanding tasks, where audio and visual features can be concatenated at the sequence level, real-time audio-visual interaction requires features from the audio and video streams to be prefilled into the LLM backbone as early as possible to minimize response latency once the user query is received.\nTo achieve this, we design a temporally-synchronized, chunk-wise audio-visual feature interleaving mechanism. Audio-visual feature chunks are structured as &#8220;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-start-token&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#8230;&#161;&#8212;audio-end-token&#8212;&#191;&#8221;, where the timestamp is represented in textual form, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS4.SSS1\" title=\"2.4.1 Video Strategy &#8227; 2.4 Video Strategy and Streaming Audio-Visual Interaction &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.4.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data</span>\nWe curate richly annotated training samples encompassing various content types, including PDFs, papers and web pages. These efforts further enrich our document parsing dataset, enabling the model to learn both content extraction and structural understanding, and resulting in more robust and accurate document parsing.\nWe further include diverse OCR datasets covering scene text, structured documents, handwriting, and mathematical expressions.\nWe also synthesize multi-page and region-level OCR samples from scene and document OCR data, with each sample comprising 2 to 6 pages.\nFor OCR-related VQA, we incorporate a diverse range of datasets for visual question answering, covering domains of text-centric VQA, document VQA, table VQA, and chart VQA.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To strengthen the model&#8217;s foundational capabilities in scientific reasoning and problem-solving, we construct a large-scale, multimodal STEM (Science, Technology, Engineering, and Mathematics) dataset.\nThe collected data are meticulously processed and structured into both multiple-choice and open-ended generative question-answer formats.\nWe implement a rigorous multi-stage filtering pipeline to ensure factual accuracy, eliminate ambiguity, and standardize formatting.\nThis effort culminates in a high-fidelity pre-training dataset comprising 15 million image-text pairs with substantial diversity in both subject matter and academic level, and covering a wide range of disciplines from K12 education to advanced university studies.\nThis foundational dataset is essential for enabling the model to achieve deep conceptual understanding and robust reasoning capabilities across a broad spectrum of scientific and technical domains.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our video dataset is primarily sourced from a broad range of publicly available corpora, covering diverse task types such as video classification, temporal grounding, detection, captioning, and question answering. We design a comprehensive data processing pipeline centered on rigorous quality filtering and targeted enhancement, resulting in a refined, high-quality dataset tailored for large-scale pre-training.\n</p>\n\n",
                "matched_terms": [
                    "sourced",
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we curate an in-house dataset from publicly available video content with three components:</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporally grounded video QA data</span> We construct a high-quality temporally grounded dataset by applying rule-based transformations to convert annotations from tasks such as temporal action detection, segmentation, video summarization, and temporal sentence grounding into question-answer (QA) pairs. Data complexity is further enriched through a proprietary model that generates more challenging and in-depth QA pairs.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our in-house long-context dataset includes image-text interleaved data and long-video QA data.\nImage-text interleaved data is constructed by concatenating single-image data with the same topic or rendering some text segments within long text into images. This data can help the model to improve in-context learning and information retrieval from long context (i.e., &#8220;a needle in a haystack&#8221;).\nLong video QA data is constructed by the following pipeline: (1) First we segment videos into multiple clips and generates rich descriptive captions for each clip. (2) For consecutive clips from the same video, we analyze their scene continuity from their captions. If the clips are identified as continuous, we concatenate and refine their captions into a coherent longer-form video caption. (3) Finally, we construct temporally grounded and detailed QA pairs to form our long-video QA dataset.\nThis data can help the model to enhance long-sequence cross-modal modeling and memory retention.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by these observations, we adopt a curriculum-inspired, progressive training strategy that gradually transitions from simpler to more complex sequence modeling tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F5\" title=\"Figure 5 &#8227; 3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nWe begin with large-scale text pretraining (<span class=\"ltx_text ltx_font_bold\">Stage-0</span>), leveraging the maturity and stability of LLMs as a strong initialization for subsequent multimodal learning. Building on this foundation, we introduce speech data, which is structurally closer to text, to align acoustic representations with the language model&#8217;s feature space and effectively integrate paralinguistic information (<span class=\"ltx_text ltx_font_bold\">Stage-1</span>). Once speech-text alignment is established, we incorporate large-scale image-caption pairs and vision-language interleaved corpora (<span class=\"ltx_text ltx_font_bold\">Stage-2</span>) for vision-language alignment, enriching the model&#8217;s visual knowledge. We then introduce the most complex video data to enable spatial-temporal reasoning (<span class=\"ltx_text ltx_font_bold\">Stage-3</span>), meanwhile integrating higher-quality and more diverse image datasets to enhance vision comprehension.\nTo further support long-context reasoning and multi-turn interactions, we extend the model&#8217;s context window from 8K to 128K tokens (<span class=\"ltx_text ltx_font_bold\">Stage-4</span>).\nFinally, to mitigate information loss in audio inputs represented by discrete speech tokens, we introduce an audio encoder alignment stage (<span class=\"ltx_text ltx_font_bold\">Stage-5</span>) that enables the model to directly process continuous audio features, thereby enhancing fidelity in downstream speech tasks.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-Text SFT Data</span>\nWe construct a large-scale video data pool of approximately 3 million videos sourced from diverse proprietary and open-source datasets, covering tasks such as general video understanding, classification, reasoning, grounding, temporal localization, segmentation, and highlight detection. From each source, representative subsets are annotated with capability tags using multimodal language models, following a predefined taxonomy encompassing visual perception, temporal and causal reasoning, and domain-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "sourced",
                    "understanding",
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "sourced",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "sourced",
                    "understanding",
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-Driven Automation</span> To comprehensively cover diverse interaction scenarios, we first establish an ability taxonomy encompassing six major dimensions: memorization, understanding, analysis, creation, application, and entertainment. Guided by this taxonomy, we collect a mixture of public and in-house videos as the source data to ensure balanced coverage across abilities. For each video, we first perform scene segmentation using PySceneDetect <cite class=\"ltx_cite ltx_citemacro_citep\">(Breakthrough, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib35\" title=\"\">2025</a>)</cite> to obtain a sequence of clips. For every clip, a powerful multimodal language model generates multiple rounds of progressively deep and context-aware QA pairs. In each round, subsequent queries build upon preceding conversations with logical progression, referential dependency, or anaphoric linkage to earlier turns. Such conversations encourage the model to reason over extended dialogue contexts, maintain entity and reference consistency in a natural, human-like audio-visual interaction setting. We then apply an automatic verification pipeline, employing an LLM-as-a-judge framework to evaluate and discard low-quality or inconsistent QA pairs. The remaining qualified samples are then composed into multi-turn dialogue, enabling the model to learn both intra-scene conversational continuity and inter-scene contextual transitions, thereby better reflecting real-world audio-visual interactions.</p>\n\n",
                "matched_terms": [
                    "public",
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance long-term memory and contextual reasoning in extended dialogue settings, we construct a long-context multi-turn video interaction dataset. In this dataset, dialogue sequences are reordered so that certain queries appear at temporal positions distant from their corresponding visual segments, encouraging the model to retain and retrieve information over extended temporal spans. After reordering, all QA sequences are refined with a strong multimodal language model to resolve anaphoric references, correct temporal inconsistencies, and improve contextual coherence, resulting in natural and logically consistent multi-turn conversations.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "marked",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, each request data packet from the user side consists of 1 second of audio and 2 corresponding video frames. Using the streaming prefill strategy, each request packet can be immediately prefilled, eliminating the need to wait until the user&#8217;s turn to finish before initiating computation. By overlapping the VAD endpoint detection (<math alttext=\"600\\sim 700ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>600</mn><mo>&#8764;</mo><mrow><mn>700</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">600\\sim 700ms</annotation></semantics></math>) and the prefill process, users can receive the model response within <math alttext=\"100ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">100ms</annotation></semantics></math> after the endpoint detection. This asynchronous pipeline enables real-time omni-modal interaction.</p>\n\n",
                "matched_terms": [
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation of image understanding across six key dimensions, leveraging a diverse suite of benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capability of video understanding, we consider three dimensions as below:</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate cross-modal understanding using publicly available benchmarks, including OmniBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib118\" title=\"\">2024c</a>)</cite>, WorldSense&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib119\" title=\"\">2025</a>)</cite>, and DailyOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib120\" title=\"\">2025</a>)</cite>. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.\nTo address the limited quality and coverage of existing benchmarks, we further introduce a new benchmark, UNO-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib121\" title=\"\">2025</a>)</cite>, comprising 1,880 human-crafted questions spanning 44 task types, with 98% of the questions requiring cross-modal reasoning.\nWe constructed this benchmark by manually annotating our in-house dataset. This approach prevents data contamination and ensures the benchmark is highly representative of real-world application scenarios.\nIn addition to conventional multiple-choice questions, the evaluation includes innovative multi-step open-ended questions, providing a more realistic and discriminative assessment of complex reasoning abilities.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data construction process involves 10 trained professional conversationalists who conducted multi-turn dialogue sessions (approximately three minutes each) with each model under evaluation. These interactions are carried out via real-time audio-visual interfaces on the official app or web platforms provided by the respective model developers.\nA total of 200 dialogue sessions have been collected for each model, covering common real-world video call scenarios across four categories: problem solving, entertainment, self-improvement, and emotional support (50 samples per category).\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video"
                ]
            }
        ]
    },
    "S7.T7": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 7: ASR and TTS performance of base models during the pre-training stages. Word error rate (WER) (for English) or character error rate (CER) (for Chinese) in percentage are reported.",
        "body": "LibriSpeech\n\n\ntest-other",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">LibriSpeech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">test-other&#8595;</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "reported",
            "base",
            "character",
            "chinese",
            "pretraining",
            "error",
            "rate",
            "asr",
            "performance",
            "during",
            "testother",
            "word",
            "english",
            "librispeech",
            "models",
            "tts",
            "stages",
            "percentage",
            "cer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
            "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we attempt to address the aforementioned challenges.\nTo overcome the first challenge, we carefully design a multi-stage large-scale pretraining pipeline. Based on an early-stage text pretraining foundation model, we progressively incorporate audio and visual data into the large-scale pretraining process. By adopting a balanced multimodal data mixture and effective ealy-fusion strategy, the model achieves deeply integrated comprehension across modalities while maintaining strong unimodal performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "during",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SOTA and Unified Omni-Modal Model:</span> LongCat-Flash-Omni achieves state-of-the-art cross-modal comprehension performance among open-source models. It seamlessly integrates powerful offline multimodal understanding with real-time audio-visual interaction within a single all-in-one framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effective Early-Fusion Training:</span> The model adopts an innovative multi-stage pretraining pipeline that progressively incorporates text, audio, and visual modalities under a balanced data strategy and early-fusion training paradigm, ensuring strong omni-modal performance without degradation in any single modality.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vision encoder serves as a critical component in multimodal language models. To effectively encode visual inputs such as images and videos, LongCat-Flash-Omni incorporates a well-designed Vision Transformer (ViT), referred to as LongCat-ViT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib10\" title=\"\">2025</a>)</cite>. LongCat-ViT achieves high performance across multimodal tasks, natively supports inputs with various resolutions and aspect ratios, while providing unified encoding capabilities for both image and video data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Design</span>\nLongCat-ViT is a Transformer-based encoder that retains the core structure of the conventional Vision Transformer while integrating several key enhancements: a unified patchification module for native image and video inputs, 2D rotary position embeddings (2D-RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite>, SwiGLU activation function, RMSNorm layer, a LayerScale module, and Query-Key normalization. Together, these refinements yield a more robust and efficient architecture compared to conventional designs.\nTo improve computational efficiency in video frame encoding during real-time interaction while maintaining model performance, we adopt a relatively lightweight model configuration.\nThe detailed hyper-parameters of the model are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Vision Encoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn line with common practice, a two-layer multilayer perceptron (MLP) with pre-normalization is employed as the vision-language projector to align visual and textual representations.\nMoreover, a 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> pixel-unshuffle operation is applied along the spatial dimension to mitigate the quadratic computational complexity associated with high-resolution inputs.</p>\n\n",
                "matched_terms": [
                    "during",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Native Resolution Encoding</span>\nConventional ViT models widely adopted in the community (e.g., CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib12\" title=\"\">2021</a>)</cite>, SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib13\" title=\"\">2023</a>)</cite>) typically resize input images to a fixed resolution, which can result in substantial information loss, particularly for images with extreme aspect ratios or high native resolutions.\nTo mitigate this limitation, LongCat-ViT encodes visual inputs at their native resolutions, circumventing the limitations of fixed-resolution ViT models. This preserves the spatial and contextual information inherent in visual data, thereby enhancing the model&#8217;s capability to comprehend and reason over complex visual inputs.\nFor each image or video frame, if the number of patches falls within a predefined range (576-5832 during training), only minimal resizing is applied to ensure both dimensions are divisible by 112. Otherwise, the image is rescaled to fit within this range while preserving its aspect ratio.</p>\n\n",
                "matched_terms": [
                    "during",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Vision-Language Pretraining</span>\nLongCat-ViT adopts a progressive training scheme that integrates two complementary adaptation strategies: (1) progressive resolution adaptation, which leverages curriculum learning by transitioning from fixed low-resolution (e.g., 224) pretraining to native-resolution fine-tuning; and (2) progressive visual modality adaptation, which postpones the incorporation of video data until the final training stage to reduce computational overhead. To further facilitate convergence in the early phases, feature distillation from a frozen pretrained vision model is introduced as an auxiliary objective, and the weight of this objective gradually reduces in later stages. The model is trained from scratch on a total of 14.6 billion samples during the contrastive pretraining phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stages",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We input audio in different formats to the LLM backbone of LongCat-Flash-Omni across training phases. Specifically, during pre-training stages 1-4, an <span class=\"ltx_text ltx_font_italic\">audio tokenizer</span> is employed to convert raw speech into four-codebook discrete tokens, enabling consistent next-token prediction and improved training efficiency.\nHowever, we observe that such discretization hinders the model&#8217;s ability to capture fine-grained acoustic details. Therefore, from pre-training stage 5 (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>) we incorporate an <span class=\"ltx_text ltx_font_italic\">audio encoder</span> to convert raw speech into continuous audio features before input to the LLM.\nFor speech generation, to align with the inherent next token prediction paradigm, the LLM consistently outputs the four-codebook discrete tokens, which are subsequently converted back into a waveform using an <span class=\"ltx_text ltx_font_italic\">audio decoder</span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stages",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic Video Frame Sampling</span> We adopt a default sampling rate of 2 frames per second (2 FPS), while also enable dynamic adjustments according to video duration.\nDuring training, shorter videos are sampled at a higher frame rate to capture denser temporal information, ensuring at least 16 frames for effective information utilization. In contrast, for excessively long videos, frames will be sampled uniformly based on a maximum frame number constraint. This frame cap further facilitates training by regulating memory consumption and preserving computational efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "during",
                    "rate",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Image-Text Data</span>\nInterleaved image-text data provides broader visual-textual coverage and improves few-shot performance, but it is often noisy and has uneven quality. To address this, we construct a high-quality dataset through a two-stage pipeline of filtering and diversity sampling from open-source data. In the filtering stage, we remove samples with noisy tokens, sensitive content, and overly complex samples, and discard corrupted or low-resolution images. We then improve image-text alignment by pairing each image with its most relevant text segment by SigLIP similarity scores, and discard pairs with low semantic alignment.\nTo sample a diverse and evenly distributed subset, we apply density-based pruning and semantic clustering similar to&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Abbas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib22\" title=\"\">2024</a>)</cite>. Samples are scored by knowledge content, quality, and similarity, then evenly sampled based on the cluster they belong to and their distances to the cluster centroid.\nThis process reduces the raw dataset by approximately 74%, while maintaining diversity and quality for multimodal pretraining.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a robust text foundation, the text pre-training stage follows the same procedure as the initial phase of LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. The model is trained on approximately 16 trillion tokens drawn from a high-quality and diverse text corpus, where a constant learning rate is applied.\nThroughout training, the proportion of high-quality reasoning data (e.g., STEM and code) is progressively increased to strengthen the model&#8217;s reasoning and programming capabilities, thereby providing a solid foundation for subsequent multimodal learning.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "during",
                    "asr",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "during",
                    "rate",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pre-training stage, the model undergoes a multimodal annealing phase, where the model&#8217;s training continues with a curated higher-quality data under an annealed learning rate to achieve superior performance.\nWe further incorporate video data, including video captioning and QA datasets, as well as a broader range of image-related data such as OCR, grounding, GUI, multi-image, and STEM datasets. We maintain the stage-2 data ratio to ensure cross-modal stability, using a text:vision:speech token ratio of 2:1:1, consuming 0.33 trillion tokens in total.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a perplexity (PPL)-gap-based signal to automatically guide data sampling allocation. The corpus is first segmented into distinct subsets according to semantics and tasks. During training, we monitor PPL convergence for each subset: if a subset&#8217;s convergence lags behind the expected reference level, its sampling weight will be dynamically increased.\nFor each subset, we build a corresponding validation set and use an off-the-shelf vision-language model to compute its per-sample PPL as the expected reference level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib32\" title=\"\">2023</a>; Michel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib33\" title=\"\">2022</a>)</cite>.\nTo prevent high-value samples from being diluted by aggregate statistics, samples for which the PPL convergence is consistent with downstream performance will be isolated and relabeled into independent data subsets.\nWith the original data partitions largely preserved while the high-value samples organized more finely, this approach significantly enhances data efficiency and downstream task performance.</p>\n\n",
                "matched_terms": [
                    "during",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For textual and speech modalities, we utilize the foundation language model&#8217;s high-quality text corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, alongside carefully curated speech data from our multilingual audio repository (over 10 million hours of processed Chinese/English recordings). Notably, we maintain the same 2:1:1 text-to-vision-to-speech ratio as established in the pre-training phase, ensuring modality balance during context extension. This comprehensive approach ensures consistent performance across all modalities while scaling to extended contexts.</p>\n\n",
                "matched_terms": [
                    "during",
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Speech Interaction Data</span> We construct a large-scale voice dialogue dataset using a two-stage approach. (1) Text-based dataset adaptation: we filter out contents like formulas, code, markdown and rewrite responses with an LLM to produce speech-friendly conversational language. (2) Voice-oriented dialogue generation: we prompt an LLM to generate diverse topics and create new multi-turn dialogues.\nTo enhance expressiveness and naturalness, professional voice actors record dialogues across a range of emotions, speaking styles, and major Chinese dialects (e.g., Sichuan, Beijing, Northeast). These recordings are then used to fine-tune a specialized TTS engine, ensuring consistent tone, high fidelity, and natural prosody. All dialogues are ultimately synthesized into high-quality speech using this engine.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During SFT, we freeze the audio encoder while updating all other modules. The frozen audio encoder retains robust acoustic representations learned from large-scale audio pre-training (i.e., pre-training stage-5 as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>), whereas the unfrozen layers allow the model to learn fine-grained multimodal alignment and instruction-following behavior. Empirically, we find this selective fine-tuning strategy stabilizes convergence and avoids catastrophic forgetting of low-level auditory features. Apart from multi-modal SFT data, we also incorporate pure text SFT data used in developing LongCat-Flash, which covers domains ranging from reasoning, mathematics, coding, tool use, and general-purpose dialogue.</p>\n\n",
                "matched_terms": [
                    "during",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimized Grouped GEMM</span>\nOur optimization strategies for Grouped GEMM achieve approximately 75% MFU in the evaluated scenarios.\n(1) <span class=\"ltx_text ltx_font_bold\">Dynamic SwapAB:</span> The WGMMA instruction is evaluated with the <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> dimension varying from 8 to 256, while the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> dimension is fixed at 64.\nWe design an optimized kernel that automatically performs SwapAB operations to maximize WGMMA performance.\n(2) <span class=\"ltx_text ltx_font_bold\">Configurable SM Usage:</span> The Grouped GEMM kernel allows configurable SM counts to avoid resource contention when overlapping with dispatch or combined communication operations.\n(3) <span class=\"ltx_text ltx_font_bold\">Fine-Tuning:</span> <span class=\"ltx_text ltx_font_italic\">TileShape</span>, <span class=\"ltx_text ltx_font_italic\">PipelineStage</span>, and <span class=\"ltx_text ltx_font_italic\">ClusterShape</span> are carefully tuned for the target workload to maximize hardware utilization and prevent register spilling.\n(4) <span class=\"ltx_text ltx_font_bold\">Scheduling and Pipeline Strategy:</span> A swizzled block mapping schedule is adopted to significantly improve L2 cache hit rates.\nIn addition, a ping-pong mechanism is introduced during the backward pass to overlap WGMMA computation with the loading and storing of weight gradients.</p>\n\n",
                "matched_terms": [
                    "during",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            }
        ]
    },
    "S7.T8": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 8: Speech continuation performance of base models across pre-training stages. Accuracy is reported as a percentage.",
        "body": "Base Model\nSpeech Continuation (CMMLU)\n\n\n\nAudio In Text Out\nAudio In Audio Out\n\n\nStage-1\n88.80\n84.80\n\n\nStage-2\n89.60\n84.80\n\n\nStage-3\n92.80\n92.00\n\n\nStage-4 (32K)\n91.20\n91.20\n\n\nStage-4 (128K)\n90.40\n90.40",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Base Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speech Continuation (CMMLU)&#8593;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Audio In Text Out</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Audio In Audio Out</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Stage-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">88.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">84.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stage-2</td>\n<td class=\"ltx_td ltx_align_center\">89.60</td>\n<td class=\"ltx_td ltx_align_center\">84.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stage-3</td>\n<td class=\"ltx_td ltx_align_center\">92.80</td>\n<td class=\"ltx_td ltx_align_center\">92.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stage-4 (32K)</td>\n<td class=\"ltx_td ltx_align_center\">91.20</td>\n<td class=\"ltx_td ltx_align_center\">91.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Stage-4 (128K)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90.40</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "out",
            "stage2",
            "speech",
            "reported",
            "base",
            "pretraining",
            "cmmlu",
            "text",
            "accuracy",
            "continuation",
            "audio",
            "stage4",
            "performance",
            "stage1",
            "32k",
            "128k",
            "across",
            "models",
            "stages",
            "percentage",
            "stage3",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "models",
                    "speech",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "speech",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we attempt to address the aforementioned challenges.\nTo overcome the first challenge, we carefully design a multi-stage large-scale pretraining pipeline. Based on an early-stage text pretraining foundation model, we progressively incorporate audio and visual data into the large-scale pretraining process. By adopting a balanced multimodal data mixture and effective ealy-fusion strategy, the model achieves deeply integrated comprehension across modalities while maintaining strong unimodal performance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "pretraining",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SOTA and Unified Omni-Modal Model:</span> LongCat-Flash-Omni achieves state-of-the-art cross-modal comprehension performance among open-source models. It seamlessly integrates powerful offline multimodal understanding with real-time audio-visual interaction within a single all-in-one framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large-Scale with Real-time Audio-Visual Interaction:</span> By leveraging an efficient LLM backbone, carefully designed lightweight modality encoders and decoder, and a chunk-wise audio-visual feature interleaving mechanism, LongCat-Flash-Omni achieves low-latency, high-quality audio-visual processing and streaming speech generation. It supports a context window of up to 128K tokens, enabling advanced capabilities in long-term memory, multi-turn dialogue, and temporal reasoning across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "128k",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effective Early-Fusion Training:</span> The model adopts an innovative multi-stage pretraining pipeline that progressively incorporates text, audio, and visual modalities under a balanced data strategy and early-fusion training paradigm, ensuring strong omni-modal performance without degradation in any single modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "pretraining",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the model architecture of LongCat-Flash-Omni. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3\" title=\"3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4\" title=\"4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> describe the pretraining and post-training datasets and pipelines, respectively. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5\" title=\"5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6\" title=\"6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> introduce training infrastructures and inference deployment. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7\" title=\"7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the experimental results. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S8\" title=\"8 Conclusion &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> draws a conclusion of this report.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vision encoder serves as a critical component in multimodal language models. To effectively encode visual inputs such as images and videos, LongCat-Flash-Omni incorporates a well-designed Vision Transformer (ViT), referred to as LongCat-ViT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib10\" title=\"\">2025</a>)</cite>. LongCat-ViT achieves high performance across multimodal tasks, natively supports inputs with various resolutions and aspect ratios, while providing unified encoding capabilities for both image and video data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Design</span>\nLongCat-ViT is a Transformer-based encoder that retains the core structure of the conventional Vision Transformer while integrating several key enhancements: a unified patchification module for native image and video inputs, 2D rotary position embeddings (2D-RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite>, SwiGLU activation function, RMSNorm layer, a LayerScale module, and Query-Key normalization. Together, these refinements yield a more robust and efficient architecture compared to conventional designs.\nTo improve computational efficiency in video frame encoding during real-time interaction while maintaining model performance, we adopt a relatively lightweight model configuration.\nThe detailed hyper-parameters of the model are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Vision Encoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn line with common practice, a two-layer multilayer perceptron (MLP) with pre-normalization is employed as the vision-language projector to align visual and textual representations.\nMoreover, a 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> pixel-unshuffle operation is applied along the spatial dimension to mitigate the quadratic computational complexity associated with high-resolution inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Vision-Language Pretraining</span>\nLongCat-ViT adopts a progressive training scheme that integrates two complementary adaptation strategies: (1) progressive resolution adaptation, which leverages curriculum learning by transitioning from fixed low-resolution (e.g., 224) pretraining to native-resolution fine-tuning; and (2) progressive visual modality adaptation, which postpones the incorporation of video data until the final training stage to reduce computational overhead. To further facilitate convergence in the early phases, feature distillation from a frozen pretrained vision model is introduced as an auxiliary objective, and the weight of this objective gradually reduces in later stages. The model is trained from scratch on a total of 14.6 billion samples during the contrastive pretraining phase.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We input audio in different formats to the LLM backbone of LongCat-Flash-Omni across training phases. Specifically, during pre-training stages 1-4, an <span class=\"ltx_text ltx_font_italic\">audio tokenizer</span> is employed to convert raw speech into four-codebook discrete tokens, enabling consistent next-token prediction and improved training efficiency.\nHowever, we observe that such discretization hinders the model&#8217;s ability to capture fine-grained acoustic details. Therefore, from pre-training stage 5 (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>) we incorporate an <span class=\"ltx_text ltx_font_italic\">audio encoder</span> to convert raw speech into continuous audio features before input to the LLM.\nFor speech generation, to align with the inherent next token prediction paradigm, the LLM consistently outputs the four-codebook discrete tokens, which are subsequently converted back into a waveform using an <span class=\"ltx_text ltx_font_italic\">audio decoder</span>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "speech",
                    "stages",
                    "pretraining",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Encoder</span>\n\nTo optimize response latency and accommodate speech inputs of arbitrary duration, the audio encoder is designed using a streaming architecture.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio encoder takes 80-dimensional Fbank features as input. The architecture incorporates a Pre-FFN module that reduces the audio sequence length by a factor of eight through frame splicing downsampling technique, with each frame representing an 80ms time window.\nThe core process is performed by a streaming encoder that maintains a Transformer-like structure while incorporating several key modifications: (1) a Pre-Norm configuration for enhanced training stability, and (2) replacing standard self-attention modules with FSMN layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib15\" title=\"\">2018</a>)</cite> to enable efficient feature processing within constrained context windows.\nTo balance latency and performance, we implement a hybrid approach where only the final six layers incorporate a one-frame look-ahead mechanism, while maintaining strict causality in the preceding layers.\nThe architecture concludes with a post-FFN module for additional feature refinement.\nThe audio encoder is trained under supervised learning using speech recognition data using the CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib16\" title=\"\">2006</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is built upon LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, a 560-billion-parameter Mixture-of-Experts (MoE) language model.\nLongCat-Flash adopts Multi-head Latent Attention (MLA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib17\" title=\"\">2024a</a>)</cite>, shortcut-connected MoE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib18\" title=\"\">2024</a>)</cite> and zero-computation experts, performing variable computation per token by activating 18.6B-31.3B parameters (27B on average), thereby unifying efficiency, performance and sparsity.\nThese characteristics are retained and extended to multimodal understanding and audio-visual interaction by LongCat-Flash-Omni.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Efficient video processing remains a significant challenge due to the substantial variability in video properties, including durations ranging from a few seconds to several hours and resolutions spanning a wide spectrum. To address these challenges, we adopt a series of strategies to effectively balance between model performance and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collect a large-scale and diverse multimodal corpus with over 2.5 trillion tokens for pre-training. This pre-training corpus consists of the following components: audio data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS1\" title=\"3.1.1 Audio Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.1</span></a>), generic image-text data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS2\" title=\"3.1.2 Generic Image-Text Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.2</span></a>), OCR, grounding and GUI data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS3\" title=\"3.1.3 OCR, Grounding and GUI Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.3</span></a>), STEM data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS4\" title=\"3.1.4 STEM Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.4</span></a>), multi-image data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS5\" title=\"3.1.5 Multi-Image Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.5</span></a>), video data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS6\" title=\"3.1.6 Video Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.6</span></a>), and long-context multimodal data (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS7\" title=\"3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.7</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "text",
                    "out",
                    "models",
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build speech-text interleaved training data, we first split each training sample into fragments using punctuation marks as delimiters: <math alttext=\"(A_{1},T_{1}),(A_{2},T_{2}),\\dots,(A_{n},T_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(A_{1},T_{1}),(A_{2},T_{2}),\\dots,(A_{n},T_{n})</annotation></semantics></math>, where <math alttext=\"A_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">A_{i}</annotation></semantics></math> and <math alttext=\"T_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">T_{i}</annotation></semantics></math> represent the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th audio fragment and its transcribed text, respectively. We then randomly mask either audio or text component of some fragments, resulting in the final training input such as <math alttext=\"(T_{1},A_{2}+T_{2},T_{3},A_{4}+T_{4},\\dots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>2</mn></msub><mo>+</mo><msub><mi>T</mi><mn>2</mn></msub></mrow><mo>,</mo><msub><mi>T</mi><mn>3</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>4</mn></msub><mo>+</mo><msub><mi>T</mi><mn>4</mn></msub></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T_{1},A_{2}+T_{2},T_{3},A_{4}+T_{4},\\dots)</annotation></semantics></math> or <math alttext=\"(A_{1},A_{2}+T_{2},A_{3},A_{4}+T_{4},\\dots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>2</mn></msub><mo>+</mo><msub><mi>T</mi><mn>2</mn></msub></mrow><mo>,</mo><msub><mi>A</mi><mn>3</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>4</mn></msub><mo>+</mo><msub><mi>T</mi><mn>4</mn></msub></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(A_{1},A_{2}+T_{2},A_{3},A_{4}+T_{4},\\dots)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply text translation as data augmentation for speech recognition datasets, various speech models for pseudo-label generation and label quality filtering, particularly for paralinguistic understanding and audio captioning datasets, and a diverse set of prompts for each task to enhance instruction variability.\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Caption Data</span>\nHigh-quality image captions are crucial for aligning visual representations with the language model&#8217;s knowledge space. To this end, we build a large-scale image-caption dataset by first applying a multi-stage cleaning pipeline across text, image, and image-text pair levels. Low-quality samples, such as those with extremely short text, abnormal resolutions, or poor image quality, are removed using elaborately-refined heuristic rules. At the pair level, we further filter samples using a SigLIP similarity threshold, discarding those below a strict minimum to preserve downstream diversity. We then improve the dataset through re-captioning to generate dense, fine-grained captions with multiple open-source vision-language models while incorporating world knowledge from original annotations. Additional filtering, including mixed-language detection, repetition removal, and truncation handling is applied to help mitigate hallucinations and ensure the final image-text pairs are accurate, informative, and contextually rich.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To broaden semantic coverage and avoid overfitting to dominant patterns, we significantly enhance data diversity and balance. We cluster image-text pairs based on joint image and text embeddings and resample from each cluster to ensure representation of long-tail content, while simultaneously removing low-quality samples frequently concentrated in specific clusters. Experimental results confirm that preserving diversity at this stage is more beneficial than overly strict quality filtering. Furthermore, to address the pronounced long-tail distribution inherent in multimodal datasets, we adopt a concept-based resampling strategy inspired by MetaCLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib21\" title=\"\">2023</a>)</cite>. By expanding the vocabulary with a 200K-scale Chinese lexicon and resampling for broader concept coverage, we achieve a more balanced distribution across semantic categories.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Image-Text Data</span>\nInterleaved image-text data provides broader visual-textual coverage and improves few-shot performance, but it is often noisy and has uneven quality. To address this, we construct a high-quality dataset through a two-stage pipeline of filtering and diversity sampling from open-source data. In the filtering stage, we remove samples with noisy tokens, sensitive content, and overly complex samples, and discard corrupted or low-resolution images. We then improve image-text alignment by pairing each image with its most relevant text segment by SigLIP similarity scores, and discard pairs with low semantic alignment.\nTo sample a diverse and evenly distributed subset, we apply density-based pruning and semantic clustering similar to&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Abbas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib22\" title=\"\">2024</a>)</cite>. Samples are scored by knowledge content, quality, and similarity, then evenly sampled based on the cluster they belong to and their distances to the cluster centroid.\nThis process reduces the raw dataset by approximately 74%, while maintaining diversity and quality for multimodal pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data</span>\nWe curate richly annotated training samples encompassing various content types, including PDFs, papers and web pages. These efforts further enrich our document parsing dataset, enabling the model to learn both content extraction and structural understanding, and resulting in more robust and accurate document parsing.\nWe further include diverse OCR datasets covering scene text, structured documents, handwriting, and mathematical expressions.\nWe also synthesize multi-page and region-level OCR samples from scene and document OCR data, with each sample comprising 2 to 6 pages.\nFor OCR-related VQA, we incorporate a diverse range of datasets for visual question answering, covering domains of text-centric VQA, document VQA, table VQA, and chart VQA.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To strengthen the model&#8217;s foundational capabilities in scientific reasoning and problem-solving, we construct a large-scale, multimodal STEM (Science, Technology, Engineering, and Mathematics) dataset.\nThe collected data are meticulously processed and structured into both multiple-choice and open-ended generative question-answer formats.\nWe implement a rigorous multi-stage filtering pipeline to ensure factual accuracy, eliminate ambiguity, and standardize formatting.\nThis effort culminates in a high-fidelity pre-training dataset comprising 15 million image-text pairs with substantial diversity in both subject matter and academic level, and covering a wide range of disciplines from K12 education to advanced university studies.\nThis foundational dataset is essential for enabling the model to achieve deep conceptual understanding and robust reasoning capabilities across a broad spectrum of scientific and technical domains.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "accuracy",
                    "across",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our in-house long-context dataset includes image-text interleaved data and long-video QA data.\nImage-text interleaved data is constructed by concatenating single-image data with the same topic or rendering some text segments within long text into images. This data can help the model to improve in-context learning and information retrieval from long context (i.e., &#8220;a needle in a haystack&#8221;).\nLong video QA data is constructed by the following pipeline: (1) First we segment videos into multiple clips and generates rich descriptive captions for each clip. (2) For consecutive clips from the same video, we analyze their scene continuity from their captions. If the clips are identified as continuous, we concatenate and refine their captions into a coherent longer-form video caption. (3) Finally, we construct temporally grounded and detailed QA pairs to form our long-video QA dataset.\nThis data can help the model to enhance long-sequence cross-modal modeling and memory retention.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most fundamental challenges in training omni-modal models lies in the significant heterogeneity of data distributions across modalities. Each modality exhibits distinct structural properties and representational characteristics.\nText, for instance, is a highly compressed and abstract symbolic representation of human knowledge, and LLMs have demonstrated remarkable success in modeling large-scale text sequences. Speech, on the other hand, is the acoustic manifestation of human thoughts and concepts, a sequential signal like text, but enriched with paralinguistic information such as speaker timbre, emotion, prosody, and accent. However, its semantic density is much lower than that of text: while a typical speech tokenizer operating at 12.5 Hz must generate roughly 12.5 tokens per second, humans only speak about 3-4 text tokens per second. This mismatch makes sequence modeling for speech inherently more challenging than for text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual information introduces a fundamentally different modality. Unlike the sequential nature of text and speech, images encode spatial structures, requiring the model to reason over spatial relationships. Video data further increases the complexity by incorporating both spatial and temporal dynamics, making it a much more complicated modality to model effectively. This requires careful consideration of effective frame sampling strategies, token compression methods, and temporal modeling approaches, as well as the ability to handle long-range sequence dependencies.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by these observations, we adopt a curriculum-inspired, progressive training strategy that gradually transitions from simpler to more complex sequence modeling tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F5\" title=\"Figure 5 &#8227; 3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nWe begin with large-scale text pretraining (<span class=\"ltx_text ltx_font_bold\">Stage-0</span>), leveraging the maturity and stability of LLMs as a strong initialization for subsequent multimodal learning. Building on this foundation, we introduce speech data, which is structurally closer to text, to align acoustic representations with the language model&#8217;s feature space and effectively integrate paralinguistic information (<span class=\"ltx_text ltx_font_bold\">Stage-1</span>). Once speech-text alignment is established, we incorporate large-scale image-caption pairs and vision-language interleaved corpora (<span class=\"ltx_text ltx_font_bold\">Stage-2</span>) for vision-language alignment, enriching the model&#8217;s visual knowledge. We then introduce the most complex video data to enable spatial-temporal reasoning (<span class=\"ltx_text ltx_font_bold\">Stage-3</span>), meanwhile integrating higher-quality and more diverse image datasets to enhance vision comprehension.\nTo further support long-context reasoning and multi-turn interactions, we extend the model&#8217;s context window from 8K to 128K tokens (<span class=\"ltx_text ltx_font_bold\">Stage-4</span>).\nFinally, to mitigate information loss in audio inputs represented by discrete speech tokens, we introduce an audio encoder alignment stage (<span class=\"ltx_text ltx_font_bold\">Stage-5</span>) that enables the model to directly process continuous audio features, thereby enhancing fidelity in downstream speech tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "stage4",
                    "128k",
                    "stage2",
                    "stage1",
                    "speech",
                    "stage3",
                    "pretraining",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a robust text foundation, the text pre-training stage follows the same procedure as the initial phase of LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. The model is trained on approximately 16 trillion tokens drawn from a high-quality and diverse text corpus, where a constant learning rate is applied.\nThroughout training, the proportion of high-quality reasoning data (e.g., STEM and code) is progressively increased to strengthen the model&#8217;s reasoning and programming capabilities, thereby providing a solid foundation for subsequent multimodal learning.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "pretraining",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math>, <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> are loss weights. <math alttext=\"\\mathcal{L}_{\\text{pure-text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>pure-text</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{pure-text}}</annotation></semantics></math> denotes the loss for pure text data, while <math alttext=\"\\mathcal{L}_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{audio}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{audio-text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>audio-text</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{audio-text}}</annotation></semantics></math> correspond to the audio loss and the text loss terms in the text-speech data. <math alttext=\"\\mathcal{L}_{\\text{first-audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>first-audio</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{first-audio}}</annotation></semantics></math> is an additional loss term applied to the semantic audio token. After carefully hyperparameter tuning and extensive preliminary experiments, we set <math alttext=\"a=1.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mn>1.75</mn></mrow><annotation encoding=\"application/x-tex\">a=1.75</annotation></semantics></math>, <math alttext=\"b=0.25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding=\"application/x-tex\">b=0.25</annotation></semantics></math>, <math alttext=\"c=1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m11\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">c=1.5</annotation></semantics></math>, and <math alttext=\"d=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">d=0.1</annotation></semantics></math>, which achieves the best balance between preserving text capabilities and enhancing audio performance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This training stage uses approximately 5.1 trillion tokens,\nwith the ratio of text tokens and audio tokens is 2:1. A slight learning rate decay is applied.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the model from stage-1, we further incorporate large-scale image-text data into the pre-training procedure, including image caption data, and interleaved image-text data. We use a vision transformer (ViT) with weights initialized from the LongCat-ViT model introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to obtain visual features from images, and employ a randomly initialized vision projector to align the visual feature with the latent space of the LLM backbone.\nWe maintain the text-to-audio data ratio the same as in stage-1, i.e., 2:1, and make the text-to-vision data ratio also 2:1. In total, this pre-training stage consumes more than 3 trillion tokens.\nThe parameters of the ViT module and projector are jointly trained with the LLM decoder parameters throughout the stage-2 training, with a nearly constant learning rate. We reuse the loss weights in stage-1 and set the additional vision-related loss weight at 0.25 during this training process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "stage1",
                    "stage2",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pre-training stage, the model undergoes a multimodal annealing phase, where the model&#8217;s training continues with a curated higher-quality data under an annealed learning rate to achieve superior performance.\nWe further incorporate video data, including video captioning and QA datasets, as well as a broader range of image-related data such as OCR, grounding, GUI, multi-image, and STEM datasets. We maintain the stage-2 data ratio to ensure cross-modal stability, using a text:vision:speech token ratio of 2:1:1, consuming 0.33 trillion tokens in total.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "stage2",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of the model is highly dependent on the quality and composition of the training data. Therefore, the domain mixture plays a crucial role in determining the final performance. Vision data, in particular, exhibits greater heterogeneity in content distribution, learning difficulty, and scale, thereby requiring more careful control over data composition.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a perplexity (PPL)-gap-based signal to automatically guide data sampling allocation. The corpus is first segmented into distinct subsets according to semantics and tasks. During training, we monitor PPL convergence for each subset: if a subset&#8217;s convergence lags behind the expected reference level, its sampling weight will be dynamically increased.\nFor each subset, we build a corresponding validation set and use an off-the-shelf vision-language model to compute its per-sample PPL as the expected reference level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib32\" title=\"\">2023</a>; Michel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib33\" title=\"\">2022</a>)</cite>.\nTo prevent high-value samples from being diluted by aggregate statistics, samples for which the PPL convergence is consistent with downstream performance will be isolated and relabeled into independent data subsets.\nWith the original data partitions largely preserved while the high-value samples organized more finely, this approach significantly enhances data efficiency and downstream task performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To augment the model&#8217;s proficiency in capturing extended sequential relationships across various modalities, we gradually extend the context length to 32K and then 128K tokens, enabling the model to process and reason over more complex tasks such as long-term memory modeling and multi-turn real-time interaction.\nWe scale the context length from 8K to 32K tokens using 100B training tokens and adjust RoPE&#8217;s base frequency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite> from 1M to 5M to preserve positional encoding quality. The context length is then further expanded to 128K tokens with an additional 20B tokens of training, requiring a proportional increase of the RoPE&#8217;s base frequency to 10M to maintain stable attention across the extended sequence length.</p>\n\n",
                "matched_terms": [
                    "128k",
                    "across",
                    "32k",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in long-sequence multimodal modeling lies in preserving fine-grained visual information across multi-image compositions or extended video sequences.\nTo address this, integrating our native resolution encoding strategy achieves dual optimization: it maintains critical visual details at their original fidelity while intelligently managing token allocation efficiency. This approach ensures the preservation of essential spatial relationships in images and temporal coherence in videos throughout extended sequences. Building upon the stage-3 vision-related data, we further augment the training mixture by incorporating 25% additional long-context multimodal data, as detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS1.SSS7\" title=\"3.1.7 Long-Context Multimodal Data &#8227; 3.1 Data Curation &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.1.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "stage3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For textual and speech modalities, we utilize the foundation language model&#8217;s high-quality text corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, alongside carefully curated speech data from our multilingual audio repository (over 10 million hours of processed Chinese/English recordings). Notably, we maintain the same 2:1:1 text-to-vision-to-speech ratio as established in the pre-training phase, ensuring modality balance during context extension. This comprehensive approach ensures consistent performance across all modalities while scaling to extended contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "speech",
                    "pretraining",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training stage is a critical phase that transforms the pre-trained foundation model into a task-adaptive, human-aligned system with strong instruction-following, multimodal reasoning, and interactive capabilities. While the pre-training phase enhances multimodal perception and world knowledge, post-training focuses on refining response alignment, controllability, and fidelity through a combination of supervised and preference-based optimization.\nThis stage consists of two components: (1) Supervised Fine-Tuning (SFT), which equips the model with multimodal instruction-following, reasoning, and spoken interaction capabilities through high-quality and diverse instruction data, and (2) Reinforcement Learning (RL), which further enhances the model&#8217;s behavioral alignment, coherence, and consistency through Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite>.\nTogether, these procedures ensure that the final model not only demonstrates robust omni-modal understanding and reasoning capabilities but also generates responses that are semantically accurate, perceptually natural, and contextually coherent across both offline and online interaction scenarios.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high data quality and strong alignment with human preferences, we employ an LLM-as-a-judge evaluation framework. In this pipeline, a strong multimodal language model evaluates candidate responses against reference answers and produces explanatory rationales. Samples labeled as inconsistent, low-quality, or semantically inaccurate are automatically filtered out, and we finally have approximately 3 million carefully curated datasets with high factual accuracy, stylistic consistency, and comprehensive task diversity.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model",
                    "out"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Speech Interaction Data</span> We construct a large-scale voice dialogue dataset using a two-stage approach. (1) Text-based dataset adaptation: we filter out contents like formulas, code, markdown and rewrite responses with an LLM to produce speech-friendly conversational language. (2) Voice-oriented dialogue generation: we prompt an LLM to generate diverse topics and create new multi-turn dialogues.\nTo enhance expressiveness and naturalness, professional voice actors record dialogues across a range of emotions, speaking styles, and major Chinese dialects (e.g., Sichuan, Beijing, Northeast). These recordings are then used to fine-tune a specialized TTS engine, ensuring consistent tone, high fidelity, and natural prosody. All dialogues are ultimately synthesized into high-quality speech using this engine.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "out"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-Driven Automation</span> To comprehensively cover diverse interaction scenarios, we first establish an ability taxonomy encompassing six major dimensions: memorization, understanding, analysis, creation, application, and entertainment. Guided by this taxonomy, we collect a mixture of public and in-house videos as the source data to ensure balanced coverage across abilities. For each video, we first perform scene segmentation using PySceneDetect <cite class=\"ltx_cite ltx_citemacro_citep\">(Breakthrough, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib35\" title=\"\">2025</a>)</cite> to obtain a sequence of clips. For every clip, a powerful multimodal language model generates multiple rounds of progressively deep and context-aware QA pairs. In each round, subsequent queries build upon preceding conversations with logical progression, referential dependency, or anaphoric linkage to earlier turns. Such conversations encourage the model to reason over extended dialogue contexts, maintain entity and reference consistency in a natural, human-like audio-visual interaction setting. We then apply an automatic verification pipeline, employing an LLM-as-a-judge framework to evaluate and discard low-quality or inconsistent QA pairs. The remaining qualified samples are then composed into multi-turn dialogue, enabling the model to learn both intra-scene conversational continuity and inter-scene contextual transitions, thereby better reflecting real-world audio-visual interactions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During SFT, we freeze the audio encoder while updating all other modules. The frozen audio encoder retains robust acoustic representations learned from large-scale audio pre-training (i.e., pre-training stage-5 as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>), whereas the unfrozen layers allow the model to learn fine-grained multimodal alignment and instruction-following behavior. Empirically, we find this selective fine-tuning strategy stabilizes convergence and avoids catastrophic forgetting of low-level auditory features. Apart from multi-modal SFT data, we also incorporate pure text SFT data used in developing LongCat-Flash, which covers domains ranging from reasoning, mathematics, coding, tool use, and general-purpose dialogue.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the model&#8217;s multimodal capabilities and improve human alignment, we employ the Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite> during the reinforcement learning stage. The LongCat-Flash-Omni model supports multimodal inputs and enables parallel streaming outputs of both text and speech. However, most existing DPO variants are designed for text-only outputs or optimize text and speech separately. We argue that such decoupled optimization is suboptimal for maintaining coherence between textual and speech responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we extend the DPO to jointly optimize text and speech outputs to improve both alignment and stability across modalities. Specifically, since the LongCat-Flash-Omni includes one text head and multiple audio heads, we modify the DPO objective to optimize all heads simultaneously. The overall loss is defined as:</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the number of audio heads. <math alttext=\"\\mathcal{L}_{\\text{DPO}}(\\text{text}_{chosen},\\text{text}_{rejected})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DPO</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mtext>text</mtext><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mtext>text</mtext><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DPO}}(\\text{text}_{chosen},\\text{text}_{rejected})</annotation></semantics></math> focuses on the semantic quality of the text head, while each audio head <math alttext=\"\\mathcal{L}_{\\text{DPO}}(\\text{audio}^{i}_{chosen},\\text{audio}^{i}_{rejected})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>DPO</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext>audio</mtext><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mtext>audio</mtext><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{DPO}}(\\text{audio}^{i}_{chosen},\\text{audio}^{i}_{rejected})</annotation></semantics></math> emphasizes both the linguistic and pronunciation stability of the corresponding speech output. This joint optimization strategy leads to more coherent and natural multimodal responses, further enhancing the expressiveness and human-aligned communication capabilities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train for one epoch with a batch size of 256, sampling each batch to achieve a well-balanced mix of modalities. The learning rate follows a cosine decay schedule, with a warm-up fraction of 0.03, gradually decreasing from <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math> to 0. To balance preference learning between the text head and the speech head, we set their loss weight ratio by setting <math alttext=\"\\alpha:\\beta=1:1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#946;</mi><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha:\\beta=1:1</annotation></semantics></math>. To mitigate drift from the SFT model, we incorporate a Kullback-Leibler (KL) divergence regularizer with a weighting factor of 0.1.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "models",
                    "speech",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach is to decouple multimodal models across different parallel dimensions. For example, DistTrain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib38\" title=\"\">2025</a>)</cite> mitigates model and data heterogeneity by separating model partitioning strategies and sorting data; PipeWeaver&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib39\" title=\"\">2025</a>)</cite> reduces PP bubbles caused by heterogeneity through fine-grained data partitioning and dynamic pipeline scheduling; and Optimus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib40\" title=\"\">2025</a>)</cite> separates the parallelization strategies for encoders and LLMs, scheduling encoder computations into the LLM&#8217;s idle periods to eliminate bubbles caused by model heterogeneity.\nBuilding on the Optimus approach, we develop modality-decoupled parallelism (MDP), a simple yet effective multimodal training strategy. The core idea is to completely decouple the modality encoders and the LLM backbone at the distributed level, enabling independent scheduling and more efficient utilization of computational resources.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Loading</span>: At the start of each training iteration, the rank with <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> fetches all micro-batches. Then it broadcasts the metadata of the data to the other ranks within the DP group. This mitigates I/O overhead and memory pressure. In this phase, all micro-batches are sorted by text data sequence length to ensure workload across DP groups remain similar, thereby reducing EP bubbles caused by workload imbalances.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these three components, we accomplish the transformation of data formats across different stages. In the forward phase, the transformation is performed according to the two-stage chunking approach, while in the backward phase, <span class=\"ltx_text ltx_font_italic\">Embedding Redistribution</span> executes the reverse process: gradients are first gathered via CP gather and then scattered to the corresponding <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks according to the global offset information for backward computation. Meanwhile, the chunking scheme reduces the peak memory usage to <math alttext=\"1/num\\_chunk\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p6.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">1/num\\_chunk</annotation></semantics></math> of the original, significantly alleviating memory pressure while ensuring bitwise numerical alignment.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In large-scale LLM pretraining, the efficiency of core operators is primarily constrained by the sequence length; increasing sequence length typically improves Model FLOPs Utilization (MFU). Accordingly, our system design increases the effective sequence length per EP rank to approach the operators&#8217; high-efficiency regime. Hardware-specific benchmarking results further indicate that reducing CP substantially improves the efficiency of the core operator, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F10\" title=\"Figure 10 &#8227; 5.2.1 Optimal Distributed Configuration &#8227; 5.2 Performance Tuning &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training infra contains multiple compute-communication overlaps, mainly including shortcut-based EP overlap and point-to-point (P2P) overlap. We tune the number of streaming multiprocessors (SMs) assigned to communication and compute kernels for each scenario to maximize hardware utilization, and we use distinct P2P groups/streams so that inter-stage pipeline-parallel (PP) communication does not interfere across stages.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language model pretraining is extremely resource-intensive, often requiring weeks and incurring millions in computational costs. Ensuring the correctness of the training framework is essential to avoid costly retraining due to implementation errors.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a highly efficient asynchronous streaming pipeline for model serving. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6.F11\" title=\"Figure 11 &#8227; 6.2 Asynchronous Streaming Pipeline &#8227; 6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, it consists of four sequentially linked and concurrently executed stages: VAD &amp; Frame Sampling, Audio-Visual Encoding, LLM Prefilling &amp; Decoding, and Audio Decoding. Each module supports incremental inference of streaming input and adaptive batching strategies, facilitating concurrent scheduling to reduce latency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "stages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> A voice activity detection (VAD) module is deployed to detect whether the user is speaking in real-time. When the user is speaking, densely sampled frames as well as the audio will be used as model input, facilitating a deeper analysis of the video content. Otherwise, only sparsely sampled frames are used to track the high-level overview of the video stream.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, each request data packet from the user side consists of 1 second of audio and 2 corresponding video frames. Using the streaming prefill strategy, each request packet can be immediately prefilled, eliminating the need to wait until the user&#8217;s turn to finish before initiating computation. By overlapping the VAD endpoint detection (<math alttext=\"600\\sim 700ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>600</mn><mo>&#8764;</mo><mrow><mn>700</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">600\\sim 700ms</annotation></semantics></math>) and the prefill process, users can receive the model response within <math alttext=\"100ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">100ms</annotation></semantics></math> after the endpoint detection. This asynchronous pipeline enables real-time omni-modal interaction.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models",
                    "32k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "stage4",
                    "across",
                    "stage2",
                    "performance",
                    "stage1",
                    "models",
                    "speech",
                    "base",
                    "stages",
                    "continuation",
                    "stage3",
                    "pretraining",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "base",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "models",
                    "speech",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "base",
                    "pretraining",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "models",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "models",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data construction process involves 10 trained professional conversationalists who conducted multi-turn dialogue sessions (approximately three minutes each) with each model under evaluation. These interactions are carried out via real-time audio-visual interfaces on the official app or web platforms provided by the respective model developers.\nA total of 200 dialogue sessions have been collected for each model, covering common real-world video call scenarios across four categories: problem solving, entertainment, self-improvement, and emotional support (50 samples per category).\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "out"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "models",
                    "accuracy",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across",
                    "performance",
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We address major challenges in building such a system: cross-modal heterogeneity, unified offline and streaming capabilities, and low-latency interaction. Through a carefully designed multi-stage early-fusion pretraining pipeline, LongCat-Flash-Omni achieves deeply integrated representations that enable synergistic multimodal reasoning while preserving unimodality strength. Our introduction of human-in-the-loop data construction and a 128K-token context window further enhances multi-turn dialogue, temporal reasoning, and memory capabilities in dynamic interactive scenarios.\nOn the architectural side, the adoption of the ScMoE backbone with zero-computation experts, together with lightweight modality encoders and decoder, allows the model to support real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            }
        ]
    },
    "S7.T9": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 9: Automatic speech recognition (ASR) and speech-to-text translation (S2TT) evaluation results.",
        "body": "WenetSpeech\n\n\ntest-meeting  test-net",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">WenetSpeech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">test-meeting &#8212; test-net</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "translation",
            "evaluation",
            "speechtotext",
            "speech",
            "asr",
            "testnet",
            "wenetspeech",
            "results",
            "testmeeting",
            "s2tt",
            "automatic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Encoder</span>\n\nTo optimize response latency and accommodate speech inputs of arbitrary duration, the audio encoder is designed using a streaming architecture.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio encoder takes 80-dimensional Fbank features as input. The architecture incorporates a Pre-FFN module that reduces the audio sequence length by a factor of eight through frame splicing downsampling technique, with each frame representing an 80ms time window.\nThe core process is performed by a streaming encoder that maintains a Transformer-like structure while incorporating several key modifications: (1) a Pre-Norm configuration for enhanced training stability, and (2) replacing standard self-attention modules with FSMN layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib15\" title=\"\">2018</a>)</cite> to enable efficient feature processing within constrained context windows.\nTo balance latency and performance, we implement a hybrid approach where only the final six layers incorporate a one-frame look-ahead mechanism, while maintaining strict causality in the preceding layers.\nThe architecture concludes with a post-FFN module for additional feature refinement.\nThe audio encoder is trained under supervised learning using speech recognition data using the CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib16\" title=\"\">2006</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply text translation as data augmentation for speech recognition datasets, various speech models for pseudo-label generation and label quality filtering, particularly for paralinguistic understanding and audio captioning datasets, and a diverse set of prompts for each task to enhance instruction variability.\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "evaluation",
                    "speech",
                    "asr",
                    "results",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "translation",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Recognition and Translation</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "results",
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "results"
                ]
            }
        ]
    },
    "S7.T10": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 10: Evaluation results of audio understanding.",
        "body": "Qwen3-Omni\n\n\nInstruct",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen3-Omni</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Instruct</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluation",
            "understanding",
            "qwen3omni",
            "results",
            "instruct",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the fourth challenge of training efficiency, we devote substantial effort to large-scale omni-modal distributed training. We propose a modality-decoupled parallelism (MDP) strategy. This approach enables independent optimization of both performance and memory usage for the LLM, vision encoder, and audio encoder. For the LLM component, we systematically optimize the distributed training configuration for computational efficiency and apply multiple memory-reduction techniques to ensure robust system stability throughout training. Experimental results demonstrate the effectiveness of this strategy, our system sustains more than 90% of the throughput achieved during pure text training.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Audio-Visual Feature Interleaving</span> Unlike offline audio-visual understanding tasks, where audio and visual features can be concatenated at the sequence level, real-time audio-visual interaction requires features from the audio and video streams to be prefilled into the LLM backbone as early as possible to minimize response latency once the user query is received.\nTo achieve this, we design a temporally-synchronized, chunk-wise audio-visual feature interleaving mechanism. Audio-visual feature chunks are structured as &#8220;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-start-token&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#8230;&#161;&#8212;audio-end-token&#8212;&#191;&#8221;, where the timestamp is represented in textual form, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS4.SSS1\" title=\"2.4.1 Video Strategy &#8227; 2.4 Video Strategy and Streaming Audio-Visual Interaction &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.4.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span> We curate an audio understanding dataset that encompasses a wide range of tasks, including audio captioning, semantic audio understanding, paralinguistic analysis, acoustic scene and event detection, audio question answering, and music understanding.\nThe dataset comprises a combination of open-source datasets and in-house proprietary datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply text translation as data augmentation for speech recognition datasets, various speech models for pseudo-label generation and label quality filtering, particularly for paralinguistic understanding and audio captioning datasets, and a diverse set of prompts for each task to enhance instruction variability.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation of image understanding across six key dimensions, leveraging a diverse suite of benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "results",
                    "understanding",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate audio-visual understanding on VideoMME under the no-subtitle protocol (w/o sub), and report results for two conditions: with audio (&#8220;VideoMME w/ audio&#8221;) and without audio (&#8220;VideoMME w/o audio&#8221;).\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "understanding",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding",
                    "instruct",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate cross-modal understanding using publicly available benchmarks, including OmniBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib118\" title=\"\">2024c</a>)</cite>, WorldSense&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib119\" title=\"\">2025</a>)</cite>, and DailyOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib120\" title=\"\">2025</a>)</cite>. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.\nTo address the limited quality and coverage of existing benchmarks, we further introduce a new benchmark, UNO-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib121\" title=\"\">2025</a>)</cite>, comprising 1,880 human-crafted questions spanning 44 task types, with 98% of the questions requiring cross-modal reasoning.\nWe constructed this benchmark by manually annotating our in-house dataset. This approach prevents data contamination and ensures the benchmark is highly representative of real-world application scenarios.\nIn addition to conventional multiple-choice questions, the evaluation includes innovative multi-step open-ended questions, providing a more realistic and discriminative assessment of complex reasoning abilities.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            }
        ]
    },
    "S7.T11": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 11: Evaluation results of audio-to-text chat.",
        "body": "Qwen3-Omni\n\n\nInstruct",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen3-Omni</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Instruct</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "chat",
            "evaluation",
            "audiototext",
            "qwen3omni",
            "results",
            "instruct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "results",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audiototext",
                    "chat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "chat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "instruct",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "qwen3omni"
                ]
            }
        ]
    },
    "S7.T12": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 12: Comparison between LongCat-Flash-Omni and other base models on text-only benchmarks. Values marked with * are sourced from public reports.",
        "body": "Kimi-K2\n\n\nBase",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Kimi-K2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Base</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "public",
            "kimik2",
            "reports",
            "benchmarks",
            "longcatflashomni",
            "models",
            "comparison",
            "from",
            "base",
            "sourced",
            "values",
            "other",
            "marked",
            "between",
            "textonly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni",
                    "models",
                    "from",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SOTA and Unified Omni-Modal Model:</span> LongCat-Flash-Omni achieves state-of-the-art cross-modal comprehension performance among open-source models. It seamlessly integrates powerful offline multimodal understanding with real-time audio-visual interaction within a single all-in-one framework.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the model architecture of LongCat-Flash-Omni. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3\" title=\"3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4\" title=\"4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> describe the pretraining and post-training datasets and pipelines, respectively. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5\" title=\"5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6\" title=\"6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> introduce training infrastructures and inference deployment. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7\" title=\"7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the experimental results. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S8\" title=\"8 Conclusion &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> draws a conclusion of this report.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "reports"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vision encoder serves as a critical component in multimodal language models. To effectively encode visual inputs such as images and videos, LongCat-Flash-Omni incorporates a well-designed Vision Transformer (ViT), referred to as LongCat-ViT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Qiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib10\" title=\"\">2025</a>)</cite>. LongCat-ViT achieves high performance across multimodal tasks, natively supports inputs with various resolutions and aspect ratios, while providing unified encoding capabilities for both image and video data.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We input audio in different formats to the LLM backbone of LongCat-Flash-Omni across training phases. Specifically, during pre-training stages 1-4, an <span class=\"ltx_text ltx_font_italic\">audio tokenizer</span> is employed to convert raw speech into four-codebook discrete tokens, enabling consistent next-token prediction and improved training efficiency.\nHowever, we observe that such discretization hinders the model&#8217;s ability to capture fine-grained acoustic details. Therefore, from pre-training stage 5 (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>) we incorporate an <span class=\"ltx_text ltx_font_italic\">audio encoder</span> to convert raw speech into continuous audio features before input to the LLM.\nFor speech generation, to align with the inherent next token prediction paradigm, the LLM consistently outputs the four-codebook discrete tokens, which are subsequently converted back into a waveform using an <span class=\"ltx_text ltx_font_italic\">audio decoder</span>.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Efficient video processing remains a significant challenge due to the substantial variability in video properties, including durations ranging from a few seconds to several hours and resolutions spanning a wide spectrum. To address these challenges, we adopt a series of strategies to effectively balance between model performance and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "between",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "between",
                    "models",
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "between",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Caption Data</span>\nHigh-quality image captions are crucial for aligning visual representations with the language model&#8217;s knowledge space. To this end, we build a large-scale image-caption dataset by first applying a multi-stage cleaning pipeline across text, image, and image-text pair levels. Low-quality samples, such as those with extremely short text, abnormal resolutions, or poor image quality, are removed using elaborately-refined heuristic rules. At the pair level, we further filter samples using a SigLIP similarity threshold, discarding those below a strict minimum to preserve downstream diversity. We then improve the dataset through re-captioning to generate dense, fine-grained captions with multiple open-source vision-language models while incorporating world knowledge from original annotations. Additional filtering, including mixed-language detection, repetition removal, and truncation handling is applied to help mitigate hallucinations and ensure the final image-text pairs are accurate, informative, and contextually rich.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our video dataset is primarily sourced from a broad range of publicly available corpora, covering diverse task types such as video classification, temporal grounding, detection, captioning, and question answering. We design a comprehensive data processing pipeline centered on rigorous quality filtering and targeted enhancement, resulting in a refined, high-quality dataset tailored for large-scale pre-training.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most fundamental challenges in training omni-modal models lies in the significant heterogeneity of data distributions across modalities. Each modality exhibits distinct structural properties and representational characteristics.\nText, for instance, is a highly compressed and abstract symbolic representation of human knowledge, and LLMs have demonstrated remarkable success in modeling large-scale text sequences. Speech, on the other hand, is the acoustic manifestation of human thoughts and concepts, a sequential signal like text, but enriched with paralinguistic information such as speaker timbre, emotion, prosody, and accent. However, its semantic density is much lower than that of text: while a typical speech tokenizer operating at 12.5 Hz must generate roughly 12.5 tokens per second, humans only speak about 3-4 text tokens per second. This mismatch makes sequence modeling for speech inherently more challenging than for text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "between",
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To augment the model&#8217;s proficiency in capturing extended sequential relationships across various modalities, we gradually extend the context length to 32K and then 128K tokens, enabling the model to process and reason over more complex tasks such as long-term memory modeling and multi-turn real-time interaction.\nWe scale the context length from 8K to 32K tokens using 100B training tokens and adjust RoPE&#8217;s base frequency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite> from 1M to 5M to preserve positional encoding quality. The context length is then further expanded to 128K tokens with an additional 20B tokens of training, requiring a proportional increase of the RoPE&#8217;s base frequency to 10M to maintain stable attention across the extended sequence length.</p>\n\n",
                "matched_terms": [
                    "base",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-Text SFT Data</span>\nWe construct a large-scale video data pool of approximately 3 million videos sourced from diverse proprietary and open-source datasets, covering tasks such as general video understanding, classification, reasoning, grounding, temporal localization, segmentation, and highlight detection. From each source, representative subsets are annotated with capability tags using multimodal language models, following a predefined taxonomy encompassing visual perception, temporal and causal reasoning, and domain-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This taxonomy, consisting of 48 subcategories (e.g., entity attributes, counting, relationship comparison, OCR, anomaly detection, spatial relations, and camera motion), guides targeted sampling to enrich underrepresented capabilities and analyze coverage gaps. For capabilities with insufficient data, we augment annotations using proprietary expert models, and for particularly challenging tasks&#8212;such as action counting, relationship comparison, event localization, and attribute change&#8212;we incorporate manually annotated samples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "between",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During SFT, we freeze the audio encoder while updating all other modules. The frozen audio encoder retains robust acoustic representations learned from large-scale audio pre-training (i.e., pre-training stage-5 as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>), whereas the unfrozen layers allow the model to learn fine-grained multimodal alignment and instruction-following behavior. Empirically, we find this selective fine-tuning strategy stabilizes convergence and avoids catastrophic forgetting of low-level auditory features. Apart from multi-modal SFT data, we also incorporate pure text SFT data used in developing LongCat-Flash, which covers domains ranging from reasoning, mathematics, coding, tool use, and general-purpose dialogue.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the model&#8217;s multimodal capabilities and improve human alignment, we employ the Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib34\" title=\"\">2023</a>)</cite> during the reinforcement learning stage. The LongCat-Flash-Omni model supports multimodal inputs and enables parallel streaming outputs of both text and speech. However, most existing DPO variants are designed for text-only outputs or optimize text and speech separately. We argue that such decoupled optimization is suboptimal for maintaining coherence between textual and speech responses.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "between",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train for one epoch with a batch size of 256, sampling each batch to achieve a well-balanced mix of modalities. The learning rate follows a cosine decay schedule, with a warm-up fraction of 0.03, gradually decreasing from <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math> to 0. To balance preference learning between the text head and the speech head, we set their loss weight ratio by setting <math alttext=\"\\alpha:\\beta=1:1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#946;</mi><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha:\\beta=1:1</annotation></semantics></math>. To mitigate drift from the SFT model, we incorporate a Kullback-Leibler (KL) divergence regularizer with a weighting factor of 0.1.</p>\n\n",
                "matched_terms": [
                    "between",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Forward</span>: The BalanceData module first distributes the modality data from the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks. Then, the modality encoders on each rank compute the corresponding vision and audio embeddings. Finally, the ModalityBridge module aggregates these embeddings at the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank, which are then passed as input to the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Backward</span>: The ModalityBridge module redistributes the modality embedding gradients from the LLM decoder to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks, and then the backward pass of the modality encoders is executed.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MDP, the ModalityBridge serves as the communication layer between the multimodal encoders and the LLM decoder.\nIt is responsible for transforming data organization formats to resolve discrepancies arising from the different parallelism strategies employed by the modality encoders and the LLM decoder. This process specifically involves handling modality embeddings during the forward phase and gradients during the backward phase.\nHowever, when processing longer context length, significant memory pressure emerges as the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank must read all micro-batches and perform gather and scatter operations on the modality embeddings and their gradients.\nTo address this challenge, we adopt chunk-based processing within ModalityBridge, which effectively alleviates the memory bottleneck while maintaining bitwise numerical consistency.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F9\" title=\"Figure 9 &#8227; 5.1.2 ModalityBridge &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this module comprises three core components:</p>\n\n",
                "matched_terms": [
                    "between",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "marked",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare LongCat-Flash-Omni with omni-models (Gemini-2.5-Pro<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/hello-gpt-4o\" title=\"\">https://openai.com/index/hello-gpt-4o</a></span></span></span>, Seed-1.6<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://seed.bytedance.com/en/seed1_6\" title=\"\">https://seed.bytedance.com/en/seed1_6</a></span></span></span>, and Qwen3-Omni<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>) and vision language models (Qwen3-VL<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite> and Qwen2.5-VL-72B<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib47\" title=\"\">2025</a>)</cite>) without thinking mode.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "base",
                    "between",
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "base",
                    "between",
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni",
                    "models",
                    "base",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "kimik2",
                    "longcatflashomni",
                    "models",
                    "comparison",
                    "base"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "between",
                    "longcatflashomni",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longcatflashomni"
                ]
            }
        ]
    },
    "S7.T13": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 13: Evaluation results of frontier chat/instruct models. Values marked with * are sourced from other public reports. Note that DeepSeek-V3.1, Qwen3-235B-A22B, Gemini2.5-Flash, and Claude Sonnet-4 are evaluated under their non-thinking mode.",
        "body": "Gemini-2.5\n\n\n-Flash",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Gemini-2.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-Flash</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "chatinstruct",
            "nonthinking",
            "evaluation",
            "gemini25",
            "results",
            "note",
            "marked",
            "gemini25flash",
            "reports",
            "deepseekv31",
            "evaluated",
            "under",
            "their",
            "sonnet4",
            "mode",
            "from",
            "public",
            "frontier",
            "models",
            "sourced",
            "qwen3235ba22b",
            "values",
            "other",
            "flash",
            "claude"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gemini25",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2\" title=\"2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the model architecture of LongCat-Flash-Omni. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3\" title=\"3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4\" title=\"4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> describe the pretraining and post-training datasets and pipelines, respectively. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5\" title=\"5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S6\" title=\"6 Inference and Deployment &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> introduce training infrastructures and inference deployment. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7\" title=\"7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the experimental results. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S8\" title=\"8 Conclusion &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> draws a conclusion of this report.</p>\n\n",
                "matched_terms": [
                    "results",
                    "reports"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Native Resolution Encoding</span>\nConventional ViT models widely adopted in the community (e.g., CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib12\" title=\"\">2021</a>)</cite>, SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib13\" title=\"\">2023</a>)</cite>) typically resize input images to a fixed resolution, which can result in substantial information loss, particularly for images with extreme aspect ratios or high native resolutions.\nTo mitigate this limitation, LongCat-ViT encodes visual inputs at their native resolutions, circumventing the limitations of fixed-resolution ViT models. This preserves the spatial and contextual information inherent in visual data, thereby enhancing the model&#8217;s capability to comprehend and reason over complex visual inputs.\nFor each image or video frame, if the number of patches falls within a predefined range (576-5832 during training), only minimal resizing is applied to ensure both dimensions are divisible by 112. Otherwise, the image is rescaled to fit within this range while preserving its aspect ratio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "from",
                    "under",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "from",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-Text Interleaved Data</span>\nOur data consists of tens of millions of hours of audio with rich diversity.\nWe employ the following pipeline to extract high-quality speech audios with consistent topics:\nFirst, we use a VAD system to split a long audio into speaking segments and remove non-speaking regions. Next, we use two proprietary ASR models for cross-validation, filtering out segments with significant discrepancies on transcription results. A multilingual speech aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib20\" title=\"\">2024</a>)</cite> is then applied for forced alignment, yielding precise transcription timestamps.\nWe further refine the dataset by computing the ratio between speech duration and text length for each segment, and discard the segments whose ratios fall outside the <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>-th to <math alttext=\"99.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mn>99.5</mn><annotation encoding=\"application/x-tex\">99.5</annotation></semantics></math>-th percentile range.\nFinally, we merge adjacent speaking segments separated by silences shorter than 10 seconds to form the training samples.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Caption Data</span>\nHigh-quality image captions are crucial for aligning visual representations with the language model&#8217;s knowledge space. To this end, we build a large-scale image-caption dataset by first applying a multi-stage cleaning pipeline across text, image, and image-text pair levels. Low-quality samples, such as those with extremely short text, abnormal resolutions, or poor image quality, are removed using elaborately-refined heuristic rules. At the pair level, we further filter samples using a SigLIP similarity threshold, discarding those below a strict minimum to preserve downstream diversity. We then improve the dataset through re-captioning to generate dense, fine-grained captions with multiple open-source vision-language models while incorporating world knowledge from original annotations. Additional filtering, including mixed-language detection, repetition removal, and truncation handling is applied to help mitigate hallucinations and ensure the final image-text pairs are accurate, informative, and contextually rich.</p>\n\n",
                "matched_terms": [
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To broaden semantic coverage and avoid overfitting to dominant patterns, we significantly enhance data diversity and balance. We cluster image-text pairs based on joint image and text embeddings and resample from each cluster to ensure representation of long-tail content, while simultaneously removing low-quality samples frequently concentrated in specific clusters. Experimental results confirm that preserving diversity at this stage is more beneficial than overly strict quality filtering. Furthermore, to address the pronounced long-tail distribution inherent in multimodal datasets, we adopt a concept-based resampling strategy inspired by MetaCLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib21\" title=\"\">2023</a>)</cite>. By expanding the vocabulary with a 200K-scale Chinese lexicon and resampling for broader concept coverage, we achieve a more balanced distribution across semantic categories.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Image-Text Data</span>\nInterleaved image-text data provides broader visual-textual coverage and improves few-shot performance, but it is often noisy and has uneven quality. To address this, we construct a high-quality dataset through a two-stage pipeline of filtering and diversity sampling from open-source data. In the filtering stage, we remove samples with noisy tokens, sensitive content, and overly complex samples, and discard corrupted or low-resolution images. We then improve image-text alignment by pairing each image with its most relevant text segment by SigLIP similarity scores, and discard pairs with low semantic alignment.\nTo sample a diverse and evenly distributed subset, we apply density-based pruning and semantic clustering similar to&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Abbas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib22\" title=\"\">2024</a>)</cite>. Samples are scored by knowledge content, quality, and similarity, then evenly sampled based on the cluster they belong to and their distances to the cluster centroid.\nThis process reduces the raw dataset by approximately 74%, while maintaining diversity and quality for multimodal pretraining.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our video dataset is primarily sourced from a broad range of publicly available corpora, covering diverse task types such as video classification, temporal grounding, detection, captioning, and question answering. We design a comprehensive data processing pipeline centered on rigorous quality filtering and targeted enhancement, resulting in a refined, high-quality dataset tailored for large-scale pre-training.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "sourced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our in-house long-context dataset includes image-text interleaved data and long-video QA data.\nImage-text interleaved data is constructed by concatenating single-image data with the same topic or rendering some text segments within long text into images. This data can help the model to improve in-context learning and information retrieval from long context (i.e., &#8220;a needle in a haystack&#8221;).\nLong video QA data is constructed by the following pipeline: (1) First we segment videos into multiple clips and generates rich descriptive captions for each clip. (2) For consecutive clips from the same video, we analyze their scene continuity from their captions. If the clips are identified as continuous, we concatenate and refine their captions into a coherent longer-form video caption. (3) Finally, we construct temporally grounded and detailed QA pairs to form our long-video QA dataset.\nThis data can help the model to enhance long-sequence cross-modal modeling and memory retention.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most fundamental challenges in training omni-modal models lies in the significant heterogeneity of data distributions across modalities. Each modality exhibits distinct structural properties and representational characteristics.\nText, for instance, is a highly compressed and abstract symbolic representation of human knowledge, and LLMs have demonstrated remarkable success in modeling large-scale text sequences. Speech, on the other hand, is the acoustic manifestation of human thoughts and concepts, a sequential signal like text, but enriched with paralinguistic information such as speaker timbre, emotion, prosody, and accent. However, its semantic density is much lower than that of text: while a typical speech tokenizer operating at 12.5 Hz must generate roughly 12.5 tokens per second, humans only speak about 3-4 text tokens per second. This mismatch makes sequence modeling for speech inherently more challenging than for text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we maintain the LLM parameters in a frozen state while exclusively training the audio encoder.\nThis approach serves dual objectives: (1) preserving the LLM&#8217;s established multimodal processing capabilities, and (2) enhancing audio comprehension while aligning continuous speech inputs with the LLM&#8217;s semantic space.\nWe investigated the necessity of separate audio projector module pre-training for semantic space alignment, but empirical results showed negligible performance differences compared to direct end-to-end audio encoder training.\nTo accelerate convergence, we initialize the audio encoder (excluding the projector) from the audio encoder trained with speech recognition introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS2\" title=\"2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, while randomly initializing the projector module parameters.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-Text SFT Data</span>\nWe construct a large-scale video data pool of approximately 3 million videos sourced from diverse proprietary and open-source datasets, covering tasks such as general video understanding, classification, reasoning, grounding, temporal localization, segmentation, and highlight detection. From each source, representative subsets are annotated with capability tags using multimodal language models, following a predefined taxonomy encompassing visual perception, temporal and causal reasoning, and domain-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "sourced",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision-Speech QA Data</span> Fluent, speech-based question answering grounded in visual inputs represents a core capability of omni-modal models. We designed a new vision-speech QA dataset to enhance this ability. It pairs visual inputs (images or videos) with spoken prompts and requires the model to output its answers in speech. To create this data, we sourced text-based QA pairs from existing SFT datasets, selecting only those suitable for TTS synthesis. The retained samples are then rewritten using an LLM to enhance fluency, naturalness, and stylistic consistency in spoken form. Finally, all text responses are converted into high-fidelity speech using a TTS engine.</p>\n\n",
                "matched_terms": [
                    "from",
                    "sourced",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "sourced",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance long-term memory and contextual reasoning in extended dialogue settings, we construct a long-context multi-turn video interaction dataset. In this dataset, dialogue sequences are reordered so that certain queries appear at temporal positions distant from their corresponding visual segments, encouraging the model to retain and retrieve information over extended temporal spans. After reordering, all QA sequences are refined with a strong multimodal language model to resolve anaphoric references, correct temporal inconsistencies, and improve contextual coherence, resulting in natural and logically consistent multi-turn conversations.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During SFT, we freeze the audio encoder while updating all other modules. The frozen audio encoder retains robust acoustic representations learned from large-scale audio pre-training (i.e., pre-training stage-5 as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.SS2.SSS6\" title=\"3.2.6 Stage-5 Audio Encoder Alignment Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3.2.6</span></a>), whereas the unfrozen layers allow the model to learn fine-grained multimodal alignment and instruction-following behavior. Empirically, we find this selective fine-tuning strategy stabilizes convergence and avoids catastrophic forgetting of low-level auditory features. Apart from multi-modal SFT data, we also incorporate pure text SFT data used in developing LongCat-Flash, which covers domains ranging from reasoning, mathematics, coding, tool use, and general-purpose dialogue.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train for one epoch with a batch size of 256, sampling each batch to achieve a well-balanced mix of modalities. The learning rate follows a cosine decay schedule, with a warm-up fraction of 0.03, gradually decreasing from <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math> to 0. To balance preference learning between the text head and the speech head, we set their loss weight ratio by setting <math alttext=\"\\alpha:\\beta=1:1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#946;</mi><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha:\\beta=1:1</annotation></semantics></math>. To mitigate drift from the SFT model, we incorporate a Kullback-Leibler (KL) divergence regularizer with a weighting factor of 0.1.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core design principles are largely inspired by the training infrastructure used in developing LongCat-Flash, with a particular emphasis on maximizing training efficiency while strictly ensuring numerical consistency. To guarantee numerical consistency, we enforce determinism, minimize errors, and maintain error interpretability, ensuring that every training run is both deterministic and reproducible. For efficiency, we decouple the components of the LLM, vision encoder, and audio encoder, enabling independent optimization of their performance and memory usage. Experimental results demonstrate the effectiveness of our approach: in multimodal settings, our system maintains over 90% of the throughput of text-only training.</p>\n\n",
                "matched_terms": [
                    "results",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Forward</span>: The BalanceData module first distributes the modality data from the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks. Then, the modality encoders on each rank compute the corresponding vision and audio embeddings. Finally, the ModalityBridge module aggregates these embeddings at the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank, which are then passed as input to the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality Encoder Backward</span>: The ModalityBridge module redistributes the modality embedding gradients from the LLM decoder to the other <math alttext=\"inner\\_dp\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">inner\\_dp</annotation></semantics></math> ranks, and then the backward pass of the modality encoders is executed.</p>\n\n",
                "matched_terms": [
                    "from",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MDP, the ModalityBridge serves as the communication layer between the multimodal encoders and the LLM decoder.\nIt is responsible for transforming data organization formats to resolve discrepancies arising from the different parallelism strategies employed by the modality encoders and the LLM decoder. This process specifically involves handling modality embeddings during the forward phase and gradients during the backward phase.\nHowever, when processing longer context length, significant memory pressure emerges as the <math alttext=\"inner\\_dp=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">inner\\_dp=0</annotation></semantics></math> rank must read all micro-batches and perform gather and scatter operations on the modality embeddings and their gradients.\nTo address this challenge, we adopt chunk-based processing within ModalityBridge, which effectively alleviates the memory bottleneck while maintaining bitwise numerical consistency.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F9\" title=\"Figure 9 &#8227; 5.1.2 ModalityBridge &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this module comprises three core components:</p>\n\n",
                "matched_terms": [
                    "from",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimized Grouped GEMM</span>\nOur optimization strategies for Grouped GEMM achieve approximately 75% MFU in the evaluated scenarios.\n(1) <span class=\"ltx_text ltx_font_bold\">Dynamic SwapAB:</span> The WGMMA instruction is evaluated with the <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> dimension varying from 8 to 256, while the <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> dimension is fixed at 64.\nWe design an optimized kernel that automatically performs SwapAB operations to maximize WGMMA performance.\n(2) <span class=\"ltx_text ltx_font_bold\">Configurable SM Usage:</span> The Grouped GEMM kernel allows configurable SM counts to avoid resource contention when overlapping with dispatch or combined communication operations.\n(3) <span class=\"ltx_text ltx_font_bold\">Fine-Tuning:</span> <span class=\"ltx_text ltx_font_italic\">TileShape</span>, <span class=\"ltx_text ltx_font_italic\">PipelineStage</span>, and <span class=\"ltx_text ltx_font_italic\">ClusterShape</span> are carefully tuned for the target workload to maximize hardware utilization and prevent register spilling.\n(4) <span class=\"ltx_text ltx_font_bold\">Scheduling and Pipeline Strategy:</span> A swizzled block mapping schedule is adopted to significantly improve L2 cache hit rates.\nIn addition, a ping-pong mechanism is introduced during the backward pass to overlap WGMMA computation with the loading and storing of weight gradients.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For features where bit alignment is not feasible, we systematically analyze all sources of numerical deviation and mitigate their impact by benchmarking against the gold reference implementation during verification. For example, we identified and aligned the accumulation order in DeepEP and All2All EP strategies, thereby achieving bit-level consistency in loss values and verifying implementation correctness.</p>\n\n",
                "matched_terms": [
                    "their",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Delivery and Interruption</span> The audio will be delivered to the user once the VAD model explicitly detects the end of a turn, which is marked as <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math> in the figure, even if the first packet of audio response is generated earlier. However, should a user interrupt the audio generation at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the process is immediately terminated. Consequently, the token sequence from the LLM is truncated at the nearest natural breakpoint, such as the most recent punctuation mark.</p>\n\n",
                "matched_terms": [
                    "marked",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare LongCat-Flash-Omni with omni-models (Gemini-2.5-Pro<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/hello-gpt-4o\" title=\"\">https://openai.com/index/hello-gpt-4o</a></span></span></span>, Seed-1.6<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://seed.bytedance.com/en/seed1_6\" title=\"\">https://seed.bytedance.com/en/seed1_6</a></span></span></span>, and Qwen3-Omni<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>) and vision language models (Qwen3-VL<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite> and Qwen2.5-VL-72B<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib47\" title=\"\">2025</a>)</cite>) without thinking mode.</p>\n\n",
                "matched_terms": [
                    "mode",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "under",
                    "evaluated",
                    "their",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "results",
                    "from",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate audio-visual understanding on VideoMME under the no-subtitle protocol (w/o sub), and report results for two conditions: with audio (&#8220;VideoMME w/ audio&#8221;) and without audio (&#8220;VideoMME w/o audio&#8221;).\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Gemini models do not provide official evaluation entries for videos without audio, we report only the &#8220;VideoMME w/ audio&#8221; results.\nAll video benchmarks are evaluated using a maximum context length of 32K tokens to ensure consistent comparison.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "evaluated",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically assess the audio capability of the base models produced in the pre-training stages (i.e., stage-1, stage-2, stage-3, and stage-4), we conduct evaluations with respect to the following three aspects: automatic speech recognition (ASR), text-to-speech (TTS), and speech continuation.\nFor ASR evaluation, we employ the SPEECHIO_ASR_ZH00002 (speechio02)&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SpeechColab/Leaderboard\" title=\"\">https://github.com/SpeechColab/Leaderboard</a></span></span></span> to test Chinese speech recognition, while utilizing the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> for English speech recognition assessment.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Remarkably, our model demonstrates competitive performance across these benchmark datasets, despite relying on discretized audio features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "evaluated",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "mode",
                    "their",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from",
                    "evaluated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data construction process involves 10 trained professional conversationalists who conducted multi-turn dialogue sessions (approximately three minutes each) with each model under evaluation. These interactions are carried out via real-time audio-visual interfaces on the official app or web platforms provided by the respective model developers.\nA total of 200 dialogue sessions have been collected for each model, covering common real-world video call scenarios across four categories: problem solving, entertainment, self-improvement, and emotional support (50 samples per category).\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "from",
                    "models"
                ]
            }
        ]
    },
    "S7.T14": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 14: Evaluation of cross-modality understanding. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.",
        "body": "Qwen2.5-Omni\n\n\nInstruct",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen2.5-Omni</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Instruct</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "deficiencies",
            "scoring",
            "contains",
            "evaluation",
            "crossmodality",
            "version",
            "understanding",
            "omnibench",
            "released",
            "qwen25omni",
            "use",
            "internally",
            "instruct",
            "publicly",
            "corrected"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "omnibench",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human-in-the-loop</span> Through manual verification, we observe that the generated QA data frequently exhibits several types of errors: factual inconsistency, response insufficiency, referential ambiguity, linguistic infelicity, and semantic irrelevance.\nIt is difficult to reuse a multimodal LLM to automatically correct these errors, as it may introduce new errors. Therefore, we employ human annotation for quality refinement.\nFor factual inconsistency, annotators are required to edit or replace any statement that contradicts the video/audio content. For response insufficiency, we ask annotators to augment the original answer with the missing information, ensuring all sub-questions are answered. For referential ambiguity, we ask annotators to replace ambiguous or incorrect pronouns with specific nouns or corrected pronouns. As a result, we obtain a high-quality interaction dataset that contains meaningful and natural conversations that align with verifiable truth.\n</p>\n\n",
                "matched_terms": [
                    "corrected",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "scoring"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training infra contains multiple compute-communication overlaps, mainly including shortcut-based EP overlap and point-to-point (P2P) overlap. We tune the number of streaming multiprocessors (SMs) assigned to communication and compute kernels for each scenario to maximize hardware utilization, and we use distinct P2P groups/streams so that inter-stage pipeline-parallel (PP) communication does not interfere across stages.</p>\n\n",
                "matched_terms": [
                    "use",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "crossmodality",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation of image understanding across six key dimensions, leveraging a diverse suite of benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "version"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "crossmodality",
                    "understanding",
                    "qwen25omni",
                    "instruct",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate cross-modal understanding using publicly available benchmarks, including OmniBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib118\" title=\"\">2024c</a>)</cite>, WorldSense&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib119\" title=\"\">2025</a>)</cite>, and DailyOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib120\" title=\"\">2025</a>)</cite>. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.\nTo address the limited quality and coverage of existing benchmarks, we further introduce a new benchmark, UNO-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib121\" title=\"\">2025</a>)</cite>, comprising 1,880 human-crafted questions spanning 44 task types, with 98% of the questions requiring cross-modal reasoning.\nWe constructed this benchmark by manually annotating our in-house dataset. This approach prevents data contamination and ensures the benchmark is highly representative of real-world application scenarios.\nIn addition to conventional multiple-choice questions, the evaluation includes innovative multi-step open-ended questions, providing a more realistic and discriminative assessment of complex reasoning abilities.\n</p>\n\n",
                "matched_terms": [
                    "deficiencies",
                    "scoring",
                    "contains",
                    "evaluation",
                    "version",
                    "omnibench",
                    "understanding",
                    "released",
                    "use",
                    "internally",
                    "publicly",
                    "corrected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "omnibench",
                    "understanding"
                ]
            }
        ]
    },
    "S7.T15": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 15: Real-time audio-visual interaction evaluation.",
        "body": "Metrics\nDoubao\nGPT-4o\niFlytek Spark\nStepFun\nChatGLM\nQwen2.5-Omni\nQwen3-Omni\nLongCat-Flash-Omni\n\n\nScore\n1.92\n1.79\n1.25\n1.22\n0.99\n0.96\n0.81\n1.37\n\n\n95% CI\n[1.85, 1.98]\n[1.72, 1.85]\n[1.18, 1.32]\n[1.15, 1.28]\n[0.93, 1.05]\n[0.89, 1.02]\n[0.75, 0.87]\n[1.30, 1.44]",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Metrics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Doubao</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">GPT-4o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">iFlytek Spark</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">StepFun</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ChatGLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Qwen2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Qwen3-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LongCat-Flash-Omni</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Score</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">95% CI</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[1.85, 1.98]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[1.72, 1.85]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[1.18, 1.32]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[1.15, 1.28]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[0.93, 1.05]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[0.89, 1.02]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">[0.75, 0.87]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[1.30, 1.44]</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "spark",
            "doubao",
            "score",
            "evaluation",
            "longcatflashomni",
            "realtime",
            "qwen25omni",
            "interaction",
            "chatglm",
            "qwen3omni",
            "gpt4o",
            "iflytek",
            "metrics",
            "audiovisual",
            "stepfun"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The quantitative ratings are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T15\" title=\"Table 15 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>.\nLongCat-Flash-Omni achieves the third-highest score for naturalness and fluency in end-to-end interaction.\nComparing with audio-visual interaction products, LongCat-Flash-Omni ranks behind Doubao and GPT-4o but outperforms iFlytek Spark and StepFun.\nNotably, LongCat-Flash-Omni demonstrates a substantial advantage over open-source alternatives, scoring 0.56 points higher than the current SOTA open-source model, Qwen3-omni.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training an omni-modal model that possesses both strong offline multimodal understanding and real-time audio-visual interaction capabilities is highly challenging. The difficulties mainly arise in the following aspects: (1) <span class=\"ltx_text ltx_font_italic\">Cross-modal heterogeneity</span>: The substantial differences among modalities require exploring effective unified representations and fusion strategies to enable synergy across modalities, ensuring that the performance of any single modality does not degrade compared to their unimodality counterparts of similar scale. (2) <span class=\"ltx_text ltx_font_italic\">Unified offline and streaming capabilities</span>: Integrating offline multimodal understanding with streaming audio-visual interaction presents a significant challenge. Streaming interaction scenarios require distinct capabilities not typically found in offline processing, such as the perception of relative time, precise synchronization of audio-visual information, and efficient management of multi-turn interaction contexts. (3) <span class=\"ltx_text ltx_font_italic\">Real-time interaction</span>: Achieving real-time audio-visual interaction presents inherent difficulties, including the necessity of supporting both streaming audio and video input, as well as streaming speech output. The stringent low-latency requirement further imposes strict constraints on computational efficiency, thus placing high demands on both model architecture design and deployment infrastructure. (4) <span class=\"ltx_text ltx_font_italic\">Training Efficiency</span>: The heterogeneity within model and data poses great challenges for the design of distributed strategies.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the second challenge of balancing offline multimodal understanding with real-time audio-visual interaction, we introduce a human-in-the-loop strategy to construct high-quality interaction data, with careful consideration for long-term memory and multi-turn dialogue handling. In addition, we derive vision-speech question-answering data from existing vision-text corpora, enabling natural speech output and facilitating the transfer of strong offline multimodal understanding capabilities into interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the third challenge of achieving low-latency audio-visual interaction in a large-scale model, we dedicate substantial effort to the design of all modules in LongCat-Flash-Omni.\nWe adopt the ScMoE architecture with zero-computation experts from LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite> as the LLM backbone. To handle streaming input, we employ efficient audio and video encoders for feature extraction and introduce a synchronized chunk-wise interleaving strategy for real-time processing. For efficient audio reconstruction, we adopted a multi-codebook audio detokenization scheme with a coarser temporal resolution. This approach significantly improves decoding efficiency while preserving reconstruction quality.\nFurthermore, we designed an efficient streaming pipeline for model serving to minimize end-to-end server-side latency. As a result, despite having up to 560B parameters (with 27B activated), LongCat-Flash-Omni achieves millisecond-level response latency in real-time interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SOTA and Unified Omni-Modal Model:</span> LongCat-Flash-Omni achieves state-of-the-art cross-modal comprehension performance among open-source models. It seamlessly integrates powerful offline multimodal understanding with real-time audio-visual interaction within a single all-in-one framework.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large-Scale with Real-time Audio-Visual Interaction:</span> By leveraging an efficient LLM backbone, carefully designed lightweight modality encoders and decoder, and a chunk-wise audio-visual feature interleaving mechanism, LongCat-Flash-Omni achieves low-latency, high-quality audio-visual processing and streaming speech generation. It supports a context window of up to 128K tokens, enabling advanced capabilities in long-term memory, multi-turn dialogue, and temporal reasoning across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, LongCat-Flash-Omni is a fully end-to-end omni-modal model. It can receive various modalities as input, including text, audio, image, video, and their any arbitrary combination, and is able to directly generates speech tokens from the LLM backbone.\nLongCat-Flash-Omni employs a vision encoder and an audio encoder as multimodal perceivers. An LLM processes the multimodal inputs and generates text and audio tokens. An audio decoder reconstructs the waveform from the speech tokens generated by the LLM, enabling natural speech interaction.\nAll modules are carefully designed to support efficient streaming inference. The audio encoder, vision encoder, and audio decoder are lightweight components, each with approximately 600M parameters. The large-scale LLM backbone uses the novel and efficient architectural design proposed in the LongCat LLM family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib9\" title=\"\">b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the reminder of this section, we first introduce each structural component of LongCat-Flash-Omni, including the vision encoder that supports inputs with arbitrary aspect ratios and native-resolution feature encoding, the audio encoder, decoder, and tokenizer, and the LLM backbone. Then we elaborate on the video processing strategy and the architectural components that enable low-latency, real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Design</span>\nLongCat-ViT is a Transformer-based encoder that retains the core structure of the conventional Vision Transformer while integrating several key enhancements: a unified patchification module for native image and video inputs, 2D rotary position embeddings (2D-RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite>, SwiGLU activation function, RMSNorm layer, a LayerScale module, and Query-Key normalization. Together, these refinements yield a more robust and efficient architecture compared to conventional designs.\nTo improve computational efficiency in video frame encoding during real-time interaction while maintaining model performance, we adopt a relatively lightweight model configuration.\nThe detailed hyper-parameters of the model are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Vision Encoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn line with common practice, a two-layer multilayer perceptron (MLP) with pre-normalization is employed as the vision-language projector to align visual and textual representations.\nMoreover, a 2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> pixel-unshuffle operation is applied along the spatial dimension to mitigate the quadratic computational complexity associated with high-resolution inputs.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is built upon LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, a 560-billion-parameter Mixture-of-Experts (MoE) language model.\nLongCat-Flash adopts Multi-head Latent Attention (MLA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib17\" title=\"\">2024a</a>)</cite>, shortcut-connected MoE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib18\" title=\"\">2024</a>)</cite> and zero-computation experts, performing variable computation per token by activating 18.6B-31.3B parameters (27B on average), thereby unifying efficiency, performance and sparsity.\nThese characteristics are retained and extended to multimodal understanding and audio-visual interaction by LongCat-Flash-Omni.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The streaming audio-visual interaction mechanism is a core component of LongCat-Flash-Omni, enabling real-time integration of video and speech signals to support interactive communication. The proposed audio-visual interaction framework is characterized by two key aspects:</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Audio-Visual Feature Interleaving</span> Unlike offline audio-visual understanding tasks, where audio and visual features can be concatenated at the sequence level, real-time audio-visual interaction requires features from the audio and video streams to be prefilled into the LLM backbone as early as possible to minimize response latency once the user query is received.\nTo achieve this, we design a temporally-synchronized, chunk-wise audio-visual feature interleaving mechanism. Audio-visual feature chunks are structured as &#8220;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-start-token&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#161;&#8212;timestamp&#8212;&#191;:&#161;&#8212;video-tokens&#8212;&#191;&#161;&#8212;audio-tokens&#8212;&#191;&#8230;&#161;&#8212;audio-end-token&#8212;&#191;&#8221;, where the timestamp is represented in textual form, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.SS4.SSS1\" title=\"2.4.1 Video Strategy &#8227; 2.4 Video Strategy and Streaming Audio-Visual Interaction &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2.4.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sparse-Dense Sampling Strategy</span> We design a sparse-dense sampling strategy to optimally balance computational cost and information loss during turn-taking interactions between the user and the model.\nSpecifically, we employ a chunk size of 1 second during the information input period to preserve as much audio-visual information as possible, using a denser video sampling rate of 2 FPS. During the model response period, video frames are buffered with a sparser sampling rate (i.e., chunk size of 2 seconds, 0.5 FPS) and prepended to the next user turn. This design effectively balances visual information retention during the model response period and computational overhead, enabling high-quality audio-visual interaction, a core capability that distinguishes it from other omni-modal models in the community.</p>\n\n",
                "matched_terms": [
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To augment the model&#8217;s proficiency in capturing extended sequential relationships across various modalities, we gradually extend the context length to 32K and then 128K tokens, enabling the model to process and reason over more complex tasks such as long-term memory modeling and multi-turn real-time interaction.\nWe scale the context length from 8K to 32K tokens using 100B training tokens and adjust RoPE&#8217;s base frequency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib11\" title=\"\">2024</a>)</cite> from 1M to 5M to preserve positional encoding quality. The context length is then further expanded to 128K tokens with an additional 20B tokens of training, requiring a proportional increase of the RoPE&#8217;s base frequency to 10M to maintain stable attention across the extended sequence length.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage focuses on two complementary objectives. First, it enhances the model&#8217;s multimodal instruction-following and reasoning capabilities by leveraging large-scale, high-quality, and diverse instruction data. Second, it strengthens the model&#8217;s multimodal interaction abilities by utilizing curated interaction data, including speech-to-speech conversational data and audio-visual interaction data.\nIn the following parts, we describe how we collect high-quality and diverse instruction data, then elaborate on the construction of multimodal interaction datasets, and finally present the SFT training recipes that unify and optimize these datasets.</p>\n\n",
                "matched_terms": [
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To equip LongCat-Flash-Omni with real-time spoken and audio-visual interaction capabilities, we construct two types of specialized interaction datasets: (1) Speech-to-Speech Interaction Data, designed to enhance the naturalness and expressiveness of voice-based dialogues, and (2) Audio-Visual Interaction Data, aimed at improving the model&#8217;s ability to manage multi-turn, context-dependent audio-visual conversations.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Interaction Data</span>\nAudio-visual speech interaction data plays a crucial role in endowing the omni model with the ability to simultaneously process audio and vision information and perform real-time voice response. Unlike offline video understanding or general audio-visual understanding, online interaction is characterized by bi-directional and multi-turn dialogues, which involve immediate feedback, dynamic barge-in, contextual memorization, logical progression, and co-reference resolution. However, collecting large-scale real-world audio-visual interaction data is resource-intensive and impractical, and synthesizing complex and reasonable interaction data is difficult for existing models that tend to generate hallucination. To address these challenges, we develop a semi-automated data production pipeline that leverages model-driven automation for initial generation, followed by a human-in-the-loop stage for verification and refinement:\n</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-Driven Automation</span> To comprehensively cover diverse interaction scenarios, we first establish an ability taxonomy encompassing six major dimensions: memorization, understanding, analysis, creation, application, and entertainment. Guided by this taxonomy, we collect a mixture of public and in-house videos as the source data to ensure balanced coverage across abilities. For each video, we first perform scene segmentation using PySceneDetect <cite class=\"ltx_cite ltx_citemacro_citep\">(Breakthrough, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib35\" title=\"\">2025</a>)</cite> to obtain a sequence of clips. For every clip, a powerful multimodal language model generates multiple rounds of progressively deep and context-aware QA pairs. In each round, subsequent queries build upon preceding conversations with logical progression, referential dependency, or anaphoric linkage to earlier turns. Such conversations encourage the model to reason over extended dialogue contexts, maintain entity and reference consistency in a natural, human-like audio-visual interaction setting. We then apply an automatic verification pipeline, employing an LLM-as-a-judge framework to evaluate and discard low-quality or inconsistent QA pairs. The remaining qualified samples are then composed into multi-turn dialogue, enabling the model to learn both intra-scene conversational continuity and inter-scene contextual transitions, thereby better reflecting real-world audio-visual interactions.</p>\n\n",
                "matched_terms": [
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we apply a TTS engine to convert the textual QA pairs into speech and integrate them into the videos to obtain the final audio-visual speech interaction dataset. The resulting data aligns the model&#8217;s response style with that of a helpful, interactive assistant and enhances the naturalness of its conversational behavior.</p>\n\n",
                "matched_terms": [
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, each request data packet from the user side consists of 1 second of audio and 2 corresponding video frames. Using the streaming prefill strategy, each request packet can be immediately prefilled, eliminating the need to wait until the user&#8217;s turn to finish before initiating computation. By overlapping the VAD endpoint detection (<math alttext=\"600\\sim 700ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>600</mn><mo>&#8764;</mo><mrow><mn>700</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">600\\sim 700ms</annotation></semantics></math>) and the prefill process, users can receive the model response within <math alttext=\"100ms\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">100ms</annotation></semantics></math> after the endpoint detection. This asynchronous pipeline enables real-time omni-modal interaction.</p>\n\n",
                "matched_terms": [
                    "realtime",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TTS evaluation framework uses text samples from the speechio02 and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite> test-clean/other datasets as input sources. The model uses discrete semantic-acoustic tokens as speech prompts, conducting speech token generation with text input as teacher-forcing guidance. The output speech tokens are reconstructed into waveform by the audio decoder. We compute the word error rate (WER) (for English) or character error rate (CER) (for Chinese) between transcription of the generated speech produced by a robust ASR system and the original input text as the measure for the evaluation. The results are shown in the right part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T7\" title=\"Table 7 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that the base models are capable of generating robust speech from textual input, providing a solid foundation for speech output in spoken and audio-visual interaction modes.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T12\" title=\"Table 12 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents the evaluation results across diverse benchmarks.\nLongCat-Flash-Omni Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Furthermore, our model demonstrates no degradation in text capabilities after extensive training with multimodal data, indicating the effectiveness of our training strategy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "qwen25omni",
                    "qwen3omni",
                    "interaction",
                    "gpt4o",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing cross-modal evaluation benchmarks still exhibit a noticeable gap from real-world user experiences, where real-time audio-visual interaction (audio-visual input with audio output) is essential. To the best of our knowledge, no prior work has systematically evaluated this form of real-time multimodal interaction.\nTo that end, we built a proprietary, end-to-end framework to measure the quality of a model&#8217;s audio-visual interaction, specifically its ability to engage users naturally and fluently in real-world scenarios.\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data construction process involves 10 trained professional conversationalists who conducted multi-turn dialogue sessions (approximately three minutes each) with each model under evaluation. These interactions are carried out via real-time audio-visual interfaces on the official app or web platforms provided by the respective model developers.\nA total of 200 dialogue sessions have been collected for each model, covering common real-world video call scenarios across four categories: problem solving, entertainment, self-improvement, and emotional support (50 samples per category).\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realtime",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "longcatflashomni",
                    "qwen25omni",
                    "qwen3omni",
                    "interaction",
                    "iflytek",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We address major challenges in building such a system: cross-modal heterogeneity, unified offline and streaming capabilities, and low-latency interaction. Through a carefully designed multi-stage early-fusion pretraining pipeline, LongCat-Flash-Omni achieves deeply integrated representations that enable synergistic multimodal reasoning while preserving unimodality strength. Our introduction of human-in-the-loop data construction and a 128K-token context window further enhances multi-turn dialogue, temporal reasoning, and memory capabilities in dynamic interactive scenarios.\nOn the architectural side, the adoption of the ScMoE backbone with zero-computation experts, together with lightweight modality encoders and decoder, allows the model to support real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "realtime",
                    "audiovisual",
                    "interaction"
                ]
            }
        ]
    },
    "S7.T16": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "Table 16: Percentage of good cases in qualitative analysis.",
        "body": "Metrics\nDoubao\nGPT-4o\niFlytek Spark\nStepFun\nChatGLM\nQwen2.5-Omni\nQwen3-Omni\nLongCat-Flash-Omni\n\n\nReal-timeness\n65.5\n71.5\n67.0\n18.0\n3.0\n4.0\n1.5\n49.5\n\n\nHuman-likeness\n93.5\n69.0\n92.5\n70.5\n76.5\n59.0\n13.5\n62.5\n\n\nParalinguistic Understanding\n88.0\n87.5\n80.0\n87.0\n84.5\n89.0\n89.0\n91.5\n\n\nRelevance\n66.5\n60.0\n25.5\n31.5\n25.0\n16.0\n8.0\n54.5\n\n\nAccuracy\n65.0\n55.0\n38.5\n42.5\n33.5\n35.5\n40.0\n36.0\n\n\nMemory Capability\n98.0\n98.0\n81.5\n92.5\n83.0\n64.0\n36.0\n94.5",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Metrics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Doubao</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">GPT-4o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">iFlytek Spark</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">StepFun</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ChatGLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Qwen2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Qwen3-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LongCat-Flash-Omni</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Real-timeness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Human-likeness</td>\n<td class=\"ltx_td ltx_align_center\">93.5</td>\n<td class=\"ltx_td ltx_align_center\">69.0</td>\n<td class=\"ltx_td ltx_align_center\">92.5</td>\n<td class=\"ltx_td ltx_align_center\">70.5</td>\n<td class=\"ltx_td ltx_align_center\">76.5</td>\n<td class=\"ltx_td ltx_align_center\">59.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">13.5</td>\n<td class=\"ltx_td ltx_align_center\">62.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Paralinguistic Understanding</td>\n<td class=\"ltx_td ltx_align_center\">88.0</td>\n<td class=\"ltx_td ltx_align_center\">87.5</td>\n<td class=\"ltx_td ltx_align_center\">80.0</td>\n<td class=\"ltx_td ltx_align_center\">87.0</td>\n<td class=\"ltx_td ltx_align_center\">84.5</td>\n<td class=\"ltx_td ltx_align_center\">89.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">89.0</td>\n<td class=\"ltx_td ltx_align_center\">91.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Relevance</td>\n<td class=\"ltx_td ltx_align_center\">66.5</td>\n<td class=\"ltx_td ltx_align_center\">60.0</td>\n<td class=\"ltx_td ltx_align_center\">25.5</td>\n<td class=\"ltx_td ltx_align_center\">31.5</td>\n<td class=\"ltx_td ltx_align_center\">25.0</td>\n<td class=\"ltx_td ltx_align_center\">16.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.0</td>\n<td class=\"ltx_td ltx_align_center\">54.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n<td class=\"ltx_td ltx_align_center\">55.0</td>\n<td class=\"ltx_td ltx_align_center\">38.5</td>\n<td class=\"ltx_td ltx_align_center\">42.5</td>\n<td class=\"ltx_td ltx_align_center\">33.5</td>\n<td class=\"ltx_td ltx_align_center\">35.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">40.0</td>\n<td class=\"ltx_td ltx_align_center\">36.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Memory Capability</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">98.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">98.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">81.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">92.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">83.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">94.5</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "relevance",
            "qwen25omni",
            "good",
            "spark",
            "doubao",
            "capability",
            "accuracy",
            "qwen3omni",
            "gpt4o",
            "cases",
            "humanlikeness",
            "metrics",
            "paralinguistic",
            "chatglm",
            "analysis",
            "memory",
            "qualitative",
            "longcatflashomni",
            "understanding",
            "percentage",
            "iflytek",
            "realtimeness",
            "stepfun"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present the qualitative analysis results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T16\" title=\"Table 16 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, together with some case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.F12\" title=\"Figure 12 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.\nLongCat-Flash-Omni excels in paralinguistic understanding, relevance, and memory capability, performing on par with top-tier models.\nNotably, LongCat-Flash-Omni actively interprets user emotions from both facial expressions and vocal cues, demonstrating its superior paralinguistic understanding.\nRegarding relevance, LongCat-Flash-Omni exhibits strong comprehension capabilities by closely tracking dialogue topics and generating highly correlated responses.\nThis comprehension capability, combined with robust memory retention, enables LongCat-Flash-Omni to recall information from many turns earlier, resulting in more fluent and natural interactions.\nHowever, certain gaps remain compared with leading models, particularly in real-timeness, human-likeness, and accuracy. Specifically, in terms of real-timeness, our model tends to be overly sensitive to user pauses, often initiating responses prematurely and interrupting users mid-conversation. Regarding human-likeness, it occasionally exhibits pronunciation errors, stuttering, and robotic or electronic audio artifacts. In terms of accuracy, while our model shows strong capability in recognizing dynamic objects, its recognition performance declines when processing text and numerical information. Additionally, we observe a tendency for our model to over-agree with users&#8217; statements while overlooking relevant visual content.\nThese aspects will be further optimized in the future work.\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction.\nBy adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability.\nBuilding upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction.\nFor training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation.\nWe provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>\n\n",
                "matched_terms": [
                    "capability",
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the second challenge of balancing offline multimodal understanding with real-time audio-visual interaction, we introduce a human-in-the-loop strategy to construct high-quality interaction data, with careful consideration for long-term memory and multi-turn dialogue handling. In addition, we derive vision-speech question-answering data from existing vision-text corpora, enabling natural speech output and facilitating the transfer of strong offline multimodal understanding capabilities into interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive evaluations demonstrate that our model delivers strong and consistent performance across both omni-modal benchmarks and real-time interactive tasks. LongCat-Flash-Omni achieves state-of-the-art results on omni-modal benchmarks such as Omni-Bench and WorldSense, while also exhibiting highly competitive performance on a wide range of unimodal tasks, including text, image, and video understanding, as well as speech comprehension and generation, establishing itself as the most powerful omni-modal model in the open-source community.\nFurthermore, extensive subjective evaluations confirm that LongCat-Flash-Omni supports high-quality audio-visual interaction with low latency.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SOTA and Unified Omni-Modal Model:</span> LongCat-Flash-Omni achieves state-of-the-art cross-modal comprehension performance among open-source models. It seamlessly integrates powerful offline multimodal understanding with real-time audio-visual interaction within a single all-in-one framework.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large-Scale with Real-time Audio-Visual Interaction:</span> By leveraging an efficient LLM backbone, carefully designed lightweight modality encoders and decoder, and a chunk-wise audio-visual feature interleaving mechanism, LongCat-Flash-Omni achieves low-latency, high-quality audio-visual processing and streaming speech generation. It supports a context window of up to 128K tokens, enabling advanced capabilities in long-term memory, multi-turn dialogue, and temporal reasoning across multiple modalities.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is built upon LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, a 560-billion-parameter Mixture-of-Experts (MoE) language model.\nLongCat-Flash adopts Multi-head Latent Attention (MLA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib17\" title=\"\">2024a</a>)</cite>, shortcut-connected MoE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib18\" title=\"\">2024</a>)</cite> and zero-computation experts, performing variable computation per token by activating 18.6B-31.3B parameters (27B on average), thereby unifying efficiency, performance and sparsity.\nThese characteristics are retained and extended to multimodal understanding and audio-visual interaction by LongCat-Flash-Omni.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is designed to seamlessly integrate robust offline multimodal understanding with low-latency audio-visual interaction.\nThe audio and visual streams are independently processed by an audio encoder and a vision encoder, respectively. Their extracted features are subsequently time-aligned and chunked into synchronized segments, which are interleaved and fed into the LLM decoder for multimodal understanding.\nHere we elaborate the video strategy adopted by LongCat-Flash-Omni and how audio-visual input is processed to support streaming interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span> We curate an audio understanding dataset that encompasses a wide range of tasks, including audio captioning, semantic audio understanding, paralinguistic analysis, acoustic scene and event detection, audio question answering, and music understanding.\nThe dataset comprises a combination of open-source datasets and in-house proprietary datasets.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "analysis",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply text translation as data augmentation for speech recognition datasets, various speech models for pseudo-label generation and label quality filtering, particularly for paralinguistic understanding and audio captioning datasets, and a diverse set of prompts for each task to enhance instruction variability.\n</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance domain knowledge and reasoning capabilities, we further curate an in-house, high-quality image-text interleaved dataset derived from video content containing educational materials.\nWe use an automated pipeline to select only instructional video segments and remove other parts.\nTextual information is extracted using ASR and OCR, then refined by an LLM to improve accuracy and consistency. Videos are segmented into meaningful scenes, and key frames are extracted and aligned with the refined text based on temporal and semantic correspondence. The resulting dataset comprises structured, context-rich sequences of synchronized visual and textual information, significantly strengthening the model&#8217;s capacity for academic reasoning and multimodal understanding.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GUI Data</span>\nGraphical User Interface (GUI) data contains rich information on visual understanding and task planning, enabling automated interactions on both mobile and desktop platforms. To this end, we utilize diverse types of GUI-related data to enhance the model&#8217;s perception, grounding, and planning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib28\" title=\"\">2025</a>)</cite>.\nFor GUI perception, we utilize a large number of image-text pairs from screenshots of various PC and mobile applications, covering tasks such as OCR, VQA, and captioning with GUI images. For GUI grounding, we construct instruction-answer pairs based on visual screenshots from diverse platforms and devices. For each screenshot we create multiple pairs, accounting for different UI elements and interaction possibilities.\nFor GUI planning, we collect a diverse set of navigational paths with rich context from both mobile and desktop. The planning data comprises key components such as screenshot observations, summarizations, reasoning traces, and corresponding actions. To enhance the model&#8217;s reasoning capability, we integrate multiple inference settings that involve different combinations of action, summarization, and reasoning outputs.\nFurthermore, to capture the underlying logic of GUI dynamics, we incorporate contextual scenarios that enable the model to predict changes across different screenshots.</p>\n\n",
                "matched_terms": [
                    "capability",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To strengthen the model&#8217;s foundational capabilities in scientific reasoning and problem-solving, we construct a large-scale, multimodal STEM (Science, Technology, Engineering, and Mathematics) dataset.\nThe collected data are meticulously processed and structured into both multiple-choice and open-ended generative question-answer formats.\nWe implement a rigorous multi-stage filtering pipeline to ensure factual accuracy, eliminate ambiguity, and standardize formatting.\nThis effort culminates in a high-fidelity pre-training dataset comprising 15 million image-text pairs with substantial diversity in both subject matter and academic level, and covering a wide range of disciplines from K12 education to advanced university studies.\nThis foundational dataset is essential for enabling the model to achieve deep conceptual understanding and robust reasoning capabilities across a broad spectrum of scientific and technical domains.\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the text foundation model obtained from stage-0, we continue pre-training using a mixture of text data, speech-text interleaved data, and ASR data. All speech samples are discretized into four-codebook token sequences. During training, we jointly optimize multiple objectives: (1) pure text next-token prediction (NTP) to preserve strong text understanding and generation capabilities; (2) text-speech interleaved NTP to align speech and text representations within a unified sequence modeling framework; and (3) ASR-style tasks to build basic speech perception capability.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S3.F6\" title=\"Figure 6 &#8227; 3.2.2 Stage-1 Text-Speech Continued Pre-Training &#8227; 3.2 Training Strategy &#8227; 3 Pre-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, text and audio embeddings are fused before being fed into the LLM decoder. We introduce four audio prediction heads, enabling LongCat-Flash-Omni to directly generate audio tokens. The model simultaneously predicts text tokens, semantic tokens, and acoustic tokens, with a one-step temporal offset between semantic and acoustic token predictions. Empirical observations indicate that ASR tasks contribute minimally to modality alignment, prompting us to include only a small proportion of ASR data in the training process.\nThe overall training objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "capability",
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video-Text SFT Data</span>\nWe construct a large-scale video data pool of approximately 3 million videos sourced from diverse proprietary and open-source datasets, covering tasks such as general video understanding, classification, reasoning, grounding, temporal localization, segmentation, and highlight detection. From each source, representative subsets are annotated with capability tags using multimodal language models, following a predefined taxonomy encompassing visual perception, temporal and causal reasoning, and domain-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "capability",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding Data</span>\nWe re-utilize a subset of the comprehensive audio understanding dataset from the previous stage (Audio Encoder Alignment Training) for the SFT phase. This sampled data specifically targets tasks such as ASR, Audio-to-Speech Translation (AST), paralinguistic understanding, and audio-conditioned captioning and question answering. The purpose of integrating this kind of data is to improve the semantic alignment between the continuous audio representations generated by the audio encoder and the language model&#8217;s semantic space.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Visual Understanding Data</span> The joint audio-visual understanding capability fundamentally distinguishes an omni model from conventional vision-only or speech-only multimodal language models. To develop this capability, we curate an in-house time-synchronized audio-visual dataset by prompting a strong multimodal language model to generate high-quality text QA pairs that are involved with both audio and visual content in the video.\nSpecifically, the videos are sourced from multiple domains, including multimodal data containing rich music and audio event contents.\nAll samples are organized in an interleaved chunk format, which enables the model to better perceive and reason over temporally aligned audio-visual content, thereby strengthening its capacity for multimodal understanding.\n</p>\n\n",
                "matched_terms": [
                    "capability",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-Driven Automation</span> To comprehensively cover diverse interaction scenarios, we first establish an ability taxonomy encompassing six major dimensions: memorization, understanding, analysis, creation, application, and entertainment. Guided by this taxonomy, we collect a mixture of public and in-house videos as the source data to ensure balanced coverage across abilities. For each video, we first perform scene segmentation using PySceneDetect <cite class=\"ltx_cite ltx_citemacro_citep\">(Breakthrough, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib35\" title=\"\">2025</a>)</cite> to obtain a sequence of clips. For every clip, a powerful multimodal language model generates multiple rounds of progressively deep and context-aware QA pairs. In each round, subsequent queries build upon preceding conversations with logical progression, referential dependency, or anaphoric linkage to earlier turns. Such conversations encourage the model to reason over extended dialogue contexts, maintain entity and reference consistency in a natural, human-like audio-visual interaction setting. We then apply an automatic verification pipeline, employing an LLM-as-a-judge framework to evaluate and discard low-quality or inconsistent QA pairs. The remaining qualified samples are then composed into multi-turn dialogue, enabling the model to learn both intra-scene conversational continuity and inter-scene contextual transitions, thereby better reflecting real-world audio-visual interactions.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our DPO training data consists of two components: general DPO data and model-generated DPO data. The general data covers samples focusing on safety, helpfulness, and response style. The model-generated data is sampled from an SFT checkpoint and used to refine broader multimodal capabilities through preference comparisons among model-produced responses.\nTo comprehensively enhance the capability, alignment, and safety of LongCat-Flash-Omni across all modalities, we utilize all prompt types from the SFT datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S4.SS1.SSS1\" title=\"4.1.1 High-quality and Diverse Instruction Data Curation &#8227; 4.1 Supervised Fine-Tuning &#8227; 4 Post-Training &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a> during DPO training.\nFor each prompt, the model generates 6 rollouts to ensure diverse candidate responses for preference pair construction.\nTo ensure data quality, we employ a hybrid evaluation strategy that combines manual annotation with automatic scoring from a robust multimodal language model, serving as a preference evaluator to assign quality scores and identify distinct and informative preference pairs. This process provides reliable supervisory signals for DPO training, enabling effective refinement of LongCat-Flash-Omni &#8217;s multimodal alignment, behavioral stability, and overall coherence.</p>\n\n",
                "matched_terms": [
                    "capability",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In multimodal scenarios, optimizing training performance is challenging due to the heterogeneity of both data and models. The challenge of data heterogeneity arises because the training data for LongCat-Flash-Omni includes speech, vision, and text, which show significant and dynamic differences in token distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nModel heterogeneity is evident in the substantially different computational workloads across the three core components of LongCat-Flash-Omni: the vision encoder, the audio encoder, and the LLM decoder (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S5.F7\" title=\"Figure 7 &#8227; 5.1 Multimodal Decoupling Framework &#8227; 5 Training Infrastructures &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nTo address the aforementioned challenges, there are two primary optimization approaches. The first relies on Fully Sharded Data Parallelism (FSDP) (e.g., OrchMLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib36\" title=\"\">2025</a>)</cite>, veOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib37\" title=\"\">2025</a>)</cite>), which reduces static memory consumption through parameter sharding, avoids pipeline parallelism (PP) bubbles caused by model heterogeneity, and mitigates data heterogeneity via data-parallel (DP) group balancing. However, for LongCat-Flash-Omni, the total number of model parameters is too large for this approach to be practical.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct comprehensive evaluations of LongCat-Flash-Omni, compare it with various proprietary and open-source multimodal models, including vision understanding, audio comprehension, text understanding and generation, cross-modality understanding, and audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that GPT-4o and Seed-1.6 only provide a vision understanding API. Therefore, we evaluate them solely on visual capabilities. To ensure a fair comparison with instruct model, we follow the evaluation setting used in Qwen3-VL benchmark, limiting Gemini-2.5-Pro&#8217;s thinking budget to 128 tokens. All models are evaluated under their official configurations.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on image understanding benchmarks are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T5\" title=\"Table 5 &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Overall, LongCat-Flash-Omni achieves performance comparable to Gemini-2.5-Flash, while outperforming the open-sourced Qwen3-Omni. Its advantage is particularly pronounced on multi-image tasks, which benefit from the model&#8217;s training on high-quality interleaved image-text, multi-image data and video datasets.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capability of video understanding, we consider three dimensions as below:</p>\n\n",
                "matched_terms": [
                    "capability",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T6\" title=\"Table 6 &#8227; 7.1.2 Video-to-Text Evaluation &#8227; 7.1 Vision Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, LongCat-Flash-Omni achieves state-of-the-art performance on video-to-text tasks. Specifically, it surpasses all compared models by a significant margin on short video understanding, demonstrating superior video comprehension capabilities. On long video tasks, LongCat-Flash-Omni demonstrates performance on par with leading models such as Gemini-2.5-Pro and Qwen3-VL. Notably, in the VideoMME benchmark, it achieves the best performance among omni-modal models. This can be attributed to a combination of an advanced video processing strategy&#8212;using dynamic frame sampling and hierarchical token aggregation&#8212;and the strong long-context modeling capacity afforded by its efficient backbone.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech continuation capabilities, we develope a specialized test set comprising <math alttext=\"1,200\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">1,200</annotation></semantics></math> synthesized speech samples derived from text data from the CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite> benchmark.\nThe assessment requires the model to generate appropriate responses to multiple-choice questions based on provided speech input. For text outputs, we directly measure response accuracy, while speech outputs undergo ASR transcription before accuracy evaluation. The evaluation protocol employs a 1-shot learning approach, with detailed results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T8\" title=\"Table 8 &#8227; 7.2.1 Base Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nOur analysis reveals that the base models in different stage perform well in in-context speech continuation evaluation and there is only a negligible performance disparity between text and speech output.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluation of the audio capabilities of LongCat-Flash-Omni, covering speech recognition and translation, audio understanding and audio-to-text chat.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Understanding</span>\nAs an omni-modal model, LongCat-Flash-Omni can effectively function as a native audio understanding model when vision input is not provided. We conduct a comprehensive evaluation of LongCat-Flash-Omni across a diverse set of audio comprehension tasks, including music, sound events, and speech understanding. Specifically, we use MMAU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib83\" title=\"\">2024</a>)</cite>, VocalSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib84\" title=\"\">2022</a>)</cite>, TUT2017&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib85\" title=\"\">2016</a>)</cite>, ClothoAQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib86\" title=\"\">2022</a>)</cite>, Nonspeech7k&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rashid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib87\" title=\"\">2023</a>)</cite>, CochlScene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong and Park, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib88\" title=\"\">2022</a>)</cite>, and MELD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib89\" title=\"\">2018</a>)</cite> as benchmarks.\nThe results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T10\" title=\"Table 10 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, show that LongCat-Flash-Omni consistently outperforms most competing models across all evaluated dimensions, and achieves state-of-the-art performance in several benchmarks. These findings highlight that LongCat-Flash-Omni possesses advanced capabilities in understanding a wide spectrum of general and complex acoustic information, going well beyond conventional speech recognition.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Text Chat</span> We evaluate the ability of LongCat-Flash-Omni to engage in text-based conversations driven by audio instructions using the OpenAudioBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>)</cite> and VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib90\" title=\"\">2024c</a>)</cite> benchmarks. These benchmarks assess a wide range of capabilities, including world knowledge, domain-specific understanding, instruction following, and reasoning.\nTo ensure standardized and reproducible results, we employ the official evaluation code provided by the benchmark authors. The results, presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T11\" title=\"Table 11 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, show that LongCat-Flash-Omni achieves strong performance across all benchmark subsets, reaching state-of-the-art results in several cases. Overall, these results demonstrate LongCat-Flash-Omni &#8217;s superior ability to conduct audio-driven conversations and perform complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding",
                    "cases"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive and rigorous text capability evaluation of the LongCat-Flash-Omni Instruct model is performed, covering diverse capability dimensions, including general domains, instruction following, mathematical reasoning,\ngeneral reasoning, and coding, using the following benchmarks:</p>\n\n",
                "matched_terms": [
                    "capability",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with some representative text chat models including DeepSeek-V3.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib43\" title=\"\">2025</a>)</cite>, Qwen3-235B-A22B (2507 version)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib46\" title=\"\">2025</a>)</cite>, Kimi-K2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MoonshotAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib115\" title=\"\">2025</a>)</cite>, GPT-4.1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib116\" title=\"\">2025b</a>)</cite>, Claude Sonnet-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib117\" title=\"\">2025</a>)</cite>, Gemini2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and LongCat-Flash<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>. For closed-source models, we conduct evaluations through their official APIs. For models supporting both thinking and non-thinking modes (Qwen3-235B-A22B, Gemini-2.5-Flash, and Claude Sonnet-4), we explicitly configure these models to operate in non-thinking mode for a fair comparison.\nThe evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T13\" title=\"Table 13 &#8227; 7.3.1 Base Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. the comprehensive evaluation demonstrates that LongCat-Flash-Omni maintains superior text capability, with consistently leading performance in different domains. Compared with LongCat-Flash, whose early base model serves as the foundation for LongCat-Flash-Omni, the latter not only exhibits no degradation in text capabilities but even achieves superior performance in certain domains. This highlights the effectiveness of our training strategy and underscores the potential synergy among different modalities in omni-modal model training.</p>\n\n",
                "matched_terms": [
                    "capability",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the aforementioned capabilities, LongCat-Flash-Omni introduces advanced cross-modal understanding and human-like speech interaction with cross-modality inputs. To evaluate these abilities, we compare our model against several strong baselines, including Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.0-Flash, Qwen3-Omni, and Qwen2.5-Omni.\nFor a fair comparison between the LongCat-Flash-Omni instruct model and Gemini-2.5-Pro with thinking mode enabled, we follow the approach of Qwen3-VL by constraining Gemini&#8217;s thinking budget to 128 tokens, denoted as Gemini-2.5-Pro-ThinkingBudget128.\nIt&#8217;s important to mention that we couldn&#8217;t use APIs to test GPT-4o and Seed-1.6 because they don&#8217;t have open ones for audio. Instead, we tested their performance on corresponding evaluations by interacting with their live applications in a real-time audio-visual scenario.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding",
                    "qwen25omni",
                    "qwen3omni",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T14\" title=\"Table 14 &#8227; 7.3.2 Instruct Model Evaluation &#8227; 7.3 Text Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, LongCat-Flash-Omni outperforms Gemini-2.5-Flash-non-thinking and achieves performance comparable to Gemini-2.5-Pro-ThinkingBudget128. In particular, on WorldSense and DailyOmni, which emphasize real-world audio-video understanding, LongCat-Flash-Omni demonstrates superior performance, significantly surpassing other open-source omni-modal models.\nOn UNO-Bench, which evaluates cross-modal perception and reasoning, LongCat-Flash-Omni also performs exceptionally well among open-source omni-modal models.\nThese results indicate that LongCat-Flash-Omni achieves highly effective multimodal integration, establishing it as the leading open-source omni-modal model.\n</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assessment process combines quantitative user ratings with qualitative expert analysis.\nFor quantitative evaluation, 250 real users independently triple-annotated the complete interaction videos, rating naturalness and fluency on a four-point scale:\n0 for <span class=\"ltx_text ltx_font_italic\">completely unnatural</span>,\n1 for <span class=\"ltx_text ltx_font_italic\">partially unnatural and affecting interaction</span>,\n2 for <span class=\"ltx_text ltx_font_italic\">partially unnatural but not affecting interaction</span>,\nand 3 for <span class=\"ltx_text ltx_font_italic\">completely natural and fluent</span>.\nFor qualitative analysis, expert annotators conduct a dimensional breakdown to identify specific factors affecting naturalness and fluency across six key factors, including <span class=\"ltx_text ltx_font_italic\">real-timeness</span>, <span class=\"ltx_text ltx_font_italic\">human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">paralinguistic understanding</span>, <span class=\"ltx_text ltx_font_italic\">relevance</span>, <span class=\"ltx_text ltx_font_italic\">accuracy</span> and <span class=\"ltx_text ltx_font_italic\">memory capability</span>.\nThis approach provides comprehensive qualitative insights into each model&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "relevance",
                    "capability",
                    "qualitative",
                    "accuracy",
                    "understanding",
                    "analysis",
                    "memory",
                    "realtimeness",
                    "humanlikeness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare LongCat-Flash-Omni with five audio-visual interaction products including Doubao<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.doubao.com/chat/\" title=\"\">https://www.doubao.com/chat/</a>, the evaluation period was August 11-15, 2025.</span></span></span>, GPT-4o<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.chatbot.app/?model=gpt-4o\" title=\"\">https://chat.chatbot.app/?model=gpt-4o</a>, the evaluation period was August 18-22, 2025.</span></span></span>, iFlytek Spark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinghuo.xfyun.cn/desk\" title=\"\">https://xinghuo.xfyun.cn/desk</a>, the evaluation period was August 25-29, 2025.</span></span></span>, StepFun<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.stepfun.com/chats/new\" title=\"\">https://www.stepfun.com/chats/new</a>, the evaluation period was September 8-12, 2025.</span></span></span> and ChatGLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://chat.z.ai/\" title=\"\">https://chat.z.ai/</a>, the evaluation period was September 1-5, 2025.</span></span></span>, as well as two open-source models Qwen2.5-Omni and Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "iflytek",
                    "longcatflashomni",
                    "qwen25omni",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quantitative ratings are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T15\" title=\"Table 15 &#8227; 7.4.2 Real-time Audio-Visual Interaction Evaluation &#8227; 7.4 Cross-modality Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>.\nLongCat-Flash-Omni achieves the third-highest score for naturalness and fluency in end-to-end interaction.\nComparing with audio-visual interaction products, LongCat-Flash-Omni ranks behind Doubao and GPT-4o but outperforms iFlytek Spark and StepFun.\nNotably, LongCat-Flash-Omni demonstrates a substantial advantage over open-source alternatives, scoring 0.56 points higher than the current SOTA open-source model, Qwen3-omni.</p>\n\n",
                "matched_terms": [
                    "spark",
                    "doubao",
                    "longcatflashomni",
                    "qwen3omni",
                    "gpt4o",
                    "iflytek",
                    "stepfun"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we present LongCat-Flash-Omni, a next-generation open-source omni-modal model that unifies robust offline multimodal understanding with real-time audio-visual interaction in a single, cohesive framework. LongCat-Flash-Omni demonstrates that large-scale models can effectively perceive, integrate, and generate across diverse modalities, including text, audio, image, and video, without sacrificing performance in any individual domain.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We address major challenges in building such a system: cross-modal heterogeneity, unified offline and streaming capabilities, and low-latency interaction. Through a carefully designed multi-stage early-fusion pretraining pipeline, LongCat-Flash-Omni achieves deeply integrated representations that enable synergistic multimodal reasoning while preserving unimodality strength. Our introduction of human-in-the-loop data construction and a 128K-token context window further enhances multi-turn dialogue, temporal reasoning, and memory capabilities in dynamic interactive scenarios.\nOn the architectural side, the adoption of the ScMoE backbone with zero-computation experts, together with lightweight modality encoders and decoder, allows the model to support real-time audio-visual interaction.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "longcatflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive evaluations show that LongCat-Flash-Omni not only achieves state-of-the-art performance on omni-modal benchmarks such as Omni-Bench and WorldSense, but also matches or exceeds closed-source systems in key unimodal tasks, including image and video understanding as well as audio comprehension. Moreover, subjective assessments confirm the model&#8217;s ability to deliver natural, low-latency, high-quality interactive experiences, highlighting its potential as a foundation for next-generation human-AI interfaces.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, LongCat-Flash-Omni lays a strong foundation for the continued evolution of omni-modal intelligence. Future work will focus on expanding the diversity and scale of training data, integrating an adaptive thinking mode, refining streaming and generation capabilities, and exploring richer forms of embodied and interactive intelligence. We believe that the release of LongCat-Flash-Omni will not only accelerate research on multimodal understanding and generation but also inspire new applications and paradigms for building human-centered, AGI-oriented systems.</p>\n\n",
                "matched_terms": [
                    "longcatflashomni",
                    "understanding"
                ]
            }
        ]
    },
    "S9.p2": {
        "source_file": "LongCat-Flash-Omni Technical Report",
        "caption": "",
        "body": "Bairui Wang\n\n\n\n\nJian Yang\n\n\n\n\nSenbin Yang\n\n\n\n\nXuezhi Cao\n\n\n\n\n\n\nBayan\n\n\n\n\nJiaxing Liu\n\n\n\n\nShanbo Xu\n\n\n\n\nXunliang Cai\n\n\n\n\n\n\nBin Xiao\n\n\n\n\nJing Huang\n\n\n\n\nShanglin Lei\n\n\n\n\nYang Yang\n\n\n\n\n\n\nBo Zhang *\n\n\n\n\nJingang Wang\n\n\n\n\nShengze Ye\n\n\n\n\nYanli Tan\n\n\n\n\n\n\nBolin Rong\n\n\n\n\nJinrui Ding\n\n\n\n\nShimin Chen\n\n\n\n\nYao Yao\n\n\n\n\n\n\nBorun Chen\n\n\n\n\nJuchao Jiang\n\n\n\n\nShuaiqi Chen\n\n\n\n\nYerui Sun\n\n\n\n\n\n\nChang Wan\n\n\n\n\nJun Kuang\n\n\n\n\nShujie Hu\n\n\n\n\nYi Chen\n\n\n\n\n\n\nChao Zhang\n\n\n\n\nJun Wang\n\n\n\n\nShuo Li\n\n\n\n\nYifan Lu\n\n\n\n\n\n\nChen Chen ()\n\n\n\n\nJunhui Mei\n\n\n\n\nSiqi Yang\n\n\n\n\nYin Gong\n\n\n\n\n\n\nChen Chen ()\n\n\n\n\nKe Ding\n\n\n\n\nSiyu Ren\n\n\n\n\nYining Zhang\n\n\n\n\n\n\nChen Huang\n\n\n\n\nKefeng Zhang\n\n\n\n\nSiyu Xu\n\n\n\n\nYitian Chen\n\n\n\n\n\n\nChengxu Yang\n\n\n\n\nLei Chen\n\n\n\n\nSong Li\n\n\n\n\nYiyang Gan\n\n\n\n\n\n\nChengzuo Yang\n\n\n\n\nLiang Shi\n\n\n\n\nSongxiang Liu\n\n\n\n\nYuchen Tang\n\n\n\n\n\n\nCong Han *\n\n\n\n\nLimeng Qiao\n\n\n\n\nTianhao Bai\n\n\n\n\nYuchen Xie\n\n\n\n\n\n\nDandan Peng\n\n\n\n\nLiming Zheng\n\n\n\n\nTianye Dai\n\n\n\n\nYueqian Wang\n\n\n\n\n\n\nDelian Ruan\n\n\n\n\nLin Ma\n\n\n\n\nWei Hong\n\n\n\n\nYuewen Zheng\n\n\n\n\n\n\nDetai Xin\n\n\n\n\nLiuyang Guo\n\n\n\n\nWei Wang\n\n\n\n\nYufei Zhang\n\n\n\n\n\n\nDisong Wang\n\n\n\n\nLiya Ma\n\n\n\n\nWeixiao Zhao\n\n\n\n\nYufeng Zhong\n\n\n\n\n\n\nDongchao Yang\n\n\n\n\nLuying Sun\n\n\n\n\nWengang Cao\n\n\n\n\nYulei Qian\n\n\n\n\n\n\nFanfan Liu\n\n\n\n\nMan Gao\n\n\n\n\nWenlong He\n\n\n\n\nYuqi Peng\n\n\n\n\n\n\nFengjiao Chen\n\n\n\n\nMengshen Zhu\n\n\n\n\nWenlong Zhu\n\n\n\n\nYuwei Jiang\n\n\n\n\n\n\nFengyu Yang\n\n\n\n\nMiao Cao\n\n\n\n\nXi Nan\n\n\n\n\nZeyang Hu\n\n\n\n\n\n\nGan Dong\n\n\n\n\nMinliang Lin\n\n\n\n\nXi Su\n\n\n\n\nZheng Zhang\n\n\n\n\n\n\nGang Huang\n\n\n\n\nNuo Xu\n\n\n\n\nXiaohan Zhao\n\n\n\n\nZhengkun Tian *\n\n\n\n\n\n\nGang Xu\n\n\n\n\nPeng Shi\n\n\n\n\nXiaohao Wang\n\n\n\n\nZhiqing Hong\n\n\n\n\n\n\nGuanglu Wan\n\n\n\n\nQi Zhang\n\n\n\n\nXiaoyu Li\n\n\n\n\nZhixiong Zeng\n\n\n\n\n\n\nGuoqiang Tan\n\n\n\n\nQian Fang\n\n\n\n\nXiaoyu Wang\n\n\n\n\nZhuqi Mi\n\n\n\n\n\n\nGuoqiao Yu\n\n\n\n\nQian Wang\n\n\n\n\nXiaoyu Zhao\n\n\n\n\nZiran Li\n\n\n\n\n\n\nHaibo Qiu\n\n\n\n\nQian Yang\n\n\n\n\nXin Chen\n\n\n\n\nZiwen Wang\n\n\n\n\n\n\nHao Lu\n\n\n\n\nQuanxiu Wang\n\n\n\n\nXin Pan\n\n\n\n\nZiyi Zhao\n\n\n\n\n\n\nHongbo Liu\n\n\n\n\nRongxiang Weng\n\n\n\n\nXiusong Sun\n\n\n\n\nZiyuan Zhuang\n\n\n\n\n\n\nHongyu Xiang\n\n\n\n\nRongxin Guo *\n\n\n\n\nXu Xiang\n\n\n\n\nZizhe Zhao\n\n\n\n\n\n\nJiaheng Wu\n\n\n\n\nRuoxuan Liang\n\n\n\n\nXudong Xing",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Bairui Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jian Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Senbin Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xuezhi Cao</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Bayan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jiaxing Liu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shanbo Xu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xunliang Cai</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Bin Xiao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jing Huang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shanglin Lei</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yang Yang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Bo Zhang *</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jingang Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shengze Ye</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yanli Tan</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Bolin Rong</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jinrui Ding</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shimin Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yao Yao</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Borun Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Juchao Jiang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shuaiqi Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yerui Sun</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chang Wan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jun Kuang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shujie Hu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yi Chen</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chao Zhang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jun Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Shuo Li</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yifan Lu</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chen Chen (&#38472;&#26216;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Junhui Mei</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Siqi Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yin Gong</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chen Chen (&#38472;&#29723;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ke Ding</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Siyu Ren</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yining Zhang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chen Huang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Kefeng Zhang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Siyu Xu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yitian Chen</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chengxu Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Lei Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Song Li</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yiyang Gan</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Chengzuo Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Liang Shi</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Songxiang Liu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yuchen Tang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Cong Han *</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Limeng Qiao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Tianhao Bai</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yuchen Xie</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Dandan Peng</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Liming Zheng</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Tianye Dai</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yueqian Wang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Delian Ruan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Lin Ma</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Wei Hong</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yuewen Zheng</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Detai Xin</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Liuyang Guo</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Wei Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yufei Zhang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Disong Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Liya Ma</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Weixiao Zhao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yufeng Zhong</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Dongchao Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Luying Sun</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Wengang Cao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yulei Qian</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Fanfan Liu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Man Gao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Wenlong He</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yuqi Peng</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Fengjiao Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Mengshen Zhu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Wenlong Zhu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Yuwei Jiang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Fengyu Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Miao Cao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xi Nan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zeyang Hu</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Gan Dong</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Minliang Lin</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xi Su</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zheng Zhang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Gang Huang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Nuo Xu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiaohan Zhao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zhengkun Tian *</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Gang Xu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Peng Shi</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiaohao Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zhiqing Hong</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Guanglu Wan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Qi Zhang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiaoyu Li</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zhixiong Zeng</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Guoqiang Tan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Qian Fang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiaoyu Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zhuqi Mi</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Guoqiao Yu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Qian Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiaoyu Zhao</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ziran Li</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Haibo Qiu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Qian Yang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xin Chen</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ziwen Wang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Hao Lu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Quanxiu Wang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xin Pan</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ziyi Zhao</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Hongbo Liu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Rongxiang Weng</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xiusong Sun</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ziyuan Zhuang</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Hongyu Xiang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Rongxin Guo *</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xu Xiang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Zizhe Zhao</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Jiaheng Wu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Ruoxuan Liang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.3pt;\">Xudong Xing</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "cong",
            "pan",
            "jiaxing",
            "chao",
            "zhuqi",
            "xiaoyu",
            "ziyi",
            "gong",
            "cai",
            "zheng",
            "yanli",
            "rongxin",
            "shimin",
            "weixiao",
            "disong",
            "tianhao",
            "shujie",
            "shanglin",
            "ziwen",
            "ding",
            "rong",
            "zhong",
            "yerui",
            "liya",
            "shuo",
            "guoqiang",
            "wang",
            "gao",
            "xiaohan",
            "hao",
            "hongbo",
            "chang",
            "xin",
            "jiaheng",
            "yulei",
            "jun",
            "bolin",
            "xie",
            "yin",
            "jian",
            "fengyu",
            "jing",
            "quanxiu",
            "chen",
            "yufei",
            "wei",
            "senbin",
            "jiang",
            "yueqian",
            "nuo",
            "wengang",
            "guo",
            "xing",
            "wan",
            "nan",
            "ziyuan",
            "shi",
            "delian",
            "han",
            "zeyang",
            "gang",
            "lei",
            "liang",
            "tianye",
            "song",
            "limeng",
            "ruan",
            "chengzuo",
            "yiyang",
            "yifan",
            "hong",
            "tang",
            "huang",
            "jingang",
            "guoqiao",
            "haibo",
            "weng",
            "peng",
            "liming",
            "liuyang",
            "ruoxuan",
            "sun",
            "gan",
            "borun",
            "luying",
            "tan",
            "siqi",
            "lin",
            "qian",
            "zhuang",
            "zhao",
            "dong",
            "songxiang",
            "xiao",
            "siyu",
            "yang",
            "dongchao",
            "ziran",
            "hongyu",
            "dai",
            "zizhe",
            "xudong",
            "tian",
            "dandan",
            "miao",
            "qiu",
            "juchao",
            "fengjiao",
            "bin",
            "shanbo",
            "kuang",
            "zhang",
            "yuqi",
            "yuwei",
            "zhengkun",
            "man",
            "detai",
            "shengze",
            "bai",
            "liu",
            "shuaiqi",
            "ren",
            "xuezhi",
            "mei",
            "yuewen",
            "xiaohao",
            "mengshen",
            "chengxu",
            "xunliang",
            "bairui",
            "yitian",
            "jinrui",
            "yuchen",
            "yao",
            "xiusong",
            "minliang",
            "xiang",
            "zhu",
            "cao",
            "bayan",
            "fang",
            "qiao",
            "yining",
            "zhixiong",
            "wenlong",
            "zeng",
            "zhiqing",
            "fanfan",
            "rongxiang",
            "junhui",
            "guanglu",
            "yufeng",
            "kefeng"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans are inherently omni-modal beings, capable of efficiently perceiving and integrating diverse forms of information, including visual and auditory inputs, to accomplish a wide range of challenging tasks. The seamless combination and transmission of multiple modalities significantly enhance the effectiveness and efficiency of human communication and interaction. In pursuit of Artificial General Intelligence (AGI), the field of large language models (LLMs) is now rapidly evolving toward the integration of richer multimodal capabilities and more efficient human-AI interaction.\nRecent pioneers such as Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib2\" title=\"\">2024</a>)</cite> have integrated text, audio, image, and video processing within a single model, enabling efficient audio-visual interaction. Following these advances, research on omni-modal models has attracted broad attention, with many subsequent efforts proposed in the community <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib4\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib5\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib6\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib7\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "guo",
                    "liu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Tokenizer and Decoder</span> We adopt LongCat-Audio-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib14\" title=\"\">2025a</a>)</cite> as our audio <span class=\"ltx_text ltx_font_italic\">tokenizer</span> and <span class=\"ltx_text ltx_font_italic\">decoder</span>, owing to its robust semantic modeling, flexible acoustic feature extraction, and low-latency streaming synthesis capabilities. The tokenizer discretizes audio waveforms into four codebooks at a frame rate of 16.67 Hz, with one codebook representing semantic information and the other three capturing acoustic details.\nTo achieve low-latency inference in real-time interaction scenarios, unlike conventional waveform reconstruction methods that rely on diffusion- or flow-matching-based code2mel models followed by vocoders, we directly employ the decoder from LongCat-Audio-Codec as the audio decoder to reconstruct waveform from tokens.\nIt supports streaming audio decoding with a look-ahead of only three frames.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S2.F4\" title=\"Figure 4 &#8227; 2.2 Audio Tokenizer, Encoder, and Decoder &#8227; 2 Architecture &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the audio decoder consists of LSTM layers, convolutional blocks, and causal transposed convolution layers, and is trained under a generative adversarial network (GAN) framework.</p>\n\n",
                "matched_terms": [
                    "zhao",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LongCat-Flash-Omni is built upon LongCat-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite>, a 560-billion-parameter Mixture-of-Experts (MoE) language model.\nLongCat-Flash adopts Multi-head Latent Attention (MLA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib17\" title=\"\">2024a</a>)</cite>, shortcut-connected MoE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib18\" title=\"\">2024</a>)</cite> and zero-computation experts, performing variable computation per token by activating 18.6B-31.3B parameters (27B on average), thereby unifying efficiency, performance and sparsity.\nThese characteristics are retained and extended to multimodal understanding and audio-visual interaction by LongCat-Flash-Omni.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "cai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Domains</span>: MMBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib48\" title=\"\">2024b</a>)</cite>, RealWorldQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(xAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib49\" title=\"\">2023</a>)</cite>, and MMStar&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib50\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-Image</span>: BLINK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib54\" title=\"\">2024a</a>)</cite>, MuirBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib55\" title=\"\">2024a</a>)</cite>, and Mantis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib56\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "jiang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Short Video</span>: MVBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib66\" title=\"\">2024b</a>)</cite>, NextQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib67\" title=\"\">2021</a>)</cite>, and TempCompass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib68\" title=\"\">2024e</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "xiao"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR evaluation, as presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we use LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib73\" title=\"\">2015</a>)</cite>, AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib75\" title=\"\">2017</a>)</cite>, AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib76\" title=\"\">2018</a>)</cite>, FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib77\" title=\"\">2023</a>)</cite>, CommonVoice15&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib78\" title=\"\">2019</a>)</cite>, and WenetSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib79\" title=\"\">2022</a>)</cite> as benchmarks. We observe that LongCat-Flash-Omni consistently achieves superior performance compared to other competitors, including Gemini-2.5-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o-Audio, Qwen3-Omni-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib3\" title=\"\">2025</a>)</cite>, Kimi-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib80\" title=\"\">2025</a>)</cite>, and Step-Audio-2-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib81\" title=\"\">2025</a>)</cite>.\nFor speech-to-text translation (S2TT) evaluation, we adopt CoVost2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib82\" title=\"\">2021</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#S7.T9\" title=\"Table 9 &#8227; 7.2.2 Instruct Model Evaluation &#8227; 7.2 Audio Capability Evaluation &#8227; 7 Evaluation &#8227; LongCat-Flash-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, LongCat-Flash-Omni exhibits strong S2TT capabilities among all models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Kimi-audio defaults to ASR rather than executing the requested translation task.</span></span></span>. Taken together, the speech recognition and translation results demonstrate that LongCat-Flash-Omni possesses robust and comprehensive fundamental speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "ding",
                    "zhang",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the LongCat-Flash-Omni Base model (i.e., after pre-training stage 5) with other strong text-based models across the following core capabilities and corresponding benchmarks.: (1) <span class=\"ltx_text ltx_font_bold\">General Tasks:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, C-Eval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>. (2) <span class=\"ltx_text ltx_font_bold\">Reasoning Tasks:</span> GPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib94\" title=\"\">2023</a>)</cite>, SuperGPQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(M-A-P Team, ByteDance., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib95\" title=\"\">2025</a>)</cite>, BBH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Suzgun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib96\" title=\"\">2023</a>)</cite>, PIQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib97\" title=\"\">2019</a>)</cite>, DROP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dua et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib98\" title=\"\">2019</a>)</cite>, CLUEWSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib99\" title=\"\">2020</a>)</cite>, and WinoGrande&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib100\" title=\"\">2019</a>)</cite>. (3) <span class=\"ltx_text ltx_font_bold\">Math Tasks:</span> GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib101\" title=\"\">2021</a>)</cite>, MATH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib102\" title=\"\">2021b</a>)</cite>. (4) <span class=\"ltx_text ltx_font_bold\">Coding Tasks:</span> MBPP+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, HumanEval+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib103\" title=\"\">2024f</a>)</cite>, MultiPL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib104\" title=\"\">2022</a>)</cite>, and CRUXEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib105\" title=\"\">2024</a>)</cite>. We follow the same evaluation protocol as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Meituan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib8\" title=\"\">2025a</a>)</cite> to ensure maximum fairness.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "huang",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Domains:</span> MMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib91\" title=\"\">2021a</a>)</cite>, MMLU-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib92\" title=\"\">2024b</a>)</cite>, CEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib93\" title=\"\">2023</a>)</cite>, and CMMLU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib74\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "huang",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following:</span> IFEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib106\" title=\"\">2023</a>)</cite>, COLLIE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib107\" title=\"\">2024</a>)</cite>, and Meeseeks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib108\" title=\"\">2025b</a>)</cite>, Meeseeks evaluates models&#8217; instruction-following capabilities in multi-turn scenarios through an iterative feedback framework that simulates realistic human-LLM interactions, enabling models to self-correct based on turn-specific failures and better reflect real-world usage patterns.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "yao"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate cross-modal understanding using publicly available benchmarks, including OmniBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib118\" title=\"\">2024c</a>)</cite>, WorldSense&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib119\" title=\"\">2025</a>)</cite>, and DailyOmni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib120\" title=\"\">2025</a>)</cite>. We use an internally corrected version of OmniBench, as the publicly released version contains scoring deficiencies.\nTo address the limited quality and coverage of existing benchmarks, we further introduce a new benchmark, UNO-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.00279v1#bib.bib121\" title=\"\">2025</a>)</cite>, comprising 1,880 human-crafted questions spanning 44 task types, with 98% of the questions requiring cross-modal reasoning.\nWe constructed this benchmark by manually annotating our in-house dataset. This approach prevents data contamination and ensures the benchmark is highly representative of real-world application scenarios.\nIn addition to conventional multiple-choice questions, the evaluation includes innovative multi-step open-ended questions, providing a more realistic and discriminative assessment of complex reasoning abilities.\n</p>\n\n",
                "matched_terms": [
                    "hong",
                    "chen"
                ]
            }
        ]
    }
}