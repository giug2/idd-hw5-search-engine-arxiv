{
    "p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_top\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Yejin Kwon, Taewoo Kang, Hyunsoo Yoon<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8224;</span></sup>, and Changouk Kim<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8224;</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\nDepartment of Industrial Engineering, Yonsei University</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Seoul, Republic of Korea</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">{beckykwon, hs.yoon, kimco}@yonsei.ac.kr, gangtaeu02@gmail.com</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "kimcoyonseiackr",
            "kwon",
            "taewoo",
            "beckykwon",
            "department",
            "gangtaeu02gmailcom",
            "yejin",
            "seoul",
            "changouk",
            "hsyoon",
            "university",
            "republic",
            "yonsei",
            "yoon†",
            "kim†",
            "kang",
            "hyunsoo",
            "industrial",
            "korea",
            "engineering"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": []
    },
    "S1.T1": {
        "caption": "Table 1: Comparison between M3-SLU and existing speech-language understanding benchmarks.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLURP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoiceBench</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMSU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMAU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AudioBench</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MSU-Bench</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\">M3-SLU (Ours)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speaker-Oriented</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\">O</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multi-speaker</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\">O</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speaker-attributed Reasoning</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><math alttext=\"\\triangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#9651;</mi><annotation encoding=\"application/x-tex\">\\triangle</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\">O</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio Source</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTS+RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTS+RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">RPC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6FF;\">RPC</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Conversation Type</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Monologue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Monologue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dialogue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dialogue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dialogue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dialogue</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6FF;\">Dialogue</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Conversation Length</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#E6E6FF;padding-top:0.7pt;padding-bottom:0.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6FF;\">Long (Over 1 Min)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mmau",
            "audiobench",
            "msubench",
            "type",
            "monologue",
            "dialogue",
            "△triangle",
            "length",
            "source",
            "voicebench",
            "comparison",
            "audio",
            "rpc",
            "understanding",
            "ttsrpc",
            "short",
            "long",
            "mmsu",
            "between",
            "over",
            "speakerattributed",
            "min",
            "reasoning",
            "m3slu",
            "conversation",
            "speechlanguage",
            "ours",
            "slurp",
            "existing",
            "speakeroriented",
            "multispeaker",
            "benchmarks"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As the performance of Large Audio-Language Models (LALMs) has advanced, developing benchmarks that evaluate their complex speech understanding capabilities has become a major research focus. Early benchmarks in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> such as SLURP <cite class=\"ltx_cite ltx_citemacro_citep\">(Bastianelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib2\" title=\"\">2020</a>)</cite> and VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>)</cite> primarily focused on single-turn, single-speaker intent classification using synthetic or short speech segments (TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>TTS (Text-to-Speech): Speech audio is synthetically generated from written text using a text-to-speech engine.</span></span></span> + RPC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>RPC (Real/Recorded Speech Corpus): Speech audio is naturally recorded from human speakers in real environments.</span></span></span>), laying the groundwork for fundamental speech-language understanding. Subsequent datasets including MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite>, MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib26\" title=\"\">2025a</a>)</cite>, and AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite> expanded their evaluation scope to multi-turn audio-based question and tasks such as intent classification and emotion interpretation, aiming to assess more nuanced aspects of speech understanding.</p>\n\n",
            "<p class=\"ltx_p\">As summarized in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MSU-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite> marked the first benchmark specifically dedicated to the rigorous evaluation of multi-speaker understanding in realistic conversational scenarios. Yet, it still focused on short dialogues and failed to capture reasoning that spans multiple turns and speakers. Building on this trajectory, we propose the <span class=\"ltx_text ltx_font_bold\">M3-SLU</span> benchmark, designed to assess long-form conversational understanding in segments over one minute long. Unlike prior benchmarks, M3-SLU comprises real multi-speaker dialogues lasting between one and three minutes and features speaker-attributed question answering tasks that require identifying concrete nouns (e.g., objects, places, times, numbers, names) instead of abstract intents or emotions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "over",
                    "audio",
                    "dialogue",
                    "speakerattributed",
                    "understanding",
                    "multispeaker",
                    "reasoning",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">M3-SLU: Evaluating Speaker-Attributed Reasoning \n<br class=\"ltx_break\"/>in Multimodal Large Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "m3slu",
                    "speakerattributed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal Large Language Models (MLLMs) have begun to blur the boundary between modalities, that is, seeing, hearing, and reasoning. Advances in models such as Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite>, Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib6\" title=\"\">2025</a>)</cite>, and GPT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> have demonstrated how far AI systems can extend their understanding beyond text, seamlessly integrating visual, auditory, and linguistic cues to perceive the world in richer ways. Particularly, Audio-Language Models (ALMs) that integrate auditory representations into large language models, including Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>, Audio Flamingo <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite>, and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, have recently achieved remarkable progress in bridging speech and language understanding. These models enable machines not only to transcribe speech but also to understand intent, summarize dialogues, and generate coherent responses.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "audio",
                    "reasoning",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their impressive multimodal capabilities, most existing MLLMs still assume single-speaker conditions, achieving strong performance in Automatic Speech Recognition (ASR) but leaving Speaker Diarization (SD) largely unaddressed <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>. Yet, real-world conversations are far more complex than such single-speaker settings. Understanding &#8220;who spoke when and what&#8221; offers a more comprehensive and meaningful perspective on real-world conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib7\" title=\"\">2025</a>)</cite>. In particular, in natural interactions, utterance sequences may be immediately continuous or may overlap temporally; overlapping talk often occurs around turn transitions and is characterized by backchannels, interruptions, and simultaneous first-starts <cite class=\"ltx_cite ltx_citemacro_citep\">(Levinson and Torreira, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib13\" title=\"\">2015</a>; Knudsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib11\" title=\"\">2020</a>; Schegloff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib22\" title=\"\">2000</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding \"Who spoke When and What\" in multi-party conversations is a crucial step toward socially intelligent AI. However, most existing speech benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib31\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite> address speaker-related and general dialogue tasks together, without separately examining the distinct challenges of speaker-centric understanding in real conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>. To close the gap between current MLLM evaluation and real-world conversational complexity, we introduce the <span class=\"ltx_text ltx_font_bold\">M3-SLU Benchmark</span> (<span class=\"ltx_text ltx_font_bold\">M</span>ulti-Speaker, <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Turn, and <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Modal <span class=\"ltx_text ltx_font_bold\">S</span>poken <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding), as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our key contributions:</p>\n\n",
                "matched_terms": [
                    "existing",
                    "dialogue",
                    "understanding",
                    "multispeaker",
                    "between",
                    "m3slu",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding has advanced beyond mere transcription toward comprehension of spoken language &#8211; capturing both meaning and paralinguistic cues such as prosody and tone. Earlier pipeline systems that linked ASR to NLP models often lost acoustic detail and propagated recognition errors, prompting a shift toward end-to-end architectures that map raw audio directly to semantic representations. This architectural shift has enhanced robustness and enabled deeper speech understanding, as shown by recent models such as SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib35\" title=\"\">2023</a>)</cite>, Salmonn <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib23\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib33\" title=\"\">2025</a>)</cite>, Glm-4-voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib34\" title=\"\">2024</a>)</cite>, and Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib24\" title=\"\">2023</a>)</cite>. These models typically follow two main approaches: (1) Using an audio adaptor, as in Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, or (2) Directly combining an audio encoder with an LLM, as in Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paradigm has also paved the way for the emergence of more specialized capabilities, such as the Speaker LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, which captures individual vocal signatures and uses them in reasoning, and the MT-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib15\" title=\"\">2025</a>)</cite>, designed to disentangle and process dialogue from concurrent speakers. Looking forward, the frontier of research is expanding into multimodal domains where models like GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> and Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite> fuse auditory streams with visual data to achieve a contextually richer, more human-aligned interaction.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However these benchmarks, while often built from long recordings, evaluated only short conversational segments (typically under 30 seconds) and were mainly oriented toward intent, emotion, or speaker recognition rather than context-grounded reasoning. Moreover, they did not address multi-speaker scenarios where understanding requires reasoning over speaker identities and interactions.</p>\n\n",
                "matched_terms": [
                    "over",
                    "understanding",
                    "multispeaker",
                    "short",
                    "long",
                    "reasoning",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our M3-SLU benchmark is designed to measure how accurately a model can understand complex multi-speaker conversations and provide appropriate answers to related questions. Therefore, <span class=\"ltx_text ltx_font_bold\">we designed tasks that require the MLLM models to listen to the conversation and distinguish between speakers in order to answer correctly</span>.\nAs illustrated in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S2.F2\" title=\"Figure 2 &#8227; 2.3. Multi-speaker Speech Understanding Benchmarks &#8227; 2. Related Work &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we propose two tasks: <span class=\"ltx_text ltx_font_bold\">Task 1. Speaker-Attributed QA</span> and <span class=\"ltx_text ltx_font_bold\">Task 2. Speaker Attribution Utterance Matching (T/F)</span>.</p>\n\n",
                "matched_terms": [
                    "speakerattributed",
                    "multispeaker",
                    "between",
                    "m3slu",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3-SLU consists of two core evaluation tasks generated from 4 public multi-speaker dialogue datasets &#8212; CHiME-6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib4\" title=\"\">2020</a>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib3\" title=\"\">2019</a>)</cite>, MultiDialog <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib2\" title=\"\">2024</a>)</cite>, and AMI <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib1\" title=\"\">2005</a>)</cite>. <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> consists of real dinner-party recordings captured in noisy environments with overlapping speech and distant microphones, making it ideal for evaluating speech robustness and diarization accuracy. <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> covers multi-topic conversations designed for contextual understanding across diverse domains. <span class=\"ltx_text ltx_font_bold\">MELD</span>, based on the Friends TV series, provides multimodal emotional dialogues that emphasize emotion recognition and sentiment analysis. Finally, <span class=\"ltx_text ltx_font_bold\">AMI</span> includes real business meeting recordings commonly used for summarization and decision-making tasks, capturing realistic multi-speaker interactions in professional settings. (Details and statistics for each dataset are provided in Appendix A.)</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "m3slu",
                    "dialogue",
                    "multispeaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Task 1 (QA) and Task 2 (T/F) were constructed from approximately 8k multi-speaker dialogue segments, each longer than one minute. As shown in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T2\" title=\"Table 2 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T3\" title=\"Table 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we finalized 12,873 challenging data instances (segments) for the benchmark, and each instance involves at least two speakers. And, every data instance must consist of at least two speakers. The two proposed tasks are as follows:</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the benchmark embodies its core characteristics of multi-speaker and multi-turn conversations, we carefully selected and collected high-quality public datasets(CHIME-6, MultiDialog, MELD, and AMI). Long dialogue recordings, mostly around or over one hour in length, were segmented into semantically coherent conversation units, each lasting between one and three and a half minutes. Only samples containing two or more speakers were retained, resulting in <span class=\"ltx_text ltx_font_bold\">refined long multi-speaker conversation chunks</span> that serve as the raw materials for the second-stage processing engine.</p>\n\n",
                "matched_terms": [
                    "over",
                    "dialogue",
                    "length",
                    "multispeaker",
                    "long",
                    "between",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both Task 1 and Task 2, around 8,000 multi-speaker conversation chunks are preprocessed. The original transcripts (ground-truth scripts) of these chunks were provided to GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib10\" title=\"\">2024</a>)</cite>, which automatically generated corresponding question and answer pairs: short noun-phrase answers for Task&#160;1 and True/False statements for Task&#160;2. For each task, a small set of manually created QA examples was also supplied as few-shot guidance.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "conversation",
                    "short"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Task 1, Task 2 additionally required GPT-4o to produce a rationale explaining the reasoning behind each True/False(boolean type) answer. This rationale was used temporarily during the validation process to assess the consistency of each candidate pair, though it was not included in the final benchmark.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The verification process for Task 2 (True/False reasoning) followed the validation and refinement loop illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.F5\" title=\"Figure 5 &#8227; Generation. &#8227; 3.3.2. Stage 2. Automated Data Curation Loop &#8227; 3.3. Benchmark Construction Pipeline &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Each candidate set (question, Boolean label, and rationale) underwent a three-step validation loop. <span class=\"ltx_text ltx_font_bold\">Step 1</span> verified whether the label was a valid Boolean (\"True\" or \"False\"). <span class=\"ltx_text ltx_font_bold\">Step 2</span> checked whether the question conformed to the required speaker-attributed form (e.g., &#8220;Is the one who &#8230; the same one who &#8230; ?&#8221;), ensuring that the reasoning explicitly involved speaker identity. <span class=\"ltx_text ltx_font_bold\">Step 3</span> evaluated the coherence between the rationale and the answer, filtering out logically inconsistent or semantically irrelevant cases.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speakerattributed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since Task 1 checks noun-phrase matching (QA) and Task 2 judges whether two utterances or actions were made by the same speaker (True/False), we propose distinct evaluation methods for assessing an MLLM&#8217;s speaker-attributed reasoning ability.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speakerattributed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So, the new evaluation metric was required for Task 1, which involves listening to speech and identifying the words that appear in the conversation. To address this, we adopted an LLM-as-a-Judge approach, in which GPT-4o evaluated model outputs by considering semantic similarity and phonetic plausibility, a strategy inspired by the evaluation framework in AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T5\" title=\"Table 5 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this allowed for minor pronunciation- or transcription-related variations&#8212;such as &#8220;NITE XML&#8221; vs. &#8220;Night XML&#8221;&#8212;to be accepted as correct, ensuring a more human-aligned and speech-aware assessment of answer quality. We conducted an LLM-as-a-Judge evaluation using prompts similar to the prompt of <cite class=\"ltx_cite ltx_citemacro_cite\">Badshah and Sajjad (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib1\" title=\"\">2024</a>)</cite>. The final evaluation score was defined as the proportion of samples that GPT-4o (LLM-as-Judge) evaluated as Correct among all instances.</p>\n\n",
                "matched_terms": [
                    "audiobench",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "audio",
                    "speakerattributed",
                    "multispeaker",
                    "reasoning",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, we evaluated cascade SD+ASR pipelines on our audio clips in M3-SLU. In particular, we used <span class=\"ltx_text ltx_font_bold\">Pyannote 3.1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bredin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib3\" title=\"\">2020</a>)</cite> and <span class=\"ltx_text ltx_font_bold\">DiariZen</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib9\" title=\"\">2025</a>)</cite> as speaker diarization (SD) modules, both widely recognized for their strong performance on English conversational audio. These were combined with <span class=\"ltx_text ltx_font_bold\">Whisper</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib19\" title=\"\">2023</a>)</cite> models of varying sizes (Medium and Large) for ASR. To further compare with end-to-end commercial systems, we also included two proprietary SDR models, <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span> and <span class=\"ltx_text ltx_font_bold\">Google STT</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, we measured two metrics, average of <span class=\"ltx_text ltx_font_bold\">WER</span> and average of <span class=\"ltx_text ltx_font_bold\">cpWER</span> for audio clips, to assess whether our benchmark achieves proper SDR performance with existing models. WER (Word Error Rate), commonly used in ASR assessment, measures the proportion of word errors between reference and predicted transcripts <cite class=\"ltx_cite ltx_citemacro_citep\">(Morris et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib16\" title=\"\">2004</a>)</cite>. And cpWER (concatenated minimum-permutation WER) adapts WER for multi-speaker data by optimally permuting speakers&#8217; transcriptions before scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "audio",
                    "existing",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio",
                    "dialogue",
                    "multispeaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU consists of long segments, rather than the short clips (within 30 seconds) used in previous studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, the reported WER and cpWER are naturally higher. This is because longer segments inherently increase the chance of accumulated transcription errors over time, leading to higher WER, while the frequent speaker transitions in extended dialogues also raise cpWER for each instance.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "over",
                    "short",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate current models&#8217; speaker-attributed reasoning ability on the M3-SLU benchmark, we adopted both cascade (SD + ASR + LLM) and end-to-end (E2E) MLLM methodologies, following prior approaches in spoken language understanding research <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "m3slu",
                    "reasoning",
                    "speakerattributed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the cascade setting, we first combined speaker diarization (SD) and automatic speech recognition (ASR) models before passing the transcribed text to a large language model (LLM) for question answering. Based on the SDR Test results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> combination exhibited consistently balanced multi-speaker transcription performance across our audio datasets, therefore we adopted this combination as the default SD + ASR configuration. The transcribed text was then passed to LLMs such as <span class=\"ltx_text ltx_font_bold\">Llama3.1-8B</span> and <span class=\"ltx_text ltx_font_bold\">Mistral-7B/24B</span> to investigate the impact of LLM scale. We also included commercial <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span>&#8217;s transcription result to analyze the difference in transcription text quality.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, we evaluated end-to-end Speech-LLM and Multimodal LLMs, which directly process raw audio inputs without intermediate transcription. For the Speech-LLM evaluation, we tested <span class=\"ltx_text ltx_font_bold\">Qwen2-Audio-7B</span> and <span class=\"ltx_text ltx_font_bold\">Voxtral-Small-24B</span>, while the Multimodal LLM evaluation included <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni-7B</span> and <span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-30B</span>, which jointly handle audio and textual reasoning in a unified framework.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "audio",
                    "speakerattributed",
                    "understanding",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the comparison between Task 1 and Task 2 results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, models demonstrated a <span class=\"ltx_text ltx_font_bold\">noticeable gap between understanding what was said and who said it</span>. While Task 1 could be partially solved by leveraging contextual cues without explicit speaker distinction, Task 2 inherently required precise speaker identification to match utterances correctly. This means that <span class=\"ltx_text ltx_font_bold\">although current models can comprehend the content of conversations, they remain largely incapable of reasoning about speaker attribution</span>, highlighting the persistent gap toward true multi-speaker understanding. Therefore, advancing and evaluating future MLLMs will require our M3-SLU benchmark as a foundation for genuine multi-speaker understanding.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "multispeaker",
                    "between",
                    "reasoning",
                    "m3slu",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T7\" title=\"Table 7 &#8227; Evaluation in Closed Models &#8227; 5.4. Results of M3-SLU Benchmark &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> below, current commercial models such as GPT-4o-Audio and Gemini-2.5-Flash-Audio completely fail to perform multi-speaker understanding, indicating that they are still unable to distinguish and reason over different speakers in conversational audio.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "audio",
                    "multispeaker",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced M3-SLU, a benchmark that reveals a key limitation of current MLLMs&#8212;the inability to comprehend \"who spoke when and what\" in long multi-speaker dialogues. Through two targeted tasks, Speaker-Attributed QA and Utterance Matching, M3-SLU isolates the challenge of speaker reasoning beyond simple transcription. Our experiments show that while existing cascaded pipelines and MLLMs can capture what was said, they consistently fail to track who said it, even with accurate transcripts. This underscores a critical gap in speaker attribution and multi-speaker reasoning. Building on real, naturally occurring conversations with speaker-attributed annotations, M3-SLU offers a structured evaluation setting that addresses the limitations of synthetic or short-turn benchmarks. The consistently low performance of current state-of-the-art models across both tasks reflects the complexity of speaker-grounded reasoning, which is unlikely to be resolved through scaling alone. M3-SLU offers a practical testbed for studying how multimodal language models handle multi-speaker conversations in realistic settings. We anticipate that it will guide the development of modeling strategies, evaluation methods, and training practices that explicitly incorporate speaker roles, turn-taking, and conversational structure, all of which are essential to dialogue comprehension.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "dialogue",
                    "speakerattributed",
                    "multispeaker",
                    "long",
                    "reasoning",
                    "m3slu",
                    "benchmarks"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Segment-level statistics of datasets used in M3-SLU, including total audio duration, average segment length, average number of utterances per segment, and the number of speakers.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CHIME-6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MELD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MultiDialog</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Total (h)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">113.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Seg. Dur (s)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">116.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">126.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Utt./Seg</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.70</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\"># of Speakers</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2&#8211;4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2&#8211;8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2&#8211;5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dur",
            "length",
            "multidialog",
            "2–5",
            "2–4",
            "audio",
            "average",
            "ami",
            "statistics",
            "seg",
            "speakers",
            "chime6",
            "used",
            "segment",
            "meld",
            "uttseg",
            "2–8",
            "segmentlevel",
            "m3slu",
            "number",
            "datasets",
            "total",
            "including",
            "duration",
            "utterances"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Task 1 (QA) and Task 2 (T/F) were constructed from approximately 8k multi-speaker dialogue segments, each longer than one minute. As shown in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T2\" title=\"Table 2 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T3\" title=\"Table 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we finalized 12,873 challenging data instances (segments) for the benchmark, and each instance involves at least two speakers. And, every data instance must consist of at least two speakers. The two proposed tasks are as follows:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "audio",
                    "ami",
                    "chime6",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal Large Language Models (MLLMs) have begun to blur the boundary between modalities, that is, seeing, hearing, and reasoning. Advances in models such as Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite>, Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib6\" title=\"\">2025</a>)</cite>, and GPT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> have demonstrated how far AI systems can extend their understanding beyond text, seamlessly integrating visual, auditory, and linguistic cues to perceive the world in richer ways. Particularly, Audio-Language Models (ALMs) that integrate auditory representations into large language models, including Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>, Audio Flamingo <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite>, and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, have recently achieved remarkable progress in bridging speech and language understanding. These models enable machines not only to transcribe speech but also to understand intent, summarize dialogues, and generate coherent responses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "ami",
                    "chime6",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the performance of Large Audio-Language Models (LALMs) has advanced, developing benchmarks that evaluate their complex speech understanding capabilities has become a major research focus. Early benchmarks in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> such as SLURP <cite class=\"ltx_cite ltx_citemacro_citep\">(Bastianelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib2\" title=\"\">2020</a>)</cite> and VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>)</cite> primarily focused on single-turn, single-speaker intent classification using synthetic or short speech segments (TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>TTS (Text-to-Speech): Speech audio is synthetically generated from written text using a text-to-speech engine.</span></span></span> + RPC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>RPC (Real/Recorded Speech Corpus): Speech audio is naturally recorded from human speakers in real environments.</span></span></span>), laying the groundwork for fundamental speech-language understanding. Subsequent datasets including MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite>, MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib26\" title=\"\">2025a</a>)</cite>, and AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite> expanded their evaluation scope to multi-turn audio-based question and tasks such as intent classification and emotion interpretation, aiming to assess more nuanced aspects of speech understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets",
                    "speakers",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MSU-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite> marked the first benchmark specifically dedicated to the rigorous evaluation of multi-speaker understanding in realistic conversational scenarios. Yet, it still focused on short dialogues and failed to capture reasoning that spans multiple turns and speakers. Building on this trajectory, we propose the <span class=\"ltx_text ltx_font_bold\">M3-SLU</span> benchmark, designed to assess long-form conversational understanding in segments over one minute long. Unlike prior benchmarks, M3-SLU comprises real multi-speaker dialogues lasting between one and three minutes and features speaker-attributed question answering tasks that require identifying concrete nouns (e.g., objects, places, times, numbers, names) instead of abstract intents or emotions.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our M3-SLU benchmark is designed to measure how accurately a model can understand complex multi-speaker conversations and provide appropriate answers to related questions. Therefore, <span class=\"ltx_text ltx_font_bold\">we designed tasks that require the MLLM models to listen to the conversation and distinguish between speakers in order to answer correctly</span>.\nAs illustrated in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S2.F2\" title=\"Figure 2 &#8227; 2.3. Multi-speaker Speech Understanding Benchmarks &#8227; 2. Related Work &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we propose two tasks: <span class=\"ltx_text ltx_font_bold\">Task 1. Speaker-Attributed QA</span> and <span class=\"ltx_text ltx_font_bold\">Task 2. Speaker Attribution Utterance Matching (T/F)</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3-SLU consists of two core evaluation tasks generated from 4 public multi-speaker dialogue datasets &#8212; CHiME-6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib4\" title=\"\">2020</a>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib3\" title=\"\">2019</a>)</cite>, MultiDialog <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib2\" title=\"\">2024</a>)</cite>, and AMI <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib1\" title=\"\">2005</a>)</cite>. <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> consists of real dinner-party recordings captured in noisy environments with overlapping speech and distant microphones, making it ideal for evaluating speech robustness and diarization accuracy. <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> covers multi-topic conversations designed for contextual understanding across diverse domains. <span class=\"ltx_text ltx_font_bold\">MELD</span>, based on the Friends TV series, provides multimodal emotional dialogues that emphasize emotion recognition and sentiment analysis. Finally, <span class=\"ltx_text ltx_font_bold\">AMI</span> includes real business meeting recordings commonly used for summarization and decision-making tasks, capturing realistic multi-speaker interactions in professional settings. (Details and statistics for each dataset are provided in Appendix A.)</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "ami",
                    "statistics",
                    "chime6",
                    "m3slu",
                    "used",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the benchmark embodies its core characteristics of multi-speaker and multi-turn conversations, we carefully selected and collected high-quality public datasets(CHIME-6, MultiDialog, MELD, and AMI). Long dialogue recordings, mostly around or over one hour in length, were segmented into semantically coherent conversation units, each lasting between one and three and a half minutes. Only samples containing two or more speakers were retained, resulting in <span class=\"ltx_text ltx_font_bold\">refined long multi-speaker conversation chunks</span> that serve as the raw materials for the second-stage processing engine.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "ami",
                    "length",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-based data pairs (QA, T/F) that pass automated stages are matched to corresponding original audio clips, packaging both text and speech information into complete multimodal datasets for human annotator use.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, we evaluated cascade SD+ASR pipelines on our audio clips in M3-SLU. In particular, we used <span class=\"ltx_text ltx_font_bold\">Pyannote 3.1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bredin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib3\" title=\"\">2020</a>)</cite> and <span class=\"ltx_text ltx_font_bold\">DiariZen</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib9\" title=\"\">2025</a>)</cite> as speaker diarization (SD) modules, both widely recognized for their strong performance on English conversational audio. These were combined with <span class=\"ltx_text ltx_font_bold\">Whisper</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib19\" title=\"\">2023</a>)</cite> models of varying sizes (Medium and Large) for ASR. To further compare with end-to-end commercial systems, we also included two proprietary SDR models, <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span> and <span class=\"ltx_text ltx_font_bold\">Google STT</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, we measured two metrics, average of <span class=\"ltx_text ltx_font_bold\">WER</span> and average of <span class=\"ltx_text ltx_font_bold\">cpWER</span> for audio clips, to assess whether our benchmark achieves proper SDR performance with existing models. WER (Word Error Rate), commonly used in ASR assessment, measures the proportion of word errors between reference and predicted transcripts <cite class=\"ltx_cite ltx_citemacro_citep\">(Morris et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib16\" title=\"\">2004</a>)</cite>. And cpWER (concatenated minimum-permutation WER) adapts WER for multi-speaker data by optimally permuting speakers&#8217; transcriptions before scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "audio",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU consists of long segments, rather than the short clips (within 30 seconds) used in previous studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, the reported WER and cpWER are naturally higher. This is because longer segments inherently increase the chance of accumulated transcription errors over time, leading to higher WER, while the frequent speaker transitions in extended dialogues also raise cpWER for each instance.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the cascade setting, we first combined speaker diarization (SD) and automatic speech recognition (ASR) models before passing the transcribed text to a large language model (LLM) for question answering. Based on the SDR Test results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> combination exhibited consistently balanced multi-speaker transcription performance across our audio datasets, therefore we adopted this combination as the default SD + ASR configuration. The transcribed text was then passed to LLMs such as <span class=\"ltx_text ltx_font_bold\">Llama3.1-8B</span> and <span class=\"ltx_text ltx_font_bold\">Mistral-7B/24B</span> to investigate the impact of LLM scale. We also included commercial <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span>&#8217;s transcription result to analyze the difference in transcription text quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the comparison between Task 1 and Task 2 results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, models demonstrated a <span class=\"ltx_text ltx_font_bold\">noticeable gap between understanding what was said and who said it</span>. While Task 1 could be partially solved by leveraging contextual cues without explicit speaker distinction, Task 2 inherently required precise speaker identification to match utterances correctly. This means that <span class=\"ltx_text ltx_font_bold\">although current models can comprehend the content of conversations, they remain largely incapable of reasoning about speaker attribution</span>, highlighting the persistent gap toward true multi-speaker understanding. Therefore, advancing and evaluating future MLLMs will require our M3-SLU benchmark as a foundation for genuine multi-speaker understanding.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T7\" title=\"Table 7 &#8227; Evaluation in Closed Models &#8227; 5.4. Results of M3-SLU Benchmark &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> below, current commercial models such as GPT-4o-Audio and Gemini-2.5-Flash-Audio completely fail to perform multi-speaker understanding, indicating that they are still unable to distinguish and reason over different speakers in conversational audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "audio",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio data used in this study are sourced from publicly available corpora (CHiME-6, MELD, MultiDialog, and AMI) that provide appropriate research licenses. No private or personally identifiable information (PII) was included. The dataset construction and experiments fully comply with the ethical use policies of the original sources.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "audio",
                    "ami",
                    "chime6",
                    "used"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Composition of the M3-SLU Benchmark",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task 1 (Q&amp;A)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task 2 (T/F)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CHiME-6</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">642</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,063</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,705</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MultiDialog</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2,793</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4,156</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6,949</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MELD</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">388</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">597</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">985</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AMI</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,103</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2,131</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3,234</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Total</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4,926</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7,947</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12,873</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "benchmark",
            "multidialog",
            "task",
            "total",
            "meld",
            "ami",
            "dataset",
            "chime6",
            "composition",
            "m3slu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Task 1 (QA) and Task 2 (T/F) were constructed from approximately 8k multi-speaker dialogue segments, each longer than one minute. As shown in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T2\" title=\"Table 2 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T3\" title=\"Table 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we finalized 12,873 challenging data instances (segments) for the benchmark, and each instance involves at least two speakers. And, every data instance must consist of at least two speakers. The two proposed tasks are as follows:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "multidialog",
                    "meld",
                    "ami",
                    "chime6",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding \"Who spoke When and What\" in multi-party conversations is a crucial step toward socially intelligent AI. However, most existing speech benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib31\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite> address speaker-related and general dialogue tasks together, without separately examining the distinct challenges of speaker-centric understanding in real conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>. To close the gap between current MLLM evaluation and real-world conversational complexity, we introduce the <span class=\"ltx_text ltx_font_bold\">M3-SLU Benchmark</span> (<span class=\"ltx_text ltx_font_bold\">M</span>ulti-Speaker, <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Turn, and <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Modal <span class=\"ltx_text ltx_font_bold\">S</span>poken <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding), as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our key contributions:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "multidialog",
                    "meld",
                    "ami",
                    "chime6",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the M3-SLU benchmark and <span class=\"ltx_text ltx_font_bold\">evaluation framework for MLLMs</span>, designed to measure performance on <span class=\"ltx_text ltx_font_bold\">two simple yet challenging tasks that can only be solved by correctly identifying the speaker</span>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MSU-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite> marked the first benchmark specifically dedicated to the rigorous evaluation of multi-speaker understanding in realistic conversational scenarios. Yet, it still focused on short dialogues and failed to capture reasoning that spans multiple turns and speakers. Building on this trajectory, we propose the <span class=\"ltx_text ltx_font_bold\">M3-SLU</span> benchmark, designed to assess long-form conversational understanding in segments over one minute long. Unlike prior benchmarks, M3-SLU comprises real multi-speaker dialogues lasting between one and three minutes and features speaker-attributed question answering tasks that require identifying concrete nouns (e.g., objects, places, times, numbers, names) instead of abstract intents or emotions.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our M3-SLU benchmark is designed to measure how accurately a model can understand complex multi-speaker conversations and provide appropriate answers to related questions. Therefore, <span class=\"ltx_text ltx_font_bold\">we designed tasks that require the MLLM models to listen to the conversation and distinguish between speakers in order to answer correctly</span>.\nAs illustrated in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S2.F2\" title=\"Figure 2 &#8227; 2.3. Multi-speaker Speech Understanding Benchmarks &#8227; 2. Related Work &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we propose two tasks: <span class=\"ltx_text ltx_font_bold\">Task 1. Speaker-Attributed QA</span> and <span class=\"ltx_text ltx_font_bold\">Task 2. Speaker Attribution Utterance Matching (T/F)</span>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3-SLU consists of two core evaluation tasks generated from 4 public multi-speaker dialogue datasets &#8212; CHiME-6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib4\" title=\"\">2020</a>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib3\" title=\"\">2019</a>)</cite>, MultiDialog <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib2\" title=\"\">2024</a>)</cite>, and AMI <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib1\" title=\"\">2005</a>)</cite>. <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> consists of real dinner-party recordings captured in noisy environments with overlapping speech and distant microphones, making it ideal for evaluating speech robustness and diarization accuracy. <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> covers multi-topic conversations designed for contextual understanding across diverse domains. <span class=\"ltx_text ltx_font_bold\">MELD</span>, based on the Friends TV series, provides multimodal emotional dialogues that emphasize emotion recognition and sentiment analysis. Finally, <span class=\"ltx_text ltx_font_bold\">AMI</span> includes real business meeting recordings commonly used for summarization and decision-making tasks, capturing realistic multi-speaker interactions in professional settings. (Details and statistics for each dataset are provided in Appendix A.)</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "m3slu",
                    "meld",
                    "ami",
                    "chime6",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 4-stage hybrid benchmark construction pipeline was implemented like Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.F3\" title=\"Figure 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, in which LLM-driven automatic screening and human review were jointly employed to maximize the reliability and accuracy of the generated dataset.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the benchmark embodies its core characteristics of multi-speaker and multi-turn conversations, we carefully selected and collected high-quality public datasets(CHIME-6, MultiDialog, MELD, and AMI). Long dialogue recordings, mostly around or over one hour in length, were segmented into semantically coherent conversation units, each lasting between one and three and a half minutes. Only samples containing two or more speakers were retained, resulting in <span class=\"ltx_text ltx_font_bold\">refined long multi-speaker conversation chunks</span> that serve as the raw materials for the second-stage processing engine.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "meld",
                    "ami",
                    "multidialog"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Task 1, Task 2 additionally required GPT-4o to produce a rationale explaining the reasoning behind each True/False(boolean type) answer. This rationale was used temporarily during the validation process to assess the consistency of each candidate pair, though it was not included in the final benchmark.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "multidialog"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate current models&#8217; speaker-attributed reasoning ability on the M3-SLU benchmark, we adopted both cascade (SD + ASR + LLM) and end-to-end (E2E) MLLM methodologies, following prior approaches in spoken language understanding research <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "dataset",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the lower part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, E2E Speech-LLM and Multimodal LLM models show relatively lower performance on Task 1, compared to the cascade setting. Only the larger models, such as MistralAI-Voxtral (24B) and Qwen3-Omni (30B), achieved scores approaching 80%, indicating that <span class=\"ltx_text ltx_font_bold\">our M3-SLU Task 1 remains a challenging problem for current E2E Speech-LLMs and MLLMs</span>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the comparison between Task 1 and Task 2 results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, models demonstrated a <span class=\"ltx_text ltx_font_bold\">noticeable gap between understanding what was said and who said it</span>. While Task 1 could be partially solved by leveraging contextual cues without explicit speaker distinction, Task 2 inherently required precise speaker identification to match utterances correctly. This means that <span class=\"ltx_text ltx_font_bold\">although current models can comprehend the content of conversations, they remain largely incapable of reasoning about speaker attribution</span>, highlighting the persistent gap toward true multi-speaker understanding. Therefore, advancing and evaluating future MLLMs will require our M3-SLU benchmark as a foundation for genuine multi-speaker understanding.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "task",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced M3-SLU, a benchmark that reveals a key limitation of current MLLMs&#8212;the inability to comprehend \"who spoke when and what\" in long multi-speaker dialogues. Through two targeted tasks, Speaker-Attributed QA and Utterance Matching, M3-SLU isolates the challenge of speaker reasoning beyond simple transcription. Our experiments show that while existing cascaded pipelines and MLLMs can capture what was said, they consistently fail to track who said it, even with accurate transcripts. This underscores a critical gap in speaker attribution and multi-speaker reasoning. Building on real, naturally occurring conversations with speaker-attributed annotations, M3-SLU offers a structured evaluation setting that addresses the limitations of synthetic or short-turn benchmarks. The consistently low performance of current state-of-the-art models across both tasks reflects the complexity of speaker-grounded reasoning, which is unlikely to be resolved through scaling alone. M3-SLU offers a practical testbed for studying how multimodal language models handle multi-speaker conversations in realistic settings. We anticipate that it will guide the development of modeling strategies, evaluation methods, and training practices that explicitly incorporate speaker roles, turn-taking, and conversational structure, all of which are essential to dialogue comprehension.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our current benchmark is primarily focused on English conversational data. Future work could expand M3-SLU to include a wider range of languages and even more complex, overlapping speech scenarios to further probe the robustness of MLLMs.\nIn addition, our evaluation framework currently relies on GPT-4o as an LLM-as-Judge to assess model outputs. While this approach enables flexible and semantic-level evaluation, it may overlook subtle variations that arise from speech input, such as minor pronunciation or transcription differences. We are actively exploring more refined evaluation strategies that can account for these speech-induced variations while maintaining fairness and consistency across models.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio data used in this study are sourced from publicly available corpora (CHiME-6, MELD, MultiDialog, and AMI) that provide appropriate research licenses. No private or personally identifiable information (PII) was included. The dataset construction and experiments fully comply with the ethical use policies of the original sources.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "ami",
                    "chime6",
                    "dataset"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Comparison of WER and cpWER across four dialogue audio sets. Both metrics indicate better performance with lower scores. Experiments marked with an asterisk (*) were conducted on 500 randomly sampled instances from each dataset due to research resource limitations.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model (SD + ASR)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CHiME-6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MELD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MultiDialog</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">cpWER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">cpWER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">cpWER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">cpWER</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pyannote + Whisper-Medium</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.631</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.712</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.601</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.712</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.356</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.335</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.472</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.451</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pyannote + Whisper-Large</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.635</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.713</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.604</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.707</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.391</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.354</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.487</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.478</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">DiariZen + Whisper-Medium</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.631</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.601</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.600</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.581</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.355</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.162</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.472</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.377</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Closed SDR) AssemblyAI</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.532</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.631</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.509</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.678</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.236</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">0.157</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.531</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.472</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">(Closed SDR) Google STT</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">*</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.710</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.720</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.545</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.662</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.394</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.542</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.552</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.450</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "four",
            "marked",
            "wer",
            "closed",
            "cpwer",
            "dialogue",
            "research",
            "each",
            "experiments",
            "comparison",
            "multidialog",
            "asterisk",
            "audio",
            "randomly",
            "ami",
            "sampled",
            "from",
            "chime6",
            "sdr",
            "whispermedium",
            "scores",
            "performance",
            "across",
            "due",
            "indicate",
            "conducted",
            "instances",
            "sets",
            "meld",
            "metrics",
            "resource",
            "google",
            "whisperlarge",
            "both",
            "lower",
            "asr",
            "diarizen",
            "assemblyai",
            "model",
            "better",
            "stt",
            "limitations",
            "dataset",
            "pyannote"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
            "<p class=\"ltx_p\">In the cascade setting, we first combined speaker diarization (SD) and automatic speech recognition (ASR) models before passing the transcribed text to a large language model (LLM) for question answering. Based on the SDR Test results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> combination exhibited consistently balanced multi-speaker transcription performance across our audio datasets, therefore we adopted this combination as the default SD + ASR configuration. The transcribed text was then passed to LLMs such as <span class=\"ltx_text ltx_font_bold\">Llama3.1-8B</span> and <span class=\"ltx_text ltx_font_bold\">Mistral-7B/24B</span> to investigate the impact of LLM scale. We also included commercial <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span>&#8217;s transcription result to analyze the difference in transcription text quality.</p>\n\n",
            "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "four",
                    "instances",
                    "meld",
                    "audio",
                    "ami",
                    "dialogue",
                    "metrics",
                    "model",
                    "research",
                    "from",
                    "chime6",
                    "both",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their impressive multimodal capabilities, most existing MLLMs still assume single-speaker conditions, achieving strong performance in Automatic Speech Recognition (ASR) but leaving Speaker Diarization (SD) largely unaddressed <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>. Yet, real-world conversations are far more complex than such single-speaker settings. Understanding &#8220;who spoke when and what&#8221; offers a more comprehensive and meaningful perspective on real-world conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib7\" title=\"\">2025</a>)</cite>. In particular, in natural interactions, utterance sequences may be immediately continuous or may overlap temporally; overlapping talk often occurs around turn transitions and is characterized by backchannels, interruptions, and simultaneous first-starts <cite class=\"ltx_cite ltx_citemacro_citep\">(Levinson and Torreira, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib13\" title=\"\">2015</a>; Knudsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib11\" title=\"\">2020</a>; Schegloff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib22\" title=\"\">2000</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "four",
                    "meld",
                    "ami",
                    "chime6"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding has advanced beyond mere transcription toward comprehension of spoken language &#8211; capturing both meaning and paralinguistic cues such as prosody and tone. Earlier pipeline systems that linked ASR to NLP models often lost acoustic detail and propagated recognition errors, prompting a shift toward end-to-end architectures that map raw audio directly to semantic representations. This architectural shift has enhanced robustness and enabled deeper speech understanding, as shown by recent models such as SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib35\" title=\"\">2023</a>)</cite>, Salmonn <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib23\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib33\" title=\"\">2025</a>)</cite>, Glm-4-voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib34\" title=\"\">2024</a>)</cite>, and Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib24\" title=\"\">2023</a>)</cite>. These models typically follow two main approaches: (1) Using an audio adaptor, as in Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, or (2) Directly combining an audio encoder with an LLM, as in Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paradigm has also paved the way for the emergence of more specialized capabilities, such as the Speaker LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, which captures individual vocal signatures and uses them in reasoning, and the MT-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib15\" title=\"\">2025</a>)</cite>, designed to disentangle and process dialogue from concurrent speakers. Looking forward, the frontier of research is expanding into multimodal domains where models like GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> and Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite> fuse auditory streams with visual data to achieve a contextually richer, more human-aligned interaction.</p>\n\n",
                "matched_terms": [
                    "research",
                    "dialogue",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the performance of Large Audio-Language Models (LALMs) has advanced, developing benchmarks that evaluate their complex speech understanding capabilities has become a major research focus. Early benchmarks in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> such as SLURP <cite class=\"ltx_cite ltx_citemacro_citep\">(Bastianelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib2\" title=\"\">2020</a>)</cite> and VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>)</cite> primarily focused on single-turn, single-speaker intent classification using synthetic or short speech segments (TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>TTS (Text-to-Speech): Speech audio is synthetically generated from written text using a text-to-speech engine.</span></span></span> + RPC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>RPC (Real/Recorded Speech Corpus): Speech audio is naturally recorded from human speakers in real environments.</span></span></span>), laying the groundwork for fundamental speech-language understanding. Subsequent datasets including MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite>, MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib26\" title=\"\">2025a</a>)</cite>, and AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite> expanded their evaluation scope to multi-turn audio-based question and tasks such as intent classification and emotion interpretation, aiming to assess more nuanced aspects of speech understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "research",
                    "from",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3-SLU consists of two core evaluation tasks generated from 4 public multi-speaker dialogue datasets &#8212; CHiME-6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib4\" title=\"\">2020</a>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib3\" title=\"\">2019</a>)</cite>, MultiDialog <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib2\" title=\"\">2024</a>)</cite>, and AMI <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib1\" title=\"\">2005</a>)</cite>. <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> consists of real dinner-party recordings captured in noisy environments with overlapping speech and distant microphones, making it ideal for evaluating speech robustness and diarization accuracy. <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> covers multi-topic conversations designed for contextual understanding across diverse domains. <span class=\"ltx_text ltx_font_bold\">MELD</span>, based on the Friends TV series, provides multimodal emotional dialogues that emphasize emotion recognition and sentiment analysis. Finally, <span class=\"ltx_text ltx_font_bold\">AMI</span> includes real business meeting recordings commonly used for summarization and decision-making tasks, capturing realistic multi-speaker interactions in professional settings. (Details and statistics for each dataset are provided in Appendix A.)</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "across",
                    "meld",
                    "ami",
                    "dialogue",
                    "each",
                    "from",
                    "chime6",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Task 1 (QA) and Task 2 (T/F) were constructed from approximately 8k multi-speaker dialogue segments, each longer than one minute. As shown in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T2\" title=\"Table 2 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T3\" title=\"Table 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we finalized 12,873 challenging data instances (segments) for the benchmark, and each instance involves at least two speakers. And, every data instance must consist of at least two speakers. The two proposed tasks are as follows:</p>\n\n",
                "matched_terms": [
                    "instances",
                    "dialogue",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the benchmark embodies its core characteristics of multi-speaker and multi-turn conversations, we carefully selected and collected high-quality public datasets(CHIME-6, MultiDialog, MELD, and AMI). Long dialogue recordings, mostly around or over one hour in length, were segmented into semantically coherent conversation units, each lasting between one and three and a half minutes. Only samples containing two or more speakers were retained, resulting in <span class=\"ltx_text ltx_font_bold\">refined long multi-speaker conversation chunks</span> that serve as the raw materials for the second-stage processing engine.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "ami",
                    "dialogue",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both Task 1 and Task 2, around 8,000 multi-speaker conversation chunks are preprocessed. The original transcripts (ground-truth scripts) of these chunks were provided to GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib10\" title=\"\">2024</a>)</cite>, which automatically generated corresponding question and answer pairs: short noun-phrase answers for Task&#160;1 and True/False statements for Task&#160;2. For each task, a small set of manually created QA examples was also supplied as few-shot guidance.</p>\n\n",
                "matched_terms": [
                    "each",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the initial generation stage, an iterative validation and refinement loop in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.F4\" title=\"Figure 4 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was applied to ensure the factual accuracy and linguistic precision of the generated QA pairs. Each iteration involved feeding the original ground-truth scripts and generated QA outputs back into GPT-4o for self-evaluation and regeneration. Through this process, only samples that consistently produced clear, contextually relevant, and noun-phrase-based answers were retained across the entire dataset.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The iteration stopped after six cycles because the proportion of samples passing Step 2 stayed around 78%, showing no further improvement. Conversation chunks that repeatedly failed Step 2 were deemed unsuitable for QA generation, as their original scripts lacked extractable noun-phrase answers. These samples were therefore excluded from the dataset. Consequently, the remaining 5,531 candidates advanced to human validation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Samples that failed any step entered the regeneration step, in which GPT-4o regenerated the candidate set. Each regenerated set was re-validated through the same three steps. After repeating the loop three times, we observed no further increase in the proportion of valid samples. So, we obtained a final set of 8,020 True/False labeled question pairs, each aligned with its corresponding audio clip and ready for subsequent human verification.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-based data pairs (QA, T/F) that pass automated stages are matched to corresponding original audio clips, packaging both text and speech information into complete multimodal datasets for human annotator use.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final and most critical stage ensured the overall integrity and contextual quality of both tasks. In this phase, human annotators manually inspected every remaining instance to identify subtle semantic or contextual errors that automated processes could not capture. For <span class=\"ltx_text ltx_font_bold\">Task&#160;1</span>, annotators carefully examined each question&#8211;answer pair to remove cases where the generated noun-phrase answers were vague or linguistically invalid (e.g., phrases like &#8220;that thing&#8221; or &#8220;something&#8221;), as well as instances where the question already contained the answer, rendering the question meaningless. For <span class=\"ltx_text ltx_font_bold\">Task&#160;2</span>, reviewers focused on verifying speaker attribution and contextual alignment, discarding samples in which the True/False reasoning relied on incorrect or ambiguous speaker identification.</p>\n\n",
                "matched_terms": [
                    "each",
                    "both",
                    "instances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional QA evaluation metrics typically rely on Exact Match (EM) and token-level F1 scores. The EM score is the percentage of predictions that match any one of the ground truth answers exactly. The F1 score measures the average overlap between the prediction and ground truth answer <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib20\" title=\"\">2016</a>)</cite>. However, these metrics assume textual inputs and do not account for variations arising from speech or pronunciation differences.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So, the new evaluation metric was required for Task 1, which involves listening to speech and identifying the words that appear in the conversation. To address this, we adopted an LLM-as-a-Judge approach, in which GPT-4o evaluated model outputs by considering semantic similarity and phonetic plausibility, a strategy inspired by the evaluation framework in AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T5\" title=\"Table 5 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this allowed for minor pronunciation- or transcription-related variations&#8212;such as &#8220;NITE XML&#8221; vs. &#8220;Night XML&#8221;&#8212;to be accepted as correct, ensuring a more human-aligned and speech-aware assessment of answer quality. We conducted an LLM-as-a-Judge evaluation using prompts similar to the prompt of <cite class=\"ltx_cite ltx_citemacro_cite\">Badshah and Sajjad (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib1\" title=\"\">2024</a>)</cite>. The final evaluation score was defined as the proportion of samples that GPT-4o (LLM-as-Judge) evaluated as Correct among all instances.</p>\n\n",
                "matched_terms": [
                    "model",
                    "instances",
                    "conducted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "asr",
                    "conducted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, we evaluated cascade SD+ASR pipelines on our audio clips in M3-SLU. In particular, we used <span class=\"ltx_text ltx_font_bold\">Pyannote 3.1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bredin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib3\" title=\"\">2020</a>)</cite> and <span class=\"ltx_text ltx_font_bold\">DiariZen</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib9\" title=\"\">2025</a>)</cite> as speaker diarization (SD) modules, both widely recognized for their strong performance on English conversational audio. These were combined with <span class=\"ltx_text ltx_font_bold\">Whisper</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib19\" title=\"\">2023</a>)</cite> models of varying sizes (Medium and Large) for ASR. To further compare with end-to-end commercial systems, we also included two proprietary SDR models, <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span> and <span class=\"ltx_text ltx_font_bold\">Google STT</span>.</p>\n\n",
                "matched_terms": [
                    "assemblyai",
                    "audio",
                    "google",
                    "stt",
                    "both",
                    "sdr",
                    "asr",
                    "pyannote",
                    "diarizen",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, we measured two metrics, average of <span class=\"ltx_text ltx_font_bold\">WER</span> and average of <span class=\"ltx_text ltx_font_bold\">cpWER</span> for audio clips, to assess whether our benchmark achieves proper SDR performance with existing models. WER (Word Error Rate), commonly used in ASR assessment, measures the proportion of word errors between reference and predicted transcripts <cite class=\"ltx_cite ltx_citemacro_citep\">(Morris et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib16\" title=\"\">2004</a>)</cite>. And cpWER (concatenated minimum-permutation WER) adapts WER for multi-speaker data by optimally permuting speakers&#8217; transcriptions before scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "audio",
                    "cpwer",
                    "metrics",
                    "sdr",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU consists of long segments, rather than the short clips (within 30 seconds) used in previous studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, the reported WER and cpWER are naturally higher. This is because longer segments inherently increase the chance of accumulated transcription errors over time, leading to higher WER, while the frequent speaker transitions in extended dialogues also raise cpWER for each instance.</p>\n\n",
                "matched_terms": [
                    "cpwer",
                    "each",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate current models&#8217; speaker-attributed reasoning ability on the M3-SLU benchmark, we adopted both cascade (SD + ASR + LLM) and end-to-end (E2E) MLLM methodologies, following prior approaches in spoken language understanding research <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "research",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The top two scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> correspond to the <span class=\"ltx_text ltx_font_italic\">GT Script(Gold)</span> configuration, where the model was provided with the original ground-truth transcription that had been perfectly segmented by speakers.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As expected, when the <span class=\"ltx_text ltx_font_italic\">GT Script(Gold)</span> was provided to the LLM, this configuration achieved the highest score, since the model received perfectly transcribed and speaker-segmented text, effectively removing any noise or cascading errors during the SD and ASR stages. In addition, the cascade setting (SD + ASR + LLM) also demonstrated reasonably strong QA performance, indicating that despite inevitable errors from the SD + ASR modules, the overall pipeline was still able to preserve a substantial amount of semantic and speaker-related information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the lower part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, E2E Speech-LLM and Multimodal LLM models show relatively lower performance on Task 1, compared to the cascade setting. Only the larger models, such as MistralAI-Voxtral (24B) and Qwen3-Omni (30B), achieved scores approaching 80%, indicating that <span class=\"ltx_text ltx_font_bold\">our M3-SLU Task 1 remains a challenging problem for current E2E Speech-LLMs and MLLMs</span>.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "lower",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown on the right side of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the Task 2 results reveal that no model configurations exceeded 70%, a surprisingly low performance considering that the Task 2 is binary (True/False) classification. This suggests that <span class=\"ltx_text ltx_font_bold\">accurate speaker-attributed utterance matching is impossible for both cascade and E2E models under the current framework</span>. Even in the cascade setting with gold transcripts provided, the models failed to accurately distinguish speakers. The best result was obtained with the Diarizen + Whisper + Mistral (24B) combination, which achieved only 0.6544 on Task 2.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diarizen",
                    "both",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "randomly",
                    "from",
                    "whispermedium",
                    "diarizen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced M3-SLU, a benchmark that reveals a key limitation of current MLLMs&#8212;the inability to comprehend \"who spoke when and what\" in long multi-speaker dialogues. Through two targeted tasks, Speaker-Attributed QA and Utterance Matching, M3-SLU isolates the challenge of speaker reasoning beyond simple transcription. Our experiments show that while existing cascaded pipelines and MLLMs can capture what was said, they consistently fail to track who said it, even with accurate transcripts. This underscores a critical gap in speaker attribution and multi-speaker reasoning. Building on real, naturally occurring conversations with speaker-attributed annotations, M3-SLU offers a structured evaluation setting that addresses the limitations of synthetic or short-turn benchmarks. The consistently low performance of current state-of-the-art models across both tasks reflects the complexity of speaker-grounded reasoning, which is unlikely to be resolved through scaling alone. M3-SLU offers a practical testbed for studying how multimodal language models handle multi-speaker conversations in realistic settings. We anticipate that it will guide the development of modeling strategies, evaluation methods, and training practices that explicitly incorporate speaker roles, turn-taking, and conversational structure, all of which are essential to dialogue comprehension.</p>\n\n",
                "matched_terms": [
                    "across",
                    "dialogue",
                    "both",
                    "limitations",
                    "experiments",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our current benchmark is primarily focused on English conversational data. Future work could expand M3-SLU to include a wider range of languages and even more complex, overlapping speech scenarios to further probe the robustness of MLLMs.\nIn addition, our evaluation framework currently relies on GPT-4o as an LLM-as-Judge to assess model outputs. While this approach enables flexible and semantic-level evaluation, it may overlook subtle variations that arise from speech input, such as minor pronunciation or transcription differences. We are actively exploring more refined evaluation strategies that can account for these speech-induced variations while maintaining fairness and consistency across models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio data used in this study are sourced from publicly available corpora (CHiME-6, MELD, MultiDialog, and AMI) that provide appropriate research licenses. No private or personally identifiable information (PII) was included. The dataset construction and experiments fully comply with the ethical use policies of the original sources.</p>\n\n",
                "matched_terms": [
                    "multidialog",
                    "meld",
                    "audio",
                    "ami",
                    "research",
                    "from",
                    "chime6",
                    "dataset",
                    "experiments"
                ]
            }
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Comparison of Evaluation Metrics",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Case</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F1 Score</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLM-as-Judge</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">(GT) </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">NITE XML</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Nite xml</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Incorrect</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Partial</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Correct</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Night xml</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Incorrect</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Partial</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Correct</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Nite x-m-l</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Incorrect</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Incorrect</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Correct</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "nite",
            "correct",
            "metrics",
            "night",
            "evaluation",
            "llmasjudge",
            "xml",
            "case",
            "partial",
            "comparison",
            "incorrect"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">So, the new evaluation metric was required for Task 1, which involves listening to speech and identifying the words that appear in the conversation. To address this, we adopted an LLM-as-a-Judge approach, in which GPT-4o evaluated model outputs by considering semantic similarity and phonetic plausibility, a strategy inspired by the evaluation framework in AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T5\" title=\"Table 5 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this allowed for minor pronunciation- or transcription-related variations&#8212;such as &#8220;NITE XML&#8221; vs. &#8220;Night XML&#8221;&#8212;to be accepted as correct, ensuring a more human-aligned and speech-aware assessment of answer quality. We conducted an LLM-as-a-Judge evaluation using prompts similar to the prompt of <cite class=\"ltx_cite ltx_citemacro_cite\">Badshah and Sajjad (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib1\" title=\"\">2024</a>)</cite>. The final evaluation score was defined as the proportion of samples that GPT-4o (LLM-as-Judge) evaluated as Correct among all instances.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "llmasjudge",
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional QA evaluation metrics typically rely on Exact Match (EM) and token-level F1 scores. The EM score is the percentage of predictions that match any one of the ground truth answers exactly. The F1 score measures the average overlap between the prediction and ground truth answer <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib20\" title=\"\">2016</a>)</cite>. However, these metrics assume textual inputs and do not account for variations arising from speech or pronunciation differences.</p>\n\n",
                "matched_terms": [
                    "score",
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "llmasjudge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our current benchmark is primarily focused on English conversational data. Future work could expand M3-SLU to include a wider range of languages and even more complex, overlapping speech scenarios to further probe the robustness of MLLMs.\nIn addition, our evaluation framework currently relies on GPT-4o as an LLM-as-Judge to assess model outputs. While this approach enables flexible and semantic-level evaluation, it may overlook subtle variations that arise from speech input, such as minor pronunciation or transcription differences. We are actively exploring more refined evaluation strategies that can account for these speech-induced variations while maintaining fairness and consistency across models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "llmasjudge"
                ]
            }
        ]
    },
    "S5.T6": {
        "caption": "Table 6: M3-SLU benchmark results comparing cascaded (SD+ASR+LLM) and E2E Speech/Multimodal LLM models. For Task 1, the evaluation scores were obtained using the LLM-as-Judge approach, while for Task 2, the values represent accuracy based on the correctness of True/False judgments. Both scores are higher-the-better metrics.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M3-SLU Benchmark</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task2</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F5F0FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F5F0FF;\">SD + ASR + LLM</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GT Script(Gold)+Llama3.1(8B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.9577</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5787</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GT Script(Gold)+Mistral(7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8717</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5409</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Diarizen+whisper+Llama3.1(8B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7863</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5620</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AssemblyAI+Llama3.1(8B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.9192</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5452</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Diarizen+whisper+Mistral(7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7665</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5490</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Diarizen+whisper+Mistral(24B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8068</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6544</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F5F0FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F5F0FF;\">E2E Speech LLM</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio(7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0602</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.4960</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MistralAI-Voxtral(24B)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8375</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5169</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F5F0FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F5F0FF;\">E2E Multimodal LLM</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni(7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6883</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5071</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen3-Omni(30B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7762</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5760</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cascaded",
            "models",
            "while",
            "task2",
            "assemblyaillama318b",
            "evaluation",
            "task1",
            "e2e",
            "diarizenwhisperllama318b",
            "higherthebetter",
            "based",
            "scriptgoldmistral7b",
            "qwen3omni30b",
            "judgments",
            "llm",
            "sdasrllm",
            "accuracy",
            "diarizenwhispermistral24b",
            "scores",
            "scriptgoldllama318b",
            "comparing",
            "qwen2audio7b",
            "metrics",
            "speechmultimodal",
            "both",
            "results",
            "asr",
            "speech",
            "m3slu",
            "mistralaivoxtral24b",
            "values",
            "benchmark",
            "truefalse",
            "multimodal",
            "task",
            "obtained",
            "represent",
            "qwen25omni7b",
            "diarizenwhispermistral7b",
            "approach",
            "llmasjudge",
            "correctness"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The top two scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> correspond to the <span class=\"ltx_text ltx_font_italic\">GT Script(Gold)</span> configuration, where the model was provided with the original ground-truth transcription that had been perfectly segmented by speakers.</p>\n\n",
            "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n",
            "<p class=\"ltx_p\">In the lower part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, E2E Speech-LLM and Multimodal LLM models show relatively lower performance on Task 1, compared to the cascade setting. Only the larger models, such as MistralAI-Voxtral (24B) and Qwen3-Omni (30B), achieved scores approaching 80%, indicating that <span class=\"ltx_text ltx_font_bold\">our M3-SLU Task 1 remains a challenging problem for current E2E Speech-LLMs and MLLMs</span>.</p>\n\n",
            "<p class=\"ltx_p\">As shown on the right side of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the Task 2 results reveal that no model configurations exceeded 70%, a surprisingly low performance considering that the Task 2 is binary (True/False) classification. This suggests that <span class=\"ltx_text ltx_font_bold\">accurate speaker-attributed utterance matching is impossible for both cascade and E2E models under the current framework</span>. Even in the cascade setting with gold transcripts provided, the models failed to accurately distinguish speakers. The best result was obtained with the Diarizen + Whisper + Mistral (24B) combination, which achieved only 0.6544 on Task 2.</p>\n\n",
            "<p class=\"ltx_p\">As shown in the comparison between Task 1 and Task 2 results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, models demonstrated a <span class=\"ltx_text ltx_font_bold\">noticeable gap between understanding what was said and who said it</span>. While Task 1 could be partially solved by leveraging contextual cues without explicit speaker distinction, Task 2 inherently required precise speaker identification to match utterances correctly. This means that <span class=\"ltx_text ltx_font_bold\">although current models can comprehend the content of conversations, they remain largely incapable of reasoning about speaker attribution</span>, highlighting the persistent gap toward true multi-speaker understanding. Therefore, advancing and evaluating future MLLMs will require our M3-SLU benchmark as a foundation for genuine multi-speaker understanding.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "cascaded",
                    "models",
                    "benchmark",
                    "while",
                    "multimodal",
                    "metrics",
                    "evaluation",
                    "llmasjudge",
                    "both",
                    "results",
                    "speech",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">M3-SLU: Evaluating Speaker-Attributed Reasoning \n<br class=\"ltx_break\"/>in Multimodal Large Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "models",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal Large Language Models (MLLMs) have begun to blur the boundary between modalities, that is, seeing, hearing, and reasoning. Advances in models such as Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite>, Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib6\" title=\"\">2025</a>)</cite>, and GPT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> have demonstrated how far AI systems can extend their understanding beyond text, seamlessly integrating visual, auditory, and linguistic cues to perceive the world in richer ways. Particularly, Audio-Language Models (ALMs) that integrate auditory representations into large language models, including Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>, Audio Flamingo <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite>, and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, have recently achieved remarkable progress in bridging speech and language understanding. These models enable machines not only to transcribe speech but also to understand intent, summarize dialogues, and generate coherent responses.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their impressive multimodal capabilities, most existing MLLMs still assume single-speaker conditions, achieving strong performance in Automatic Speech Recognition (ASR) but leaving Speaker Diarization (SD) largely unaddressed <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>. Yet, real-world conversations are far more complex than such single-speaker settings. Understanding &#8220;who spoke when and what&#8221; offers a more comprehensive and meaningful perspective on real-world conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib7\" title=\"\">2025</a>)</cite>. In particular, in natural interactions, utterance sequences may be immediately continuous or may overlap temporally; overlapping talk often occurs around turn transitions and is characterized by backchannels, interruptions, and simultaneous first-starts <cite class=\"ltx_cite ltx_citemacro_citep\">(Levinson and Torreira, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib13\" title=\"\">2015</a>; Knudsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib11\" title=\"\">2020</a>; Schegloff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib22\" title=\"\">2000</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multimodal",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding \"Who spoke When and What\" in multi-party conversations is a crucial step toward socially intelligent AI. However, most existing speech benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib31\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite> address speaker-related and general dialogue tasks together, without separately examining the distinct challenges of speaker-centric understanding in real conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>. To close the gap between current MLLM evaluation and real-world conversational complexity, we introduce the <span class=\"ltx_text ltx_font_bold\">M3-SLU Benchmark</span> (<span class=\"ltx_text ltx_font_bold\">M</span>ulti-Speaker, <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Turn, and <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Modal <span class=\"ltx_text ltx_font_bold\">S</span>poken <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding), as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our key contributions:</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "benchmark",
                    "evaluation",
                    "speech",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the M3-SLU benchmark and <span class=\"ltx_text ltx_font_bold\">evaluation framework for MLLMs</span>, designed to measure performance on <span class=\"ltx_text ltx_font_bold\">two simple yet challenging tasks that can only be solved by correctly identifying the speaker</span>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding has advanced beyond mere transcription toward comprehension of spoken language &#8211; capturing both meaning and paralinguistic cues such as prosody and tone. Earlier pipeline systems that linked ASR to NLP models often lost acoustic detail and propagated recognition errors, prompting a shift toward end-to-end architectures that map raw audio directly to semantic representations. This architectural shift has enhanced robustness and enabled deeper speech understanding, as shown by recent models such as SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib35\" title=\"\">2023</a>)</cite>, Salmonn <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib23\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib33\" title=\"\">2025</a>)</cite>, Glm-4-voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib34\" title=\"\">2024</a>)</cite>, and Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib24\" title=\"\">2023</a>)</cite>. These models typically follow two main approaches: (1) Using an audio adaptor, as in Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, or (2) Directly combining an audio encoder with an LLM, as in Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "llm",
                    "both",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paradigm has also paved the way for the emergence of more specialized capabilities, such as the Speaker LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, which captures individual vocal signatures and uses them in reasoning, and the MT-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib15\" title=\"\">2025</a>)</cite>, designed to disentangle and process dialogue from concurrent speakers. Looking forward, the frontier of research is expanding into multimodal domains where models like GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> and Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite> fuse auditory streams with visual data to achieve a contextually richer, more human-aligned interaction.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the performance of Large Audio-Language Models (LALMs) has advanced, developing benchmarks that evaluate their complex speech understanding capabilities has become a major research focus. Early benchmarks in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> such as SLURP <cite class=\"ltx_cite ltx_citemacro_citep\">(Bastianelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib2\" title=\"\">2020</a>)</cite> and VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>)</cite> primarily focused on single-turn, single-speaker intent classification using synthetic or short speech segments (TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>TTS (Text-to-Speech): Speech audio is synthetically generated from written text using a text-to-speech engine.</span></span></span> + RPC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>RPC (Real/Recorded Speech Corpus): Speech audio is naturally recorded from human speakers in real environments.</span></span></span>), laying the groundwork for fundamental speech-language understanding. Subsequent datasets including MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite>, MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib26\" title=\"\">2025a</a>)</cite>, and AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite> expanded their evaluation scope to multi-turn audio-based question and tasks such as intent classification and emotion interpretation, aiming to assess more nuanced aspects of speech understanding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MSU-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite> marked the first benchmark specifically dedicated to the rigorous evaluation of multi-speaker understanding in realistic conversational scenarios. Yet, it still focused on short dialogues and failed to capture reasoning that spans multiple turns and speakers. Building on this trajectory, we propose the <span class=\"ltx_text ltx_font_bold\">M3-SLU</span> benchmark, designed to assess long-form conversational understanding in segments over one minute long. Unlike prior benchmarks, M3-SLU comprises real multi-speaker dialogues lasting between one and three minutes and features speaker-attributed question answering tasks that require identifying concrete nouns (e.g., objects, places, times, numbers, names) instead of abstract intents or emotions.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our M3-SLU benchmark is designed to measure how accurately a model can understand complex multi-speaker conversations and provide appropriate answers to related questions. Therefore, <span class=\"ltx_text ltx_font_bold\">we designed tasks that require the MLLM models to listen to the conversation and distinguish between speakers in order to answer correctly</span>.\nAs illustrated in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S2.F2\" title=\"Figure 2 &#8227; 2.3. Multi-speaker Speech Understanding Benchmarks &#8227; 2. Related Work &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we propose two tasks: <span class=\"ltx_text ltx_font_bold\">Task 1. Speaker-Attributed QA</span> and <span class=\"ltx_text ltx_font_bold\">Task 2. Speaker Attribution Utterance Matching (T/F)</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "task",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3-SLU consists of two core evaluation tasks generated from 4 public multi-speaker dialogue datasets &#8212; CHiME-6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib4\" title=\"\">2020</a>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib3\" title=\"\">2019</a>)</cite>, MultiDialog <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib2\" title=\"\">2024</a>)</cite>, and AMI <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#biba.bib1\" title=\"\">2005</a>)</cite>. <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> consists of real dinner-party recordings captured in noisy environments with overlapping speech and distant microphones, making it ideal for evaluating speech robustness and diarization accuracy. <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> covers multi-topic conversations designed for contextual understanding across diverse domains. <span class=\"ltx_text ltx_font_bold\">MELD</span>, based on the Friends TV series, provides multimodal emotional dialogues that emphasize emotion recognition and sentiment analysis. Finally, <span class=\"ltx_text ltx_font_bold\">AMI</span> includes real business meeting recordings commonly used for summarization and decision-making tasks, capturing realistic multi-speaker interactions in professional settings. (Details and statistics for each dataset are provided in Appendix A.)</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "multimodal",
                    "based",
                    "evaluation",
                    "speech",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Task 1 (QA) and Task 2 (T/F) were constructed from approximately 8k multi-speaker dialogue segments, each longer than one minute. As shown in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T2\" title=\"Table 2 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.T3\" title=\"Table 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we finalized 12,873 challenging data instances (segments) for the benchmark, and each instance involves at least two speakers. And, every data instance must consist of at least two speakers. The two proposed tasks are as follows:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 4-stage hybrid benchmark construction pipeline was implemented like Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.F3\" title=\"Figure 3 &#8227; 3.2. Overview of Benchmarks &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, in which LLM-driven automatic screening and human review were jointly employed to maximize the reliability and accuracy of the generated dataset.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both Task 1 and Task 2, around 8,000 multi-speaker conversation chunks are preprocessed. The original transcripts (ground-truth scripts) of these chunks were provided to GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib10\" title=\"\">2024</a>)</cite>, which automatically generated corresponding question and answer pairs: short noun-phrase answers for Task&#160;1 and True/False statements for Task&#160;2. For each task, a small set of manually created QA examples was also supplied as few-shot guidance.</p>\n\n",
                "matched_terms": [
                    "task",
                    "truefalse",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Task 1, Task 2 additionally required GPT-4o to produce a rationale explaining the reasoning behind each True/False(boolean type) answer. This rationale was used temporarily during the validation process to assess the consistency of each candidate pair, though it was not included in the final benchmark.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The verification process for Task 2 (True/False reasoning) followed the validation and refinement loop illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S3.F5\" title=\"Figure 5 &#8227; Generation. &#8227; 3.3.2. Stage 2. Automated Data Curation Loop &#8227; 3.3. Benchmark Construction Pipeline &#8227; 3. M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Each candidate set (question, Boolean label, and rationale) underwent a three-step validation loop. <span class=\"ltx_text ltx_font_bold\">Step 1</span> verified whether the label was a valid Boolean (\"True\" or \"False\"). <span class=\"ltx_text ltx_font_bold\">Step 2</span> checked whether the question conformed to the required speaker-attributed form (e.g., &#8220;Is the one who &#8230; the same one who &#8230; ?&#8221;), ensuring that the reasoning explicitly involved speaker identity. <span class=\"ltx_text ltx_font_bold\">Step 3</span> evaluated the coherence between the rationale and the answer, filtering out logically inconsistent or semantically irrelevant cases.</p>\n\n",
                "matched_terms": [
                    "task",
                    "truefalse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Samples that failed any step entered the regeneration step, in which GPT-4o regenerated the candidate set. Each regenerated set was re-validated through the same three steps. After repeating the loop three times, we observed no further increase in the proportion of valid samples. So, we obtained a final set of 8,020 True/False labeled question pairs, each aligned with its corresponding audio clip and ready for subsequent human verification.</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "truefalse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-based data pairs (QA, T/F) that pass automated stages are matched to corresponding original audio clips, packaging both text and speech information into complete multimodal datasets for human annotator use.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multimodal",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final and most critical stage ensured the overall integrity and contextual quality of both tasks. In this phase, human annotators manually inspected every remaining instance to identify subtle semantic or contextual errors that automated processes could not capture. For <span class=\"ltx_text ltx_font_bold\">Task&#160;1</span>, annotators carefully examined each question&#8211;answer pair to remove cases where the generated noun-phrase answers were vague or linguistically invalid (e.g., phrases like &#8220;that thing&#8221; or &#8220;something&#8221;), as well as instances where the question already contained the answer, rendering the question meaningless. For <span class=\"ltx_text ltx_font_bold\">Task&#160;2</span>, reviewers focused on verifying speaker attribution and contextual alignment, discarding samples in which the True/False reasoning relied on incorrect or ambiguous speaker identification.</p>\n\n",
                "matched_terms": [
                    "task",
                    "truefalse",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since Task 1 checks noun-phrase matching (QA) and Task 2 judges whether two utterances or actions were made by the same speaker (True/False), we propose distinct evaluation methods for assessing an MLLM&#8217;s speaker-attributed reasoning ability.</p>\n\n",
                "matched_terms": [
                    "task",
                    "truefalse",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional QA evaluation metrics typically rely on Exact Match (EM) and token-level F1 scores. The EM score is the percentage of predictions that match any one of the ground truth answers exactly. The F1 score measures the average overlap between the prediction and ground truth answer <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib20\" title=\"\">2016</a>)</cite>. However, these metrics assume textual inputs and do not account for variations arising from speech or pronunciation differences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "metrics",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So, the new evaluation metric was required for Task 1, which involves listening to speech and identifying the words that appear in the conversation. To address this, we adopted an LLM-as-a-Judge approach, in which GPT-4o evaluated model outputs by considering semantic similarity and phonetic plausibility, a strategy inspired by the evaluation framework in AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T5\" title=\"Table 5 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this allowed for minor pronunciation- or transcription-related variations&#8212;such as &#8220;NITE XML&#8221; vs. &#8220;Night XML&#8221;&#8212;to be accepted as correct, ensuring a more human-aligned and speech-aware assessment of answer quality. We conducted an LLM-as-a-Judge evaluation using prompts similar to the prompt of <cite class=\"ltx_cite ltx_citemacro_cite\">Badshah and Sajjad (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib1\" title=\"\">2024</a>)</cite>. The final evaluation score was defined as the proportion of samples that GPT-4o (LLM-as-Judge) evaluated as Correct among all instances.</p>\n\n",
                "matched_terms": [
                    "task",
                    "approach",
                    "evaluation",
                    "llmasjudge",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Task&#160;2, each prediction was evaluated using the standard <span class=\"ltx_text ltx_font_bold\">Accuracy</span> metric, which measures the proportion of correctly classified True/False label.</p>\n\n",
                "matched_terms": [
                    "task",
                    "truefalse",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "cascaded",
                    "llm",
                    "asr",
                    "e2e",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, we evaluated cascade SD+ASR pipelines on our audio clips in M3-SLU. In particular, we used <span class=\"ltx_text ltx_font_bold\">Pyannote 3.1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bredin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib3\" title=\"\">2020</a>)</cite> and <span class=\"ltx_text ltx_font_bold\">DiariZen</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib9\" title=\"\">2025</a>)</cite> as speaker diarization (SD) modules, both widely recognized for their strong performance on English conversational audio. These were combined with <span class=\"ltx_text ltx_font_bold\">Whisper</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib19\" title=\"\">2023</a>)</cite> models of varying sizes (Medium and Large) for ASR. To further compare with end-to-end commercial systems, we also included two proprietary SDR models, <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span> and <span class=\"ltx_text ltx_font_bold\">Google STT</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, we measured two metrics, average of <span class=\"ltx_text ltx_font_bold\">WER</span> and average of <span class=\"ltx_text ltx_font_bold\">cpWER</span> for audio clips, to assess whether our benchmark achieves proper SDR performance with existing models. WER (Word Error Rate), commonly used in ASR assessment, measures the proportion of word errors between reference and predicted transcripts <cite class=\"ltx_cite ltx_citemacro_citep\">(Morris et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib16\" title=\"\">2004</a>)</cite>. And cpWER (concatenated minimum-permutation WER) adapts WER for multi-speaker data by optimally permuting speakers&#8217; transcriptions before scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "metrics",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "based",
                    "both",
                    "asr",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU consists of long segments, rather than the short clips (within 30 seconds) used in previous studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, the reported WER and cpWER are naturally higher. This is because longer segments inherently increase the chance of accumulated transcription errors over time, leading to higher WER, while the frequent speaker transitions in extended dialogues also raise cpWER for each instance.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate current models&#8217; speaker-attributed reasoning ability on the M3-SLU benchmark, we adopted both cascade (SD + ASR + LLM) and end-to-end (E2E) MLLM methodologies, following prior approaches in spoken language understanding research <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "llm",
                    "both",
                    "asr",
                    "e2e",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the cascade setting, we first combined speaker diarization (SD) and automatic speech recognition (ASR) models before passing the transcribed text to a large language model (LLM) for question answering. Based on the SDR Test results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> combination exhibited consistently balanced multi-speaker transcription performance across our audio datasets, therefore we adopted this combination as the default SD + ASR configuration. The transcribed text was then passed to LLMs such as <span class=\"ltx_text ltx_font_bold\">Llama3.1-8B</span> and <span class=\"ltx_text ltx_font_bold\">Mistral-7B/24B</span> to investigate the impact of LLM scale. We also included commercial <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span>&#8217;s transcription result to analyze the difference in transcription text quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "based",
                    "llm",
                    "asr",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, we evaluated end-to-end Speech-LLM and Multimodal LLMs, which directly process raw audio inputs without intermediate transcription. For the Speech-LLM evaluation, we tested <span class=\"ltx_text ltx_font_bold\">Qwen2-Audio-7B</span> and <span class=\"ltx_text ltx_font_bold\">Voxtral-Small-24B</span>, while the Multimodal LLM evaluation included <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni-7B</span> and <span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-30B</span>, which jointly handle audio and textual reasoning in a unified framework.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "qwen2audio7b",
                    "while",
                    "qwen3omni30b",
                    "llm",
                    "qwen25omni7b",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As expected, when the <span class=\"ltx_text ltx_font_italic\">GT Script(Gold)</span> was provided to the LLM, this configuration achieved the highest score, since the model received perfectly transcribed and speaker-segmented text, effectively removing any noise or cascading errors during the SD and ASR stages. In addition, the cascade setting (SD + ASR + LLM) also demonstrated reasonably strong QA performance, indicating that despite inevitable errors from the SD + ASR modules, the overall pipeline was still able to preserve a substantial amount of semantic and speaker-related information.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "task",
                    "judgments",
                    "evaluation",
                    "llmasjudge",
                    "results",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced M3-SLU, a benchmark that reveals a key limitation of current MLLMs&#8212;the inability to comprehend \"who spoke when and what\" in long multi-speaker dialogues. Through two targeted tasks, Speaker-Attributed QA and Utterance Matching, M3-SLU isolates the challenge of speaker reasoning beyond simple transcription. Our experiments show that while existing cascaded pipelines and MLLMs can capture what was said, they consistently fail to track who said it, even with accurate transcripts. This underscores a critical gap in speaker attribution and multi-speaker reasoning. Building on real, naturally occurring conversations with speaker-attributed annotations, M3-SLU offers a structured evaluation setting that addresses the limitations of synthetic or short-turn benchmarks. The consistently low performance of current state-of-the-art models across both tasks reflects the complexity of speaker-grounded reasoning, which is unlikely to be resolved through scaling alone. M3-SLU offers a practical testbed for studying how multimodal language models handle multi-speaker conversations in realistic settings. We anticipate that it will guide the development of modeling strategies, evaluation methods, and training practices that explicitly incorporate speaker roles, turn-taking, and conversational structure, all of which are essential to dialogue comprehension.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "cascaded",
                    "models",
                    "multimodal",
                    "while",
                    "evaluation",
                    "both",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our current benchmark is primarily focused on English conversational data. Future work could expand M3-SLU to include a wider range of languages and even more complex, overlapping speech scenarios to further probe the robustness of MLLMs.\nIn addition, our evaluation framework currently relies on GPT-4o as an LLM-as-Judge to assess model outputs. While this approach enables flexible and semantic-level evaluation, it may overlook subtle variations that arise from speech input, such as minor pronunciation or transcription differences. We are actively exploring more refined evaluation strategies that can account for these speech-induced variations while maintaining fairness and consistency across models.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "while",
                    "approach",
                    "evaluation",
                    "llmasjudge",
                    "speech",
                    "m3slu"
                ]
            }
        ]
    },
    "S5.T7": {
        "caption": "Table 7: M3-SLU benchmark results using closed commercial models. Due to limited research resources, both models were evaluated on a randomly selected 100 samples.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M3-SLU Benchmark</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task2</span></th>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F5F0FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F5F0FF;\">Closed Models</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GPT-4o-Audio*</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemini-2.5-Flash-Audio*</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "evaluated",
            "task2",
            "closed",
            "gemini25flashaudio",
            "research",
            "task1",
            "commercial",
            "selected",
            "randomly",
            "resources",
            "gpt4oaudio",
            "due",
            "samples",
            "both",
            "results",
            "m3slu",
            "benchmark",
            "limited"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T7\" title=\"Table 7 &#8227; Evaluation in Closed Models &#8227; 5.4. Results of M3-SLU Benchmark &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> below, current commercial models such as GPT-4o-Audio and Gemini-2.5-Flash-Audio completely fail to perform multi-speaker understanding, indicating that they are still unable to distinguish and reason over different speakers in conversational audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations.\nM3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Multi-Speaker Spoken Language Understanding, Speech-LLM Benchmark, Evaluation</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "evaluated",
                    "research",
                    "both",
                    "results",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">M3-SLU: Evaluating Speaker-Attributed Reasoning \n<br class=\"ltx_break\"/>in Multimodal Large Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding \"Who spoke When and What\" in multi-party conversations is a crucial step toward socially intelligent AI. However, most existing speech benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib31\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite> address speaker-related and general dialogue tasks together, without separately examining the distinct challenges of speaker-centric understanding in real conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>. To close the gap between current MLLM evaluation and real-world conversational complexity, we introduce the <span class=\"ltx_text ltx_font_bold\">M3-SLU Benchmark</span> (<span class=\"ltx_text ltx_font_bold\">M</span>ulti-Speaker, <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Turn, and <span class=\"ltx_text ltx_font_bold\">M</span>ulti-Modal <span class=\"ltx_text ltx_font_bold\">S</span>poken <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding), as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our key contributions:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed M3-SLU benchmark using four open multi-speaker corpora &#8212; <span class=\"ltx_text ltx_font_bold\">CHiME-6</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MELD</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Poria et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib18\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">MultiDialog</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib17\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">AMI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kraaij et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib12\" title=\"\">2005</a>)</cite>, reflecting diverse acoustic conditions and conversational patterns such as overlaps and rapid turns.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the M3-SLU benchmark and <span class=\"ltx_text ltx_font_bold\">evaluation framework for MLLMs</span>, designed to measure performance on <span class=\"ltx_text ltx_font_bold\">two simple yet challenging tasks that can only be solved by correctly identifying the speaker</span>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding has advanced beyond mere transcription toward comprehension of spoken language &#8211; capturing both meaning and paralinguistic cues such as prosody and tone. Earlier pipeline systems that linked ASR to NLP models often lost acoustic detail and propagated recognition errors, prompting a shift toward end-to-end architectures that map raw audio directly to semantic representations. This architectural shift has enhanced robustness and enabled deeper speech understanding, as shown by recent models such as SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib35\" title=\"\">2023</a>)</cite>, Salmonn <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib23\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib33\" title=\"\">2025</a>)</cite>, Glm-4-voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib34\" title=\"\">2024</a>)</cite>, and Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib24\" title=\"\">2023</a>)</cite>. These models typically follow two main approaches: (1) Using an audio adaptor, as in Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib8\" title=\"\">2025</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib14\" title=\"\">2025</a>)</cite>, or (2) Directly combining an audio encoder with an LLM, as in Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paradigm has also paved the way for the emergence of more specialized capabilities, such as the Speaker LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, which captures individual vocal signatures and uses them in reasoning, and the MT-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib15\" title=\"\">2025</a>)</cite>, designed to disentangle and process dialogue from concurrent speakers. Looking forward, the frontier of research is expanding into multimodal domains where models like GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib27\" title=\"\">2025b</a>)</cite> and Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib30\" title=\"\">2025</a>)</cite> fuse auditory streams with visual data to achieve a contextually richer, more human-aligned interaction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the performance of Large Audio-Language Models (LALMs) has advanced, developing benchmarks that evaluate their complex speech understanding capabilities has become a major research focus. Early benchmarks in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> such as SLURP <cite class=\"ltx_cite ltx_citemacro_citep\">(Bastianelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib2\" title=\"\">2020</a>)</cite> and VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib4\" title=\"\">2024</a>)</cite> primarily focused on single-turn, single-speaker intent classification using synthetic or short speech segments (TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>TTS (Text-to-Speech): Speech audio is synthetically generated from written text using a text-to-speech engine.</span></span></span> + RPC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>RPC (Real/Recorded Speech Corpus): Speech audio is naturally recorded from human speakers in real environments.</span></span></span>), laying the groundwork for fundamental speech-language understanding. Subsequent datasets including MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib21\" title=\"\">2024</a>)</cite>, MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib26\" title=\"\">2025a</a>)</cite>, and AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite> expanded their evaluation scope to multi-turn audio-based question and tasks such as intent classification and emotion interpretation, aiming to assess more nuanced aspects of speech understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, MSU-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite> marked the first benchmark specifically dedicated to the rigorous evaluation of multi-speaker understanding in realistic conversational scenarios. Yet, it still focused on short dialogues and failed to capture reasoning that spans multiple turns and speakers. Building on this trajectory, we propose the <span class=\"ltx_text ltx_font_bold\">M3-SLU</span> benchmark, designed to assess long-form conversational understanding in segments over one minute long. Unlike prior benchmarks, M3-SLU comprises real multi-speaker dialogues lasting between one and three minutes and features speaker-attributed question answering tasks that require identifying concrete nouns (e.g., objects, places, times, numbers, names) instead of abstract intents or emotions.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our M3-SLU benchmark is designed to measure how accurately a model can understand complex multi-speaker conversations and provide appropriate answers to related questions. Therefore, <span class=\"ltx_text ltx_font_bold\">we designed tasks that require the MLLM models to listen to the conversation and distinguish between speakers in order to answer correctly</span>.\nAs illustrated in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S2.F2\" title=\"Figure 2 &#8227; 2.3. Multi-speaker Speech Understanding Benchmarks &#8227; 2. Related Work &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we propose two tasks: <span class=\"ltx_text ltx_font_bold\">Task 1. Speaker-Attributed QA</span> and <span class=\"ltx_text ltx_font_bold\">Task 2. Speaker Attribution Utterance Matching (T/F)</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the benchmark embodies its core characteristics of multi-speaker and multi-turn conversations, we carefully selected and collected high-quality public datasets(CHIME-6, MultiDialog, MELD, and AMI). Long dialogue recordings, mostly around or over one hour in length, were segmented into semantically coherent conversation units, each lasting between one and three and a half minutes. Only samples containing two or more speakers were retained, resulting in <span class=\"ltx_text ltx_font_bold\">refined long multi-speaker conversation chunks</span> that serve as the raw materials for the second-stage processing engine.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "samples",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final and most critical stage ensured the overall integrity and contextual quality of both tasks. In this phase, human annotators manually inspected every remaining instance to identify subtle semantic or contextual errors that automated processes could not capture. For <span class=\"ltx_text ltx_font_bold\">Task&#160;1</span>, annotators carefully examined each question&#8211;answer pair to remove cases where the generated noun-phrase answers were vague or linguistically invalid (e.g., phrases like &#8220;that thing&#8221; or &#8220;something&#8221;), as well as instances where the question already contained the answer, rendering the question meaningless. For <span class=\"ltx_text ltx_font_bold\">Task&#160;2</span>, reviewers focused on verifying speaker attribution and contextual alignment, discarding samples in which the True/False reasoning relied on incorrect or ambiguous speaker identification.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So, the new evaluation metric was required for Task 1, which involves listening to speech and identifying the words that appear in the conversation. To address this, we adopted an LLM-as-a-Judge approach, in which GPT-4o evaluated model outputs by considering semantic similarity and phonetic plausibility, a strategy inspired by the evaluation framework in AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib25\" title=\"\">2024</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T5\" title=\"Table 5 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this allowed for minor pronunciation- or transcription-related variations&#8212;such as &#8220;NITE XML&#8221; vs. &#8220;Night XML&#8221;&#8212;to be accepted as correct, ensuring a more human-aligned and speech-aware assessment of answer quality. We conducted an LLM-as-a-Judge evaluation using prompts similar to the prompt of <cite class=\"ltx_cite ltx_citemacro_cite\">Badshah and Sajjad (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib1\" title=\"\">2024</a>)</cite>. The final evaluation score was defined as the proportion of samples that GPT-4o (LLM-as-Judge) evaluated as Correct among all instances.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "evaluated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we conducted an Speaker Diarization and Recognition(SDR) Test to verify whether the audio clips in M3-SLU can be accurately transcribed with correct speaker attribution by existing models. Then, we evaluated the M3-SLU benchmark to assess the capability of current E2E MLLMs and cascaded SD + ASR + LLM pipelines in performing speaker-attributed reasoning across multi-speaker dialogues.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "evaluated",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>)</cite>, we evaluated cascade SD+ASR pipelines on our audio clips in M3-SLU. In particular, we used <span class=\"ltx_text ltx_font_bold\">Pyannote 3.1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bredin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib3\" title=\"\">2020</a>)</cite> and <span class=\"ltx_text ltx_font_bold\">DiariZen</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib9\" title=\"\">2025</a>)</cite> as speaker diarization (SD) modules, both widely recognized for their strong performance on English conversational audio. These were combined with <span class=\"ltx_text ltx_font_bold\">Whisper</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib19\" title=\"\">2023</a>)</cite> models of varying sizes (Medium and Large) for ASR. To further compare with end-to-end commercial systems, we also included two proprietary SDR models, <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span> and <span class=\"ltx_text ltx_font_bold\">Google STT</span>.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "evaluated",
                    "both",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, we measured two metrics, average of <span class=\"ltx_text ltx_font_bold\">WER</span> and average of <span class=\"ltx_text ltx_font_bold\">cpWER</span> for audio clips, to assess whether our benchmark achieves proper SDR performance with existing models. WER (Word Error Rate), commonly used in ASR assessment, measures the proportion of word errors between reference and predicted transcripts <cite class=\"ltx_cite ltx_citemacro_citep\">(Morris et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib16\" title=\"\">2004</a>)</cite>. And cpWER (concatenated minimum-permutation WER) adapts WER for multi-speaker data by optimally permuting speakers&#8217; transcriptions before scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Watanabe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib29\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the SDR performance of different models across four multi-speaker dialogue audio sets in M3-SLU. Among the open cascade SD + ASR pipelines, <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> consistently achieved the most balanced WER and cpWER. Based on this observation, we adopted this Diarizen + Whisper-Medium configuration as our default SD + ASR setting and integrated it with LLMs to conduct the final experiments of our benchmark. Also, closed SDR models such as AssemblyAI achieved further reductions in both WER and cpWER on MultiDialog audio sets.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "closed",
                    "both",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate current models&#8217; speaker-attributed reasoning ability on the M3-SLU benchmark, we adopted both cascade (SD + ASR + LLM) and end-to-end (E2E) MLLM methodologies, following prior approaches in spoken language understanding research <cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#bib.bib28\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "m3slu",
                    "research",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the cascade setting, we first combined speaker diarization (SD) and automatic speech recognition (ASR) models before passing the transcribed text to a large language model (LLM) for question answering. Based on the SDR Test results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the <span class=\"ltx_text ltx_font_bold\">Diarizen + Whisper-Medium</span> combination exhibited consistently balanced multi-speaker transcription performance across our audio datasets, therefore we adopted this combination as the default SD + ASR configuration. The transcribed text was then passed to LLMs such as <span class=\"ltx_text ltx_font_bold\">Llama3.1-8B</span> and <span class=\"ltx_text ltx_font_bold\">Mistral-7B/24B</span> to investigate the impact of LLM scale. We also included commercial <span class=\"ltx_text ltx_font_bold\">AssemblyAI</span>&#8217;s transcription result to analyze the difference in transcription text quality.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the upper part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results demonstrate how the quality of the SD + ASR pipeline directly influences the final QA performance in the cascade setting. Although both settings use the same LLM (Llama 3.1-8B), AssemblyAI + Llama 3.1-8B achieves 0.9192, substantially outperforming Diarizen + Whisper + Llama 3.1-8B (0.7863). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S4.T4\" title=\"Table 4 &#8227; Task 1 (QA) Evaluation &#8227; 4. Evaluation of M3-SLU Benchmark &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, AssemblyAI exhibits remarkably higher transcription accuracy, particularly on the MultiDialog Audio dataset, showing a clear margin over the Diarizen + Whisper-Medium results. That is, <span class=\"ltx_text ltx_font_bold\">more precise transcriptions and speaker labels allow the LLM to better understand who said what</span>, thereby yielding QA results nearly comparable to those obtained with gold transcriptions (GT Script + LLM). Also, the performance difference between Mistral-7B (0.7665) and Mistral-24B (0.8068) shows that increasing the model size leads to a moderate improvement in Task 1 results, suggesting that <span class=\"ltx_text ltx_font_bold\">larger LLM better leverage the transcribed and diarized input for understanding speaker-attributed content</span>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the lower part of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, E2E Speech-LLM and Multimodal LLM models show relatively lower performance on Task 1, compared to the cascade setting. Only the larger models, such as MistralAI-Voxtral (24B) and Qwen3-Omni (30B), achieved scores approaching 80%, indicating that <span class=\"ltx_text ltx_font_bold\">our M3-SLU Task 1 remains a challenging problem for current E2E Speech-LLMs and MLLMs</span>.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown on the right side of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the Task 2 results reveal that no model configurations exceeded 70%, a surprisingly low performance considering that the Task 2 is binary (True/False) classification. This suggests that <span class=\"ltx_text ltx_font_bold\">accurate speaker-attributed utterance matching is impossible for both cascade and E2E models under the current framework</span>. Even in the cascade setting with gold transcripts provided, the models failed to accurately distinguish speakers. The best result was obtained with the Diarizen + Whisper + Mistral (24B) combination, which achieved only 0.6544 on Task 2.</p>\n\n",
                "matched_terms": [
                    "models",
                    "both",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the comparison between Task 1 and Task 2 results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19358v1#S5.T6\" title=\"Table 6 &#8227; 5.3. Experiment Setting for M3-SLU Benchmark Evaluation &#8227; 5. Experiments and Results &#8227; M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, models demonstrated a <span class=\"ltx_text ltx_font_bold\">noticeable gap between understanding what was said and who said it</span>. While Task 1 could be partially solved by leveraging contextual cues without explicit speaker distinction, Task 2 inherently required precise speaker identification to match utterances correctly. This means that <span class=\"ltx_text ltx_font_bold\">although current models can comprehend the content of conversations, they remain largely incapable of reasoning about speaker attribution</span>, highlighting the persistent gap toward true multi-speaker understanding. Therefore, advancing and evaluating future MLLMs will require our M3-SLU benchmark as a foundation for genuine multi-speaker understanding.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "benchmark",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since M3-SLU Task 1 involves predicting noun phrases from audio inputs, we adopted an LLM-as-a-Judge evaluation method using GPT-4o. To verify its reliability, we manually compared GPT-4o&#8217;s judgments with human judgments on 200 randomly selected samples. Specifically, we used GT noun answers and predictions from the Diarizen + Whisper-Medium + LLaMA 3.1 (8B) experiment. The results showed 96.5% agreement between GPT-4o and human evaluators, demonstrating that our LLM-as-Judge evaluation is consistent with human judgment and not arbitrarily biased.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "randomly",
                    "samples",
                    "results",
                    "m3slu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced M3-SLU, a benchmark that reveals a key limitation of current MLLMs&#8212;the inability to comprehend \"who spoke when and what\" in long multi-speaker dialogues. Through two targeted tasks, Speaker-Attributed QA and Utterance Matching, M3-SLU isolates the challenge of speaker reasoning beyond simple transcription. Our experiments show that while existing cascaded pipelines and MLLMs can capture what was said, they consistently fail to track who said it, even with accurate transcripts. This underscores a critical gap in speaker attribution and multi-speaker reasoning. Building on real, naturally occurring conversations with speaker-attributed annotations, M3-SLU offers a structured evaluation setting that addresses the limitations of synthetic or short-turn benchmarks. The consistently low performance of current state-of-the-art models across both tasks reflects the complexity of speaker-grounded reasoning, which is unlikely to be resolved through scaling alone. M3-SLU offers a practical testbed for studying how multimodal language models handle multi-speaker conversations in realistic settings. We anticipate that it will guide the development of modeling strategies, evaluation methods, and training practices that explicitly incorporate speaker roles, turn-taking, and conversational structure, all of which are essential to dialogue comprehension.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "both",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our current benchmark is primarily focused on English conversational data. Future work could expand M3-SLU to include a wider range of languages and even more complex, overlapping speech scenarios to further probe the robustness of MLLMs.\nIn addition, our evaluation framework currently relies on GPT-4o as an LLM-as-Judge to assess model outputs. While this approach enables flexible and semantic-level evaluation, it may overlook subtle variations that arise from speech input, such as minor pronunciation or transcription differences. We are actively exploring more refined evaluation strategies that can account for these speech-induced variations while maintaining fairness and consistency across models.</p>\n\n",
                "matched_terms": [
                    "m3slu",
                    "models",
                    "benchmark"
                ]
            }
        ]
    }
}