{
    "S3.T1": {
        "caption": "Table 1: Parameters and instructions used for simulated oracle generation. A simulated oracle sentence after obtaining the nn-th word of an NN-word utterance is generated with the instruction defined in the ℓ\\ell-th row, where the completeness is defined to be r​(t)=n/Nr(t)=n/N.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Progress indicator</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Information provided for generation (as a prompt)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Lv. (</span><math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Completeness (</span><math alttext=\"r(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">t</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r(t)</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">History</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Hint</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Instruction on hint-usage</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"[0,0.5)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">0</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">0.5</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0,0.5)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[0.5,0.65)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">0.5</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">0.65</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0.5,0.65)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Refer only to keywords from the hint string.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[0.65,0.8)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">0.65</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">0.8</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0.65,0.8)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Include content different from the hint.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[0.8,0.95)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">0.8</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">0.95</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0.8,0.95)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Don&#8217;t copy the hint verbatim.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[0.95,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">0.95</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">1</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0.95,1)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Use the hint.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">1</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,1]</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A; No LLM generation performed. &#8220;Hint&#8221; is directly used as the generated text.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text\" style=\"font-size:70%;\">History: All previous words up to timestamp <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> in the session. Hint: Transcription of the next response.</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "copy",
            "word",
            "content",
            "defined",
            "include",
            "history",
            "keywords",
            "indicator",
            "words",
            "row",
            "instructions",
            "instruction",
            "information",
            "oracle",
            "completeness",
            "“hint”",
            "nnth",
            "after",
            "hintusage",
            "llm",
            "directly",
            "generation",
            "ℓellth",
            "r​tnnrtnn",
            "obtaining",
            "from",
            "next",
            "used",
            "sentence",
            "text",
            "timestamp",
            "don’t",
            "r​trt",
            "refer",
            "response",
            "utterance",
            "provided",
            "simulated",
            "hint",
            "ℓell",
            "where",
            "parameters",
            "only",
            "verbatim",
            "previous",
            "nnword",
            "performed",
            "transcription",
            "use",
            "prompt",
            "generated",
            "session",
            "all",
            "string",
            "different",
            "progress"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To generate the simulated oracle text, we use a separate &#8220;simulator&#8221; LLM.\nThe goal is to control how closely the simulator&#8217;s output matches the ground-truth response based on how much of the user&#8217;s input has been heard.\nWe first quantify the completeness of the user&#8217;s input at any given timestep </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by calculating the ratio of words heard so far and denote it as </span>\n  <math alttext=\"r(t)=n(t)/N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mrow>\n            <mi mathsize=\"0.900em\">n</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r(t)=n(t)/N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"n(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">n(t)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the number of words observed before time </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the total number of words in the full utterance.\nBased on this ratio </span>\n  <math alttext=\"r(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r(t)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we determine a hint level </span>\n  <math alttext=\"\\ell(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\ell(t)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from 0 to 5, as defined in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Simulated Oracle Augmentation &#8227; 3 Training Method &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis hint level indicates how we prompt the simulator LLM to generate simulated oracle texts:</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Real-time speech-to-speech (S2S) models excel at generating natural, low-latency conversational responses but often lack deep knowledge and semantic understanding. Conversely, cascaded systems combining automatic speech recognition, a text-based Large Language Model (LLM), and text-to-speech synthesis offer superior knowledge representation at the cost of high latency, which disrupts the flow of natural interaction. This paper introduces a novel hybrid architecture that bridges the gap between these two paradigms. Our framework processes user speech through an S2S transformer for immediate responsiveness while concurrently relaying the query to a powerful back-end LLM. The LLM&#8217;s text-based response is then injected in real time to guide the S2S model&#8217;s speech generation, effectively infusing its output with rich knowledge without the full latency penalty of a cascaded system. We evaluated our method using a speech-synthesized variant of the MT-Bench benchmark that consists of multi-turn question-answering sessions. The results demonstrate that our system substantially outperforms a baseline S2S model in response correctness, approaching that of a cascaded system, while maintaining a latency on par with the baseline.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "response",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, because the inputs and outputs of S2S models are information-rich acoustic signals, auto-regressive modeling presents a fundamental challenge related to model capacity.\nIn other words, unlike in text-only large language models (LLMs), S2S modeling must capture not only the verbal content of speech but also paralinguistic features, such as speaking style, emotion, etc.\nThis creates a fundamental inefficiency for S2S models with regard to knowledge acquisition.\nGiven a similar model size, a text-only LLM can learn more knowledge because its capacity is dedicated solely to text, where information is densely represented. An S2S model, by contrast, must expend significant resources on capturing expensive, non-verbal features.\nSimply scaling the model is not a straightforward solution, as this presents its own challenges in training stability and resource requirements, particularly because an S2S model inference must operate in real time.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "llm",
                    "words",
                    "only",
                    "text",
                    "where",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">On the other hand, a cascaded architecture excels at knowledge integration.\nThis approach first transcribes a user&#8217;s complete utterance using an Automatic Speech Recognition (ASR) model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe resulting text is then fed into a text-based LLM, and the response is synthesized back into speech with a Text-to-Speech (TTS) engine&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe primary advantage of this modular design is knowledge extensibility.\nThat is, the most advanced LLMs can be easily &#8220;plugged in&#8221; to keep the system state-of-the-art.\nHowever, this sequential process introduces latency, a critical drawback in conversational AI.\nBecause the system must wait for the endpoint of the users&#8217; utterances </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before the ASR and LLM can even begin their work, the resulting delay disrupts the natural flow of conversation.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "llm",
                    "utterance",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To bridge the gap between these paradigms, we introduce a novel hybrid architecture that achieves both the low-latency interaction of S2S models and the knowledge extensibility of cascaded systems.\nWe call our architecture the </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">K</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">nowledge-</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ccess </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">M</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">odel </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">xtension (KAME).\nKAME operates as a &#8220;tandem&#8221; system with a front-end S2S model and a back-end text LLM.\nThe front-end model processes the user&#8217;s speech in real time for an immediate response.\nSimultaneously, it streams an interim transcription to the back-end LLM.\nAs the LLM formulates a more knowledgeable response, its text is sent back to the front-end through an &#8220;oracle&#8221; stream.\nThe front-end model is specifically trained to condition its speech output on both its own internal context and this incoming oracle guidance, effectively infusing real-time responsiveness with deep knowledge.\nEvaluations on a speech-synthesized variant of the multi-turn MT-Bench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark confirm our approach&#8217;s effectiveness, showing that KAME achieves conversational quality comparable to the latest cascaded system while preserving the responsiveness of a direct S2S model (see Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "transcription",
                    "llm",
                    "response",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our work is conceptually related to prior research on layered and multi-component architectures for response generation.\nQwen2.5-Omni </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, for example, employs a Thinker-Talker architecture that enables real-time synthesis of the text generated by an LLM module (called &#8220;Thinker&#8221; in the paper).\nWhile a layered architecture is advantageous in terms of response time, this still requires complete queries represented in the text.\nTheir tight integration between &#8220;Thinker&#8221; and &#8220;Talker&#8221; also makes it difficult to switch the back-end &#8220;Thinker&#8221; alone.\nMinions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> realized a cost-efficient response generation in a large-context scenario by splitting the context-handling tasks between a large back-end LLM and local small LMs.\nFor ASR, the deliberation model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs a heterogeneous architecture with two components aiming at correcting the results of a smaller front-end model by using a larger back-end system.\nOur proposed approach follows this same collaborative strategy.\nIts key distinction, however, is that the front-end and back-end components operate on different representations. Concretely, our front-end is an S2S model, while the back-end is a text-based language model.\nThis loose coupling in our proposed architecture is advantageous because KAME can be made back-end agnostic.\nSince frontier LLMs are known to differ widely in their areas of expertise </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, back-end agnosticism enables flexible selection of the back-end LLM and is essential to achieve optimal results in diverse application areas.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "response",
                    "generated",
                    "generation",
                    "different",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the architecture of KAME.\nOur design consists of two modules that operate on distinct time scales: the front-end S2S module runs at the cycle of discrete audio tokens (e.g., 80 ms), while the back-end LLM is updated at a relatively slower cycle (e.g., 100 - 500 ms).\nThe key strength of this architecture is that the front-end S2S can generate responses immediately and can refine them using supervision from the back-end (detailed in the following sections).\nUnlike cascaded approaches, our method allows the two modules to run independently and remain asynchronously connected.\nThis design minimizes initial response latency while improving response quality with the assistance of the back-end LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "llm",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The back-end LLM module consists of a streaming speech-to-text (STT) component and a back-end LLM.\nAs a user speaks, the streaming STT component continuously transcribes their speech and periodically feeds the partial transcripts to the back-end LLM.\nFor each partial transcript it receives, the back-end LLM generates a candidate response and sends it to the front-end module.\nThis architecture allows the front-end model to generate higher-quality, more informed responses by leveraging the superior knowledge and reasoning of the larger back-end LLM in near real time.\nThe data flow of the back-end module is illustrated by the red arrows in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "response",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Specifically, these candidate responses are fed into the front-end S2S transformer as &#8220;oracle&#8221; tokens to guide its output\n</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the term &#8220;oracle&#8221; to refer to the supervision provided by the back-end LLM, not to manual annotations.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nA timing challenge arises because the S2S transformer consumes tokens at fixed intervals (e.g., every 80 ms), while the asynchronous responses from the back-end LLM can vary in length and frequency.\nThis can cause outputs from consecutive calls to overlap.\nTo resolve this, we prioritize the most recent response, as it is generated from a longer, more complete user transcript and is therefore expected to be the most informative.</span>\n</p>\n\n",
                "matched_terms": [
                    "refer",
                    "use",
                    "llm",
                    "response",
                    "generated",
                    "provided",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Building on the three streams in the original Moshi architecture (output audio, inner monologue, and input audio), our KAME architecture introduces a fourth oracle stream.\nThis new stream incorporates the gradually evolving candidate responses (i.e., oracle tokens) from the back-end LLM, providing guidance to the generation process.\nThe complete data flow of the front-end module is depicted by the blue arrows in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "oracle",
                    "llm",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this four-stream system, the input audio stream contains the user&#8217;s discretized speech.\nFrom this, the model generates both the text representation of its speech in the inner monologue stream, and the corresponding discretized audio in the output audio stream.\nOur oracle stream introduces external knowledge from the back-end LLM directly into this process.\nTo effectively integrate this information and overcome timing challenges, the front-end module must be trained using those oracle tokens. We detail a practical method in the following section.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "llm",
                    "directly",
                    "from",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Training a conventional S2S model is straightforward, as any two-party dialogue dataset can be used by treating one user&#8217;s speech as the input and the other&#8217;s as the target response.\nHowever, this simplicity does not apply for KAME.\nOur architecture requires oracle tokens that not only provide external information but also evolve in real time based on partial user speech.\nSince collecting natural conversation data with these specific properties is a significant challenge,\nwe propose a method to create it by converting a standard two-party dialogue dataset into the required format.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "response",
                    "used",
                    "only",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since the oracle text doesn&#8217;t naturally exist, our solution is to simulate it using a standard conversational dataset that already contains a user&#8217;s input and a corresponding ground-truth response.\nThe design principle is to make the simulated oracle mimic how a real-time LLM would behave, for example, its predictions should get progressively better as it learns more of the user&#8217;s speech.\nAs illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 Back-end LLM Module &#8227; 2 Tandem Architecture &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the process works as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "llm",
                    "response",
                    "simulated",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Early Input:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> At the beginning of the user&#8217;s utterance, the simulated oracle text is a general, plausible sentence. This simulates an LLM that has only heard the first few words and is making an educated guess.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "llm",
                    "utterance",
                    "words",
                    "simulated",
                    "only",
                    "sentence",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Progressive Refinement:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> As more of the input utterance is processed, we generate new simulated oracle text that is more specific and closer to the final, correct answer.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "oracle",
                    "utterance",
                    "simulated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Final Convergence:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> By the time the user finishes speaking, the simulated oracle text is designed to converge and become nearly identical to the actual ground-truth response recorded in the dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "oracle",
                    "response",
                    "simulated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Level 0 (<math alttext=\"r(t)\\in[0,0.5)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>0.5</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r(t)\\in[0,0.5)</annotation></semantics></math>):</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> When very little has been heard, the LLM is prompted with only the speech history. It receives no hint and must generate a plausible response from scratch.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "history",
                    "response",
                    "from",
                    "only",
                    "hint"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Level 1-4 (<math alttext=\"r(t)\\in[0.5,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0.5</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r(t)\\in[0.5,1)</annotation></semantics></math>):</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> As the input progresses, the LLM receives both the history and the ground-truth response as &#8220;hint&#8221;, a specific instruction to guide its usage of the history and refine the simulated oracle sentences.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "“hint”",
                    "llm",
                    "history",
                    "response",
                    "simulated",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Level 5 (<math alttext=\"r(t)=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">r(t)=1</annotation></semantics></math>):</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> When the input is complete, no LLM generation is performed. The ground-truth &#8220;hint&#8221; text is used directly as the final simulated oracle sentence.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "“hint”",
                    "performed",
                    "llm",
                    "directly",
                    "generation",
                    "simulated",
                    "used",
                    "sentence",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This hint-level mechanism allows us to generate simulated oracle text that starts as an unguided prediction and smoothly converges to the correct response by gradually relaxing the constraints on how it uses the ground-truth hint.\nMoreover, by employing a TTS system, text-based question answering datasets can also be used to train our KAME system.\nIn our experiments, we collected questions and answers on diverse topics and converted them into a conversational style.\nWe then applied a TTS system and word-aligner to transform the dataset into a form suitable for the procedure above.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "response",
                    "simulated",
                    "used",
                    "hint",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the data generation complete, we can now train the front-end S2S module.\nThe training data consists of audio inputs paired with the corresponding simulated oracle sentences, which are injected at fixed intervals to mimic their real-time arrival.\nSeveral key modifications were made to prepare this data for training:\n(1) We tokenize the simulated oracle sentences using the same tokenizer as the inner monologue stream, ensuring consistency within the model;\n(2) To help the model distinguish between consecutively arriving simulated oracle sentences, each one is prefixed with a dedicated special token that marks its boundary;\n(3) To improve robustness, we add random jitter to the arrival timing of the simulated oracle tokens during training, simulating the small delays and variations that would occur in a live system.\nFor the training objective, we use a combined loss function of text and audio, weighting the audio loss by a factor of 1.5.\nAll other hyper-parameters and settings are identical to the original Moshi paper.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "use",
                    "all",
                    "generation",
                    "simulated",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct the base conversational dataset, we seed from benchmark Q&amp;A pairs and generate synthetic dialogue sessions: 22,800 from 11,996 MMLU-Pro entries&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 11,742 from 7,428 GSM8K entries&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and 22,040 from 12,012 HSSBench entries&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then add simulated oracles to these datasets. We fine-tuned Moshi on the same base conversational data for fair comparison.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "simulated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compared two metrics across these systems: Response latency and answer quality over a subset of the MT-Bench benchmark </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe selected the subset by excluding the question categories that are not suitable for speech interaction.\nSpecifically, &#8220;Coding&#8221;, &#8220;Extraction&#8221;, &#8220;Math&#8221;, &#8220;Roleplay&#8221;, and &#8220;Writing&#8221; are excluded from the evaluation.\nResponse latency is computed by measuring the duration between the end of a user&#8217;s utterance (question) and the beginning of a system&#8217;s utterance (answer).\nFollowing the standard procedure described in the MT-Bench paper, we adopt &#8220;LLM-as-a-Judge&#8221; to measure the quality of the answers.\nAll experiments are conducted six times, and we report the average results.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "from",
                    "response",
                    "utterance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Experimental Setups &#8227; 4 Experiments &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the MT-Bench scores and latencies in our experiments.\nWe observe that our proposed architecture (KAME) significantly outperforms Moshi in question answering.\nThe MT-Bench score improved from 2.05 (Moshi) to 6.43 (KAME) by adding external LLMs as a knowledge-access model.\nThe median latency was confirmed to be the same after introducing the knowledge-access model.\nIn other words, for both Moshi and KAME, for at least half of the sessions, the model starts responding before the questions end.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "after",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In comparison with a cascaded system, our method is significantly faster than Unmute.\nHowever, the MT-Bench score is degraded compared to it.\nThis is mainly because our method starts responding with significantly less information.\nRegardless, we notice that KAME can correct its speech content after getting contradictory oracles from the external model.\nWhile this is a natural and desired behavior in real-world conversations, this also produces redundant expressions in written text that affect the LLM-as-a-Judge evaluation scheme, and result in a degradation in the MT-Bench score.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "after",
                    "from",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In principle, a cascaded system should be able to leverage the full capability of the back-end LLMs if there are no speech recognition errors.\nOn the other hand, our system can be degraded due to the premature response generation.\nTo better understand the capability of the back-end LLMs in isolation, we retrieve sentences of the last oracle injection in each session from the experiments of KAME with GPT-4.1, and evaluate the back-end&#8217;s text responses directly.\nThe results are: reasoning 6.48, STEM 8.34, and humanities 8.56, with an average of 7.79.\nThe cascaded Unmute system (with GPT-4.1) achieves comparable scores, whereas KAME (with GPT-4.1 as the back-end LLM) performs worse.\nThis implies that the main reason for KAME&#8217;s gap to cascaded systems is not the back-end LLM&#8217;s capability but the timing of its early responses.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "llm",
                    "directly",
                    "response",
                    "session",
                    "generation",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the trade-off between response latency and quality, we conducted an experiment where we forced our model to delay its response.\nThis is achieved by compelling the S2S model to output only silence tokens (for audio) and padding tokens (for the inner monologue) until a specified time has passed.\nThe results, shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, plot the MT-Bench scores as a function of this forced delay.\nInitially, delaying the response improves the model&#8217;s quality.\nHowever, the benefit diminishes in higher-latency settings, a limitation we attribute to the lack of long-pause examples in our training.\nThis could be mitigated by corresponding data expansion and curation in the future.\nMost importantly, this analysis verifies that the performance gap to a cascaded model is due to premature generation, confirming that KAME strikes an effective balance between the competing demands of low latency and high-quality output.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "only",
                    "where",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduced KAME, a novel tandem architecture for real-time conversational AI.\nKAME successfully integrates the advanced capabilities of an LLM with the low latency of a direct S2S system via the introduction of oracle tokens.\nWe also proposed practical training methods for this architecture, including a technique for generating the necessary synthetic oracle data.</span>\n</p>\n\n",
                "matched_terms": [
                    "oracle",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our experiments show that KAME significantly outperforms conventional approaches.\nCompared to a cascaded system, KAME dramatically reduces latency while keeping quality degradation minimal.\nConversely, compared to a standard S2S model, our method substantially improves response quality without increasing latency.\nOur results demonstrate that this tandem architecture provides an effective and balanced solution for building powerful, real-time conversational AI.\nAn interesting challenge for future work is to extend KAME to support scenarios where the front-end model alone does not perform well, such as multi-party conversations.</span>\n</p>\n\n",
                "matched_terms": [
                    "where",
                    "response"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: MT-Bench scores. Scores over reasoning (reas.), STEM (stem), humanities (human.) subsets of MT-Bench. Turn one and two scores are averaged. The “avg.” column contains the averages of all scores of the subsets extracted. “latency” is the median response latency in seconds.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">reas.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">stem</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">human.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">avg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">latency</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Moshi</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">KAME (Ours)</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/ GPT-4.1</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.44</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/ Claude-opus-4.1</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Unmute (w/ GPT-4.1)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "claudeopus41",
            "column",
            "seconds",
            "unmute",
            "moshi",
            "extracted",
            "subsets",
            "humanities",
            "two",
            "gpt41",
            "reas",
            "turn",
            "“latency”",
            "scores",
            "over",
            "mtbench",
            "avg",
            "averaged",
            "response",
            "one",
            "reasoning",
            "averages",
            "stem",
            "ours",
            "all",
            "kame",
            "“avg”",
            "latency",
            "human",
            "method",
            "contains",
            "median"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Experimental Setups &#8227; 4 Experiments &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the MT-Bench scores and latencies in our experiments.\nWe observe that our proposed architecture (KAME) significantly outperforms Moshi in question answering.\nThe MT-Bench score improved from 2.05 (Moshi) to 6.43 (KAME) by adding external LLMs as a knowledge-access model.\nThe median latency was confirmed to be the same after introducing the knowledge-access model.\nIn other words, for both Moshi and KAME, for at least half of the sessions, the model starts responding before the questions end.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Real-time speech-to-speech (S2S) models excel at generating natural, low-latency conversational responses but often lack deep knowledge and semantic understanding. Conversely, cascaded systems combining automatic speech recognition, a text-based Large Language Model (LLM), and text-to-speech synthesis offer superior knowledge representation at the cost of high latency, which disrupts the flow of natural interaction. This paper introduces a novel hybrid architecture that bridges the gap between these two paradigms. Our framework processes user speech through an S2S transformer for immediate responsiveness while concurrently relaying the query to a powerful back-end LLM. The LLM&#8217;s text-based response is then injected in real time to guide the S2S model&#8217;s speech generation, effectively infusing its output with rich knowledge without the full latency penalty of a cascaded system. We evaluated our method using a speech-synthesized variant of the MT-Bench benchmark that consists of multi-turn question-answering sessions. The results demonstrate that our system substantially outperforms a baseline S2S model in response correctness, approaching that of a cascaded system, while maintaining a latency on par with the baseline.</span>\n</p>\n\n",
                "matched_terms": [
                    "mtbench",
                    "response",
                    "latency",
                    "method",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">On the other hand, a cascaded architecture excels at knowledge integration.\nThis approach first transcribes a user&#8217;s complete utterance using an Automatic Speech Recognition (ASR) model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe resulting text is then fed into a text-based LLM, and the response is synthesized back into speech with a Text-to-Speech (TTS) engine&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe primary advantage of this modular design is knowledge extensibility.\nThat is, the most advanced LLMs can be easily &#8220;plugged in&#8221; to keep the system state-of-the-art.\nHowever, this sequential process introduces latency, a critical drawback in conversational AI.\nBecause the system must wait for the endpoint of the users&#8217; utterances </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before the ASR and LLM can even begin their work, the resulting delay disrupts the natural flow of conversation.</span>\n</p>\n\n",
                "matched_terms": [
                    "latency",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To bridge the gap between these paradigms, we introduce a novel hybrid architecture that achieves both the low-latency interaction of S2S models and the knowledge extensibility of cascaded systems.\nWe call our architecture the </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">K</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">nowledge-</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ccess </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">M</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">odel </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">xtension (KAME).\nKAME operates as a &#8220;tandem&#8221; system with a front-end S2S model and a back-end text LLM.\nThe front-end model processes the user&#8217;s speech in real time for an immediate response.\nSimultaneously, it streams an interim transcription to the back-end LLM.\nAs the LLM formulates a more knowledgeable response, its text is sent back to the front-end through an &#8220;oracle&#8221; stream.\nThe front-end model is specifically trained to condition its speech output on both its own internal context and this incoming oracle guidance, effectively infusing real-time responsiveness with deep knowledge.\nEvaluations on a speech-synthesized variant of the multi-turn MT-Bench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark confirm our approach&#8217;s effectiveness, showing that KAME achieves conversational quality comparable to the latest cascaded system while preserving the responsiveness of a direct S2S model (see Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "mtbench",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our work is conceptually related to prior research on layered and multi-component architectures for response generation.\nQwen2.5-Omni </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, for example, employs a Thinker-Talker architecture that enables real-time synthesis of the text generated by an LLM module (called &#8220;Thinker&#8221; in the paper).\nWhile a layered architecture is advantageous in terms of response time, this still requires complete queries represented in the text.\nTheir tight integration between &#8220;Thinker&#8221; and &#8220;Talker&#8221; also makes it difficult to switch the back-end &#8220;Thinker&#8221; alone.\nMinions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> realized a cost-efficient response generation in a large-context scenario by splitting the context-handling tasks between a large back-end LLM and local small LMs.\nFor ASR, the deliberation model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs a heterogeneous architecture with two components aiming at correcting the results of a smaller front-end model by using a larger back-end system.\nOur proposed approach follows this same collaborative strategy.\nIts key distinction, however, is that the front-end and back-end components operate on different representations. Concretely, our front-end is an S2S model, while the back-end is a text-based language model.\nThis loose coupling in our proposed architecture is advantageous because KAME can be made back-end agnostic.\nSince frontier LLMs are known to differ widely in their areas of expertise </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, back-end agnosticism enables flexible selection of the back-end LLM and is essential to achieve optimal results in diverse application areas.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "response",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the rest of this paper, we first present our tandem architecture, which enables real-time communication between the front- and back-end components.\nThen, we describe the detailed training method for our system.\nIn the experimental section, we evaluate KAME in MT-Bench and discuss the results.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "mtbench",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the architecture of KAME.\nOur design consists of two modules that operate on distinct time scales: the front-end S2S module runs at the cycle of discrete audio tokens (e.g., 80 ms), while the back-end LLM is updated at a relatively slower cycle (e.g., 100 - 500 ms).\nThe key strength of this architecture is that the front-end S2S can generate responses immediately and can refine them using supervision from the back-end (detailed in the following sections).\nUnlike cascaded approaches, our method allows the two modules to run independently and remain asynchronously connected.\nThis design minimizes initial response latency while improving response quality with the assistance of the back-end LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "response",
                    "kame",
                    "latency",
                    "method",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The back-end LLM module consists of a streaming speech-to-text (STT) component and a back-end LLM.\nAs a user speaks, the streaming STT component continuously transcribes their speech and periodically feeds the partial transcripts to the back-end LLM.\nFor each partial transcript it receives, the back-end LLM generates a candidate response and sends it to the front-end module.\nThis architecture allows the front-end model to generate higher-quality, more informed responses by leveraging the superior knowledge and reasoning of the larger back-end LLM in near real time.\nThe data flow of the back-end module is illustrated by the red arrows in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Building on the three streams in the original Moshi architecture (output audio, inner monologue, and input audio), our KAME architecture introduces a fourth oracle stream.\nThis new stream incorporates the gradually evolving candidate responses (i.e., oracle tokens) from the back-end LLM, providing guidance to the generation process.\nThe complete data flow of the front-end module is depicted by the blue arrows in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "moshi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this four-stream system, the input audio stream contains the user&#8217;s discretized speech.\nFrom this, the model generates both the text representation of its speech in the inner monologue stream, and the corresponding discretized audio in the output audio stream.\nOur oracle stream introduces external knowledge from the back-end LLM directly into this process.\nTo effectively integrate this information and overcome timing challenges, the front-end module must be trained using those oracle tokens. We detail a practical method in the following section.</span>\n</p>\n\n",
                "matched_terms": [
                    "contains",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Training a conventional S2S model is straightforward, as any two-party dialogue dataset can be used by treating one user&#8217;s speech as the input and the other&#8217;s as the target response.\nHowever, this simplicity does not apply for KAME.\nOur architecture requires oracle tokens that not only provide external information but also evolve in real time based on partial user speech.\nSince collecting natural conversation data with these specific properties is a significant challenge,\nwe propose a method to create it by converting a standard two-party dialogue dataset into the required format.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "one",
                    "response",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since the oracle text doesn&#8217;t naturally exist, our solution is to simulate it using a standard conversational dataset that already contains a user&#8217;s input and a corresponding ground-truth response.\nThe design principle is to make the simulated oracle mimic how a real-time LLM would behave, for example, its predictions should get progressively better as it learns more of the user&#8217;s speech.\nAs illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 Back-end LLM Module &#8227; 2 Tandem Architecture &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the process works as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "contains",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This hint-level mechanism allows us to generate simulated oracle text that starts as an unguided prediction and smoothly converges to the correct response by gradually relaxing the constraints on how it uses the ground-truth hint.\nMoreover, by employing a TTS system, text-based question answering datasets can also be used to train our KAME system.\nIn our experiments, we collected questions and answers on diverse topics and converted them into a conversational style.\nWe then applied a TTS system and word-aligner to transform the dataset into a form suitable for the procedure above.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the data generation complete, we can now train the front-end S2S module.\nThe training data consists of audio inputs paired with the corresponding simulated oracle sentences, which are injected at fixed intervals to mimic their real-time arrival.\nSeveral key modifications were made to prepare this data for training:\n(1) We tokenize the simulated oracle sentences using the same tokenizer as the inner monologue stream, ensuring consistency within the model;\n(2) To help the model distinguish between consecutively arriving simulated oracle sentences, each one is prefixed with a dedicated special token that marks its boundary;\n(3) To improve robustness, we add random jitter to the arrival timing of the simulated oracle tokens during training, simulating the small delays and variations that would occur in a live system.\nFor the training objective, we use a combined loss function of text and audio, weighting the audio loss by a factor of 1.5.\nAll other hyper-parameters and settings are identical to the original Moshi paper.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "one",
                    "moshi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate KAME and compare it with several baselines.\nOur baselines include Moshi </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a representative S2S system, and Unmute </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a pioneering cascaded system.\nWe modified Unmute&#8217;s back-end so that it works with external LLMs for the sake of comparisons.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "unmute",
                    "moshi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compared two metrics across these systems: Response latency and answer quality over a subset of the MT-Bench benchmark </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe selected the subset by excluding the question categories that are not suitable for speech interaction.\nSpecifically, &#8220;Coding&#8221;, &#8220;Extraction&#8221;, &#8220;Math&#8221;, &#8220;Roleplay&#8221;, and &#8220;Writing&#8221; are excluded from the evaluation.\nResponse latency is computed by measuring the duration between the end of a user&#8217;s utterance (question) and the beginning of a system&#8217;s utterance (answer).\nFollowing the standard procedure described in the MT-Bench paper, we adopt &#8220;LLM-as-a-Judge&#8221; to measure the quality of the answers.\nAll experiments are conducted six times, and we report the average results.</span>\n</p>\n\n",
                "matched_terms": [
                    "over",
                    "mtbench",
                    "response",
                    "all",
                    "latency",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">KAME also proves to be back-end agnostic.\nOur training was done using GPT-4.1-nano as a primary back-end, however, the KAME&#8217;s two rows indicate that swapping the back-end models does not hurt the performance significantly.\nWe also observe that the category-wise relative advantages differ depending on the back-end LLM choice.\nWith KAME, one can choose the optimal back-end model depending on the context in which it would be deployed.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "one",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In comparison with a cascaded system, our method is significantly faster than Unmute.\nHowever, the MT-Bench score is degraded compared to it.\nThis is mainly because our method starts responding with significantly less information.\nRegardless, we notice that KAME can correct its speech content after getting contradictory oracles from the external model.\nWhile this is a natural and desired behavior in real-world conversations, this also produces redundant expressions in written text that affect the LLM-as-a-Judge evaluation scheme, and result in a degradation in the MT-Bench score.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "mtbench",
                    "unmute",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In principle, a cascaded system should be able to leverage the full capability of the back-end LLMs if there are no speech recognition errors.\nOn the other hand, our system can be degraded due to the premature response generation.\nTo better understand the capability of the back-end LLMs in isolation, we retrieve sentences of the last oracle injection in each session from the experiments of KAME with GPT-4.1, and evaluate the back-end&#8217;s text responses directly.\nThe results are: reasoning 6.48, STEM 8.34, and humanities 8.56, with an average of 7.79.\nThe cascaded Unmute system (with GPT-4.1) achieves comparable scores, whereas KAME (with GPT-4.1 as the back-end LLM) performs worse.\nThis implies that the main reason for KAME&#8217;s gap to cascaded systems is not the back-end LLM&#8217;s capability but the timing of its early responses.</span>\n</p>\n\n",
                "matched_terms": [
                    "stem",
                    "gpt41",
                    "response",
                    "unmute",
                    "kame",
                    "humanities",
                    "reasoning",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the trade-off between response latency and quality, we conducted an experiment where we forced our model to delay its response.\nThis is achieved by compelling the S2S model to output only silence tokens (for audio) and padding tokens (for the inner monologue) until a specified time has passed.\nThe results, shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02327v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; KAME: TANDEM ARCHITECTURE FOR ENHANCING KNOWLEDGE IN REAL-TIME SPEECH-TO-SPEECH CONVERSATIONAL AI\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, plot the MT-Bench scores as a function of this forced delay.\nInitially, delaying the response improves the model&#8217;s quality.\nHowever, the benefit diminishes in higher-latency settings, a limitation we attribute to the lack of long-pause examples in our training.\nThis could be mitigated by corresponding data expansion and curation in the future.\nMost importantly, this analysis verifies that the performance gap to a cascaded model is due to premature generation, confirming that KAME strikes an effective balance between the competing demands of low latency and high-quality output.</span>\n</p>\n\n",
                "matched_terms": [
                    "mtbench",
                    "response",
                    "kame",
                    "latency",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduced KAME, a novel tandem architecture for real-time conversational AI.\nKAME successfully integrates the advanced capabilities of an LLM with the low latency of a direct S2S system via the introduction of oracle tokens.\nWe also proposed practical training methods for this architecture, including a technique for generating the necessary synthetic oracle data.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our experiments show that KAME significantly outperforms conventional approaches.\nCompared to a cascaded system, KAME dramatically reduces latency while keeping quality degradation minimal.\nConversely, compared to a standard S2S model, our method substantially improves response quality without increasing latency.\nOur results demonstrate that this tandem architecture provides an effective and balanced solution for building powerful, real-time conversational AI.\nAn interesting challenge for future work is to extend KAME to support scenarios where the front-end model alone does not perform well, such as multi-party conversations.</span>\n</p>\n\n",
                "matched_terms": [
                    "kame",
                    "latency",
                    "response",
                    "method"
                ]
            }
        ]
    }
}