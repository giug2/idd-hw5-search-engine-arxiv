{
    "S4.T1": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 1: Speech training datasets with total speech hours and the amount of Hubert tokens.",
        "body": "Dataset\nHours\nHubert Tokens (B)\n\n\n\n\nLibriLight (Kahn et al., 2020)\n\n44,174\n3.7\n\n\nPeople Speech (Galvez et al., 2021)\n\n14,699\n1.2\n\n\nMultilingual LibriSpeech (Pratap et al., 2020)\n\n50,601\n4.2\n\n\nSpotify (Clifton et al., 2020)\n\n55,309\n4.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hubert Tokens (B)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LibriLight <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44,174</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">People Speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Galvez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib10\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">14,699</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Multilingual LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib26\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">50,601</td>\n<td class=\"ltx_td ltx_align_center\">4.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Spotify <cite class=\"ltx_cite ltx_citemacro_citep\">(Clifton et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib5\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">55,309</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "training",
            "multilingual",
            "pratap",
            "librilight",
            "hubert",
            "kahn",
            "librispeech",
            "amount",
            "dataset",
            "total",
            "clifton",
            "datasets",
            "hours",
            "spotify",
            "people",
            "tokens",
            "galvez"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech.</span>\nOur speech training data includes speech which is discretized into HuBERT tokens (501-entry codebook at 25Hz) together with paired text transcriptions. We use LibriLight (60k hours), People&#8217;s Speech (30k hours), Multilingual LibriSpeech (50k hours), and Spotify (60k hours), detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Training and Evaluation Datasets &#8227; 4 Experimental Setup &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All corpora are aligned using the Wav2Vec2 + CTC framework to provide token-level correspondence between speech and text (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "amount"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hubert",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}</annotation></semantics></math> be speech token embeddings obtained using a learned embedding matrix applied to speech tokens <math alttext=\"\\{s_{0},\\dots,s_{T}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{T}\\}</annotation></semantics></math> .\nThe process of patching maps <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> to a shorter sequence of patch embeddings\n<math alttext=\"\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>z</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math> by aggregating local frame segments. For a frame-index set <math alttext=\"\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo>&#8838;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}</annotation></semantics></math>, a patch embedding is formed via the local encoder:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment Patching.</span>\nTo better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let <math alttext=\"\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}</annotation></semantics></math> denotes the aligned frame ranges, where <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math> spans the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th textual unit.\nThe corresponding patch is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our pre-training data comprises a mixture of text and interleaved speech datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text.</span> Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite>, totaling 1.8T tokens. We follow the LLaMA&#160;2 setup and apply its SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> BPE tokenizer with a 32K vocabulary.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze and Topic StoryCloze (SC/TSC).</span> SC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite> and its topic-based extension TSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> are widely used in prior multimodal work (e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a></cite>) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "hubert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned Patching.</span> Uses Wav2Vec2+CTC boundaries (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). For each text span <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math>, we form patch set <math alttext=\"\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}</annotation></semantics></math>, synchronizing speech and text tokens (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base SpeechLLM.</span> Processes speech tokens directly with text tokens, without patching, similar to SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To balance modalities, we set speech tokens to account for about one third (33%) of the total training data, while the rest (67%) is text-only. This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.\nFor comparison, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> adopts a different composition: 33% pure speech, 33% interleaved, and 33% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33% interleaved and 66% text tokens yields better performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS4\" title=\"A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "total",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hubert",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "amount"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "kahn",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work adheres to the ICLR Code of Ethics. We use only publicly available datasets (e.g., LibriLight, People&#8217;s Speech, Multilingual LibriSpeech, Spotify) and did not collect any new human-subject data. No personally identifiable information is included. Our methods aim to improve efficiency and generalization in speech&#8211;text modeling, with potential positive impacts on accessibility and multilingual applications. As with all large language models, there remain general risks of misuse, and we encourage responsible use of our work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "librilight",
                    "librispeech",
                    "datasets",
                    "spotify",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span> We obtain alignment information by using Wav2Vec2 + CTC to determine the boundaries linking text tokens to their corresponding spans of speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Span selection.</span> For each training example, we randomly select a contiguous span of words. The selected span is replaced by text tokens, while the following span of approximately half that length is kept as speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality markers.</span> We insert special tokens <span class=\"ltx_text ltx_font_typewriter\">&lt;t&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;s&gt;</span> to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the model using the AdamW optimizer (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, weight decay = 0.1). The learning rate was initialized at <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math> and scheduled with cosine decay, including a warmup period of 2,000 steps and a minimum ratio of 0.01 at the final step. For the 1B model, training was performed on 32 H100 GPUs with a per-GPU batch size of 4 sequences (sequence length = 4,096 units), leading to a total batch size of 0.5M units. Mixed-precision training with bfloat16 was used for efficiency. Gradient clipping was applied at 1.0, and gradient accumulation was set to 1. Model parallelism used a single partition, and Fully Sharded Data Parallel (FSDP) was enabled for memory efficiency. No dropout was applied.\nThe 1B model was trained for 200k steps, corresponding to approximately 1 trillion units, and required around 17 hours to complete on 32 H100 GPUs.</p>\n\n",
                "matched_terms": [
                    "total",
                    "training",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 2: Evaluation datasets for story completion (MC = Multiple Choice).",
        "body": "Dataset\nFormat\nFocus\n\n\n\n\nHellaSwag (Zellers et al., 2019)\n\n1-in-4 MC\nCommonsense reasoning\n\n\nStoryCloze (Mostafazadeh et al., 2016)\n\n1-in-2 MC\nNarrative coherence\n\n\nTopicStoryCloze (Hassid et al., 2023)\n\n1-in-2 MC\nTopic consistency",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Format</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Focus</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1-in-4 MC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Commonsense reasoning</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">1-in-2 MC</td>\n<td class=\"ltx_td ltx_align_center\">Narrative coherence</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">TopicStoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1-in-2 MC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Topic consistency</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reasoning",
            "coherence",
            "commonsense",
            "topic",
            "datasets",
            "format",
            "topicstorycloze",
            "completion",
            "1in2",
            "zellers",
            "focus",
            "mostafazadeh",
            "storycloze",
            "choice",
            "hassid",
            "narrative",
            "evaluation",
            "multiple",
            "dataset",
            "hellaswag",
            "consistency",
            "1in4",
            "story"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "story",
                    "hellaswag",
                    "evaluation",
                    "completion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "hassid",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "zellers",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next describe the datasets, models, and evaluation protocols used in our experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "coherence",
                    "narrative",
                    "commonsense",
                    "evaluation",
                    "dataset",
                    "topic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">sHellaSWAG (HS).</span> We create a speech version of HellaSwag&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite> with Kokoro TTS. This benchmark evaluates everyday commonsense reasoning with spoken inputs and outputs. To ensure fairness, we generate the speech for prompts and responses independently and concatenate them afterwards, so that all responses are evaluated against the same speech prompt.</p>\n\n",
                "matched_terms": [
                    "zellers",
                    "reasoning",
                    "commonsense",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze and Topic StoryCloze (SC/TSC).</span> SC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite> and its topic-based extension TSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> are widely used in prior multimodal work (e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a></cite>) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.</p>\n\n",
                "matched_terms": [
                    "mostafazadeh",
                    "storycloze",
                    "hassid",
                    "reasoning",
                    "coherence",
                    "topic",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We omit sWUGGY and sBLiMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite>, as they target lexical and syntactic judgments on very short speech segments. Such settings are less aligned with our focus on narrative reasoning, where story-level coherence and commonsense understanding are required.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "coherence",
                    "narrative",
                    "commonsense",
                    "focus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "evaluation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag",
                    "storycloze"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "hellaswag",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "hassid",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hassid",
                    "reasoning",
                    "completion",
                    "story",
                    "commonsense",
                    "hellaswag",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag",
                    "storycloze"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag",
                    "storycloze"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 3: Main comparison of LST models and baselines under the same computation budget scheme. Each dataset reports both S→\\rightarrowS and T→\\rightarrowT.",
        "body": "Model\nTokens (B)\nHellaSwag\nStoryCloze\nTopicStoryCloze\n\n\n\nInt.\nText\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\n\n\n\n\nBase SpeechLLM\n87\n175\n39.0\n47.0\n59.1\n67.8\n87.5\n95.7\n\n\nBPE SpeechLLM\n95\n190\n38.0\n47.5\n58.0\n66.4\n87.0\n93.5\n\n\nLST (Static)\n108\n217\n44.3\n51.1\n60.5\n70.3\n87.7\n96.2\n\n\nLST (Aligned)\n108\n217\n42.7\n51.7\n60.4\n70.4\n86.6\n95.7\n\n\nLST (Mixed)\n108\n217\n44.3\n51.9\n61.4\n70.8\n88.0\n95.9\n\n\nLST (Curriculum)\n108\n217\n45.5\n52.2\n61.2\n71.6\n87.9\n96.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Tokens (B)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">StoryCloze</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">TopicStoryCloze</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Int.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Base SpeechLLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">175</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">67.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BPE SpeechLLM</td>\n<td class=\"ltx_td ltx_align_center\">95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">190</td>\n<td class=\"ltx_td ltx_align_center\">38.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">47.5</td>\n<td class=\"ltx_td ltx_align_center\">58.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">66.4</td>\n<td class=\"ltx_td ltx_align_center\">87.0</td>\n<td class=\"ltx_td ltx_align_center\">93.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Static)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">108</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">51.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">70.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">96.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (Aligned)</td>\n<td class=\"ltx_td ltx_align_center\">108</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">217</td>\n<td class=\"ltx_td ltx_align_center\">42.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">51.7</td>\n<td class=\"ltx_td ltx_align_center\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">70.4</td>\n<td class=\"ltx_td ltx_align_center\">86.6</td>\n<td class=\"ltx_td ltx_align_center\">95.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (Mixed)</td>\n<td class=\"ltx_td ltx_align_center\">108</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">217</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">70.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.0</span></td>\n<td class=\"ltx_td ltx_align_center\">95.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LST (Curriculum)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">108</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">45.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">52.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">87.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">96.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "computation",
            "same",
            "s→rightarrows",
            "each",
            "topicstorycloze",
            "base",
            "main",
            "both",
            "reports",
            "storycloze",
            "model",
            "hellaswag",
            "dataset",
            "speechllm",
            "int",
            "static",
            "aligned",
            "curriculum",
            "models",
            "under",
            "t→rightarrowt",
            "bpe",
            "lst",
            "budget",
            "tokens",
            "baselines",
            "comparison",
            "scheme",
            "mixed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "under",
                    "hellaswag",
                    "both",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "lst",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "hellaswag",
                    "both",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sequence Modeling</span> Similar to LLMs for text, speech token modeling is typically done using a large transformer decoder model using causal self-attention, to maximize the likelihood of sequences from a large speech pre-training corpus (<math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>) in an auto-regressive fashion:</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "text",
                    "models",
                    "t→rightarrowt",
                    "dataset",
                    "same",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "lst",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "tokens",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "each",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment Patching.</span>\nTo better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let <math alttext=\"\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}</annotation></semantics></math> denotes the aligned frame ranges, where <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math> spans the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th textual unit.\nThe corresponding patch is</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "bpe",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtain alignments with Wav2Vec2+CTC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib2\" title=\"\">2020</a>)</cite>, yielding one patch per text unit and silence segment (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. <em class=\"ltx_emph ltx_font_italic\">Curriculum patching</em> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mitigates this by gradually shifting from aligned to static patching during training.</p>\n\n",
                "matched_terms": [
                    "curriculum",
                    "model",
                    "text",
                    "static",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span>\n\nCurriculum patching interpolates between alignment-based and static patching during training. Let <math alttext=\"P(u)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)\\in[0,1]</annotation></semantics></math> denote the probability of using alignment at training step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m2\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text.</span> Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite>, totaling 1.8T tokens. We follow the LLaMA&#160;2 setup and apply its SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> BPE tokenizer with a 32K vocabulary.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech.</span>\nOur speech training data includes speech which is discretized into HuBERT tokens (501-entry codebook at 25Hz) together with paired text transcriptions. We use LibriLight (60k hours), People&#8217;s Speech (30k hours), Multilingual LibriSpeech (50k hours), and Spotify (60k hours), detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Training and Evaluation Datasets &#8227; 4 Experimental Setup &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All corpora are aligned using the Wav2Vec2 + CTC framework to provide token-level correspondence between speech and text (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "t→rightarrowt",
                    "dataset",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">sHellaSWAG (HS).</span> We create a speech version of HellaSwag&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite> with Kokoro TTS. This benchmark evaluates everyday commonsense reasoning with spoken inputs and outputs. To ensure fairness, we generate the speech for prompts and responses independently and concatenate them afterwards, so that all responses are evaluated against the same speech prompt.</p>\n\n",
                "matched_terms": [
                    "same",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze and Topic StoryCloze (SC/TSC).</span> SC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite> and its topic-based extension TSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> are widely used in prior multimodal work (e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a></cite>) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned Patching.</span> Uses Wav2Vec2+CTC boundaries (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). For each text span <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math>, we form patch set <math alttext=\"\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}</annotation></semantics></math>, synchronizing speech and text tokens (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "tokens",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mixed Patching.</span> Randomly applies static or aligned patching per sequence, combining the robustness of static patching with the fine-grained sync of aligned.</p>\n\n",
                "matched_terms": [
                    "static",
                    "aligned",
                    "mixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span> Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum",
                    "aligned",
                    "mixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nWe include two speechLLM systems as baselines:</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "baselines"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base SpeechLLM.</span> Processes speech tokens directly with text tokens, without patching, similar to SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "base",
                    "tokens",
                    "speechllm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To balance modalities, we set speech tokens to account for about one third (33%) of the total training data, while the rest (67%) is text-only. This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.\nFor comparison, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> adopts a different composition: 33% pure speech, 33% interleaved, and 33% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33% interleaved and 66% text tokens yields better performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS4\" title=\"A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "storycloze",
                    "curriculum",
                    "budget",
                    "baselines",
                    "text",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "computation",
                    "bpe",
                    "both",
                    "same",
                    "speechllm",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "under",
                    "lst",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "both",
                    "same",
                    "budget",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "both",
                    "static",
                    "s→rightarrows",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "bpe",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "topicstorycloze",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations. First, we focus on half-duplex speech&#8211;text modeling, where speech and text alternate in turns, and do not yet address full-duplex interaction required for real-time dialogue such as Moshi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. Second, our analysis is restricted to the pre-training stage, without exploring instruction fine-tuning or downstream adaptation, which we leave for future work. Third, some of our patching strategies, such as alignment and curriculum, rely on forced alignments during pre-training; although curriculum patching reduces this dependency at inference, fully alignment-free approaches remain an open challenge. Finally, our experiments are limited to the speech&#8211;text modality, and we have not yet extended LST to additional modalities such as image or video, which represent a promising next direction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "curriculum",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "curriculum",
                    "model",
                    "text",
                    "under",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span> We obtain alignment information by using Wav2Vec2 + CTC to determine the boundaries linking text tokens to their corresponding spans of speech tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Span selection.</span> For each training example, we randomly select a contiguous span of words. The selected span is replaced by text tokens, while the following span of approximately half that length is kept as speech tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality markers.</span> We insert special tokens <span class=\"ltx_text ltx_font_typewriter\">&lt;t&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;s&gt;</span> to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "t→rightarrowt",
                    "same",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "under",
                    "hellaswag",
                    "both",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "text",
                    "lst",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "both",
                    "same",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "topicstorycloze",
                    "curriculum",
                    "text",
                    "hellaswag",
                    "t→rightarrowt",
                    "bpe",
                    "same",
                    "tokens",
                    "s→rightarrows",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "each",
                    "storycloze",
                    "topicstorycloze",
                    "curriculum",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "static",
                    "s→rightarrows"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 4: Main comparison of LST models and baselines under the same speech/text tokens scheme. Each dataset reports both S→\\rightarrowS and T→\\rightarrowT.",
        "body": "Model\nCompute Savings\nHellaSwag\nStoryCloze\nTopicStoryCloze\n\n\n\n(%)\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\n\n\n\n\nBase SpeechLLM\n-\n40.2\n49.6\n60.2\n69.1\n87.5\n95.2\n\n\nBPE SpeechLLM\n8.2%\n39.4\n48.4\n58.3\n66.3\n86.5\n93.9\n\n\nLST (Static)\n19.3%\n44.3\n51.1\n60.5\n70.3\n87.7\n96.2\n\n\nLST (Curriculum)\n19.7%\n45.5\n52.2\n61.2\n71.6\n87.9\n96.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Compute Savings</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">StoryCloze</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">TopicStoryCloze</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Base SpeechLLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">69.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BPE SpeechLLM</td>\n<td class=\"ltx_td ltx_align_center\">8.2%</td>\n<td class=\"ltx_td ltx_align_center\">39.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">48.4</td>\n<td class=\"ltx_td ltx_align_center\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">66.3</td>\n<td class=\"ltx_td ltx_align_center\">86.5</td>\n<td class=\"ltx_td ltx_align_center\">93.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Static)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">51.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">70.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">96.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LST (Curriculum)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">19.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">45.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">52.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">87.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">96.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "same",
            "s→rightarrows",
            "each",
            "topicstorycloze",
            "base",
            "main",
            "both",
            "reports",
            "compute",
            "storycloze",
            "model",
            "hellaswag",
            "dataset",
            "speechllm",
            "static",
            "speechtext",
            "curriculum",
            "models",
            "under",
            "t→rightarrowt",
            "savings",
            "bpe",
            "lst",
            "tokens",
            "baselines",
            "comparison",
            "scheme"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "models",
                    "under",
                    "hellaswag",
                    "both",
                    "lst",
                    "tokens",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "same",
                    "tokens",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "models",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "models",
                    "lst",
                    "same",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "models",
                    "hellaswag",
                    "both",
                    "lst",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "models",
                    "t→rightarrowt",
                    "dataset",
                    "same",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "models",
                    "lst",
                    "savings",
                    "same",
                    "tokens",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "each",
                    "tokens",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "each",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment Patching.</span>\nTo better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let <math alttext=\"\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}</annotation></semantics></math> denotes the aligned frame ranges, where <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math> spans the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th textual unit.\nThe corresponding patch is</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtain alignments with Wav2Vec2+CTC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib2\" title=\"\">2020</a>)</cite>, yielding one patch per text unit and silence segment (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. <em class=\"ltx_emph ltx_font_italic\">Curriculum patching</em> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mitigates this by gradually shifting from aligned to static patching during training.</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span>\n\nCurriculum patching interpolates between alignment-based and static patching during training. Let <math alttext=\"P(u)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)\\in[0,1]</annotation></semantics></math> denote the probability of using alignment at training step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m2\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text.</span> Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite>, totaling 1.8T tokens. We follow the LLaMA&#160;2 setup and apply its SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> BPE tokenizer with a 32K vocabulary.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "t→rightarrowt",
                    "dataset",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">sHellaSWAG (HS).</span> We create a speech version of HellaSwag&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite> with Kokoro TTS. This benchmark evaluates everyday commonsense reasoning with spoken inputs and outputs. To ensure fairness, we generate the speech for prompts and responses independently and concatenate them afterwards, so that all responses are evaluated against the same speech prompt.</p>\n\n",
                "matched_terms": [
                    "same",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze and Topic StoryCloze (SC/TSC).</span> SC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite> and its topic-based extension TSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> are widely used in prior multimodal work (e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a></cite>) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned Patching.</span> Uses Wav2Vec2+CTC boundaries (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). For each text span <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math>, we form patch set <math alttext=\"\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}</annotation></semantics></math>, synchronizing speech and text tokens (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span> Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nWe include two speechLLM systems as baselines:</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "baselines"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base SpeechLLM.</span> Processes speech tokens directly with text tokens, without patching, similar to SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "base",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To balance modalities, we set speech tokens to account for about one third (33%) of the total training data, while the rest (67%) is text-only. This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.\nFor comparison, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> adopts a different composition: 33% pure speech, 33% interleaved, and 33% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33% interleaved and 66% text tokens yields better performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS4\" title=\"A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "curriculum",
                    "tokens",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "both",
                    "same",
                    "static",
                    "s→rightarrows",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "under",
                    "lst",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "both",
                    "same",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "both",
                    "static",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "bpe",
                    "tokens",
                    "compute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "topicstorycloze",
                    "compute",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations. First, we focus on half-duplex speech&#8211;text modeling, where speech and text alternate in turns, and do not yet address full-duplex interaction required for real-time dialogue such as Moshi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. Second, our analysis is restricted to the pre-training stage, without exploring instruction fine-tuning or downstream adaptation, which we leave for future work. Third, some of our patching strategies, such as alignment and curriculum, rely on forced alignments during pre-training; although curriculum patching reduces this dependency at inference, fully alignment-free approaches remain an open challenge. Finally, our experiments are limited to the speech&#8211;text modality, and we have not yet extended LST to additional modalities such as image or video, which represent a promising next direction.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "curriculum",
                    "under",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Span selection.</span> For each training example, we randomly select a contiguous span of words. The selected span is replaced by text tokens, while the following span of approximately half that length is kept as speech tokens.</p>\n\n",
                "matched_terms": [
                    "each",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality markers.</span> We insert special tokens <span class=\"ltx_text ltx_font_typewriter\">&lt;t&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;s&gt;</span> to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "same",
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "hellaswag",
                    "both",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hellaswag",
                    "t→rightarrowt",
                    "lst",
                    "both",
                    "same",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "t→rightarrowt",
                    "bpe",
                    "same",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "each",
                    "storycloze",
                    "topicstorycloze",
                    "curriculum",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "static",
                    "s→rightarrows"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 5: Scaling trends of baseline SpeechLLM and LST models at 1B and 7B parameter scales. Each dataset reports both S→\\rightarrowS and T→\\rightarrowT.",
        "body": "Model\nBatch\nIters\nHellaSwag\n\n\n\n\n\n(M)\n(k)\nS→\\rightarrowS\nT→\\rightarrowT\n\n\nBaseline (1B)\n0.5\n200\n36.8\n47.1\n\n\nLST (1B)\n0.5\n200\n41.3\n49.2\n\n\nBaseline (7B)\n4.0\n25\n42.0\n54.8\n\n\nLST (7B)\n4.0\n25\n44.2\n55.3",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Batch</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Iters</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(M)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(k)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Baseline (1B)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (1B)</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">49.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Baseline (7B)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LST (7B)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">44.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">55.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "each",
            "model",
            "models",
            "t→rightarrowt",
            "parameter",
            "dataset",
            "batch",
            "hellaswag",
            "reports",
            "scaling",
            "baseline",
            "both",
            "iters",
            "lst",
            "speechllm",
            "s→rightarrows",
            "scales",
            "trends"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hellaswag",
                    "both",
                    "scaling",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "scaling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "both",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "scaling",
                    "model",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "models",
                    "t→rightarrowt",
                    "dataset",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "each",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "t→rightarrowt",
                    "dataset",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "lst",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speechllm",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "baseline",
                    "lst",
                    "s→rightarrows",
                    "trends"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "baseline",
                    "lst",
                    "speechllm",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "s→rightarrows",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "both",
                    "model",
                    "scaling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "both",
                    "model",
                    "scaling",
                    "trends"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "baseline",
                    "model",
                    "scaling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the model using the AdamW optimizer (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, weight decay = 0.1). The learning rate was initialized at <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math> and scheduled with cosine decay, including a warmup period of 2,000 steps and a minimum ratio of 0.01 at the final step. For the 1B model, training was performed on 32 H100 GPUs with a per-GPU batch size of 4 sequences (sequence length = 4,096 units), leading to a total batch size of 0.5M units. Mixed-precision training with bfloat16 was used for efficiency. Gradient clipping was applied at 1.0, and gradient accumulation was set to 1. Model parallelism used a single partition, and Fully Sharded Data Parallel (FSDP) was enabled for memory efficiency. No dropout was applied.\nThe 1B model was trained for 200k steps, corresponding to approximately 1 trillion units, and required around 17 hours to complete on 32 H100 GPUs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hellaswag",
                    "baseline",
                    "scaling",
                    "both",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "both",
                    "model",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "baseline",
                    "lst",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "t→rightarrowt",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "each",
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "baseline",
                    "s→rightarrows"
                ]
            }
        ]
    },
    "S6.T6": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 6: Comparison of patching strategies with approximately matched patch sizes. Static uses fixed patch lengths. Align (sil sep.) treats silence as separate patches, Align (sil merged) merges silence into words, and Curriculum starts with Align (sil sep.) and gradually shifts to Static during training.",
        "body": "Model\nAve Patch Size\nHellaSwag\nStoryCloze\nTopicStoryCloze\n\n\n\n(tokens)\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\n\n\n\n\nLST (Static)\n4\n40.5\n48.8\n58.2\n69.4\n86.2\n95.1\n\n\nLST (Curriculum)\n5.8* →\\rightarrow 4\n41.3\n49.2\n58.6\n67.8\n86.6\n95.4\n\n\nLST (Align, sil sep.)\n5.8*\n\n39.9\n49.3\n60.3\n69.9\n85.7\n95.3\n\n\nLST (Static)\n6\n39.4\n49.2\n58.7\n69.6\n84.9\n94.9\n\n\nLST (Static)\n9\n37.2\n49.4\n57.5\n69.7\n84.7\n95.9\n\n\nLST (Align, sil merged)\n9.4\n38.5\n49.0\n58.8\n69.7\n86.9\n96.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Ave Patch Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">StoryCloze</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">TopicStoryCloze</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(tokens)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Static)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">69.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">86.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (<span class=\"ltx_text ltx_font_bold\">Curriculum</span>)</td>\n<td class=\"ltx_td ltx_align_center\">5.8<sup class=\"ltx_sup\">*</sup> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">58.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">67.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">86.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (<span class=\"ltx_text ltx_font_bold\">Align, sil sep.</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.8<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">39.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">49.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">69.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">95.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (Static)</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_align_center\">39.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">49.2</td>\n<td class=\"ltx_td ltx_align_center\">58.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">69.6</td>\n<td class=\"ltx_td ltx_align_center\">84.9</td>\n<td class=\"ltx_td ltx_align_center\">94.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Static)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">49.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">69.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">84.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LST (<span class=\"ltx_text ltx_font_bold\">Align, sil merged</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">9.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">38.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">49.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">58.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">69.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">86.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">96.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "strategies",
            "matched",
            "training",
            "size",
            "fixed",
            "ave",
            "patch",
            "approximately",
            "words",
            "→rightarrow",
            "s→rightarrows",
            "sizes",
            "patching",
            "topicstorycloze",
            "shifts",
            "merged",
            "patches",
            "treats",
            "sep",
            "storycloze",
            "model",
            "starts",
            "lengths",
            "hellaswag",
            "merges",
            "into",
            "gradually",
            "align",
            "static",
            "curriculum",
            "t→rightarrowt",
            "silence",
            "separate",
            "during",
            "comparison",
            "sil",
            "lst",
            "tokens",
            "uses"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "training",
                    "hellaswag",
                    "patches",
                    "during",
                    "into",
                    "lst",
                    "align",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "during",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "sizes",
                    "model",
                    "training",
                    "patches",
                    "into",
                    "lst",
                    "align",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "training",
                    "patches",
                    "during",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "lst",
                    "hellaswag",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) We introduce different variations of speech patching schemes, including fixed-size static patching and alignment-based patching, and analyze their effectiveness.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "t→rightarrowt",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "patches",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "strategies",
                    "patch",
                    "into",
                    "lst",
                    "tokens",
                    "uses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "patches",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}</annotation></semantics></math> be speech token embeddings obtained using a learned embedding matrix applied to speech tokens <math alttext=\"\\{s_{0},\\dots,s_{T}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{T}\\}</annotation></semantics></math> .\nThe process of patching maps <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> to a shorter sequence of patch embeddings\n<math alttext=\"\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>z</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math> by aggregating local frame segments. For a frame-index set <math alttext=\"\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo>&#8838;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}</annotation></semantics></math>, a patch embedding is formed via the local encoder:</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">integrating the frames indexed by <math alttext=\"\\mathcal{P}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}</annotation></semantics></math> into a single patch embedding. Different patching strategies correspond to different segmentation <math alttext=\"\\{\\mathcal{P}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\mathcal{P}_{i}\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "strategies",
                    "into",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "patching",
                    "size",
                    "fixed",
                    "patch",
                    "into",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <math alttext=\"p=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">p=3</annotation></semantics></math> and input embeddings <math alttext=\"\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo>,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]</annotation></semantics></math>, the first patch is <math alttext=\"\\{x_{0},x_{1},x_{2}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{0},x_{1},x_{2}\\}</annotation></semantics></math>, the second <math alttext=\"\\{x_{3},x_{4},x_{5}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{3},x_{4},x_{5}\\}</annotation></semantics></math>, and so on. Each segment is encoded into a single patch embedding <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> by the local encoder.\nThis provides a uniform compression ratio independent of alignment information.</p>\n\n",
                "matched_terms": [
                    "into",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment Patching.</span>\nTo better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let <math alttext=\"\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}</annotation></semantics></math> denotes the aligned frame ranges, where <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math> spans the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th textual unit.\nThe corresponding patch is</p>\n\n",
                "matched_terms": [
                    "words",
                    "patching",
                    "tokens",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Frames outside text spans (e.g., silence) are grouped into separate patches (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).\nFor instance, if one word aligns to <math alttext=\"[2,4]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>2</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[2,4]</annotation></semantics></math> and the next to <math alttext=\"[6,7]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>6</mn><mo>,</mo><mn>7</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[6,7]</annotation></semantics></math>, patches are <math alttext=\"\\{x_{2},x_{3},x_{4}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{2},x_{3},x_{4}\\}</annotation></semantics></math> and <math alttext=\"\\{x_{6},x_{7}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>6</mn></msub><mo>,</mo><msub><mi>x</mi><mn>7</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{6},x_{7}\\}</annotation></semantics></math>,\nwith silence forming <math alttext=\"\\{x_{0},x_{1}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{0},x_{1}\\}</annotation></semantics></math> and <math alttext=\"\\{x_{5}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>5</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{5}\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "patches",
                    "separate",
                    "silence",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtain alignments with Wav2Vec2+CTC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib2\" title=\"\">2020</a>)</cite>, yielding one patch per text unit and silence segment (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. <em class=\"ltx_emph ltx_font_italic\">Curriculum patching</em> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mitigates this by gradually shifting from aligned to static patching during training.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "model",
                    "training",
                    "silence",
                    "patch",
                    "during",
                    "static",
                    "gradually"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span>\n\nCurriculum patching interpolates between alignment-based and static patching during training. Let <math alttext=\"P(u)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)\\in[0,1]</annotation></semantics></math> denote the probability of using alignment at training step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m2\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "training",
                    "during",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m3\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>, we choose alignment patches with probability <math alttext=\"P(u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)</annotation></semantics></math> and static patches otherwise.\nThis retains alignment benefits during early training while enabling simple static-only inference.</p>\n\n",
                "matched_terms": [
                    "patches",
                    "static",
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text.</span> Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite>, totaling 1.8T tokens. We follow the LLaMA&#160;2 setup and apply its SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> BPE tokenizer with a 32K vocabulary.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech.</span>\nOur speech training data includes speech which is discretized into HuBERT tokens (501-entry codebook at 25Hz) together with paired text transcriptions. We use LibriLight (60k hours), People&#8217;s Speech (30k hours), Multilingual LibriSpeech (50k hours), and Spotify (60k hours), detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Training and Evaluation Datasets &#8227; 4 Experimental Setup &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All corpora are aligned using the Wav2Vec2 + CTC framework to provide token-level correspondence between speech and text (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "lst",
                    "patching",
                    "strategies",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patches",
                    "patching",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned Patching.</span> Uses Wav2Vec2+CTC boundaries (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). For each text span <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math>, we form patch set <math alttext=\"\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}</annotation></semantics></math>, synchronizing speech and text tokens (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching",
                    "uses",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mixed Patching.</span> Randomly applies static or aligned patching per sequence, combining the robustness of static patching with the fine-grained sync of aligned.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span> Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "training",
                    "shifts",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base SpeechLLM.</span> Processes speech tokens directly with text tokens, without patching, similar to SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To balance modalities, we set speech tokens to account for about one third (33%) of the total training data, while the rest (67%) is text-only. This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.\nFor comparison, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> adopts a different composition: 33% pure speech, 33% interleaved, and 33% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33% interleaved and 66% text tokens yields better performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS4\" title=\"A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "starts",
                    "training",
                    "tokens",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "training",
                    "tokens",
                    "hellaswag",
                    "t→rightarrowt",
                    "patches",
                    "lst",
                    "static",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "patching",
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "t→rightarrowt",
                    "patch",
                    "patches",
                    "into",
                    "lst",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "words",
                    "lst",
                    "model",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "t→rightarrowt",
                    "hellaswag",
                    "lst",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "model",
                    "training",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average patch length is 5.8 for words in Align (sil sep.), while silence has an average of 3.7.</p>\n\n",
                "matched_terms": [
                    "sep",
                    "silence",
                    "patch",
                    "sil",
                    "words",
                    "align"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations. First, we focus on half-duplex speech&#8211;text modeling, where speech and text alternate in turns, and do not yet address full-duplex interaction required for real-time dialogue such as Moshi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. Second, our analysis is restricted to the pre-training stage, without exploring instruction fine-tuning or downstream adaptation, which we leave for future work. Third, some of our patching strategies, such as alignment and curriculum, rely on forced alignments during pre-training; although curriculum patching reduces this dependency at inference, fully alignment-free approaches remain an open challenge. Finally, our experiments are limited to the speech&#8211;text modality, and we have not yet extended LST to additional modalities such as image or video, which represent a promising next direction.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "strategies",
                    "curriculum",
                    "during",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "model",
                    "training",
                    "into",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Span selection.</span> For each training example, we randomly select a contiguous span of words. The selected span is replaced by text tokens, while the following span of approximately half that length is kept as speech tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "approximately",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality markers.</span> We insert special tokens <span class=\"ltx_text ltx_font_typewriter\">&lt;t&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;s&gt;</span> to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the model using the AdamW optimizer (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, weight decay = 0.1). The learning rate was initialized at <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math> and scheduled with cosine decay, including a warmup period of 2,000 steps and a minimum ratio of 0.01 at the final step. For the 1B model, training was performed on 32 H100 GPUs with a per-GPU batch size of 4 sequences (sequence length = 4,096 units), leading to a total batch size of 0.5M units. Mixed-precision training with bfloat16 was used for efficiency. Gradient clipping was applied at 1.0, and gradient accumulation was set to 1. Model parallelism used a single partition, and Fully Sharded Data Parallel (FSDP) was enabled for memory efficiency. No dropout was applied.\nThe 1B model was trained for 200k steps, corresponding to approximately 1 trillion units, and required around 17 hours to complete on 32 H100 GPUs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "size",
                    "approximately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T7\" title=\"Table 7 &#8227; A.2.2 Model Architecture &#8227; A.2 Hyperparameters &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> summarizes the hierarchical architecture used in our experiments.\nThe model consists of a shallow local encoder, a deep global transformer, and a moderately deep local decoder.\nThe local modules operate with restricted attention windows to capture fine-grained context, while the global transformer\nuses block-causal attention with rotary position embeddings (RoPE) to handle long-range dependencies efficiently.\nThis design balances local detail preservation with scalable long-context modeling.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "sizes",
                    "model",
                    "training",
                    "size",
                    "hellaswag",
                    "lst",
                    "tokens",
                    "uses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "training",
                    "t→rightarrowt",
                    "hellaswag",
                    "lst",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "patching",
                    "storycloze",
                    "curriculum",
                    "training",
                    "hellaswag",
                    "t→rightarrowt",
                    "patches",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average patch length is 5.8 for words, 5.0 for BPEs, and 3.7 for silence spans.</p>\n\n",
                "matched_terms": [
                    "words",
                    "silence",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "strategies",
                    "patching",
                    "storycloze",
                    "training",
                    "curriculum",
                    "ave",
                    "hellaswag",
                    "t→rightarrowt",
                    "static",
                    "s→rightarrows"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 7: Model architecture configuration. Each module is shown with its depth, hidden dimension, number of attention heads, and other relevant settings.",
        "body": "Module\nLayers\nDim.\nHeads\nNotes\n\n\n\n\nLocal Encoder\n1\n1024\n16\nLocal window = 512\n\n\nGlobal Transformer\n25\n2048\n16\nBlock-causal; RoPE (θ=5×105\\theta=5\\!\\times\\!10^{5})\n\n\nLocal Decoder\n9\n1024\n16\nLocal window = 512",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Module</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Layers</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dim.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Heads</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Notes</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Local Encoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Local window = 512</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Global Transformer</td>\n<td class=\"ltx_td ltx_align_center\">25</td>\n<td class=\"ltx_td ltx_align_center\">2048</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">Block-causal; RoPE (<math alttext=\"\\theta=5\\!\\times\\!10^{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><mrow><mi>&#952;</mi><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mn>5</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta=5\\!\\times\\!10^{5}</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Local Decoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Local window = 512</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "global",
            "blockcausal",
            "architecture",
            "rope",
            "relevant",
            "transformer",
            "encoder",
            "each",
            "decoder",
            "notes",
            "local",
            "dim",
            "layers",
            "model",
            "settings",
            "depth",
            "module",
            "hidden",
            "configuration",
            "θ5×105theta5times105",
            "number",
            "dimension",
            "attention",
            "window",
            "its",
            "heads"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T7\" title=\"Table 7 &#8227; A.2.2 Model Architecture &#8227; A.2 Hyperparameters &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> summarizes the hierarchical architecture used in our experiments.\nThe model consists of a shallow local encoder, a deep global transformer, and a moderately deep local decoder.\nThe local modules operate with restricted attention windows to capture fine-grained context, while the global transformer\nuses block-causal attention with rotary position embeddings (RoPE) to handle long-range dependencies efficiently.\nThis design balances local detail preservation with scalable long-context modeling.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "number",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "number",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "global",
                    "model",
                    "decoder",
                    "architecture",
                    "transformer",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sequence Modeling</span> Similar to LLMs for text, speech token modeling is typically done using a large transformer decoder model using causal self-attention, to maximize the likelihood of sequences from a large speech pre-training corpus (<math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>) in an auto-regressive fashion:</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "global",
                    "model",
                    "decoder",
                    "architecture",
                    "transformer",
                    "local",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "local",
                    "layers",
                    "encoder",
                    "window"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "each",
                    "layers",
                    "decoder",
                    "window",
                    "transformer",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}</annotation></semantics></math> be speech token embeddings obtained using a learned embedding matrix applied to speech tokens <math alttext=\"\\{s_{0},\\dots,s_{T}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{T}\\}</annotation></semantics></math> .\nThe process of patching maps <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> to a shorter sequence of patch embeddings\n<math alttext=\"\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>z</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math> by aggregating local frame segments. For a frame-index set <math alttext=\"\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo>&#8838;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}</annotation></semantics></math>, a patch embedding is formed via the local encoder:</p>\n\n",
                "matched_terms": [
                    "local",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "each",
                    "encoder",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <math alttext=\"p=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">p=3</annotation></semantics></math> and input embeddings <math alttext=\"\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo>,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]</annotation></semantics></math>, the first patch is <math alttext=\"\\{x_{0},x_{1},x_{2}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{0},x_{1},x_{2}\\}</annotation></semantics></math>, the second <math alttext=\"\\{x_{3},x_{4},x_{5}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{3},x_{4},x_{5}\\}</annotation></semantics></math>, and so on. Each segment is encoded into a single patch embedding <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> by the local encoder.\nThis provides a uniform compression ratio independent of alignment information.</p>\n\n",
                "matched_terms": [
                    "each",
                    "encoder",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "its",
                    "model",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "other",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "other",
                    "each",
                    "its"
                ]
            }
        ]
    },
    "A1.T8": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 8: Comparison of aligned patching strategies under a speech-to-text token ratio of 1:4.\nWord Align uses word-level forced alignment,\nBPE Align uses BPE segmentation,\nand Curriculum gradually shifts from alignment-based to static patching.",
        "body": "Model\nAve Patch Size\nHellaSwag\nStoryCloze\nTopicStoryCloze\n\n\n\n(tokens)\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\nS→\\rightarrowS\nT→\\rightarrowT\n\n\n\n\nLST (Word Align)\n5.8*\n\n40.0\n49.9\n59.4\n68.6\n84.8\n94.6\n\n\nLST (BPE Align)\n5.0*\n\n39.2\n50.1\n55.6\n69.1\n79.6\n95.6\n\n\nLST (Word Curr.)\n5.8→\\rightarrow4\n41.5\n49.5\n57.9\n68.9\n86.8\n95.1\n\n\nLST (BPE Curr.)\n5.0→\\rightarrow4\n41.3\n48.6\n59.1\n67.1\n86.5\n95.4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Ave Patch Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">StoryCloze</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">TopicStoryCloze</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(tokens)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Word Align)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.8<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">40.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">59.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">68.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">94.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LST (BPE Align)</td>\n<td class=\"ltx_td ltx_align_center\">5.0<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">50.1</span></td>\n<td class=\"ltx_td ltx_align_center\">55.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">69.1</span></td>\n<td class=\"ltx_td ltx_align_center\">79.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LST (Word Curr.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.8<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">41.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">49.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">68.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">86.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LST (BPE Curr.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.0<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">41.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">48.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">59.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">67.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">95.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "strategies",
            "size",
            "alignmentbased",
            "ave",
            "patch",
            "s→rightarrows",
            "topicstorycloze",
            "patching",
            "shifts",
            "58→rightarrow4",
            "segmentation",
            "from",
            "wordlevel",
            "storycloze",
            "model",
            "curr",
            "speechtotext",
            "hellaswag",
            "word",
            "ratio",
            "token",
            "forced",
            "alignment",
            "gradually",
            "align",
            "static",
            "aligned",
            "curriculum",
            "50→rightarrow4",
            "under",
            "t→rightarrowt",
            "bpe",
            "comparison",
            "lst",
            "tokens",
            "uses"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "under",
                    "hellaswag",
                    "from",
                    "alignment",
                    "lst",
                    "align",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite>, pre-training with interleaved speech-text data <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, and modeling speech and text in multiple streams to leverage the textual chain of thought or &#8220;inner monologue&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. All these works attempted to some extent to achieve <em class=\"ltx_emph ltx_font_italic\">representational alignment</em> between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "from",
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome the aforementioned challenges, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST) based on the byte-latent transformer (BLT) architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite>, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.\nThese speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "lst",
                    "align",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models with fixed-size speech patching schemes similar to what <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite> did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "patching",
                    "token",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "lst",
                    "hellaswag",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) We introduce different variations of speech patching schemes, including fixed-size static patching and alignment-based patching, and analyze their effectiveness.</p>\n\n",
                "matched_terms": [
                    "alignmentbased",
                    "patching",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "from",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech tokenization</span> Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib14\" title=\"\">2021</a>)</cite>, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>); Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> we do not need to deduplicate Hubert tokens as this is organically handled by the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "from",
                    "tokens",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sequence Modeling</span> Similar to LLMs for text, speech token modeling is typically done using a large transformer decoder model using causal self-attention, to maximize the likelihood of sequences from a large speech pre-training corpus (<math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>) in an auto-regressive fashion:</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "t→rightarrowt",
                    "from",
                    "token",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Encoder.</span> Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST, we only patch spans of speech tokens using strategies described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.SS1\" title=\"3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> and similar to them, failed to improve performance in our experiments (ablations in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5\" title=\"5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "strategies",
                    "patch",
                    "bpe",
                    "token",
                    "lst",
                    "tokens",
                    "uses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},\\dots,x_{T}]\\in\\mathbb{R}^{T\\times d}</annotation></semantics></math> be speech token embeddings obtained using a learned embedding matrix applied to speech tokens <math alttext=\"\\{s_{0},\\dots,s_{T}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{T}\\}</annotation></semantics></math> .\nThe process of patching maps <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> to a shorter sequence of patch embeddings\n<math alttext=\"\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>z</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[z_{0},\\dots,z_{T^{\\prime}}]\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math> by aggregating local frame segments. For a frame-index set <math alttext=\"\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo>&#8838;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}\\subseteq\\{0,\\dots,T\\}</annotation></semantics></math>, a patch embedding is formed via the local encoder:</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching",
                    "token",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">integrating the frames indexed by <math alttext=\"\\mathcal{P}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}</annotation></semantics></math> into a single patch embedding. Different patching strategies correspond to different segmentation <math alttext=\"\\{\\mathcal{P}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\mathcal{P}_{i}\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "patching",
                    "strategies",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "patching",
                    "size",
                    "patch",
                    "from",
                    "token",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <math alttext=\"p=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">p=3</annotation></semantics></math> and input embeddings <math alttext=\"\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo>,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{0},x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},\\dots]</annotation></semantics></math>, the first patch is <math alttext=\"\\{x_{0},x_{1},x_{2}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{0},x_{1},x_{2}\\}</annotation></semantics></math>, the second <math alttext=\"\\{x_{3},x_{4},x_{5}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><msub><mi>x</mi><mn>4</mn></msub><mo>,</mo><msub><mi>x</mi><mn>5</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{3},x_{4},x_{5}\\}</annotation></semantics></math>, and so on. Each segment is encoded into a single patch embedding <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> by the local encoder.\nThis provides a uniform compression ratio independent of alignment information.</p>\n\n",
                "matched_terms": [
                    "ratio",
                    "alignment",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment Patching.</span>\nTo better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let <math alttext=\"\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}=\\{(b_{k},e_{k})\\}_{k=1}^{K}</annotation></semantics></math> denotes the aligned frame ranges, where <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math> spans the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th textual unit.\nThe corresponding patch is</p>\n\n",
                "matched_terms": [
                    "patching",
                    "patch",
                    "bpe",
                    "forced",
                    "alignment",
                    "tokens",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtain alignments with Wav2Vec2+CTC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib2\" title=\"\">2020</a>)</cite>, yielding one patch per text unit and silence segment (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. <em class=\"ltx_emph ltx_font_italic\">Curriculum patching</em> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mitigates this by gradually shifting from aligned to static patching during training.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "model",
                    "patch",
                    "from",
                    "static",
                    "gradually",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span>\n\nCurriculum patching interpolates between alignment-based and static patching during training. Let <math alttext=\"P(u)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)\\in[0,1]</annotation></semantics></math> denote the probability of using alignment at training step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m2\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "alignmentbased",
                    "alignment",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m3\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>, we choose alignment patches with probability <math alttext=\"P(u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)</annotation></semantics></math> and static patches otherwise.\nThis retains alignment benefits during early training while enabling simple static-only inference.</p>\n\n",
                "matched_terms": [
                    "static",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text.</span> Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite>, totaling 1.8T tokens. We follow the LLaMA&#160;2 setup and apply its SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> BPE tokenizer with a 32K vocabulary.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "from",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech.</span>\nOur speech training data includes speech which is discretized into HuBERT tokens (501-entry codebook at 25Hz) together with paired text transcriptions. We use LibriLight (60k hours), People&#8217;s Speech (30k hours), Multilingual LibriSpeech (50k hours), and Spotify (60k hours), detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Training and Evaluation Datasets &#8227; 4 Experimental Setup &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All corpora are aligned using the Wav2Vec2 + CTC framework to provide token-level correspondence between speech and text (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LST Models.</span> We explore four patching strategies for speech tokens:</p>\n\n",
                "matched_terms": [
                    "lst",
                    "patching",
                    "strategies",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching",
                    "static",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned Patching.</span> Uses Wav2Vec2+CTC boundaries (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). For each text span <math alttext=\"[b_{k},e_{k}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[b_{k},e_{k}]</annotation></semantics></math>, we form patch set <math alttext=\"\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>b</mi><mi>k</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{k}=\\{b_{k},\\dots,e_{k}\\}</annotation></semantics></math>, synchronizing speech and text tokens (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf1\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>).</p>\n\n",
                "matched_terms": [
                    "patching",
                    "patch",
                    "tokens",
                    "uses",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mixed Patching.</span> Randomly applies static or aligned patching per sequence, combining the robustness of static patching with the fine-grained sync of aligned.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "aligned",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span> Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "shifts",
                    "from",
                    "alignment",
                    "static",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base SpeechLLM.</span> Processes speech tokens directly with text tokens, without patching, similar to SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "patching"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BPE SpeechLLM.</span> Maps speech tokens into 1k BPE units using a SentencePiece tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib17\" title=\"\">2018</a>)</cite> trained on 100k random speech sequences, replacing speech tokens with BPE-derived units<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "bpe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To balance modalities, we set speech tokens to account for about one third (33%) of the total training data, while the rest (67%) is text-only. This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.\nFor comparison, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> adopts a different composition: 33% pure speech, 33% interleaved, and 33% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33% interleaved and 66% text tokens yields better performance (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS4\" title=\"A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "alignment",
                    "tokens",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "tokens",
                    "hellaswag",
                    "t→rightarrowt",
                    "word",
                    "token",
                    "alignment",
                    "lst",
                    "static",
                    "s→rightarrows",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "patching",
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "t→rightarrowt",
                    "patch",
                    "from",
                    "bpe",
                    "alignment",
                    "lst",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "patch",
                    "word",
                    "from",
                    "lst",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "from",
                    "token",
                    "lst",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "storycloze",
                    "patching",
                    "strategies",
                    "curriculum",
                    "size",
                    "shifts",
                    "hellaswag",
                    "patch",
                    "word",
                    "from",
                    "alignment",
                    "align",
                    "static",
                    "s→rightarrows",
                    "gradually",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "from",
                    "model",
                    "uses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "from",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The average patch length is 5.8 for words in Align (sil sep.), while silence has an average of 3.7.</p>\n\n",
                "matched_terms": [
                    "align",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech model efficiency.</span> Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib1\" title=\"\">Baade et&#160;al., </a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib31\" title=\"\">2025</a>)</cite>, hierarchical generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite>, and producing residual tokens using parallel streams <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib6\" title=\"\">2023</a>)</cite>. Attempts at text-inspired approaches to compress token sequences such as BPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib27\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib19\" title=\"\">2024</a>)</cite> achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>; Videau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib35\" title=\"\">2025</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Pang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib25\" title=\"\">2024</a>; Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib3\" title=\"\">2023</a>)</cite>, and extend these methods to speech-text LLMs.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "model",
                    "from",
                    "bpe",
                    "token",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations. First, we focus on half-duplex speech&#8211;text modeling, where speech and text alternate in turns, and do not yet address full-duplex interaction required for real-time dialogue such as Moshi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite>. Second, our analysis is restricted to the pre-training stage, without exploring instruction fine-tuning or downstream adaptation, which we leave for future work. Third, some of our patching strategies, such as alignment and curriculum, rely on forced alignments during pre-training; although curriculum patching reduces this dependency at inference, fully alignment-free approaches remain an open challenge. Finally, our experiments are limited to the speech&#8211;text modality, and we have not yet extended LST to additional modalities such as image or video, which represent a promising next direction.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "strategies",
                    "curriculum",
                    "forced",
                    "alignment",
                    "lst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "patching",
                    "curriculum",
                    "model",
                    "under",
                    "lst",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span> We obtain alignment information by using Wav2Vec2 + CTC to determine the boundaries linking text tokens to their corresponding spans of speech tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality markers.</span> We insert special tokens <span class=\"ltx_text ltx_font_typewriter\">&lt;t&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;s&gt;</span> to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the model using the AdamW optimizer (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>, weight decay = 0.1). The learning rate was initialized at <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math> and scheduled with cosine decay, including a warmup period of 2,000 steps and a minimum ratio of 0.01 at the final step. For the 1B model, training was performed on 32 H100 GPUs with a per-GPU batch size of 4 sequences (sequence length = 4,096 units), leading to a total batch size of 0.5M units. Mixed-precision training with bfloat16 was used for efficiency. Gradient clipping was applied at 1.0, and gradient accumulation was set to 1. Model parallelism used a single partition, and Fully Sharded Data Parallel (FSDP) was enabled for memory efficiency. No dropout was applied.\nThe 1B model was trained for 200k steps, corresponding to approximately 1 trillion units, and required around 17 hours to complete on 32 H100 GPUs.</p>\n\n",
                "matched_terms": [
                    "ratio",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T7\" title=\"Table 7 &#8227; A.2.2 Model Architecture &#8227; A.2 Hyperparameters &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> summarizes the hierarchical architecture used in our experiments.\nThe model consists of a shallow local encoder, a deep global transformer, and a moderately deep local decoder.\nThe local modules operate with restricted attention windows to capture fine-grained context, while the global transformer\nuses block-causal attention with rotary position embeddings (RoPE) to handle long-range dependencies efficiently.\nThis design balances local detail preservation with scalable long-context modeling.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "size",
                    "under",
                    "hellaswag",
                    "ratio",
                    "from",
                    "lst",
                    "tokens",
                    "uses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "lst",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "t→rightarrowt",
                    "ratio",
                    "from",
                    "token",
                    "lst",
                    "tokens",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "strategies",
                    "patching",
                    "storycloze",
                    "curriculum",
                    "ave",
                    "under",
                    "hellaswag",
                    "t→rightarrowt",
                    "static",
                    "s→rightarrows"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "Latent Speech-Text Transformer",
        "caption": "Table 9: Average (Ave) and standard deviation (Std) across three runs. Each task is reported with both S→\\rightarrowS and T→\\rightarrowT directions.",
        "body": "Model\nEvaluation\nHellaSwag\nStoryCloze\nTopicStoryCloze\n\n\n\n\n\n\nAve\nStd\nAve\nStd\nAve\nStd\n\n\nCurriculum\nS→\\rightarrowS\n41.4\n0.13\n59.2\n0.68\n87.1\n0.45\n\n\n\nT→\\rightarrowT\n49.1\n0.06\n69.5\n1.56\n95.6\n0.45\n\n\nStatic (4)\nS→\\rightarrowS\n40.9\n0.67\n58.5\n0.50\n86.6\n0.52\n\n\n\nT→\\rightarrowT\n48.5\n0.37\n69.4\n0.11\n95.1\n0.19\n\n\nBaseline\nS→\\rightarrowS\n36.5\n0.22\n58.3\n0.21\n86.3\n0.52\n\n\n\nT→\\rightarrowT\n46.3\n0.78\n66.6\n1.56\n93.9\n1.44",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Evaluation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">HellaSwag</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">StoryCloze</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">TopicStoryCloze</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Ave</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Std</td>\n<td class=\"ltx_td ltx_align_center\">Ave</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Std</td>\n<td class=\"ltx_td ltx_align_center\">Ave</td>\n<td class=\"ltx_td ltx_align_center\">Std</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Curriculum</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">41.4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.13</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">59.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.68</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">87.1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.45</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">49.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.06</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.56</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.6</span></td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Static (4)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">40.9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.67</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">58.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">86.6</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.52</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</td>\n<td class=\"ltx_td ltx_align_center\">48.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">69.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.11</td>\n<td class=\"ltx_td ltx_align_center\">95.1</td>\n<td class=\"ltx_td ltx_align_center\">0.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">36.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.22</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">58.3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.21</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">86.3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.52</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">46.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">1.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">93.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.44</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "task",
            "ave",
            "std",
            "s→rightarrows",
            "standard",
            "three",
            "each",
            "directions",
            "topicstorycloze",
            "across",
            "deviation",
            "both",
            "baseline",
            "runs",
            "storycloze",
            "model",
            "evaluation",
            "hellaswag",
            "average",
            "static",
            "reported",
            "curriculum",
            "t→rightarrowt"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T9\" title=\"Table 9 &#8227; A.6 Stability Analysis Across Tasks &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS6.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the <span class=\"ltx_text ltx_font_smallcaps\">L</span>atent Speech-Text Transformer (<span class=\"ltx_text ltx_font_smallcaps\">L</span>ST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "both",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, <cite class=\"ltx_cite ltx_citemacro_cite\">Lakhotia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We show improved performance of <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite>, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib38\" title=\"\">2019</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "hellaswag",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) We demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.</p>\n\n",
                "matched_terms": [
                    "model",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generalized spoken language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> to a sequence of speech tokens <math alttext=\"\\{s_{0},\\dots,s_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{0},\\dots,s_{n}\\}</annotation></semantics></math>, (2) a decoder-only transformer model <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib34\" title=\"\">2017</a>)</cite> with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> that models the distribution of the next speech token given the previous context i.e. <math alttext=\"p_{\\theta}(s_{i}|s_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(s_{i}|s_{&lt;i})</annotation></semantics></math>, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "three",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Data</span> Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models <cite class=\"ltx_cite ltx_citemacro_citep\">(Cuervo &amp; Marxer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite>. In order to bridge the gap, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> find that training on interleaved sequences of text and speech data directly correlates with improved performance.\nFor a subset of the pre-training dataset that contains the textual sequence <math alttext=\"\\{t_{0},\\dots,t_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{0},\\dots,t_{m}\\}</annotation></semantics></math>, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib30\" title=\"\">2023</a>)</cite> in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS1\" title=\"A.1 Interleaved Data Construction &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "each",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core idea of the <span class=\"ltx_text ltx_font_smallcaps\">L</span>ST architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pagnoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib24\" title=\"\">2024</a>)</cite> which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S2.F2\" title=\"Figure 2 &#8227; 2 Background &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Decoder.</span> A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).</p>\n\n",
                "matched_terms": [
                    "each",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span>\nSpeech sequence is split into non-overlapping segments of a fixed length <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:</p>\n\n",
                "matched_terms": [
                    "each",
                    "static"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtain alignments with Wav2Vec2+CTC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib2\" title=\"\">2020</a>)</cite>, yielding one patch per text unit and silence segment (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F3.sf2\" title=\"In Figure 3 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. <em class=\"ltx_emph ltx_font_italic\">Curriculum patching</em> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mitigates this by gradually shifting from aligned to static patching during training.</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span>\n\nCurriculum patching interpolates between alignment-based and static patching during training. Let <math alttext=\"P(u)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(u)\\in[0,1]</annotation></semantics></math> denote the probability of using alignment at training step <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p7.m2\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and text-to-text (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) modes. For the speech mode, we apply Kokoro TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(hexgrad, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib12\" title=\"\">2025</a>)</cite> to generate the speech for evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "three",
                    "model",
                    "evaluation",
                    "t→rightarrowt",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StoryCloze and Topic StoryCloze (SC/TSC).</span> SC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib20\" title=\"\">2016</a>)</cite> and its topic-based extension TSC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> are widely used in prior multimodal work (e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a></cite>) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Static Patching.</span> Fixed-length patches (4 HuBERT tokens) as in <cite class=\"ltx_cite ltx_citemacro_cite\">Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib36\" title=\"\">2023</a>)</cite>, independent of alignment and consistent across training/inference.</p>\n\n",
                "matched_terms": [
                    "static",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum Patching.</span> Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.</p>\n\n",
                "matched_terms": [
                    "static",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Compute-controlled.</span>\nWe fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens <math alttext=\"=\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">=</annotation></semantics></math> LST patches).\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T3\" title=\"Table 3 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows three trends on HellaSwag.\nFirst, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T by +5.2 (47.0&#8594;52.2) and S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S by +6.5 (39.0&#8594;45.5).\nSecond, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.\nFinally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.</p>\n\n",
                "matched_terms": [
                    "three",
                    "curriculum",
                    "across",
                    "evaluation",
                    "hellaswag",
                    "t→rightarrowt",
                    "baseline",
                    "both",
                    "static",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data-controlled.</span>\nHere we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T4\" title=\"Table 4 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S from 40.2 to 45.5.\nSimilar improvements are observed on StoryCloze and TopicStoryCloze.\nOverall, LST with Curriculum Patching reduces the speech&#8211;text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "t→rightarrowt",
                    "both",
                    "baseline",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visualization of Word-Level Speech Patch Embeddings</span>\nWe use t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;der Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib33\" title=\"\">2008</a>)</cite> to project embeddings of representative word groups from the aligned-patching LST model (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Patching &#8227; 3 Latent Speech-Text Transformers &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nAcross different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.\nEach word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail&#8211;sailing show stability under inflection, while semantically similar pairs like scissors&#8211;shears also appear nearby despite being distinct words.\nThese qualitative patterns match quantitative results: within-word similarity is high (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.87), between-word similarity is much lower (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.43), and silhouette scores (0.65&#8211;0.68)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rousseeuw, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib28\" title=\"\">1987</a>)</cite> confirm well-separated clusters.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Trends.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.T5\" title=\"Table 5 &#8227; Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes results at both 1B and 7B scales, while Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S5.F5\" title=\"Figure 5 &#8227; 5 Results &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides the training curve of the 7B model on HellaSwag.\nScaling consistently improves performance across all datasets.\nAt 1B, LST already outperforms the baseline (e.g., 41.3 vs. 36.8 on S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and 49.2 vs. 47.1 on T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T).\nAt 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline&#8217;s 42.0/54.8.\nThe figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "hellaswag",
                    "t→rightarrowt",
                    "baseline",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation on Patching Strategies</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#S6.T6\" title=\"Table 6 &#8227; 6 Related Work &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> compares static and aligned patching.\nAligned patching uses word boundaries from alignment, producing semantically coherent patches.\nWe consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words.\nBoth outperform static patching at similar patch sizes&#8212;for instance, Align (sil sep.) reaches 60.3 on StoryCloze S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 58.7 for static size&#160;6, and Align (sil merged) scores 38.5 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S vs. 37.2 for static size&#160;9.\n<em class=\"ltx_emph ltx_font_italic\">Curriculum</em> starts with <em class=\"ltx_emph ltx_font_italic\">Align (sil sep.)</em> and gradually shifts to <em class=\"ltx_emph ltx_font_italic\">Static</em> during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S).\nOverall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.\nFor completeness, we also report BPE-aligned patching experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.SS5\" title=\"A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "evaluation",
                    "both",
                    "static",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLMs using speech tokens.</span> Early neural audio generation methods included direct auto-regressive generation of the speech waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(van&#160;den Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib32\" title=\"\">2016</a>)</cite>, or using adversarial approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib16\" title=\"\">2020</a>)</cite>. Following this, <span class=\"ltx_text ltx_font_italic\">textless NLP</span> work <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib18\" title=\"\">2021</a>)</cite> showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs. AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib4\" title=\"\">2023</a>)</cite> further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib37\" title=\"\">2021</a>)</cite>, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib23\" title=\"\">2025</a>)</cite> also introduced interleaving speech modeling with text-tokens. More recently, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> propose a hierarchical <span class=\"ltx_text ltx_font_italic\">inner monologue</span> method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Cuervo &amp; Marxer (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib7\" title=\"\">2024</a>)</cite> fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferring textual knowledge into speech LMs.</span> Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib29\" title=\"\">2023</a>; Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech&#8211;text training significantly improves inter-modality knowledge transfer. Spectron <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib21\" title=\"\">Nachmani et&#160;al., </a>)</cite> uses a &#8220;Chain-of-Modality&#8221; pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib8\" title=\"\">2024</a>)</cite> uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib9\" title=\"\">2024</a>)</cite> style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.</p>\n\n",
                "matched_terms": [
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Understanding Benchmarks.</span> Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib15\" title=\"\">2020</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">Nguyen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib22\" title=\"\">2020</a>)</cite> established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute approaches such as ours, do not yield significant improvements. However, subsequently, <cite class=\"ltx_cite ltx_citemacro_cite\">Hassid et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib11\" title=\"\">2023</a>)</cite> introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "hellaswag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech&#8211;text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "baseline",
                    "across",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dynamic sampling.</span> Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T, T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S, and T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T tasks.</p>\n\n",
                "matched_terms": [
                    "t→rightarrowt",
                    "s→rightarrows",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F6\" title=\"Figure 6 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of <cite class=\"ltx_cite ltx_citemacro_citet\">Hoffmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#bib.bib13\" title=\"\">2022</a>)</cite>, allocating 20&#215; the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.\nAt the smallest scale (420M), LST already outperforms the baseline, reaching 29.2% vs. 28.4% for speech and 31.6% vs. 31.1% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0% compared to 35.3%, while LST (Text) achieves 46.3% over 45.7%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "hellaswag",
                    "baseline",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze model preference using the\n<em class=\"ltx_emph ltx_font_italic\">NLL difference</em> in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones.\nMore negative values indicate stronger separation, i.e., the model assigns\nlower NLL to the correct choice relative to distractors.\nLST consistently yields\nlarger-magnitude gaps than the baseline across both text and speech.\nFor speech (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf1\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>), the gap expands from about\n<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math> to <math alttext=\"-1.80\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.80</mn></mrow><annotation encoding=\"application/x-tex\">-1.80</annotation></semantics></math>, while the baseline only improves from <math alttext=\"-0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">-0.45</annotation></semantics></math> to <math alttext=\"-1.39\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.39</mn></mrow><annotation encoding=\"application/x-tex\">-1.39</annotation></semantics></math>.\nFor text (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F7.sf2\" title=\"In Figure 7 &#8227; A.3 Compute-Optimal Scaling &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>), the difference deepens from <math alttext=\"-5.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.8</mn></mrow><annotation encoding=\"application/x-tex\">-5.8</annotation></semantics></math> at 420M to <math alttext=\"-15.6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.6</mn></mrow><annotation encoding=\"application/x-tex\">-15.6</annotation></semantics></math> at 1.8B, compared to the baseline&#8217;s <math alttext=\"-5.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>5.3</mn></mrow><annotation encoding=\"application/x-tex\">-5.3</annotation></semantics></math> to <math alttext=\"-15.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>15.0</mn></mrow><annotation encoding=\"application/x-tex\">-15.0</annotation></semantics></math>.\nSince NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "baseline",
                    "both",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.F8\" title=\"Figure 8 &#8227; A.4 Effect of Speech Proportion &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the effect of varying the training speech&#8211;to&#8211;text token ratio on HellaSwag.\nAcross all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech&#8211;text trade-off at the <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> ratio.\nMoving from <math alttext=\"1{:}3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">1{:}3</annotation></semantics></math> to <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> improves LST (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) from <math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math> to <math alttext=\"41.3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mn>41.3</mn><annotation encoding=\"application/x-tex\">41.3</annotation></semantics></math> while keeping LST (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T) high at <math alttext=\"49.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mn>49.7</mn><annotation encoding=\"application/x-tex\">49.7</annotation></semantics></math>; pushing further to <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> does not provide speech gain (<math alttext=\"41.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mn>41.2</mn><annotation encoding=\"application/x-tex\">41.2</annotation></semantics></math>) but a large text drop (<math alttext=\"47.2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mn>47.2</mn><annotation encoding=\"application/x-tex\">47.2</annotation></semantics></math>, <math alttext=\"-2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m12\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.5</mn></mrow><annotation encoding=\"application/x-tex\">-2.5</annotation></semantics></math>).\nThe baseline shows the same pattern: at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> it reaches <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m14\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math> (S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S) and <math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math> (T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T), whereas <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m18\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> gives only <math alttext=\"37.0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m19\" intent=\":literal\"><semantics><mn>37.0</mn><annotation encoding=\"application/x-tex\">37.0</annotation></semantics></math> on speech (+0.2) but lowers text to <math alttext=\"45.4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m20\" intent=\":literal\"><semantics><mn>45.4</mn><annotation encoding=\"application/x-tex\">45.4</annotation></semantics></math> (<math alttext=\"-1.7\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m21\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.7</mn></mrow><annotation encoding=\"application/x-tex\">-1.7</annotation></semantics></math>).\nAveraging speech and text accuracies, the macro score peaks at <math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m22\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math> for both LST and the baseline.\nThese results indicate that allocating about one-third of tokens to speech (<math alttext=\"1{:}2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m23\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1{:}2</annotation></semantics></math>) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at <math alttext=\"1{:}1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m24\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1{:}1</annotation></semantics></math> while securing clear gains over lower speech ratios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "hellaswag",
                    "t→rightarrowt",
                    "baseline",
                    "both",
                    "s→rightarrows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to word-aligned patching, we also explored BPE-aligned patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06195v1#A1.T8\" title=\"Table 8 &#8227; A.5 BPE-Aligned Patching &#8227; Appendix A Appendix &#8227; Latent Speech-Text Transformer\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, word alignment generally outperforms BPE alignment in S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S (e.g., 59.4 vs. 55.6 on StoryCloze and 84.8 vs. 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>T results, likely because its patching\nis directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.</p>\n\n",
                "matched_terms": [
                    "topicstorycloze",
                    "storycloze",
                    "curriculum",
                    "hellaswag",
                    "t→rightarrowt",
                    "s→rightarrows"
                ]
            }
        ]
    }
}