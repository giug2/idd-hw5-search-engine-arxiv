{
    "S1.T1": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 1: Categories of stuttering with representative phonological examples.",
        "body": "Classification\n\n\nExample pattern\n\n\n\n\nSound repetition (SoundRep)\n\n\n“a-a-and”\n\n\n\n\nWord repetition (WordRep)\n\n\n“and and”\n\n\n\n\nInterjection\n\n\n“I think that—uhmm”\n\n\n\n\nBlock\n\n\n“I think… <pause> …that”\n\n\n\n\nProlongation\n\n\n“sooooo”",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Classification</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Example pattern</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Sound repetition (SoundRep)</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8220;a-a-and&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Word repetition (WordRep)</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8220;and and&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Interjection</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8220;I think that&#8212;uhmm&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Block</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8220;I think&#8230; </span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">&lt;pause&gt;</span><span class=\"ltx_text\" style=\"font-size:80%;\"> &#8230;that&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Prolongation</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\" style=\"padding-top:0.2pt;padding-bottom:0.2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8220;sooooo&#8221;</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "and”",
            "prolongation",
            "categories",
            "soundrep",
            "“aaand”",
            "classification",
            "example",
            "pause",
            "repetition",
            "“and",
            "phonological",
            "examples",
            "block",
            "word",
            "that—uhmm”",
            "representative",
            "“sooooo”",
            "wordrep",
            "stuttering",
            "sound",
            "…that”",
            "think",
            "interjection",
            "think…",
            "pattern"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Stuttering is a prevalent speech disorder that affects more than 70 million individuals worldwide <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ghai2021</span>]</cite>. It manifests as interruptions in speech flow, including unintentional repetitions, prolongations, and pauses. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the five main phonological classifications of stuttering and examples of their symptoms <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kourkounakis2020a</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">basak2023</span>]</cite>. Such interruptions can significantly hinder effective communication, frequently causing social anxiety, depression, and a diminished quality of life <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2009</span>]</cite>. In children, stuttering may lead to bullying and social isolation, exacerbating the emotional and psychological difficulties they encounter <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2016</span>]</cite>.</p>\n\n",
            "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "word",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "word",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "prolongation",
                    "repetition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, StutterNet, introduced by Sheikh et al., is a multi-class Time Delay Neural Network (TDNN). Because TDNNs use windows of time-delayed inputs, they are especially well-suited to temporal dependencies such as MFCC speech features. While StutterNet was only able to detect stutters, it is plausible that similar systems could be used to flag sections of stuttered audio for removal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2021</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T3\" title=\"Table 3 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes representative DSP-based systems, their methodological choices, and reported performance.</p>\n\n",
                "matched_terms": [
                    "example",
                    "representative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "classification",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "word",
                    "pattern"
                ]
            }
        ]
    },
    "S1.T2": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 2: Performance of DSP-based stutter removal methods using MFCC and LPC features across two stuttering types [kn2020].",
        "body": "Stuttering Type\n\n\nFeatures Used\n\n\nTotal\nRemoved\nRetained\nAccuracy\n\n\nRepetition\n\n\nMFCC\n\n\n60\n54\n6\n90.0%\n\n\n\n\nLPC\n\n\n60\n52\n8\n86.7%\n\n\nProlongation\n\n\nMFCC\n\n\n70\n67\n3\n95.7%\n\n\n\n\nLPC\n\n\n70\n65\n5\n92.9%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Stuttering Type</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Features Used</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Removed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Retained</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Repetition</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">MFCC</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">90.0%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">LPC</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.7%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Prolongation</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">MFCC</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">95.7%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">LPC</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">92.9%</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "features",
            "prolongation",
            "types",
            "mfcc",
            "used",
            "across",
            "repetition",
            "methods",
            "lpc",
            "removal",
            "performance",
            "accuracy",
            "dspbased",
            "retained",
            "stuttering",
            "kn2020",
            "two",
            "total",
            "removed",
            "stutter"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "stuttering",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fluent speech plays a critical role in daily communication, both in interpersonal situations and when engaging with intelligent voice-based technologies. Through a quality-of-life survey, people who stutter (PWS) showed a statistically significant decrease in emotional health and social function metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kasbi2015</span>]</cite>. Stuttering can also cause feelings of shame, fear, anxiety, and guilt <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bloodstein2021</span>]</cite>. These challenges are exacerbated by the growing dependence on intelligent voice assistants, such as Google Home, Amazon Echo, and Apple Siri, which are designed to process fluent speech. As a result, people who stutter frequently encounter recognition errors, limited functionality, and exclusion when interacting with voice-controlled technologies.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DSP approaches utilize audio feature extraction methods to obtain condensed numerical features from complex audio signals. Then, a ruleset or a set of predetermined filters is applied to the features to determine which timeframes contain a stutter. Finally, these timeframes are cut out of the audio, removing the stutter. Some frequently utilized feature sets include Mel-Frequency Cepstral Coefficients (MFCCs), Linear Predictive Coding (LPC), and Linear Predictive Cepstral Coefficients (LPCCs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite>. For instance, MFCC features are generated by first applying the Fast Fourier Transform (FFT) to convert an audio sample from the amplitude domain to the frequency domain, generating the power spectrum. After that, the Mel filter bank is used to map the power spectrum to Mel frequencies, employing a set of nonlinear triangular filters. This aligns the intensity of frequencies to match the nonlinearity of human auditory perception. Finally, a Discrete Cosine Transform (DCT) is used to generate the cepstral coefficients through the decorrelation of features. This pipeline is applied to every window of audio, usually 20&#8211;50 ms long <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "lpc",
                    "kn2020",
                    "features",
                    "methods",
                    "mfcc",
                    "used",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some cases, low-level features such as MFCC, LPC, and LPCC are extracted from the audio and used as input to neural networks or machine learning models, which then classify whether a stutter has occurred at each point in time.</p>\n\n",
                "matched_terms": [
                    "lpc",
                    "features",
                    "mfcc",
                    "used",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, StutterNet, introduced by Sheikh et al., is a multi-class Time Delay Neural Network (TDNN). Because TDNNs use windows of time-delayed inputs, they are especially well-suited to temporal dependencies such as MFCC speech features. While StutterNet was only able to detect stutters, it is plausible that similar systems could be used to flag sections of stuttered audio for removal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2021</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T3\" title=\"Table 3 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes representative DSP-based systems, their methodological choices, and reported performance.</p>\n\n",
                "matched_terms": [
                    "dspbased",
                    "features",
                    "removal",
                    "mfcc",
                    "performance",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach to stutter correction involves fine-tuning or training an ASR model on stuttered speech such that the ASR model can generate transcripts of that speech. The ASR model can either explicitly transcribe the stutter into text or ignore the stuttered portions, producing a fluent transcript. That transcript can be filtered with rule-based systems and finally passed through a TTS model to produce fluent audio sequences. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.F1\" title=\"Figure 1 &#8227; 1.0.2 ASR &amp; TTS-based Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the general pipeline to achieve ASR &amp; TTS-based stutter correction. This approach helps to omit artifacts caused by splicing audio in the DSP-based approaches, since TTS models are generating new audio sequences. However, DSP methods can preserve speaker prosody more naturally, as they simply edit the original speech. For a TTS model to mimic the tone and prosody of the original speaker, it would need more utterances to fine-tune on.</p>\n\n",
                "matched_terms": [
                    "dspbased",
                    "methods",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While there is considerable work on end-to-end speech conversion of fluent speech and other speech impairments such as dysarthria, there is little research on deep learning or end-to-end stutter correction. This paper first reviews deep learning and end-to-end methods for fluent speech recognition, conversion, and the correction of dysarthric speech.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of end-to-end models stems from their ability to consolidate the entire training and inference pipeline into a single model optimized with one objective function that directly reflects the training goal. In contrast, a traditional DSP-based pipeline might first extract MFCC features from audio and then train a neural network to classify whether each frame contains a stutter. The neural network in this case is trained for frame-level accuracy in detecting stutters, which does not align with removing stutters at the speech sequence level <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chan2016</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">graves2014</span>]</cite>. The lack of manual feature engineering in end-to-end models also allows for greater generalizability and adaptability to similar problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hannun2014</span>]</cite>. Finally, end-to-end models such as encoder-decoder and RNN-Transducer models are suitable for sequence-to-sequence tasks such as speech conversion, as they do not require alignment of acoustic sequences to ground truth transcripts and are very flexible with input and output sequence lengths <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dspbased",
                    "features",
                    "mfcc",
                    "accuracy",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite> introduced an end-to-end encoder-decoder for dysarthric speech correction. Three multitask encoders were used: a content encoder to learn underlying semantic meaning, a prosody encoder to learn and correct audio features salient to dysarthria, and a speaker encoder to capture prosody and recreate the tone and voice of the original speaker. A decoder aggregates hidden representations from all three layers and generates acoustic features from which audio can be reconstructed using a vocoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "across",
                    "two",
                    "accuracy",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "used",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "stutter",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that all audio contained speech, audio files labeled &#8221;Music&#8221; or &#8221;NoSpeech&#8221; were removed from the SEP-28K dataset. To account for bias or skew in the frequency of each type of stutter, data resampling was conducted on SEP-28K to balance out stutter categories that may have been less common.</p>\n\n",
                "matched_terms": [
                    "removed",
                    "type",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After cleaning, approximately 34 hours of stuttered audio were retained across the SEP-28K and LibriStutter datasets, with no overlap in speakers or transcripts between the two corpora.</p>\n\n",
                "matched_terms": [
                    "retained",
                    "two",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The University College London Archive of Stuttered Speech (UCLASS) and the FluencyBank dataset are the two other well-known stuttering datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zotero-item-1105</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bernsteinratner2018</span>]</cite>. However, both datasets have some limitations. The UCLASS dataset does not provide fluent &#8221;ground truth&#8221; transcripts for the stuttered speech, so there are no reference transcripts to fine-tune Whisper-Small on.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small&#8217;s WhisperFeatureExtractor uses a Short-Time Fourier Transform (STFT), computing the Fast Fourier Transform (FFT) on overlapping segments, or &#8220;windows,&#8221; of the audio as it slides across the entire signal. Then, the spectrogram is converted into a log-Mel spectrogram by mapping the frequencies from the &#8220;vanilla&#8221; spectrogram onto the Mel scale. This is done because human hearing does not perceive pitch in a linear manner; humans are more attuned to changes in lower pitches than in higher pitches. The linear frequency spectrogram is passed through a Mel filter bank, which applies a series of triangular filters to aggregate spectral energy within perceptually relevant frequency bands. Once a spectrogram is transformed into a log-Mel spectrogram, equal intervals on the Mel scale reflect equal perceived differences in pitch. The practical effect is that Mel spectrograms highlight the frequencies of human speech while minimizing the intensity of background noise, allowing the model to concentrate solely on the speech signals <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stevens1937</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F2\" title=\"Figure 2 &#8227; 2.3 Whisper-Small Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> displays visualized spectrograms of the conversion process used to obtain a log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "used",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "used",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "types",
                    "two",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "two",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the previously predicted spectrogram and the current context vector as inputs. Two fully connected layers form a pre-net that compress and transform the previous spectrogram frames into a lower-dimensional representation. A well-trained pre-net simplifies the input and extracts the most salient features. If the raw spectrogram frames were used, the decoder might &#8221;shortcut&#8221; the learning process by copying the previous frame or minimally modifying it <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "two",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that all three proposed models&#8212;ASR-based, StutterZero, and StutterFormer&#8212;outperform the best Whisper baseline (Whisper-Medium) across both WER and CER metrics. The Whisper-Medium model achieves a WER of 0.361 and a CER of 0.162, while the ASR-based approach dramatically reduces these errors to 0.04 and 0.02, respectively. This indicates a substantial improvement in transcription accuracy and character-level precision.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "removed",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces three stutter correction systems, including the first two end-to-end encoder&#8211;decoder models, and provides evidence that direct conversion of stuttered to fluent audio is both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "across",
                    "types",
                    "performance",
                    "accuracy",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The observed performance improvements resonate with trends in related domains, such as dysarthric and accented speech recognition, where multitask and end-to-end approaches have consistently demonstrated robustness to atypical input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>. By showing that stutter correction benefits strongly from the integration of textual and acoustic objectives, our findings suggest that linguistic supervision is not merely auxiliary but foundational. This echoes psycholinguistic accounts that disfluency cannot be treated as random noise but reflects systematic deviations in speech planning and motor execution. In this sense, our work challenges the assumption &#8211; common in early DSP and ASR pipelines &#8211; that acoustic correction can be divorced from lexical anchoring.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The superior performance of StutterFormer over StutterZero cannot be attributed solely to increased model capacity. Rather, Transformer attention provides specific advantages for disfluency correction. Multi-head self-attention captures long-range dependencies, allowing the model to flexibly relate repeated or prolonged segments of speech to their fluent counterparts. This mechanism facilitates alignment across syllables and words, which is essential when disfluencies span multiple phonemic units. In addition, attention layers can simultaneously model both global and local phonetic details, helping to preserve intonation and rhythm while removing interruptions. These properties explain why attention-based architectures are particularly well-suited for mapping stuttered to fluent speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeyer2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity of the training corpus was constrained by limited access to datasets such as UCLASS or FluencyBank. The absence of these datasets restricted coverage of various speaker demographics, accents, and tones, which in turn limits generalizability. There may be risks of overfitting and undergeneralization when training on only the SEP-28K data from the LibriStutter datasets. Even though five-fold cross-validation and testing on a completely new dataset was used to produce a candid estimate of the true performance both approaches, future steps should focus on data augmentation and training on larger datasets. Additional experiments using diverse corpora such as UCLASS or AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite> are needed.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "stutter",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research originated from personal initiative, as the student researcher has a stutter and sought to address this challenge through computational methods.\nThe author expresses sincere gratitude to Mr. Cook, teacher of the Millburn High School Science Research Program, for his invaluable mentorship. Mr. Cook provided detailed guidance on the scientific process &#8212; from formulating research questions and establishing goals to conducting literature reviews and writing in accordance with academic standards. Equally important, he encouraged students to reach out to professors and domain experts, a practice that significantly shaped the independent yet rigorous nature of this work.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "methods"
                ]
            }
        ]
    },
    "S1.T3": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 3: Summary of studies on stutter detection, including datasets, feature extraction methods, detection approaches, stutter types analyzed, and best reported accuracies [kn2020].",
        "body": "Author(s)\n\n\n\n\nDataset\n\n\n\n\nFeature Extraction\n\n\n\n\nDetection Method\n\n\n\n\nStutter Type(s)\n\n\n\n\nBest Accuracy\n\n\n\n\n\n\nMishra et al., 2021\n\n\n\n\nUCLASS\n\n\n\n\nMFCC, RMSE\n\n\n\n\nDeep neural network\n\n\n\n\nRepetition, Prolongation, Block\n\n\n\n\n86.67%\n\n\n\n\n\n\nDash et al., 2018\n\n\n\n\nPrivate human speech samples\n\n\n\n\nAmplitude\n\n\n\n\nDeep neural network\n\n\n\n\nProlongation\n\n\n\n\n86%\n\n\n\n\n\n\nShonibare et al., 2022\n\n\n\n\nPrivate human speech samples\n\n\n\n\nlog-Mel Spectrograms\n\n\n\n\nCNN\n\n\n\n\nRepetition, Block, Prolongation\n\n\n\n\n71.24% reduction in WER",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Author(s)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Feature Extraction</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Detection Method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Stutter Type(s)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Best Accuracy</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Mishra et al., 2021</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">UCLASS</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">MFCC, RMSE</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Deep neural network</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Repetition, Prolongation, Block</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">86.67%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Dash et al., 2018</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Private human speech samples</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Amplitude</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Deep neural network</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Prolongation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">86%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Shonibare et al., 2022</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Private human speech samples</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">log-Mel Spectrograms</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">CNN</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Repetition, Block, Prolongation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:0.45pt 4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">71.24% reduction in WER</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "detection",
            "cnn",
            "logmel",
            "prolongation",
            "types",
            "datasets",
            "including",
            "studies",
            "accuracies",
            "mfcc",
            "spectrograms",
            "speech",
            "repetition",
            "neural",
            "methods",
            "human",
            "wer",
            "extraction",
            "analyzed",
            "summary",
            "private",
            "deep",
            "network",
            "amplitude",
            "dataset",
            "block",
            "rmse",
            "samples",
            "accuracy",
            "approaches",
            "feature",
            "reported",
            "kn2020",
            "authors",
            "uclass",
            "reduction",
            "mishra",
            "best",
            "method",
            "dash",
            "shonibare",
            "stutter"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For example, StutterNet, introduced by Sheikh et al., is a multi-class Time Delay Neural Network (TDNN). Because TDNNs use windows of time-delayed inputs, they are especially well-suited to temporal dependencies such as MFCC speech features. While StutterNet was only able to detect stutters, it is plausible that similar systems could be used to flag sections of stuttered audio for removal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2021</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T3\" title=\"Table 3 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes representative DSP-based systems, their methodological choices, and reported performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature",
                    "dataset",
                    "methods",
                    "wer",
                    "extraction",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">automatic speech recognition, deep learning, human-computer interaction, speech correction, speech processing, stuttering\n\n<span class=\"ltx_ERROR undefined\">\\IEEEpeerreviewmaketitle</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "deep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stuttering is a prevalent speech disorder that affects more than 70 million individuals worldwide <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ghai2021</span>]</cite>. It manifests as interruptions in speech flow, including unintentional repetitions, prolongations, and pauses. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the five main phonological classifications of stuttering and examples of their symptoms <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kourkounakis2020a</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">basak2023</span>]</cite>. Such interruptions can significantly hinder effective communication, frequently causing social anxiety, depression, and a diminished quality of life <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2009</span>]</cite>. In children, stuttering may lead to bullying and social isolation, exacerbating the emotional and psychological difficulties they encounter <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2016</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fluent speech plays a critical role in daily communication, both in interpersonal situations and when engaging with intelligent voice-based technologies. Through a quality-of-life survey, people who stutter (PWS) showed a statistically significant decrease in emotional health and social function metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kasbi2015</span>]</cite>. Stuttering can also cause feelings of shame, fear, anxiety, and guilt <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bloodstein2021</span>]</cite>. These challenges are exacerbated by the growing dependence on intelligent voice assistants, such as Google Home, Amazon Echo, and Apple Siri, which are designed to process fluent speech. As a result, people who stutter frequently encounter recognition errors, limited functionality, and exclusion when interacting with voice-controlled technologies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "reported",
                    "deep",
                    "wer",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work in the field of stutter correction can be separated into three categories: (1) Digital signal processing (DSP) and rule-based classifiers, (2) ASR and text-to-speech (TTS) pipelines, and (3) deep learning (DL) approaches.</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "stutter",
                    "deep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DSP approaches utilize audio feature extraction methods to obtain condensed numerical features from complex audio signals. Then, a ruleset or a set of predetermined filters is applied to the features to determine which timeframes contain a stutter. Finally, these timeframes are cut out of the audio, removing the stutter. Some frequently utilized feature sets include Mel-Frequency Cepstral Coefficients (MFCCs), Linear Predictive Coding (LPC), and Linear Predictive Cepstral Coefficients (LPCCs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite>. For instance, MFCC features are generated by first applying the Fast Fourier Transform (FFT) to convert an audio sample from the amplitude domain to the frequency domain, generating the power spectrum. After that, the Mel filter bank is used to map the power spectrum to Mel frequencies, employing a set of nonlinear triangular filters. This aligns the intensity of frequencies to match the nonlinearity of human auditory perception. Finally, a Discrete Cosine Transform (DCT) is used to generate the cepstral coefficients through the decorrelation of features. This pipeline is applied to every window of audio, usually 20&#8211;50 ms long <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "kn2020",
                    "amplitude",
                    "human",
                    "methods",
                    "extraction",
                    "mfcc",
                    "stutter",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "kn2020",
                    "prolongation",
                    "repetition",
                    "mfcc",
                    "accuracy",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some cases, low-level features such as MFCC, LPC, and LPCC are extracted from the audio and used as input to neural networks or machine learning models, which then classify whether a stutter has occurred at each point in time.</p>\n\n",
                "matched_terms": [
                    "mfcc",
                    "neural",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach to stutter correction involves fine-tuning or training an ASR model on stuttered speech such that the ASR model can generate transcripts of that speech. The ASR model can either explicitly transcribe the stutter into text or ignore the stuttered portions, producing a fluent transcript. That transcript can be filtered with rule-based systems and finally passed through a TTS model to produce fluent audio sequences. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.F1\" title=\"Figure 1 &#8227; 1.0.2 ASR &amp; TTS-based Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the general pipeline to achieve ASR &amp; TTS-based stutter correction. This approach helps to omit artifacts caused by splicing audio in the DSP-based approaches, since TTS models are generating new audio sequences. However, DSP methods can preserve speaker prosody more naturally, as they simply edit the original speech. For a TTS model to mimic the tone and prosody of the original speaker, it would need more utterances to fine-tune on.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter",
                    "methods",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "reported",
                    "private",
                    "datasets",
                    "methods",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the adaptability of fine-tuned ASR models, there has been research into fine-tuning pre-trained ASR models for other speech impairments such as dysarthria. Wang et al. fine-tuned wav2vec2.0 and HuBERT ASR models on dysarthria datasets. They achieved the best WER of 16.53% by fine-tuning a pre-trained wav2vec2.0 ASR model on a dysarthria corpus augmented by a generative adversarial network. Even without any data augmentation, they obtained a WER of 22.25% on a fine-tuned wav2vec2.0 model and a WER of 21.10% on a fine-tuned HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024a</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "network",
                    "best",
                    "datasets",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning approaches for speech recognition and speech conversion have been the subject of extensive investigation. Speech conversion focuses on modifying or transforming a speech signal, such as changing the speaker&#8217;s voice or style, while preserving the original linguistic content. In contrast, speech recognition involves accurately transcribing spoken language into text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">triantafyllopoulos2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">walczyna2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>. Deep learning approaches for speech processing typically involve training neural networks or other computational models, eliminating the need for manual feature engineering. Recent advancements in deep learning have popularized end-to-end models, systems that learn to perform the entire correction process directly from input data without requiring handcrafted features or intermediate steps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature",
                    "deep",
                    "neural",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While there is considerable work on end-to-end speech conversion of fluent speech and other speech impairments such as dysarthria, there is little research on deep learning or end-to-end stutter correction. This paper first reviews deep learning and end-to-end methods for fluent speech recognition, conversion, and the correction of dysarthric speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter",
                    "methods",
                    "deep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of end-to-end models stems from their ability to consolidate the entire training and inference pipeline into a single model optimized with one objective function that directly reflects the training goal. In contrast, a traditional DSP-based pipeline might first extract MFCC features from audio and then train a neural network to classify whether each frame contains a stutter. The neural network in this case is trained for frame-level accuracy in detecting stutters, which does not align with removing stutters at the speech sequence level <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chan2016</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">graves2014</span>]</cite>. The lack of manual feature engineering in end-to-end models also allows for greater generalizability and adaptability to similar problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hannun2014</span>]</cite>. Finally, end-to-end models such as encoder-decoder and RNN-Transducer models are suitable for sequence-to-sequence tasks such as speech conversion, as they do not require alignment of acoustic sequences to ground truth transcripts and are very flexible with input and output sequence lengths <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature",
                    "network",
                    "neural",
                    "mfcc",
                    "accuracy",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, Toshniwal et al. developed an attention-based encoder-decoder model inspired by recurrent neural networks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite>. A speech encoder is built on a deep bidirectional long short-term memory (BiLSTM) network, which produces a sequence of abstract hidden representations. These hidden representations are passed into the character decoder, which is a single-layer LSTM that predicts the most likely letter uttered <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "neural",
                    "deep",
                    "network"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "detection",
                    "deep",
                    "accuracy",
                    "stutter",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that all audio contained speech, audio files labeled &#8221;Music&#8221; or &#8221;NoSpeech&#8221; were removed from the SEP-28K dataset. To account for bias or skew in the frequency of each type of stutter, data resampling was conducted on SEP-28K to balance out stutter categories that may have been less common.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The University College London Archive of Stuttered Speech (UCLASS) and the FluencyBank dataset are the two other well-known stuttering datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zotero-item-1105</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bernsteinratner2018</span>]</cite>. However, both datasets have some limitations. The UCLASS dataset does not provide fluent &#8221;ground truth&#8221; transcripts for the stuttered speech, so there are no reference transcripts to fine-tune Whisper-Small on.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uclass",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain fluent speech data to train StutterZero and StutterFormer as end-to-end models, this research also developed an auxiliary pipeline as described below to first generate fluent speech data, effectively &#8221;completing&#8221; both datasets:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study fine-tuned Whisper-Small, a lightweight derivative of the Whisper family of ASR models. Audio data from both datasets are in the amplitude domain, though Whisper-Small accepts a spectrogram in the frequency domain as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "amplitude",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small&#8217;s WhisperFeatureExtractor uses a Short-Time Fourier Transform (STFT), computing the Fast Fourier Transform (FFT) on overlapping segments, or &#8220;windows,&#8221; of the audio as it slides across the entire signal. Then, the spectrogram is converted into a log-Mel spectrogram by mapping the frequencies from the &#8220;vanilla&#8221; spectrogram onto the Mel scale. This is done because human hearing does not perceive pitch in a linear manner; humans are more attuned to changes in lower pitches than in higher pitches. The linear frequency spectrogram is passed through a Mel filter bank, which applies a series of triangular filters to aggregate spectral energy within perceptually relevant frequency bands. Once a spectrogram is transformed into a log-Mel spectrogram, equal intervals on the Mel scale reflect equal perceived differences in pitch. The practical effect is that Mel spectrograms highlight the frequencies of human speech while minimizing the intensity of background noise, allowing the model to concentrate solely on the speech signals <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stevens1937</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F2\" title=\"Figure 2 &#8227; 2.3 Whisper-Small Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> displays visualized spectrograms of the conversion process used to obtain a log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "human",
                    "spectrograms",
                    "logmel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small computes log-Mel spectrograms with 25-ms-long windows that move forward by 10 milliseconds at every time step.</p>\n\n",
                "matched_terms": [
                    "spectrograms",
                    "logmel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "spectrograms",
                    "types",
                    "logmel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "spectrograms",
                    "logmel",
                    "network",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like StutterZero, StutterFormer employs a hybrid loss function that combines multiple weighted losses.\nThe spectrogram decoder also uses MSE loss between the predicted and ground truth spectrograms. In addition to MSE loss, the spectrogram decoder also computes a mean absolute error (MAE) loss. MAE loss measures the absolute difference of the predicted values and target values without squaring. When tested with spectrogram applications, MAE loss promotes sharper spectrograms that prevent the predicted fluent speech from sounding slurred <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guso2022</span>]</cite>. The transcript decoder also uses cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spectrograms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR-based and end-to-end approaches used in this study, state-of-the-art fluent-speech ASR models were also evaluated to establish a baseline for how accurately current speech recognition systems transcribe stuttered speech. This study used the Whisper-Tiny, Whisper-Small, and Whisper-Medium pretrained fluent ASR models as baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. Any larger models such as Whisper-Large could not be tested due to memory and hardware limitations. The 10% validation data split was used to assess each of the six models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because both approaches in this research produce audio signals, the audio signals must be converted back to text before any word or character-level evaluation. This study used an unmodified, pretrained copy of the Whisper-Small ASR model to transcribe the predicted fluent sequences from both the ASR and end-to-end approaches, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.F5\" title=\"Figure 5 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. This was an attempt to simulate what an untrained, average English-speaking person would interpret the fluent speech to be. The predicted transcripts of the fluent speech were compared with ground truth transcripts to calculate the final metrics. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the mean WER, CER, BERTScore Precision, and their standard deviations in the validation data split.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "approaches",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that all three proposed models&#8212;ASR-based, StutterZero, and StutterFormer&#8212;outperform the best Whisper baseline (Whisper-Medium) across both WER and CER metrics. The Whisper-Medium model achieves a WER of 0.361 and a CER of 0.162, while the ASR-based approach dramatically reduces these errors to 0.04 and 0.02, respectively. This indicates a substantial improvement in transcription accuracy and character-level precision.</p>\n\n",
                "matched_terms": [
                    "best",
                    "accuracy",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">FluencyBank is a subset of the TalkBank corpus, an open repository for spoken language data. FluencyBank specifically focuses on disfluencies, including stuttering. Because the FluencyBank dataset is password-protected and it is not possible to automate the scraping of data from the website, this research manually downloaded audio clips from a randomly selected recording along with their transcripts from the Voice-AdultsWhoStutter (Voices-AWS) subset. After downloading and splitting each audio file to be 30 seconds or less, there were 800 stuttered audio samples.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "dataset",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "dataset",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T9\" title=\"Table 9 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> presents a qualitative comparison between the stuttered and fluent spectrograms for three speech samples from the FluencyBank dataset. The regions enclosed by green rectangles highlight sections containing a stutter in the original audio or the corresponding fluent segments after correction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "samples",
                    "spectrograms",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "spectrograms",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces three stutter correction systems, including the first two end-to-end encoder&#8211;decoder models, and provides evidence that direct conversion of stuttered to fluent audio is both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer significantly outperformed state-of-the-art fluent speech recognition models in WER, CER, and BERTScore. The Wilcoxon Signed-Rank Test demonstrated the improvements (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>) of StutterZero and StutterFormer over the next best model, Whisper-Medium. This demonstrates the effectiveness of multitask, end-to-end models for stuttered speech recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "best",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "types",
                    "datasets",
                    "accuracy",
                    "stutter",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The observed performance improvements resonate with trends in related domains, such as dysarthric and accented speech recognition, where multitask and end-to-end approaches have consistently demonstrated robustness to atypical input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>. By showing that stutter correction benefits strongly from the integration of textual and acoustic objectives, our findings suggest that linguistic supervision is not merely auxiliary but foundational. This echoes psycholinguistic accounts that disfluency cannot be treated as random noise but reflects systematic deviations in speech planning and motor execution. In this sense, our work challenges the assumption &#8211; common in early DSP and ASR pipelines &#8211; that acoustic correction can be divorced from lexical anchoring.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A substantial portion of training and evaluation relied on fluent audio generated by TTS systems. While this strategy enabled efficient creation of large-scale paired datasets, it introduced a prosodic mismatch between synthetic and natural speech. As a result, models may have partially learned to adapt to synthetic rhythms and intonation rather than capturing the full variability of natural stuttering. This could limit robustness when applied to spontaneous, emotionally nuanced speech. Future research should mitigate this gap by curating larger collections of natural stutter&#8211;fluent pairs, leveraging prosody encoders to capture expressive detail, and employing domain adaptation techniques to reduce distributional bias.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity of the training corpus was constrained by limited access to datasets such as UCLASS or FluencyBank. The absence of these datasets restricted coverage of various speaker demographics, accents, and tones, which in turn limits generalizability. There may be risks of overfitting and undergeneralization when training on only the SEP-28K data from the LibriStutter datasets. Even though five-fold cross-validation and testing on a completely new dataset was used to produce a candid estimate of the true performance both approaches, future steps should focus on data augmentation and training on larger datasets. Additional experiments using diverse corpora such as UCLASS or AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite> are needed.</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "uclass",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate the prosodic loss due to training on TTS-generated fluent speech samples, using multiple encoders to capture the tonal and prosodic content of the stuttered speech could be explored. Multi-encoder models have been explored in dysarthic speech conversion, specifically using a prosody encoder to extract prosodic features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>. This would enable fluent outputs that retain the speaker&#8217;s identity, pitch, and expressive nuance. Additionally, expanding to multilingual and low-resource languages through cross-lingual pretraining and transfer learning would extend accessibility to a wider global population <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The current study is limited by access to a small number of corpora. Expanding to diverse datasets such as UCLASS, FluencyBank, and AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>, as well as developing multilingual training resources, would enhance model generalizability. Cross-lingual pretraining and transfer learning offer promising strategies to extend accessibility to low-resource and global language communities, ensuring that stutter correction technology benefits a wider population.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "datasets",
                    "uclass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "neural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond therapy, optimized versions of StutterZero and StutterFormer could be deployed in real-time communication settings. With techniques such as model quantization, pruning, and knowledge distillation, lightweight implementations may run on mobile or embedded devices. Potential applications include live correction during phone calls, video conferences, and online meetings, where disfluent speech is automatically converted to fluent audio for listeners. Similar methods could also be applied to voice recording, broadcasting, and sound engineering, eliminating the need for re-recording when disfluencies occur.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research originated from personal initiative, as the student researcher has a stutter and sought to address this challenge through computational methods.\nThe author expresses sincere gratitude to Mr. Cook, teacher of the Millburn High School Science Research Program, for his invaluable mentorship. Mr. Cook provided detailed guidance on the scientific process &#8212; from formulating research questions and establishing goals to conducting literature reviews and writing in accordance with academic standards. Equally important, he encouraged students to reach out to professors and domain experts, a practice that significantly shaped the independent yet rigorous nature of this work.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Special thanks are also extended to Professor JoAnne Cascia and Dr. Iyad Ghanim of Kean University, specialists in speech-language pathology, who provided critical insights into the clinical relevance and potential applications of this research. Their perspectives grounded the project in real-world practice, highlighting how computational models may complement therapeutic approaches and better serve individuals who stutter. Their contributions were advisory only, without involvement in coding or writing, and were provided entirely without compensation.</p>\n\n",
                "matched_terms": [
                    "stutter",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the author acknowledges the broader research community that made this project possible. The datasets employed &#8212; including SEP-28k and LibriStutter &#8212; are publicly available and de-identified, developed by teams committed to advancing accessibility in speech technology. Without these resources, this project would not have been feasible.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets",
                    "including"
                ]
            }
        ]
    },
    "S2.T4": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 4: Training hyperparameters used for the StutterZero Adam optimizer.",
        "body": "Parameter\nValue\n\n\n\nLearning Rate\n1​e−41e^{-4}\n\n\n\nWeight Decay\n1​e−61e^{-6}\n\n\n\nBatch Size\n33\n\n\n\nBetas (Momentum parameters)\n(0.9,0.999)(0.9,0.999)\n\n\n\nEpsilon\n1​e−61e^{-6}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Parameter</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Value</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Learning Rate</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T4.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Weight Decay</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T4.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Batch Size</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T4.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Betas (Momentum parameters)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"(0.9,0.999)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T4.m4\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mn mathsize=\"0.800em\">0.9</mn><mo mathsize=\"0.800em\">,</mo><mn mathsize=\"0.800em\">0.999</mn><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(0.9,0.999)</annotation></semantics></math></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Epsilon</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T4.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "size",
            "rate",
            "parameter",
            "betas",
            "used",
            "hyperparameters",
            "stutterzero",
            "adam",
            "momentum",
            "decay",
            "learning",
            "epsilon",
            "weight",
            "1​e−41e4",
            "optimizer",
            "value",
            "batch",
            "1​e−61e6",
            "parameters"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">StutterZero used standard stochastic gradient descent with the Adam optimizer for training, starting with a learning rate of <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> and a weight decay of <math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math>. The weight decay discourages large weights by incorporating the L2 norm of the weights into the loss function. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.T4\" title=\"Table 4 &#8227; 2.5.4 StutterZero Training &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows all the hyperparameters used in the Adam optimizer.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "used",
                    "value"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some cases, low-level features such as MFCC, LPC, and LPCC are extracted from the audio and used as input to neural networks or machine learning models, which then classify whether a stutter has occurred at each point in time.</p>\n\n",
                "matched_terms": [
                    "used",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning approaches for speech recognition and speech conversion have been the subject of extensive investigation. Speech conversion focuses on modifying or transforming a speech signal, such as changing the speaker&#8217;s voice or style, while preserving the original linguistic content. In contrast, speech recognition involves accurately transcribing spoken language into text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">triantafyllopoulos2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">walczyna2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>. Deep learning approaches for speech processing typically involve training neural networks or other computational models, eliminating the need for manual feature engineering. Recent advancements in deep learning have popularized end-to-end models, systems that learn to perform the entire correction process directly from input data without requiring handcrafted features or intermediate steps.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size",
                    "learning",
                    "rate",
                    "batch",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the previously predicted spectrogram and the current context vector as inputs. Two fully connected layers form a pre-net that compress and transform the previous spectrogram frames into a lower-dimensional representation. A well-trained pre-net simplifies the input and extracts the most salient features. If the raw spectrogram frames were used, the decoder might &#8221;shortcut&#8221; the learning process by copying the previous frame or minimally modifying it <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero employs a location-based attention mechanism, which is an extension of classical content-based attention. In content-based attention, the model computes an attention score between a query (representing the decoder&#8217;s current state) and a key (representing the encoder&#8217;s output at any time step). The score measures how relevant each encoder state is to the current decoding step. The attention weights are then obtained by normalizing these scores using a softmax function. These weights form a probability distribution that determines how much &#8221;attention&#8221; the decoder should pay to each encoder state when predicting the next output. Finally, a weighted context vector is used for output prediction <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder input remains a log-Mel spectrogram. Because a Transformer processes all input frames in parallel, it does not have a sense of order by default. Sinusoidal positional encoding gives each time step a deterministic vector (based on the position index) for the embedding at each time step. Residual and layer normalization layers are used between the multi-head attention and feed-forward layers to mitigate vanishing or exploding gradients <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borawar2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2015</span>]</cite>. Three multi-head attention units are utilized in the encoder, each consisting of four heads. Similarly to StutterZero, the encoder outputs a context vector &#8211; a learned hidden representation of the input.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the context vector from the input and uses a similar pre-net architecture as StutterZero to learn a lower-dimension representation. Masked multi-head self-attention is used to prevent the decoder from &#8221;looking ahead&#8221; at future frames that have not been predicted yet. After applying the mask, the decoder can only attend to itself and past frames. Cross attention utilizes queries from the preceding decoder layer, while the keys and values are derived from the original encoder context vector. This allows the decoder to observe the entire encoder context when deciding the next frame.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "learning",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "stutterzero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity of the training corpus was constrained by limited access to datasets such as UCLASS or FluencyBank. The absence of these datasets restricted coverage of various speaker demographics, accents, and tones, which in turn limits generalizability. There may be risks of overfitting and undergeneralization when training on only the SEP-28K data from the LibriStutter datasets. Even though five-fold cross-validation and testing on a completely new dataset was used to produce a candid estimate of the true performance both approaches, future steps should focus on data augmentation and training on larger datasets. Additional experiments using diverse corpora such as UCLASS or AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite> are needed.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training process was constrained by significant hardware limitations. Since the model was trained on a single GPU with only 10 GB of VRAM, both the batch size and model complexity had to be reduced to make training feasible. This, however, resulted in slower training and unstable convergence of the training loss. With access to more powerful hardware, it would be possible to incorporate more advanced architectural choices &#8211; such as increasing the number of heads in the multi-head attention mechanism &#8211; potentially resulting in improved model accuracy.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate the prosodic loss due to training on TTS-generated fluent speech samples, using multiple encoders to capture the tonal and prosodic content of the stuttered speech could be explored. Multi-encoder models have been explored in dysarthic speech conversion, specifically using a prosody encoder to extract prosodic features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>. This would enable fluent outputs that retain the speaker&#8217;s identity, pitch, and expressive nuance. Additionally, expanding to multilingual and low-resource languages through cross-lingual pretraining and transfer learning would extend accessibility to a wider global population <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The current study is limited by access to a small number of corpora. Expanding to diverse datasets such as UCLASS, FluencyBank, and AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>, as well as developing multilingual training resources, would enhance model generalizability. Cross-lingual pretraining and transfer learning offer promising strategies to extend accessibility to low-resource and global language communities, ensuring that stutter correction technology benefits a wider population.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "rate"
                ]
            }
        ]
    },
    "S2.T5": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 5: Training hyperparameters used for the cosine annealing scheduler and optimizer.",
        "body": "Parameter\nValue\n\n\n\nLearning Rate\n1​e−41e^{-4}\n\n\n\nWeight Decay\n1​e−51e^{-5}\n\n\n\nBatch Size\n33\n\n\n\nBetas (Momentum parameters)\n(0.9,0.98)(0.9,0.98)\n\n\n\nEpsilon\n1​e−61e^{-6}\n\n\n\n\nT0T_{0} (Initial restart epochs)\n\n50\n\n\n\n\nTm​u​l​tT_{mult} (Period multiplication factor)\n\n2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Parameter</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Value</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Learning Rate</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Weight Decay</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Batch Size</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Betas (Momentum parameters)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"(0.9,0.98)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m4\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mn mathsize=\"0.800em\">0.9</mn><mo mathsize=\"0.800em\">,</mo><mn mathsize=\"0.800em\">0.98</mn><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(0.9,0.98)</annotation></semantics></math></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Epsilon</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathsize=\"0.800em\">e</mi><mrow><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.3pt 1.5pt;\">\n<math alttext=\"T_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m6\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">0</mn></msub><annotation encoding=\"application/x-tex\">T_{0}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> (Initial restart epochs)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">50</span></td>\n<td class=\"ltx_td\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\">\n<math alttext=\"T_{mult}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T5.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.800em\">T</mi><mrow><mi mathsize=\"0.800em\">m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.800em\">u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.800em\">l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.800em\">t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{mult}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> (Period multiplication factor)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1.3pt 1.5pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "size",
            "rate",
            "parameter",
            "betas",
            "factor",
            "used",
            "multiplication",
            "hyperparameters",
            "momentum",
            "decay",
            "learning",
            "t0t0",
            "scheduler",
            "period",
            "cosine",
            "epsilon",
            "weight",
            "annealing",
            "restart",
            "epochs",
            "1​e−41e4",
            "initial",
            "optimizer",
            "value",
            "batch",
            "1​e−61e6",
            "tm​u​l​ttmult",
            "1​e−51e5",
            "parameters"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DSP approaches utilize audio feature extraction methods to obtain condensed numerical features from complex audio signals. Then, a ruleset or a set of predetermined filters is applied to the features to determine which timeframes contain a stutter. Finally, these timeframes are cut out of the audio, removing the stutter. Some frequently utilized feature sets include Mel-Frequency Cepstral Coefficients (MFCCs), Linear Predictive Coding (LPC), and Linear Predictive Cepstral Coefficients (LPCCs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite>. For instance, MFCC features are generated by first applying the Fast Fourier Transform (FFT) to convert an audio sample from the amplitude domain to the frequency domain, generating the power spectrum. After that, the Mel filter bank is used to map the power spectrum to Mel frequencies, employing a set of nonlinear triangular filters. This aligns the intensity of frequencies to match the nonlinearity of human auditory perception. Finally, a Discrete Cosine Transform (DCT) is used to generate the cepstral coefficients through the decorrelation of features. This pipeline is applied to every window of audio, usually 20&#8211;50 ms long <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cosine",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "used",
                    "value",
                    "factor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some cases, low-level features such as MFCC, LPC, and LPCC are extracted from the audio and used as input to neural networks or machine learning models, which then classify whether a stutter has occurred at each point in time.</p>\n\n",
                "matched_terms": [
                    "used",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning approaches for speech recognition and speech conversion have been the subject of extensive investigation. Speech conversion focuses on modifying or transforming a speech signal, such as changing the speaker&#8217;s voice or style, while preserving the original linguistic content. In contrast, speech recognition involves accurately transcribing spoken language into text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">triantafyllopoulos2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">walczyna2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>. Deep learning approaches for speech processing typically involve training neural networks or other computational models, eliminating the need for manual feature engineering. Recent advancements in deep learning have popularized end-to-end models, systems that learn to perform the entire correction process directly from input data without requiring handcrafted features or intermediate steps.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size",
                    "learning",
                    "rate",
                    "batch",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "multiplication",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the previously predicted spectrogram and the current context vector as inputs. Two fully connected layers form a pre-net that compress and transform the previous spectrogram frames into a lower-dimensional representation. A well-trained pre-net simplifies the input and extracts the most salient features. If the raw spectrogram frames were used, the decoder might &#8221;shortcut&#8221; the learning process by copying the previous frame or minimally modifying it <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero used standard stochastic gradient descent with the Adam optimizer for training, starting with a learning rate of <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> and a weight decay of <math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math>. The weight decay discourages large weights by incorporating the L2 norm of the weights into the loss function. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.T4\" title=\"Table 4 &#8227; 2.5.4 StutterZero Training &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows all the hyperparameters used in the Adam optimizer.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "training",
                    "weight",
                    "1​e−41e4",
                    "decay",
                    "rate",
                    "learning",
                    "optimizer",
                    "1​e−61e6",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "learning",
                    "rate",
                    "scheduler",
                    "cosine",
                    "epochs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cosine",
                    "used",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity of the training corpus was constrained by limited access to datasets such as UCLASS or FluencyBank. The absence of these datasets restricted coverage of various speaker demographics, accents, and tones, which in turn limits generalizability. There may be risks of overfitting and undergeneralization when training on only the SEP-28K data from the LibriStutter datasets. Even though five-fold cross-validation and testing on a completely new dataset was used to produce a candid estimate of the true performance both approaches, future steps should focus on data augmentation and training on larger datasets. Additional experiments using diverse corpora such as UCLASS or AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite> are needed.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training process was constrained by significant hardware limitations. Since the model was trained on a single GPU with only 10 GB of VRAM, both the batch size and model complexity had to be reduced to make training feasible. This, however, resulted in slower training and unstable convergence of the training loss. With access to more powerful hardware, it would be possible to incorporate more advanced architectural choices &#8211; such as increasing the number of heads in the multi-head attention mechanism &#8211; potentially resulting in improved model accuracy.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate the prosodic loss due to training on TTS-generated fluent speech samples, using multiple encoders to capture the tonal and prosodic content of the stuttered speech could be explored. Multi-encoder models have been explored in dysarthic speech conversion, specifically using a prosody encoder to extract prosodic features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>. This would enable fluent outputs that retain the speaker&#8217;s identity, pitch, and expressive nuance. Additionally, expanding to multilingual and low-resource languages through cross-lingual pretraining and transfer learning would extend accessibility to a wider global population <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The current study is limited by access to a small number of corpora. Expanding to diverse datasets such as UCLASS, FluencyBank, and AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>, as well as developing multilingual training resources, would enhance model generalizability. Cross-lingual pretraining and transfer learning offer promising strategies to extend accessibility to low-resource and global language communities, ensuring that stutter correction technology benefits a wider population.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            }
        ]
    },
    "S3.T6": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 6: Combined WER, CER, and Mean BERTScore Precision metrics for all models.",
        "body": "Model\n\n\nMean WER\nMean CER\nMean BERTScore Precision\n\n\n\n\nWhisper-Tiny\n\n\n0.415±0.0490.415\\pm 0.049\n0.227±0.0330.227\\pm 0.033\n0.5768±0.0270.5768\\pm 0.027\n\n\n\n\nWhisper-Small\n\n\n0.370±0.0370.370\\pm 0.037\n0.171±0.0270.171\\pm 0.027\n0.5956±0.0170.5956\\pm 0.017\n\n\n\n\nWhisper-Medium\n\n\n0.361±0.0320.361\\pm 0.032\n0.162±0.0220.162\\pm 0.022\n0.601±0.0170.601\\pm 0.017\n\n\n\n\nASR-based\n\n\n0.04±0.010.04\\pm 0.01\n0.02±0.010.02\\pm 0.01\n0.9516±0.040.9516\\pm 0.04\n\n\n\n\nStutterZero (end-to-end)\n\n\n0.116±0.0130.116\\pm 0.013\n0.110±0.0520.110\\pm 0.052\n0.9174±0.0340.9174\\pm 0.034\n\n\n\n\nStutterFormer (end-to-end)\n\n\n0.08±0.030.08\\pm 0.03\n0.07±0.0110.07\\pm 0.011\n0.9411±0.0120.9411\\pm 0.012",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mean WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mean CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mean BERTScore Precision</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Whisper-Tiny</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.415\\pm 0.049\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.415</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.049</mn></mrow><annotation encoding=\"application/x-tex\">0.415\\pm 0.049</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.227\\pm 0.033\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.227</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.033</mn></mrow><annotation encoding=\"application/x-tex\">0.227\\pm 0.033</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.5768\\pm 0.027\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.5768</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.027</mn></mrow><annotation encoding=\"application/x-tex\">0.5768\\pm 0.027</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Whisper-Small</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.370\\pm 0.037\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.370</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.037</mn></mrow><annotation encoding=\"application/x-tex\">0.370\\pm 0.037</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.171\\pm 0.027\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.171</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.027</mn></mrow><annotation encoding=\"application/x-tex\">0.171\\pm 0.027</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.5956\\pm 0.017\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m6\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.5956</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.017</mn></mrow><annotation encoding=\"application/x-tex\">0.5956\\pm 0.017</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Whisper-Medium</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.361\\pm 0.032\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m7\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.361</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.032</mn></mrow><annotation encoding=\"application/x-tex\">0.361\\pm 0.032</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.162\\pm 0.022\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m8\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.162</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.022</mn></mrow><annotation encoding=\"application/x-tex\">0.162\\pm 0.022</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.601\\pm 0.017\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m9\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.601</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.017</mn></mrow><annotation encoding=\"application/x-tex\">0.601\\pm 0.017</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASR-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.04\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.04</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.01</mn></mrow><annotation encoding=\"application/x-tex\">0.04\\pm 0.01</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.02\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m11\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.02</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.01</mn></mrow><annotation encoding=\"application/x-tex\">0.02\\pm 0.01</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.9516\\pm 0.04\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m12\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.9516</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.04</mn></mrow><annotation encoding=\"application/x-tex\">0.9516\\pm 0.04</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">StutterZero (end-to-end)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.116\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m13\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.116</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.116\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.110\\pm 0.052\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m14\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.110</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.052</mn></mrow><annotation encoding=\"application/x-tex\">0.110\\pm 0.052</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.9174\\pm 0.034\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m15\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.9174</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.034</mn></mrow><annotation encoding=\"application/x-tex\">0.9174\\pm 0.034</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">StutterFormer (end-to-end)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.08\\pm 0.03\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m16\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.08</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.03</mn></mrow><annotation encoding=\"application/x-tex\">0.08\\pm 0.03</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.07\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m17\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.07</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.07\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.9411\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T6.m18\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.9411</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">0.9411\\pm 0.012</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "0370±00370370pm",
            "09516±00409516pm",
            "004±001004pm",
            "whispertiny",
            "combined",
            "whispersmall",
            "stutterzero",
            "all",
            "metrics",
            "asrbased",
            "mean",
            "wer",
            "007±0011007pm",
            "05956±001705956pm",
            "bertscore",
            "0415±00490415pm",
            "model",
            "endtoend",
            "0227±00330227pm",
            "008±003008pm",
            "09411±001209411pm",
            "cer",
            "stutterformer",
            "0361±00320361pm",
            "0601±00170601pm",
            "0110±00520110pm",
            "0171±00270171pm",
            "0162±00220162pm",
            "models",
            "0116±00130116pm",
            "whispermedium",
            "05768±002705768pm",
            "002±001002pm",
            "09174±003409174pm",
            "precision"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Because both approaches in this research produce audio signals, the audio signals must be converted back to text before any word or character-level evaluation. This study used an unmodified, pretrained copy of the Whisper-Small ASR model to transcribe the predicted fluent sequences from both the ASR and end-to-end approaches, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.F5\" title=\"Figure 5 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. This was an attempt to simulate what an untrained, average English-speaking person would interpret the fluent speech to be. The predicted transcripts of the fluent speech were compared with ground truth transcripts to calculate the final metrics. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the mean WER, CER, BERTScore Precision, and their standard deviations in the validation data split.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that all three proposed models&#8212;ASR-based, StutterZero, and StutterFormer&#8212;outperform the best Whisper baseline (Whisper-Medium) across both WER and CER metrics. The Whisper-Medium model achieves a WER of 0.361 and a CER of 0.162, while the ASR-based approach dramatically reduces these errors to 0.04 and 0.02, respectively. This indicates a substantial improvement in transcription accuracy and character-level precision.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "endtoend",
                    "models",
                    "all",
                    "whispermedium",
                    "wer",
                    "bertscore",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach to stutter correction involves fine-tuning or training an ASR model on stuttered speech such that the ASR model can generate transcripts of that speech. The ASR model can either explicitly transcribe the stutter into text or ignore the stuttered portions, producing a fluent transcript. That transcript can be filtered with rule-based systems and finally passed through a TTS model to produce fluent audio sequences. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.F1\" title=\"Figure 1 &#8227; 1.0.2 ASR &amp; TTS-based Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the general pipeline to achieve ASR &amp; TTS-based stutter correction. This approach helps to omit artifacts caused by splicing audio in the DSP-based approaches, since TTS models are generating new audio sequences. However, DSP methods can preserve speaker prosody more naturally, as they simply edit the original speech. For a TTS model to mimic the tone and prosody of the original speaker, it would need more utterances to fine-tune on.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the adaptability of fine-tuned ASR models, there has been research into fine-tuning pre-trained ASR models for other speech impairments such as dysarthria. Wang et al. fine-tuned wav2vec2.0 and HuBERT ASR models on dysarthria datasets. They achieved the best WER of 16.53% by fine-tuning a pre-trained wav2vec2.0 ASR model on a dysarthria corpus augmented by a generative adversarial network. Even without any data augmentation, they obtained a WER of 22.25% on a fine-tuned wav2vec2.0 model and a WER of 21.10% on a fine-tuned HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024a</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning approaches for speech recognition and speech conversion have been the subject of extensive investigation. Speech conversion focuses on modifying or transforming a speech signal, such as changing the speaker&#8217;s voice or style, while preserving the original linguistic content. In contrast, speech recognition involves accurately transcribing spoken language into text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">triantafyllopoulos2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">walczyna2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>. Deep learning approaches for speech processing typically involve training neural networks or other computational models, eliminating the need for manual feature engineering. Recent advancements in deep learning have popularized end-to-end models, systems that learn to perform the entire correction process directly from input data without requiring handcrafted features or intermediate steps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of end-to-end models stems from their ability to consolidate the entire training and inference pipeline into a single model optimized with one objective function that directly reflects the training goal. In contrast, a traditional DSP-based pipeline might first extract MFCC features from audio and then train a neural network to classify whether each frame contains a stutter. The neural network in this case is trained for frame-level accuracy in detecting stutters, which does not align with removing stutters at the speech sequence level <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chan2016</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">graves2014</span>]</cite>. The lack of manual feature engineering in end-to-end models also allows for greater generalizability and adaptability to similar problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hannun2014</span>]</cite>. Finally, end-to-end models such as encoder-decoder and RNN-Transducer models are suitable for sequence-to-sequence tasks such as speech conversion, as they do not require alignment of acoustic sequences to ground truth transcripts and are very flexible with input and output sequence lengths <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite> introduced an end-to-end encoder-decoder for dysarthric speech correction. Three multitask encoders were used: a content encoder to learn underlying semantic meaning, a prosody encoder to learn and correct audio features salient to dysarthria, and a speaker encoder to capture prosody and recreate the tone and voice of the original speaker. A decoder aggregates hidden representations from all three layers and generates acoustic features from which audio can be reconstructed using a vocoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "models",
                    "all",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "model",
                    "models",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FluencyBank dataset contains the Voices-AdultsWhoStutter (Voices-AWS) corpus, which was de-identified and made publicly available to researchers who created a free account. StutterZero and StutterFormer were later tested on the Voices-AWS subset of the FluencyBank dataset for validation purposes.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "stutterzero",
                    "all",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain fluent speech data to train StutterZero and StutterFormer as end-to-end models, this research also developed an auxiliary pipeline as described below to first generate fluent speech data, effectively &#8221;completing&#8221; both datasets:</p>\n\n",
                "matched_terms": [
                    "models",
                    "stutterzero",
                    "endtoend",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stuttered audio and fluent transcripts from LibriStutter were used to fine-tune a Whisper-Small ASR model that was originally trained on fluent speech.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The fine-tuned Whisper-Small model was applied to the SEP-28K dataset, generating fluent transcripts for all SEP-28K audio clips.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A pretrained MeloTTS text-to-speech model was applied to all fluent transcripts from both datasets, generating accurate and clear fluent audio sample counterparts.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study fine-tuned Whisper-Small, a lightweight derivative of the Whisper family of ASR models. Audio data from both datasets are in the amplitude domain, though Whisper-Small accepts a spectrogram in the frequency domain as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "whispersmall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small is a Transformer encoder-decoder model featuring 12 layers in both the encoder and decoder, a hidden width of 768 units, and 12 attention heads, amounting to around 244 million parameters in total. The encoder layers consist of a self-attention mechanism and fully connected hidden layers. The encoder generates a context vector, which is passed via cross-attention to each of the 12 decoder attention blocks. Each decoder layer contains self-attention, cross-attention, and a fully connected network <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. At every time step, the autoregressive decoder is fed the aggregated output tokens from all previous time steps so that it can predict the most likely following token.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fine-tune Whisper-Small, this study aggregated all stuttered audio samples and normalized the sampling rate to 16 kHz. For LibriStutter audio clips, longer audio samples are truncated after 30 seconds.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "model",
                    "precision",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end models have seen significant usage in other speech-based tasks such as translation and transcription. These models are characterized by directly converting an input signal to an output signal without any intermediate feature engineering or representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "model",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero employs a location-based attention mechanism, which is an extension of classical content-based attention. In content-based attention, the model computes an attention score between a query (representing the decoder&#8217;s current state) and a key (representing the encoder&#8217;s output at any time step). The score measures how relevant each encoder state is to the current decoding step. The attention weights are then obtained by normalizing these scores using a softmax function. These weights form a probability distribution that determines how much &#8221;attention&#8221; the decoder should pay to each encoder state when predicting the next output. Finally, a weighted context vector is used for output prediction <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses mean squared error (MSE) loss, where the model minimizes the difference between the ground-truth spectrogram and the predicted fluent spectrogram across all frequency bins and time steps <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafaely2025</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2009</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-entropy loss is used for the transcript decoder because grapheme prediction is a categorical task. This loss measures how well the model predicts the correct token given all previous tokens and the input log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero used standard stochastic gradient descent with the Adam optimizer for training, starting with a learning rate of <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> and a weight decay of <math alttext=\"1e^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.SSS4.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-6}</annotation></semantics></math>. The weight decay discourages large weights by incorporating the L2 norm of the weights into the loss function. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.T4\" title=\"Table 4 &#8227; 2.5.4 StutterZero Training &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows all the hyperparameters used in the Adam optimizer.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer maintains the same multitask architecture as StutterZero, but switches out internal layers for the Attention mechanism found in Transformers as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F4\" title=\"Figure 4 &#8227; 2.6 StutterFormer Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> displays a high-level summary of StutterFormer&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-head attention serves as the fundamental building blocks of StutterFormer. Compared to single-head attention, which computes attention in a single attention distribution, multi-head attention projects queries, keys, and values into multiple subspaces, applies attention in parallel, and then recombines the results. This makes it possible for the model to take a joint attention function to data from several learned subspaces. For instance, one head may focus on short-range syntactic dependencies only while another head tracks long-range semantic connections. This increase in flexibility allows multi-head attention to learn a greater breadth of information while keeping the parameter count relatively equal to a larger single-head attention module <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder input remains a log-Mel spectrogram. Because a Transformer processes all input frames in parallel, it does not have a sense of order by default. Sinusoidal positional encoding gives each time step a deterministic vector (based on the position index) for the embedding at each time step. Residual and layer normalization layers are used between the multi-head attention and feed-forward layers to mitigate vanishing or exploding gradients <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borawar2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2015</span>]</cite>. Three multi-head attention units are utilized in the encoder, each consisting of four heads. Similarly to StutterZero, the encoder outputs a context vector &#8211; a learned hidden representation of the input.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like StutterZero, StutterFormer employs a hybrid loss function that combines multiple weighted losses.\nThe spectrogram decoder also uses MSE loss between the predicted and ground truth spectrograms. In addition to MSE loss, the spectrogram decoder also computes a mean absolute error (MAE) loss. MAE loss measures the absolute difference of the predicted values and target values without squaring. When tested with spectrogram applications, MAE loss promotes sharper spectrograms that prevent the predicted fluent speech from sounding slurred <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guso2022</span>]</cite>. The transcript decoder also uses cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR-based and end-to-end approaches used in this study, state-of-the-art fluent-speech ASR models were also evaluated to establish a baseline for how accurately current speech recognition systems transcribe stuttered speech. This study used the Whisper-Tiny, Whisper-Small, and Whisper-Medium pretrained fluent ASR models as baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. Any larger models such as Whisper-Large could not be tested due to memory and hardware limitations. The 10% validation data split was used to assess each of the six models.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "endtoend",
                    "models",
                    "whispertiny",
                    "whispermedium",
                    "asrbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "all",
                    "metrics",
                    "wer",
                    "cer",
                    "bertscore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "model",
                    "models",
                    "whispermedium",
                    "asrbased",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "stutterzero",
                    "all",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This significant difference in every metric after the ablation in both models demonstrates the importance of the transcript decoder. Observing StutterZero, the WER increased by 22.3% whilst the CER only increased by 15%. This shows the ablated StutterZero predicted most of the characters in a word correctly, but perhaps missed a few characters in some more words. The same phenomenon is seen in StutterFormer ablation, but to a lesser degree. This aligns with the functionality of the transcript decoder &#8211; to help both end-to-end models capture more detailed orthographical features in words. A greater increase in WER than in CER could mean StutterZero/StutterFormer incorrectly predicted more allophones and homophones, which have similar character-level spellings but are different words.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "models",
                    "mean",
                    "wer",
                    "cer",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "metrics",
                    "wer",
                    "bertscore",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces three stutter correction systems, including the first two end-to-end encoder&#8211;decoder models, and provides evidence that direct conversion of stuttered to fluent audio is both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer significantly outperformed state-of-the-art fluent speech recognition models in WER, CER, and BERTScore. The Wilcoxon Signed-Rank Test demonstrated the improvements (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>) of StutterZero and StutterFormer over the next best model, Whisper-Medium. This demonstrates the effectiveness of multitask, end-to-end models for stuttered speech recognition.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "endtoend",
                    "model",
                    "models",
                    "whispermedium",
                    "wer",
                    "cer",
                    "bertscore",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "all",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "stutterzero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The superior performance of StutterFormer over StutterZero cannot be attributed solely to increased model capacity. Rather, Transformer attention provides specific advantages for disfluency correction. Multi-head self-attention captures long-range dependencies, allowing the model to flexibly relate repeated or prolonged segments of speech to their fluent counterparts. This mechanism facilitates alignment across syllables and words, which is essential when disfluencies span multiple phonemic units. In addition, attention layers can simultaneously model both global and local phonetic details, helping to preserve intonation and rhythm while removing interruptions. These properties explain why attention-based architectures are particularly well-suited for mapping stuttered to fluent speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeyer2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this research achieves impressive preliminary results, it also acts as a proof-of-concept, opening the doors for more advanced end-to-end models in stutter correction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond therapy, optimized versions of StutterZero and StutterFormer could be deployed in real-time communication settings. With techniques such as model quantization, pruning, and knowledge distillation, lightweight implementations may run on mobile or embedded devices. Potential applications include live correction during phone calls, video conferences, and online meetings, where disfluent speech is automatically converted to fluent audio for listeners. Similar methods could also be applied to voice recording, broadcasting, and sound engineering, eliminating the need for re-recording when disfluencies occur.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "stutterzero",
                    "endtoend",
                    "model",
                    "models",
                    "whispermedium",
                    "asrbased",
                    "stutterformer"
                ]
            }
        ]
    },
    "S3.T7": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 7: Ablation Study Results on Validation Data.",
        "body": "Model\n\n\nWER\nCER\nMean BERTScore Precision\n\n\n\n\nStutterZero (with transcript decoder)\n\n\n0.116±0.0130.116\\pm 0.013\n0.110±0.0520.110\\pm 0.052\n0.9174±0.0340.9174\\pm 0.034\n\n\n\n\nAblated StutterZero (without transcript decoder)\n\n\n0.339±0.0110.339\\pm 0.011\n0.260±0.0120.260\\pm 0.012\n0.437±0.0100.437\\pm 0.010\n\n\n\n\nStutterFormer (with transcript decoder)\n\n\n0.08±0.030.08\\pm 0.03\n0.07±0.0110.07\\pm 0.011\n0.9411±0.0120.9411\\pm 0.012\n\n\n\n\nAblated StutterFormer (without transcript decoder)\n\n\n0.221±0.0160.221\\pm 0.016\n0.194±0.0190.194\\pm 0.019\n0.552±0.0150.552\\pm 0.015",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mean BERTScore Precision</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">StutterZero (with transcript decoder)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.116\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.116</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.116\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.110\\pm 0.052\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.110</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.052</mn></mrow><annotation encoding=\"application/x-tex\">0.110\\pm 0.052</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.9174\\pm 0.034\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.9174</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.034</mn></mrow><annotation encoding=\"application/x-tex\">0.9174\\pm 0.034</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ablated StutterZero (without transcript decoder)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.339\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.339</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.339\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.260\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.260</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">0.260\\pm 0.012</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.437\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m6\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.437</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.437\\pm 0.010</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">StutterFormer (with transcript decoder)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.08\\pm 0.03\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m7\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.08</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.03</mn></mrow><annotation encoding=\"application/x-tex\">0.08\\pm 0.03</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.07\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m8\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.07</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.07\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.9411\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m9\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.9411</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">0.9411\\pm 0.012</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ablated StutterFormer (without transcript decoder)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.221\\pm 0.016\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.221</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.016</mn></mrow><annotation encoding=\"application/x-tex\">0.221\\pm 0.016</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.194\\pm 0.019\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m11\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.194</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.019</mn></mrow><annotation encoding=\"application/x-tex\">0.194\\pm 0.019</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.3pt;padding-bottom:1.3pt;\"><math alttext=\"0.552\\pm 0.015\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T7.m12\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.552</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.015</mn></mrow><annotation encoding=\"application/x-tex\">0.552\\pm 0.015</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ablated",
            "ablation",
            "0552±00150552pm",
            "0339±00110339pm",
            "0437±00100437pm",
            "stutterzero",
            "decoder",
            "0194±00190194pm",
            "mean",
            "wer",
            "007±0011007pm",
            "bertscore",
            "results",
            "validation",
            "model",
            "008±003008pm",
            "09411±001209411pm",
            "without",
            "0221±00160221pm",
            "cer",
            "stutterformer",
            "0110±00520110pm",
            "transcript",
            "study",
            "0116±00130116pm",
            "09174±003409174pm",
            "data",
            "0260±00120260pm",
            "precision"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "wer",
                    "data",
                    "bertscore",
                    "results",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "study",
                    "transcript",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "without",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach to stutter correction involves fine-tuning or training an ASR model on stuttered speech such that the ASR model can generate transcripts of that speech. The ASR model can either explicitly transcribe the stutter into text or ignore the stuttered portions, producing a fluent transcript. That transcript can be filtered with rule-based systems and finally passed through a TTS model to produce fluent audio sequences. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.F1\" title=\"Figure 1 &#8227; 1.0.2 ASR &amp; TTS-based Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the general pipeline to achieve ASR &amp; TTS-based stutter correction. This approach helps to omit artifacts caused by splicing audio in the DSP-based approaches, since TTS models are generating new audio sequences. However, DSP methods can preserve speaker prosody more naturally, as they simply edit the original speech. For a TTS model to mimic the tone and prosody of the original speaker, it would need more utterances to fine-tune on.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "without",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the adaptability of fine-tuned ASR models, there has been research into fine-tuning pre-trained ASR models for other speech impairments such as dysarthria. Wang et al. fine-tuned wav2vec2.0 and HuBERT ASR models on dysarthria datasets. They achieved the best WER of 16.53% by fine-tuning a pre-trained wav2vec2.0 ASR model on a dysarthria corpus augmented by a generative adversarial network. Even without any data augmentation, they obtained a WER of 22.25% on a fine-tuned wav2vec2.0 model and a WER of 21.10% on a fine-tuned HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024a</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "without",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning approaches for speech recognition and speech conversion have been the subject of extensive investigation. Speech conversion focuses on modifying or transforming a speech signal, such as changing the speaker&#8217;s voice or style, while preserving the original linguistic content. In contrast, speech recognition involves accurately transcribing spoken language into text <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">triantafyllopoulos2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">walczyna2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>. Deep learning approaches for speech processing typically involve training neural networks or other computational models, eliminating the need for manual feature engineering. Recent advancements in deep learning have popularized end-to-end models, systems that learn to perform the entire correction process directly from input data without requiring handcrafted features or intermediate steps.</p>\n\n",
                "matched_terms": [
                    "data",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, Toshniwal et al. developed an attention-based encoder-decoder model inspired by recurrent neural networks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite>. A speech encoder is built on a deep bidirectional long short-term memory (BiLSTM) network, which produces a sequence of abstract hidden representations. These hidden representations are passed into the character decoder, which is a single-layer LSTM that predicts the most likely letter uttered <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "study",
                    "data",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FluencyBank dataset contains the Voices-AdultsWhoStutter (Voices-AWS) corpus, which was de-identified and made publicly available to researchers who created a free account. StutterZero and StutterFormer were later tested on the Voices-AWS subset of the FluencyBank dataset for validation purposes.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "validation",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "validation",
                    "stutterformer",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain fluent speech data to train StutterZero and StutterFormer as end-to-end models, this research also developed an auxiliary pipeline as described below to first generate fluent speech data, effectively &#8221;completing&#8221; both datasets:</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "data",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study fine-tuned Whisper-Small, a lightweight derivative of the Whisper family of ASR models. Audio data from both datasets are in the amplitude domain, though Whisper-Small accepts a spectrogram in the frequency domain as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small is a Transformer encoder-decoder model featuring 12 layers in both the encoder and decoder, a hidden width of 768 units, and 12 attention heads, amounting to around 244 million parameters in total. The encoder layers consist of a self-attention mechanism and fully connected hidden layers. The encoder generates a context vector, which is passed via cross-attention to each of the 12 decoder attention blocks. Each decoder layer contains self-attention, cross-attention, and a fully connected network <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. At every time step, the autoregressive decoder is fed the aggregated output tokens from all previous time steps so that it can predict the most likely following token.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "model",
                    "precision",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "transcript",
                    "study",
                    "decoder",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder generates the spectrogram one frame at a time, using the context vector along with the previously predicted frames as contextual input to append each new predicted frame to the output spectrogram. Similarly, the transcript decoder uses previously predicted tokens to predict the following grapheme tokens. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F3\" title=\"Figure 3 &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays an overview of the model architecture.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero employs a location-based attention mechanism, which is an extension of classical content-based attention. In content-based attention, the model computes an attention score between a query (representing the decoder&#8217;s current state) and a key (representing the encoder&#8217;s output at any time step). The score measures how relevant each encoder state is to the current decoding step. The attention weights are then obtained by normalizing these scores using a softmax function. These weights form a probability distribution that determines how much &#8221;attention&#8221; the decoder should pay to each encoder state when predicting the next output. Finally, a weighted context vector is used for output prediction <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "decoder",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because speech data are only read in one direction (monotonically), classical content-based attention may &#8221;jump around&#8221; erratically without respecting the flow of time. Location-based attention also computes a set of features using concatenated previously calculated attention weights via a one-dimensional convolution.</p>\n\n",
                "matched_terms": [
                    "data",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The location-based features are included in the attention score along with another set of trainable weights. In this way, the decoder is informed by past attention behavior and adjusts its focus accordingly to maintain a monotonic flow of data.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once the spectrogram decoder predicts a fluent spectrogram signal, the Griffin&#8211;Lim algorithm is used to reconstruct the phase data and generate an audio signal in the amplitude domain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">griffin1984</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work in text-to-speech and fluent speech conversion models has demonstrated the efficacy of multitask decoder architectures. Multitask training sums the loss for both the spectrogram and transcript decoders, creating a joint loss function that forces the training process to optimize the loss on both decoders <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the case of the transcript decoder, adding an objective function at the grapheme level may allow StutterZero to learn more intricate orthographic features of a word to correctly distinguish between allophones and homophones.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "decoder",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transcript decoder also uses a teacher-forced embedding of the previous text as its input during training. Instead of using its previous prediction as input for generating the next token, the true ground-truth token from the training data is fed as input to the next step. This stabilizes and speeds up training because the model always conditions on the correct previous tokens. Additionally, it avoids extreme divergence and error in the early stages of training, when incorrect predictions may accumulate.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "transcript",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses mean squared error (MSE) loss, where the model minimizes the difference between the ground-truth spectrogram and the predicted fluent spectrogram across all frequency bins and time steps <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafaely2025</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2009</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-entropy loss is used for the transcript decoder because grapheme prediction is a categorical task. This loss measures how well the model predicts the correct token given all previous tokens and the input log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer maintains the same multitask architecture as StutterZero, but switches out internal layers for the Attention mechanism found in Transformers as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F4\" title=\"Figure 4 &#8227; 2.6 StutterFormer Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> displays a high-level summary of StutterFormer&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-head attention serves as the fundamental building blocks of StutterFormer. Compared to single-head attention, which computes attention in a single attention distribution, multi-head attention projects queries, keys, and values into multiple subspaces, applies attention in parallel, and then recombines the results. This makes it possible for the model to take a joint attention function to data from several learned subspaces. For instance, one head may focus on short-range syntactic dependencies only while another head tracks long-range semantic connections. This increase in flexibility allows multi-head attention to learn a greater breadth of information while keeping the parameter count relatively equal to a larger single-head attention module <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "results",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the context vector from the input and uses a similar pre-net architecture as StutterZero to learn a lower-dimension representation. Masked multi-head self-attention is used to prevent the decoder from &#8221;looking ahead&#8221; at future frames that have not been predicted yet. After applying the mask, the decoder can only attend to itself and past frames. Cross attention utilizes queries from the preceding decoder layer, while the keys and values are derived from the original encoder context vector. This allows the decoder to observe the entire encoder context when deciding the next frame.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transcript decoder uses a very similar architecture to the spectrogram decoder and employs the same Transformer decoder architecture.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like StutterZero, StutterFormer employs a hybrid loss function that combines multiple weighted losses.\nThe spectrogram decoder also uses MSE loss between the predicted and ground truth spectrograms. In addition to MSE loss, the spectrogram decoder also computes a mean absolute error (MAE) loss. MAE loss measures the absolute difference of the predicted values and target values without squaring. When tested with spectrogram applications, MAE loss promotes sharper spectrograms that prevent the predicted fluent speech from sounding slurred <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guso2022</span>]</cite>. The transcript decoder also uses cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "transcript",
                    "decoder",
                    "without",
                    "mean",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR-based and end-to-end approaches used in this study, state-of-the-art fluent-speech ASR models were also evaluated to establish a baseline for how accurately current speech recognition systems transcribe stuttered speech. This study used the Whisper-Tiny, Whisper-Small, and Whisper-Medium pretrained fluent ASR models as baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. Any larger models such as Whisper-Large could not be tested due to memory and hardware limitations. The 10% validation data split was used to assess each of the six models.</p>\n\n",
                "matched_terms": [
                    "data",
                    "validation",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "bertscore",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because both approaches in this research produce audio signals, the audio signals must be converted back to text before any word or character-level evaluation. This study used an unmodified, pretrained copy of the Whisper-Small ASR model to transcribe the predicted fluent sequences from both the ASR and end-to-end approaches, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.F5\" title=\"Figure 5 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. This was an attempt to simulate what an untrained, average English-speaking person would interpret the fluent speech to be. The predicted transcripts of the fluent speech were compared with ground truth transcripts to calculate the final metrics. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the mean WER, CER, BERTScore Precision, and their standard deviations in the validation data split.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "study",
                    "data",
                    "mean",
                    "wer",
                    "cer",
                    "bertscore",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that all three proposed models&#8212;ASR-based, StutterZero, and StutterFormer&#8212;outperform the best Whisper baseline (Whisper-Medium) across both WER and CER metrics. The Whisper-Medium model achieves a WER of 0.361 and a CER of 0.162, while the ASR-based approach dramatically reduces these errors to 0.04 and 0.02, respectively. This indicates a substantial improvement in transcription accuracy and character-level precision.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "wer",
                    "cer",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "transcript",
                    "ablation",
                    "study",
                    "decoder",
                    "data",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This significant difference in every metric after the ablation in both models demonstrates the importance of the transcript decoder. Observing StutterZero, the WER increased by 22.3% whilst the CER only increased by 15%. This shows the ablated StutterZero predicted most of the characters in a word correctly, but perhaps missed a few characters in some more words. The same phenomenon is seen in StutterFormer ablation, but to a lesser degree. This aligns with the functionality of the transcript decoder &#8211; to help both end-to-end models capture more detailed orthographical features in words. A greater increase in WER than in CER could mean StutterZero/StutterFormer incorrectly predicted more allophones and homophones, which have similar character-level spellings but are different words.</p>\n\n",
                "matched_terms": [
                    "ablated",
                    "stutterzero",
                    "transcript",
                    "ablation",
                    "decoder",
                    "mean",
                    "wer",
                    "cer",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "wer",
                    "data",
                    "bertscore",
                    "results",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer significantly outperformed state-of-the-art fluent speech recognition models in WER, CER, and BERTScore. The Wilcoxon Signed-Rank Test demonstrated the improvements (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>) of StutterZero and StutterFormer over the next best model, Whisper-Medium. This demonstrates the effectiveness of multitask, end-to-end models for stuttered speech recognition.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "wer",
                    "cer",
                    "bertscore",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "validation",
                    "results",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "validation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The superior performance of StutterFormer over StutterZero cannot be attributed solely to increased model capacity. Rather, Transformer attention provides specific advantages for disfluency correction. Multi-head self-attention captures long-range dependencies, allowing the model to flexibly relate repeated or prolonged segments of speech to their fluent counterparts. This mechanism facilitates alignment across syllables and words, which is essential when disfluencies span multiple phonemic units. In addition, attention layers can simultaneously model both global and local phonetic details, helping to preserve intonation and rhythm while removing interruptions. These properties explain why attention-based architectures are particularly well-suited for mapping stuttered to fluent speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeyer2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The current study is limited by access to a small number of corpora. Expanding to diverse datasets such as UCLASS, FluencyBank, and AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>, as well as developing multilingual training resources, would enhance model generalizability. Cross-lingual pretraining and transfer learning offer promising strategies to extend accessibility to low-resource and global language communities, ensuring that stutter correction technology benefits a wider population.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond therapy, optimized versions of StutterZero and StutterFormer could be deployed in real-time communication settings. With techniques such as model quantization, pruning, and knowledge distillation, lightweight implementations may run on mobile or embedded devices. Potential applications include live correction during phone calls, video conferences, and online meetings, where disfluent speech is automatically converted to fluent audio for listeners. Similar methods could also be applied to voice recording, broadcasting, and sound engineering, eliminating the need for re-recording when disfluencies occur.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "model",
                    "decoder",
                    "without",
                    "stutterformer"
                ]
            }
        ]
    },
    "S3.T8": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 8: Performance comparison of StutterZero (SZ) and StutterFormer (SF) on the FluencyBank dataset using WER and BERTScore Precision metrics.",
        "body": "Audio ID\nSZ WER\nSF WER\nSZ BERT\nSF BERT\n\n\nMean\n0.161±0.0340.161\\pm 0.034\n0.120±0.0130.120\\pm 0.013\n0.915±0.0150.915\\pm 0.015\n0.937±0.0230.937\\pm 0.023",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Audio ID</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SZ WER</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SF WER</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SZ BERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SF BERT</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Mean</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"0.161\\pm 0.034\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.161</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.034</mn></mrow><annotation encoding=\"application/x-tex\">0.161\\pm 0.034</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"0.120\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.120</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.120\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"0.915\\pm 0.015\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.915</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.015</mn></mrow><annotation encoding=\"application/x-tex\">0.915\\pm 0.015</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:1.3pt 1.5pt;\"><math alttext=\"0.937\\pm 0.023\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">0.937</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.023</mn></mrow><annotation encoding=\"application/x-tex\">0.937\\pm 0.023</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "bert",
            "stutterzero",
            "audio",
            "0161±00340161pm",
            "metrics",
            "wer",
            "dataset",
            "0120±00130120pm",
            "0937±00230937pm",
            "mean",
            "0915±00150915pm",
            "fluencybank",
            "performance",
            "bertscore",
            "comparison",
            "precision",
            "stutterformer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "wer",
                    "dataset",
                    "fluencybank",
                    "bertscore",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, StutterNet, introduced by Sheikh et al., is a multi-class Time Delay Neural Network (TDNN). Because TDNNs use windows of time-delayed inputs, they are especially well-suited to temporal dependencies such as MFCC speech features. While StutterNet was only able to detect stutters, it is plausible that similar systems could be used to flag sections of stuttered audio for removal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2021</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T3\" title=\"Table 3 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes representative DSP-based systems, their methodological choices, and reported performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "fluencybank",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">About 20 hours of artificially produced stuttered audio are included in the LibriStutter dataset. This corpus was created by splicing, cutting, duplicating, and performing other manipulations on the fluent LibriSpeech corpus to mimic the prosodic characteristics of stuttering <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kourkounakis2020a</span>]</cite>. SEP-28K does not include fluent reference transcripts, whereas LibriStutter contains paired fluent transcriptions that facilitate supervised training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that all audio contained speech, audio files labeled &#8221;Music&#8221; or &#8221;NoSpeech&#8221; were removed from the SEP-28K dataset. To account for bias or skew in the frequency of each type of stutter, data resampling was conducted on SEP-28K to balance out stutter categories that may have been less common.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The University College London Archive of Stuttered Speech (UCLASS) and the FluencyBank dataset are the two other well-known stuttering datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zotero-item-1105</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bernsteinratner2018</span>]</cite>. However, both datasets have some limitations. The UCLASS dataset does not provide fluent &#8221;ground truth&#8221; transcripts for the stuttered speech, so there are no reference transcripts to fine-tune Whisper-Small on.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FluencyBank dataset contains the Voices-AdultsWhoStutter (Voices-AWS) corpus, which was de-identified and made publicly available to researchers who created a free account. StutterZero and StutterFormer were later tested on the Voices-AWS subset of the FluencyBank dataset for validation purposes.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer",
                    "dataset",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "comparison",
                    "stutterformer",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain fluent speech data to train StutterZero and StutterFormer as end-to-end models, this research also developed an auxiliary pipeline as described below to first generate fluent speech data, effectively &#8221;completing&#8221; both datasets:</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The fine-tuned Whisper-Small model was applied to the SEP-28K dataset, generating fluent transcripts for all SEP-28K audio clips.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training runs for a maximum of 10,000 steps with a learning rate of 1e-5 and a batch size of 8. Gradient accumulation over two steps is used to simulate a larger batch size. Gradient checkpointing and mixed precision are enabled to save memory and speed up training. Evaluation occurs every 1,000 steps, and the model weights with the lowest WER are saved.</p>\n\n",
                "matched_terms": [
                    "precision",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer maintains the same multitask architecture as StutterZero, but switches out internal layers for the Attention mechanism found in Transformers as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F4\" title=\"Figure 4 &#8227; 2.6 StutterFormer Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> displays a high-level summary of StutterFormer&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like StutterZero, StutterFormer employs a hybrid loss function that combines multiple weighted losses.\nThe spectrogram decoder also uses MSE loss between the predicted and ground truth spectrograms. In addition to MSE loss, the spectrogram decoder also computes a mean absolute error (MAE) loss. MAE loss measures the absolute difference of the predicted values and target values without squaring. When tested with spectrogram applications, MAE loss promotes sharper spectrograms that prevent the predicted fluent speech from sounding slurred <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guso2022</span>]</cite>. The transcript decoder also uses cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three metrics were used to evaluate all models: Word Error Rate (WER), Character Error Rate (CER), and BERTScore. CER is defined in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.E3\" title=\"In 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) as the proportion of incorrectly predicted characters compared to the ground truth string. To assess the semantic similarity between the ground truth and predicted utterances, a BERTScore is calculated using pre-trained contextual embeddings from the Bidirectional Encoder Representations from Transformers (BERT) language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>. Cosine similarity is used as a metric to quantify similarity between high-dimensional embedding vectors. The BERTScore defines how similar the two strings are semantically, meaning it is more forgiving towards minor transcription mistakes that still preserve the overall meaning of the utterance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "metrics",
                    "bertscore",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because both approaches in this research produce audio signals, the audio signals must be converted back to text before any word or character-level evaluation. This study used an unmodified, pretrained copy of the Whisper-Small ASR model to transcribe the predicted fluent sequences from both the ASR and end-to-end approaches, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.F5\" title=\"Figure 5 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. This was an attempt to simulate what an untrained, average English-speaking person would interpret the fluent speech to be. The predicted transcripts of the fluent speech were compared with ground truth transcripts to calculate the final metrics. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the mean WER, CER, BERTScore Precision, and their standard deviations in the validation data split.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "mean",
                    "wer",
                    "bertscore",
                    "precision",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that all three proposed models&#8212;ASR-based, StutterZero, and StutterFormer&#8212;outperform the best Whisper baseline (Whisper-Medium) across both WER and CER metrics. The Whisper-Medium model achieves a WER of 0.361 and a CER of 0.162, while the ASR-based approach dramatically reduces these errors to 0.04 and 0.02, respectively. This indicates a substantial improvement in transcription accuracy and character-level precision.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "metrics",
                    "precision",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "performance",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This significant difference in every metric after the ablation in both models demonstrates the importance of the transcript decoder. Observing StutterZero, the WER increased by 22.3% whilst the CER only increased by 15%. This shows the ablated StutterZero predicted most of the characters in a word correctly, but perhaps missed a few characters in some more words. The same phenomenon is seen in StutterFormer ablation, but to a lesser degree. This aligns with the functionality of the transcript decoder &#8211; to help both end-to-end models capture more detailed orthographical features in words. A greater increase in WER than in CER could mean StutterZero/StutterFormer incorrectly predicted more allophones and homophones, which have similar character-level spellings but are different words.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer",
                    "mean",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">FluencyBank is a subset of the TalkBank corpus, an open repository for spoken language data. FluencyBank specifically focuses on disfluencies, including stuttering. Because the FluencyBank dataset is password-protected and it is not possible to automate the scraping of data from the website, this research manually downloaded audio clips from a randomly selected recording along with their transcripts from the Voice-AdultsWhoStutter (Voices-AWS) subset. After downloading and splitting each audio file to be 30 seconds or less, there were 800 stuttered audio samples.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T9\" title=\"Table 9 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> presents a qualitative comparison between the stuttered and fluent spectrograms for three speech samples from the FluencyBank dataset. The regions enclosed by green rectangles highlight sections containing a stutter in the original audio or the corresponding fluent segments after correction.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "audio",
                    "dataset",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer significantly outperformed state-of-the-art fluent speech recognition models in WER, CER, and BERTScore. The Wilcoxon Signed-Rank Test demonstrated the improvements (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>) of StutterZero and StutterFormer over the next best model, Whisper-Medium. This demonstrates the effectiveness of multitask, end-to-end models for stuttered speech recognition.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "bertscore",
                    "stutterformer",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "performance",
                    "audio",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The superior performance of StutterFormer over StutterZero cannot be attributed solely to increased model capacity. Rather, Transformer attention provides specific advantages for disfluency correction. Multi-head self-attention captures long-range dependencies, allowing the model to flexibly relate repeated or prolonged segments of speech to their fluent counterparts. This mechanism facilitates alignment across syllables and words, which is essential when disfluencies span multiple phonemic units. In addition, attention layers can simultaneously model both global and local phonetic details, helping to preserve intonation and rhythm while removing interruptions. These properties explain why attention-based architectures are particularly well-suited for mapping stuttered to fluent speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeyer2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "performance",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity of the training corpus was constrained by limited access to datasets such as UCLASS or FluencyBank. The absence of these datasets restricted coverage of various speaker demographics, accents, and tones, which in turn limits generalizability. There may be risks of overfitting and undergeneralization when training on only the SEP-28K data from the LibriStutter datasets. Even though five-fold cross-validation and testing on a completely new dataset was used to produce a candid estimate of the true performance both approaches, future steps should focus on data augmentation and training on larger datasets. Additional experiments using diverse corpora such as UCLASS or AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite> are needed.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "dataset",
                    "fluencybank"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond therapy, optimized versions of StutterZero and StutterFormer could be deployed in real-time communication settings. With techniques such as model quantization, pruning, and knowledge distillation, lightweight implementations may run on mobile or embedded devices. Potential applications include live correction during phone calls, video conferences, and online meetings, where disfluent speech is automatically converted to fluent audio for listeners. Similar methods could also be applied to voice recording, broadcasting, and sound engineering, eliminating the need for re-recording when disfluencies occur.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "performance",
                    "audio",
                    "stutterformer"
                ]
            }
        ]
    },
    "S3.T9": {
        "source_file": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
        "caption": "Table 9: Spectrograms of stuttered audio, StutterZero-predicted fluent audio, and StutterFormer-predicted fluent audio. Areas enclosed by green rectangles indicate either a stutter or the corresponding fluent region after correction.",
        "body": "ID\n\n\n\n\nTranscript\n\n\n\n\nStuttered Spectrogram\n\n\n\n\nStutterZero Predicted Fluent Spectrogram\n\n\n\n\nStutterFormer Predicted Fluent Spectrogram\n\n\n\n\n\n\n29mb_1.wav\n\n\n\n\nso can you can you talk about the impact that stuttering has had on your everyday life thinking about things like education your job your family your friends\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29mb_2.wav\n\n\n\n\ni’d say s-s-s uhm s-s-s-stuttering kind of impacted me or like I-I-I-I had highs and lows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29mb_3.wav\n\n\n\n\nwith with s-s-stuttering early on d-d-d-definitely in my elementary school",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:27.6pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">ID</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:51.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Transcript</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Stuttered Spectrogram</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">StutterZero Predicted Fluent Spectrogram</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">StutterFormer Predicted Fluent Spectrogram</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:27.6pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">29mb_1.wav</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:51.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">so <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">can you can you</span> talk about the impact that stuttering has had on your everyday life thinking about things like education your job your family your friends</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g1\" src=\"1_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g2\" src=\"stutterzero_1_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g3\" src=\"stutterformer_1_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:27.6pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">29mb_2.wav</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:51.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">i&#8217;d say <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">s-s-s</span> uhm <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">s-s-s-stuttering</span> kind of impacted me or like <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">I-I-I-I</span> had highs and lows</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g4\" src=\"2_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g5\" src=\"stutterzero_2_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g6\" src=\"stutterformer_2_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"width:27.6pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">29mb_3.wav</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t\" style=\"width:51.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">with with</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">s-s-stuttering</span> early on <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">d-d-d-definitely</span> in my elementary school</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g7\" src=\"3_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g8\" src=\"stutterzero_3_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t\" style=\"width:103.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"61\" id=\"S3.T9.g9\" src=\"stutterformer_3_spectrogram.png\" width=\"153\"/>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "about",
            "elementary",
            "ddddefinitely",
            "29mb2wav",
            "spectrograms",
            "has",
            "ssstuttering",
            "stutterzero",
            "stutterzeropredicted",
            "predicted",
            "like",
            "i’d",
            "stutter",
            "rectangles",
            "education",
            "you",
            "school",
            "stuttered",
            "indicate",
            "spectrogram",
            "family",
            "lows",
            "job",
            "had",
            "either",
            "uhm",
            "fluent",
            "sssstuttering",
            "stutterformerpredicted",
            "impact",
            "thinking",
            "iiii",
            "green",
            "stutterformer",
            "29mb1wav",
            "impacted",
            "stuttering",
            "corresponding",
            "transcript",
            "your",
            "highs",
            "enclosed",
            "region",
            "everyday",
            "correction",
            "areas",
            "life",
            "friends",
            "talk",
            "things",
            "29mb3wav",
            "kind",
            "after",
            "early",
            "audio",
            "say",
            "sss"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T9\" title=\"Table 9 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> presents a qualitative comparison between the stuttered and fluent spectrograms for three speech samples from the FluencyBank dataset. The regions enclosed by green rectangles highlight sections containing a stutter in the original audio or the corresponding fluent segments after correction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions.\nThis work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional&#8211;bidirectional LSTM encoder&#8211;decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic&#8211;linguistic representations. Both architectures are trained on paired stuttered&#8211;fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset.\nAcross all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore.\nThe results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human&#8211;computer interaction, speech therapy, and accessibility-oriented AI systems.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stuttering",
                    "stutter",
                    "correction",
                    "stuttered",
                    "had",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">automatic speech recognition, deep learning, human-computer interaction, speech correction, speech processing, stuttering\n\n<span class=\"ltx_ERROR undefined\">\\IEEEpeerreviewmaketitle</span></p>\n\n",
                "matched_terms": [
                    "correction",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stuttering is a prevalent speech disorder that affects more than 70 million individuals worldwide <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ghai2021</span>]</cite>. It manifests as interruptions in speech flow, including unintentional repetitions, prolongations, and pauses. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the five main phonological classifications of stuttering and examples of their symptoms <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kourkounakis2020a</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">basak2023</span>]</cite>. Such interruptions can significantly hinder effective communication, frequently causing social anxiety, depression, and a diminished quality of life <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2009</span>]</cite>. In children, stuttering may lead to bullying and social isolation, exacerbating the emotional and psychological difficulties they encounter <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iverach2016</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "life",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fluent speech plays a critical role in daily communication, both in interpersonal situations and when engaging with intelligent voice-based technologies. Through a quality-of-life survey, people who stutter (PWS) showed a statistically significant decrease in emotional health and social function metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kasbi2015</span>]</cite>. Stuttering can also cause feelings of shame, fear, anxiety, and guilt <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bloodstein2021</span>]</cite>. These challenges are exacerbated by the growing dependence on intelligent voice assistants, such as Google Home, Amazon Echo, and Apple Siri, which are designed to process fluent speech. As a result, people who stutter frequently encounter recognition errors, limited functionality, and exclusion when interacting with voice-controlled technologies.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "stuttering",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For more severe forms of stuttering, automatic speech recognition (ASR) models, which transcribe speech, may insert unintended words or even truncate the speech due to a blocking stutter. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite> evaluated the speech of individuals with PWS using the Apple Speech framework, a production-level automatic speech recognition (ASR) system designed for fluent speakers. They reported a Word Error Rate (WER) of 19.8%, indicating that 19.8% of words in the ASR-generated transcript did not match the reference ground truth. They also reported a truncation rate of 23.8%, where 23.8% of utterances from these PWS were prematurely cut off <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. Addressing this challenge, this study develops deep learning models that convert stuttered speech into fluent audio, going beyond transcription to reconstruct corrected acoustic signals. By generating fluent speech that preserves semantic content and improves intelligibility, these models aim to enhance communication for PWS and increase accessibility in human-machine interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stuttering",
                    "transcript",
                    "stutter",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work in the field of stutter correction can be separated into three categories: (1) Digital signal processing (DSP) and rule-based classifiers, (2) ASR and text-to-speech (TTS) pipelines, and (3) deep learning (DL) approaches.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DSP approaches utilize audio feature extraction methods to obtain condensed numerical features from complex audio signals. Then, a ruleset or a set of predetermined filters is applied to the features to determine which timeframes contain a stutter. Finally, these timeframes are cut out of the audio, removing the stutter. Some frequently utilized feature sets include Mel-Frequency Cepstral Coefficients (MFCCs), Linear Predictive Coding (LPC), and Linear Predictive Cepstral Coefficients (LPCCs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite>. For instance, MFCC features are generated by first applying the Fast Fourier Transform (FFT) to convert an audio sample from the amplitude domain to the frequency domain, generating the power spectrum. After that, the Mel filter bank is used to map the power spectrum to Mel frequencies, employing a set of nonlinear triangular filters. This aligns the intensity of frequencies to match the nonlinearity of human auditory perception. Finally, a Discrete Cosine Transform (DCT) is used to generate the cepstral coefficients through the decorrelation of features. This pipeline is applied to every window of audio, usually 20&#8211;50 ms long <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "after",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">K. N. et al. introduced a rule-based approach for stutter detection and removal by computing a &#8220;correlation factor&#8221; between adjacent audio windows using either MFCC or LPC features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. A high correlation value, empirically determined to be 0.92, was used to identify repeated or prolonged segments, prompting the deletion of redundant frames. The unusually high correlation factor was used to detect repeated audio patterns, such as recurring phonemes or extended silences. However, this approach was evaluated exclusively on repetition and prolongation stutters, without accounting for other common disfluencies such as blocks or interjections. Additionally, the experimental scope of the study was confined to only 60 repetition and 70 prolongation events obtained from a limited selection of audio files. As a result, its generalizability across diverse speakers, accents, or spontaneous conversational settings remains uncertain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kn2020</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T2\" title=\"Table 2 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the performance, demonstrating high within-sample accuracy.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "either",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In some cases, low-level features such as MFCC, LPC, and LPCC are extracted from the audio and used as input to neural networks or machine learning models, which then classify whether a stutter has occurred at each point in time.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stutter",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, StutterNet, introduced by Sheikh et al., is a multi-class Time Delay Neural Network (TDNN). Because TDNNs use windows of time-delayed inputs, they are especially well-suited to temporal dependencies such as MFCC speech features. While StutterNet was only able to detect stutters, it is plausible that similar systems could be used to flag sections of stuttered audio for removal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheikh2021</span>]</cite>. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T3\" title=\"Table 3 &#8227; 1.0.1 Conventional Digital Signal Processing Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes representative DSP-based systems, their methodological choices, and reported performance.</p>\n\n",
                "matched_terms": [
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second approach to stutter correction involves fine-tuning or training an ASR model on stuttered speech such that the ASR model can generate transcripts of that speech. The ASR model can either explicitly transcribe the stutter into text or ignore the stuttered portions, producing a fluent transcript. That transcript can be filtered with rule-based systems and finally passed through a TTS model to produce fluent audio sequences. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.F1\" title=\"Figure 1 &#8227; 1.0.2 ASR &amp; TTS-based Approaches &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the general pipeline to achieve ASR &amp; TTS-based stutter correction. This approach helps to omit artifacts caused by splicing audio in the DSP-based approaches, since TTS models are generating new audio sequences. However, DSP methods can preserve speaker prosody more naturally, as they simply edit the original speech. For a TTS model to mimic the tone and prosody of the original speaker, it would need more utterances to fine-tune on.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transcript",
                    "stutter",
                    "either",
                    "correction",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mujtaba &amp; Mahapatra describe fine-tuning Whisper-small, a state-of-the-art ASR model pretrained on fluent speech corpora. To allow for efficient training, low-rank adaptation (LoRA) was used as a form of parameter-efficient fine-tuning. Whisper-small was fine-tuned on ground truth transcripts provided by the FluencyBank and private HeardAI datasets. They reported that Whisper-Small achieved a WER of 33.88% without any training or fine-tuning. After fine-tuning Whisper-Small using the aforementioned LoRA methods, it achieved a WER of 9.39% <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "after",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While there is considerable work on end-to-end speech conversion of fluent speech and other speech impairments such as dysarthria, there is little research on deep learning or end-to-end stutter correction. This paper first reviews deep learning and end-to-end methods for fluent speech recognition, conversion, and the correction of dysarthric speech.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "stutter",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of end-to-end models stems from their ability to consolidate the entire training and inference pipeline into a single model optimized with one objective function that directly reflects the training goal. In contrast, a traditional DSP-based pipeline might first extract MFCC features from audio and then train a neural network to classify whether each frame contains a stutter. The neural network in this case is trained for frame-level accuracy in detecting stutters, which does not align with removing stutters at the speech sequence level <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chan2016</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">graves2014</span>]</cite>. The lack of manual feature engineering in end-to-end models also allows for greater generalizability and adaptability to similar problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hannun2014</span>]</cite>. Finally, end-to-end models such as encoder-decoder and RNN-Transducer models are suitable for sequence-to-sequence tasks such as speech conversion, as they do not require alignment of acoustic sequences to ground truth transcripts and are very flexible with input and output sequence lengths <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite> introduced an end-to-end encoder-decoder for dysarthric speech correction. Three multitask encoders were used: a content encoder to learn underlying semantic meaning, a prosody encoder to learn and correct audio features salient to dysarthria, and a speaker encoder to capture prosody and recreate the tone and voice of the original speaker. A decoder aggregates hidden representations from all three layers and generates acoustic features from which audio can be reconstructed using a vocoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, work on stuttering remains limited. Most deep learning research to date has focused on event-level detection and classification of disfluencies rather than the synthesis of corrected audio. Approaches that combine automatic speech recognition with text-to-speech generation can yield fluent renditions, but they are constrained by a lack of transcription accuracy or generalization across all categories of stuttering. Direct acoustic-to-acoustic correction of stuttered speech into fluent speech remains an underexplored problem. This gap motivates the present study, which introduces two end-to-end models, <em class=\"ltx_emph ltx_font_italic\">StutterZero</em> and <em class=\"ltx_emph ltx_font_italic\">StutterFormer</em>, that jointly address disfluency detection, transcription, and fluency restoration. By explicitly targeting corrected audio generation while preserving semantic content, these models extend prior deep learning approaches toward real-time stutter correction and inclusive speech technology.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stuttering",
                    "stutterformer",
                    "stutter",
                    "correction",
                    "stuttered",
                    "fluent",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the methodological framework of the study, which is organized into four main parts. First, the datasets used for training and evaluation are described, along with the corresponding data preprocessing and cleaning procedures. Second, the ASR&#8211;TTS baseline pipeline is outlined, in which a pretrained ASR system is adapted to stuttered speech and fluent output is resynthesized using a TTS model. Third, two proposed end-to-end models, StutterZero and StutterFormer, are introduced; these models directly transform disfluent speech into fluent speech through multitask encoder&#8211;decoder architectures. Finally, the training configuration, optimization procedures, and cross-validation strategy employed to ensure robustness and reproducibility are detailed.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "corresponding",
                    "stuttered",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research uses two datasets for training: Stuttering Events in Podcasts (SEP-28K) and LibriStutter. The SEP-28K dataset contains 23 hours of naturally occurring stuttering events, divided into 28,000 clips, each lasting 3 seconds <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lea2023</span>]</cite>. While this dataset does not contain any ground truth for what the fluent speech should be, it includes labels classifying every stutter into the categories shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. All audio recordings were taken from public podcasts with people who stutter at a standard sampling rate of 16 kHz. After all cleaning and processing, about 14 hours of raw audio data were left.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stuttering",
                    "about",
                    "fluent",
                    "after",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">About 20 hours of artificially produced stuttered audio are included in the LibriStutter dataset. This corpus was created by splicing, cutting, duplicating, and performing other manipulations on the fluent LibriSpeech corpus to mimic the prosodic characteristics of stuttering <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kourkounakis2020a</span>]</cite>. SEP-28K does not include fluent reference transcripts, whereas LibriStutter contains paired fluent transcriptions that facilitate supervised training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stuttering",
                    "about",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that all audio contained speech, audio files labeled &#8221;Music&#8221; or &#8221;NoSpeech&#8221; were removed from the SEP-28K dataset. To account for bias or skew in the frequency of each type of stutter, data resampling was conducted on SEP-28K to balance out stutter categories that may have been less common.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After cleaning, approximately 34 hours of stuttered audio were retained across the SEP-28K and LibriStutter datasets, with no overlap in speakers or transcripts between the two corpora.</p>\n\n",
                "matched_terms": [
                    "after",
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The University College London Archive of Stuttered Speech (UCLASS) and the FluencyBank dataset are the two other well-known stuttering datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zotero-item-1105</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bernsteinratner2018</span>]</cite>. However, both datasets have some limitations. The UCLASS dataset does not provide fluent &#8221;ground truth&#8221; transcripts for the stuttered speech, so there are no reference transcripts to fine-tune Whisper-Small on.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FluencyBank dataset contains the Voices-AdultsWhoStutter (Voices-AWS) corpus, which was de-identified and made publicly available to researchers who created a free account. StutterZero and StutterFormer were later tested on the Voices-AWS subset of the FluencyBank dataset for validation purposes.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train and validate all three approaches (ASR-TTS, StutterZero, and StutterFormer) introduced in this paper, the combined dataset of audio-transcript pairs from SEP-28K and LibriStutter was randomly sampled and split into training (80%), test (10%), and validation (10%) sets. The training and testing splits were used in a five-fold cross-validation setup, with the validation set being used for a fair comparison between all three approaches after training. All training for this study was conducted on an NVIDIA RTX 3080 with 10 gigabytes of VRAM.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "after",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain fluent speech data to train StutterZero and StutterFormer as end-to-end models, this research also developed an auxiliary pipeline as described below to first generate fluent speech data, effectively &#8221;completing&#8221; both datasets:</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stuttered audio and fluent transcripts from LibriStutter were used to fine-tune a Whisper-Small ASR model that was originally trained on fluent speech.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The fine-tuned Whisper-Small model was applied to the SEP-28K dataset, generating fluent transcripts for all SEP-28K audio clips.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A pretrained MeloTTS text-to-speech model was applied to all fluent transcripts from both datasets, generating accurate and clear fluent audio sample counterparts.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study fine-tuned Whisper-Small, a lightweight derivative of the Whisper family of ASR models. Audio data from both datasets are in the amplitude domain, though Whisper-Small accepts a spectrogram in the frequency domain as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "spectrogram",
                    "family",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper-Small&#8217;s WhisperFeatureExtractor uses a Short-Time Fourier Transform (STFT), computing the Fast Fourier Transform (FFT) on overlapping segments, or &#8220;windows,&#8221; of the audio as it slides across the entire signal. Then, the spectrogram is converted into a log-Mel spectrogram by mapping the frequencies from the &#8220;vanilla&#8221; spectrogram onto the Mel scale. This is done because human hearing does not perceive pitch in a linear manner; humans are more attuned to changes in lower pitches than in higher pitches. The linear frequency spectrogram is passed through a Mel filter bank, which applies a series of triangular filters to aggregate spectral energy within perceptually relevant frequency bands. Once a spectrogram is transformed into a log-Mel spectrogram, equal intervals on the Mel scale reflect equal perceived differences in pitch. The practical effect is that Mel spectrograms highlight the frequencies of human speech while minimizing the intensity of background noise, allowing the model to concentrate solely on the speech signals <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stevens1937</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F2\" title=\"Figure 2 &#8227; 2.3 Whisper-Small Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> displays visualized spectrograms of the conversion process used to obtain a log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "spectrogram",
                    "spectrograms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fine-tune Whisper-Small, this study aggregated all stuttered audio samples and normalized the sampling rate to 16 kHz. For LibriStutter audio clips, longer audio samples are truncated after 30 seconds.</p>\n\n",
                "matched_terms": [
                    "after",
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study introduces an autoregressive, end-to-end, multitask model named StutterZero. Inputs consist of 80-channel log-Mel spectrograms computed with a 50 ms window length and a 12.5 ms frame shift. The encoder converts the input log-Mel spectrogram into a higher-level representation called a context vector. The context vector is a numerical representation of features that the trained encoder determines to be relevant. While typical encoder-decoder models use one encoder and one decoder, this study proposes a multitask decoder in which two decoders are forced to predict different data types. During training, a multitask model minimizes a joint loss function, forcing the decoders to learn more generalized patterns that benefit all constituent losses <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>. The spectrogram decoder predicts the fluent spectrogram signal, while the transcript decoder predicts the grapheme being uttered.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "transcript",
                    "spectrogram",
                    "spectrograms",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder generates the spectrogram one frame at a time, using the context vector along with the previously predicted frames as contextual input to append each new predicted frame to the output spectrogram. Similarly, the transcript decoder uses previously predicted tokens to predict the following grapheme tokens. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F3\" title=\"Figure 3 &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays an overview of the model architecture.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "spectrogram",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder consists of two convolutional blocks, each including a two-dimensional convolution layer with a 3&#215;3 kernel, a 2&#215;2 stride, and batch normalization. To effectively capture both spatial and temporal features of the input sequence, a convolution-augmented bidirectional long short-term memory network (ConvBiLSTM) is employed. Log-Mel spectrograms provide information about both the frequency content of a signal and the timing of its occurrence. Unlike traditional LSTMs, the ConvBiLSTM replaces standard matrix multiplication with a convolution operation, which promotes the learning of local spatial patterns in two-dimensional data. The network includes mechanisms analogous to the standard LSTM gates&#8212;input, forget, and output gates&#8212;as well as candidate cell states and hidden states. Each gate and state use convolutional kernels and trainable biases to process the current input in combination with the hidden state from the previous time step, enabling the model to capture complex spatiotemporal dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "about",
                    "spectrograms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the previously predicted spectrogram and the current context vector as inputs. Two fully connected layers form a pre-net that compress and transform the previous spectrogram frames into a lower-dimensional representation. A well-trained pre-net simplifies the input and extracts the most salient features. If the raw spectrogram frames were used, the decoder might &#8221;shortcut&#8221; the learning process by copying the previous frame or minimally modifying it <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "spectrogram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once the spectrogram decoder predicts a fluent spectrogram signal, the Griffin&#8211;Lim algorithm is used to reconstruct the phase data and generate an audio signal in the amplitude domain <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">griffin1984</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "spectrogram",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work in text-to-speech and fluent speech conversion models has demonstrated the efficacy of multitask decoder architectures. Multitask training sums the loss for both the spectrogram and transcript decoders, creating a joint loss function that forces the training process to optimize the loss on both decoders <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">toshniwal2017</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "fluent",
                    "spectrogram",
                    "transcript",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the case of the transcript decoder, adding an objective function at the grapheme level may allow StutterZero to learn more intricate orthographic features of a word to correctly distinguish between allophones and homophones.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transcript decoder also uses a teacher-forced embedding of the previous text as its input during training. Instead of using its previous prediction as input for generating the next token, the true ground-truth token from the training data is fed as input to the next step. This stabilizes and speeds up training because the model always conditions on the correct previous tokens. Additionally, it avoids extreme divergence and error in the early stages of training, when incorrect predictions may accumulate.</p>\n\n",
                "matched_terms": [
                    "early",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses mean squared error (MSE) loss, where the model minimizes the difference between the ground-truth spectrogram and the predicted fluent spectrogram across all frequency bins and time steps <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafaely2025</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2009</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "spectrogram",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-entropy loss is used for the transcript decoder because grapheme prediction is a categorical task. This loss measures how well the model predicts the correct token given all previous tokens and the input log-Mel spectrogram.</p>\n\n",
                "matched_terms": [
                    "spectrogram",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To combine the loss functions for both decoders, a bespoke loss function (described in Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.E2\" title=\"In 2.5.4 StutterZero Training &#8227; 2.5 StutterZero Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) is defined by summing two components: the mean-squared error (MSE) loss between the fluent and stuttered spectrogram frequency bins, and the cross-entropy loss between the predicted token probability distribution and the ground-truth tokens.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "spectrogram",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer maintains the same multitask architecture as StutterZero, but switches out internal layers for the Attention mechanism found in Transformers as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2023</span>]</cite>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S2.F4\" title=\"Figure 4 &#8227; 2.6 StutterFormer Model Architecture &#8227; 2 Methodology &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> displays a high-level summary of StutterFormer&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder input remains a log-Mel spectrogram. Because a Transformer processes all input frames in parallel, it does not have a sense of order by default. Sinusoidal positional encoding gives each time step a deterministic vector (based on the position index) for the embedding at each time step. Residual and layer normalization layers are used between the multi-head attention and feed-forward layers to mitigate vanishing or exploding gradients <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borawar2023</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">he2015</span>]</cite>. Three multi-head attention units are utilized in the encoder, each consisting of four heads. Similarly to StutterZero, the encoder outputs a context vector &#8211; a learned hidden representation of the input.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "spectrogram"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram decoder uses the context vector from the input and uses a similar pre-net architecture as StutterZero to learn a lower-dimension representation. Masked multi-head self-attention is used to prevent the decoder from &#8221;looking ahead&#8221; at future frames that have not been predicted yet. After applying the mask, the decoder can only attend to itself and past frames. Cross attention utilizes queries from the preceding decoder layer, while the keys and values are derived from the original encoder context vector. This allows the decoder to observe the entire encoder context when deciding the next frame.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "after",
                    "spectrogram",
                    "predicted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, a post-net consisting of five 1-dimensional convolutions refines the predicted mel spectrogram to denoise and sharpen the final audio signal <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "spectrogram",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transcript decoder uses a very similar architecture to the spectrogram decoder and employs the same Transformer decoder architecture.</p>\n\n",
                "matched_terms": [
                    "spectrogram",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like StutterZero, StutterFormer employs a hybrid loss function that combines multiple weighted losses.\nThe spectrogram decoder also uses MSE loss between the predicted and ground truth spectrograms. In addition to MSE loss, the spectrogram decoder also computes a mean absolute error (MAE) loss. MAE loss measures the absolute difference of the predicted values and target values without squaring. When tested with spectrogram applications, MAE loss promotes sharper spectrograms that prevent the predicted fluent speech from sounding slurred <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guso2022</span>]</cite>. The transcript decoder also uses cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "predicted",
                    "like",
                    "transcript",
                    "spectrogram",
                    "spectrograms",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterFormer is trained on a cosine annealing with warm restarts scheduler. This approach periodically &#8221;restarts&#8221; the learning rate to help escape local minima and explore the loss landscape more effectively <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2024</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cazenave2022a</span>]</cite>. Like StutterZero, StutterFormer was trained for 1000 epochs.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "like",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the ASR-based and end-to-end approaches used in this study, state-of-the-art fluent-speech ASR models were also evaluated to establish a baseline for how accurately current speech recognition systems transcribe stuttered speech. This study used the Whisper-Tiny, Whisper-Small, and Whisper-Medium pretrained fluent ASR models as baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2022</span>]</cite>. Any larger models such as Whisper-Large could not be tested due to memory and hardware limitations. The 10% validation data split was used to assess each of the six models.</p>\n\n",
                "matched_terms": [
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because both approaches in this research produce audio signals, the audio signals must be converted back to text before any word or character-level evaluation. This study used an unmodified, pretrained copy of the Whisper-Small ASR model to transcribe the predicted fluent sequences from both the ASR and end-to-end approaches, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.F5\" title=\"Figure 5 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. This was an attempt to simulate what an untrained, average English-speaking person would interpret the fluent speech to be. The predicted transcripts of the fluent speech were compared with ground truth transcripts to calculate the final metrics. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T6\" title=\"Table 6 &#8227; 3.1 Benchmarking &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> displays the mean WER, CER, BERTScore Precision, and their standard deviations in the validation data split.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "fluent",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The non-parametric, two-sided Wilcoxon Signed-Rank Test is used to assess the statistical significance of the improvements achieved by this research compared to the baseline Whisper performance. This test is chosen due to its widespread use and established precedent in evaluating significant differences between models <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rainio2024</span>]</cite>. Comparing the performance of the ASR-based approach with Whisper-Medium (the best performing Whisper model), the Wilcoxson Test returns a test statistic of <math alttext=\"77631.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mn>77631.0</mn><annotation encoding=\"application/x-tex\">77631.0</annotation></semantics></math>, a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. Running the test comparing the performance of the end-to-end StutterZero and StutterFormer models against Whisper-Medium also returns a p-value of <math alttext=\"&lt;1\\mathrm{e}^{-100}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>&#8722;</mo><mn>100</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;1\\mathrm{e}^{-100}</annotation></semantics></math>, significant at <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math>. This shows that both StutterZero and StutterFormer demonstrate a significant improvement over state-of-the-art fluent speech models.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study was conducted to determine the impact of the multitask architecture and transcript decoder. StutterZero and StutterFormer were re-trained across a five-fold cross-validation with all hyperparameters, data splits, and other tunable values kept constant. However, the transcript decoder was removed from both models during the ablation.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "impact",
                    "stutterformer",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This significant difference in every metric after the ablation in both models demonstrates the importance of the transcript decoder. Observing StutterZero, the WER increased by 22.3% whilst the CER only increased by 15%. This shows the ablated StutterZero predicted most of the characters in a word correctly, but perhaps missed a few characters in some more words. The same phenomenon is seen in StutterFormer ablation, but to a lesser degree. This aligns with the functionality of the transcript decoder &#8211; to help both end-to-end models capture more detailed orthographical features in words. A greater increase in WER than in CER could mean StutterZero/StutterFormer incorrectly predicted more allophones and homophones, which have similar character-level spellings but are different words.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "predicted",
                    "transcript",
                    "after",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">FluencyBank is a subset of the TalkBank corpus, an open repository for spoken language data. FluencyBank specifically focuses on disfluencies, including stuttering. Because the FluencyBank dataset is password-protected and it is not possible to automate the scraping of data from the website, this research manually downloaded audio clips from a randomly selected recording along with their transcripts from the Voice-AdultsWhoStutter (Voices-AWS) subset. After downloading and splitting each audio file to be 30 seconds or less, there were 800 stuttered audio samples.</p>\n\n",
                "matched_terms": [
                    "stuttering",
                    "after",
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because FluencyBank is an entirely new dataset with unique audio characteristics, StutterZero and StutterFormer are tested on data they have never encountered before. This ensures that no bias from the training data influences the results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18938v2#S3.T8\" title=\"Table 8 &#8227; 3.4 FluencyBank Validation Test &#8227; 3 Results &#8227; StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the WER and BERTScore metrics after testing StutterZero and StuttFormer on samples of the FluencyBank dataset.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "after",
                    "audio",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effects of stutter correction are most apparent in the first row (audio 29mb_1.wav). In the stuttered spectrogram, the doubled upward-sloping signal represents a word repetition (&#8220;can you&#8211;can you&#8221;). Both StutterZero and StutterFormer successfully remove this repetition in their predicted fluent spectrograms, producing a single, continuous signal in place of the duplicated pattern.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "predicted",
                    "audio",
                    "stutterformer",
                    "correction",
                    "fluent",
                    "stuttered",
                    "spectrogram",
                    "spectrograms",
                    "stutter",
                    "29mb1wav"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces three stutter correction systems, including the first two end-to-end encoder&#8211;decoder models, and provides evidence that direct conversion of stuttered to fluent audio is both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stutter",
                    "correction",
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer significantly outperformed state-of-the-art fluent speech recognition models in WER, CER, and BERTScore. The Wilcoxon Signed-Rank Test demonstrated the improvements (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>) of StutterZero and StutterFormer over the next best model, Whisper-Medium. This demonstrates the effectiveness of multitask, end-to-end models for stuttered speech recognition.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stuttered",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike existing stutter correction approaches, which struggle to address all five stuttering types, StutterFormer achieves an average transcription accuracy of 90% on the combined SEP-28K and LibriStutter validation set. It maintains robust performance on entirely new data with different speakers and recording conditions, achieving 88% accuracy. These results highlight StutterFormer&#8217;s resilience to variations in audio quality and speaker prosody, enabling consistently high performance across diverse datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stuttering",
                    "correction",
                    "stutter",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero is not far behind, with an 88% transcription accuracy on the combined validation set and 84% on the FluencyBank subset. The slight decrease in accuracy may be attributed to the Transformer architecture&#8217;s efficiency in modeling speech and audio sequences. Indeed, correcting stuttered speech is a sequence-to-sequence task, similar to machine translation and other language tasks where Transformers have shown exceptional performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chorowski2015</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stuttered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The observed performance improvements resonate with trends in related domains, such as dysarthric and accented speech recognition, where multitask and end-to-end approaches have consistently demonstrated robustness to atypical input <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mujtaba2025</span>]</cite>. By showing that stutter correction benefits strongly from the integration of textual and acoustic objectives, our findings suggest that linguistic supervision is not merely auxiliary but foundational. This echoes psycholinguistic accounts that disfluency cannot be treated as random noise but reflects systematic deviations in speech planning and motor execution. In this sense, our work challenges the assumption &#8211; common in early DSP and ASR pipelines &#8211; that acoustic correction can be divorced from lexical anchoring.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "early",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The superior performance of StutterFormer over StutterZero cannot be attributed solely to increased model capacity. Rather, Transformer attention provides specific advantages for disfluency correction. Multi-head self-attention captures long-range dependencies, allowing the model to flexibly relate repeated or prolonged segments of speech to their fluent counterparts. This mechanism facilitates alignment across syllables and words, which is essential when disfluencies span multiple phonemic units. In addition, attention layers can simultaneously model both global and local phonetic details, helping to preserve intonation and rhythm while removing interruptions. These properties explain why attention-based architectures are particularly well-suited for mapping stuttered to fluent speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeyer2019</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "correction",
                    "stuttered",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A substantial portion of training and evaluation relied on fluent audio generated by TTS systems. While this strategy enabled efficient creation of large-scale paired datasets, it introduced a prosodic mismatch between synthetic and natural speech. As a result, models may have partially learned to adapt to synthetic rhythms and intonation rather than capturing the full variability of natural stuttering. This could limit robustness when applied to spontaneous, emotionally nuanced speech. Future research should mitigate this gap by curating larger collections of natural stutter&#8211;fluent pairs, leveraging prosody encoders to capture expressive detail, and employing domain adaptation techniques to reduce distributional bias.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "stuttering",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this research achieves impressive preliminary results, it also acts as a proof-of-concept, opening the doors for more advanced end-to-end models in stutter correction.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate the prosodic loss due to training on TTS-generated fluent speech samples, using multiple encoders to capture the tonal and prosodic content of the stuttered speech could be explored. Multi-encoder models have been explored in dysarthic speech conversion, specifically using a prosody encoder to extract prosodic features <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2022</span>]</cite>. This would enable fluent outputs that retain the speaker&#8217;s identity, pitch, and expressive nuance. Additionally, expanding to multilingual and low-resource languages through cross-lingual pretraining and transfer learning would extend accessibility to a wider global population <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stuttered",
                    "fluent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The current study is limited by access to a small number of corpora. Expanding to diverse datasets such as UCLASS, FluencyBank, and AS-70 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2024</span>]</cite>, as well as developing multilingual training resources, would enhance model generalizability. Cross-lingual pretraining and transfer learning offer promising strategies to extend accessibility to low-resource and global language communities, ensuring that stutter correction technology benefits a wider population.</p>\n\n",
                "matched_terms": [
                    "correction",
                    "stutter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StutterZero and StutterFormer have great potential in automating and assisting with delayed auditory feedback (DAF), a common technique used in speech-language therapy. It involves recording and playing back a PWS&#8217;s speech after a brief delay (usually a few milliseconds to fractions of a second) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ozker2025</span>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">buzzeti2018</span>]</cite>. Playback of the fluent speech can demonstrate how fluent speech is supposed to sound, thereby reinforcing self-monitoring and reducing disfluencies. Instead of the individual hearing their disfluent utterances delayed, the model could provide them with a fluent version of what they intended to say. This would supply the brain with consistent, fluent auditory feedback, potentially reducing the reinforcement of stuttered patterns while strengthening neural pathways associated with fluent production. Indeed, speaking in unison with a fluent signal (an external &#8220;fluent version&#8221; of one&#8217;s speech content) reliably induces near-instant fluency in most PWS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kalinowski2003</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "stuttered",
                    "say",
                    "after",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond therapy, optimized versions of StutterZero and StutterFormer could be deployed in real-time communication settings. With techniques such as model quantization, pruning, and knowledge distillation, lightweight implementations may run on mobile or embedded devices. Potential applications include live correction during phone calls, video conferences, and online meetings, where disfluent speech is automatically converted to fluent audio for listeners. Similar methods could also be applied to voice recording, broadcasting, and sound engineering, eliminating the need for re-recording when disfluencies occur.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "correction",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The research presents several contributions to the field of stutter correction and speech conversion. First, an ASR-based pipeline utilizing fine-tuned Whisper-Small that achieved significant performance improvements, reducing Word Error Rate to 4% compared to 36.1% for the baseline Whisper-Medium model. Second, and more importantly, this research introduced StutterZero and StutterFormer, the first two end-to-end stutter correction models that directly convert stuttered speech to fluent audio without intermediate transcription steps. StutterZero was a multitask encoder-decoder architecture using conventional convolution and LSTM layers, reducing Word Error Rate to 11%. StutterFormer was based on a modern Transformer architecture and reduced Word Error Rate even further to 8%. Being the first end-to-end models for stutter correction, both StutterZero and StutterFormer pave the way for future development of larger and more accurate end-to-end models. Specifically, the encoder-decoder architecture allows for great flexibility in the choice of encoder and decoder, allowing several multitask encoders and decoders to work in unison to learn different representations of the audio. In industry, this research may pave the way for more accessible human-machine interaction and communication.</p>\n\n",
                "matched_terms": [
                    "stutterzero",
                    "audio",
                    "stutter",
                    "correction",
                    "stuttered",
                    "fluent",
                    "stutterformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research originated from personal initiative, as the student researcher has a stutter and sought to address this challenge through computational methods.\nThe author expresses sincere gratitude to Mr. Cook, teacher of the Millburn High School Science Research Program, for his invaluable mentorship. Mr. Cook provided detailed guidance on the scientific process &#8212; from formulating research questions and establishing goals to conducting literature reviews and writing in accordance with academic standards. Equally important, he encouraged students to reach out to professors and domain experts, a practice that significantly shaped the independent yet rigorous nature of this work.</p>\n\n",
                "matched_terms": [
                    "school",
                    "stutter",
                    "has"
                ]
            }
        ]
    }
}