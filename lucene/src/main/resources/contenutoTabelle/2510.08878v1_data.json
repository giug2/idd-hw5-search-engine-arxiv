{
    "S4.T1": {
        "caption": "Table 1: Objective and subjective evaluation results on the AudioCondition test set. For each metric, the best result is bold and the second-best is underlined. * denotes models trained by ourselves. ‡ denotes models evaluated under a different SED model. ControlAudio full denotes evaluation on prompts covering all event classes in the test set.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Temporal (Obj.)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Generation (Obj.)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Subjective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Efficiency</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Eb&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">At&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FAD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">KL&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CLAP&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Temporal&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">OVL&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">RTF&#8595;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.377</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AudioLDM Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.260</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.141</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioLDM 2 Large</td>\n<td class=\"ltx_td ltx_align_center\">7.75</td>\n<td class=\"ltx_td ltx_align_center\">42.41</td>\n<td class=\"ltx_td ltx_align_center\">3.07</td>\n<td class=\"ltx_td ltx_align_center\">1.92</td>\n<td class=\"ltx_td ltx_align_center\">0.279</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.496</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioLDM 2 Full Large</td>\n<td class=\"ltx_td ltx_align_center\">6.93</td>\n<td class=\"ltx_td ltx_align_center\">20.47</td>\n<td class=\"ltx_td ltx_align_center\">3.68</td>\n<td class=\"ltx_td ltx_align_center\">2.15</td>\n<td class=\"ltx_td ltx_align_center\">0.283</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.496</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Tango</td>\n<td class=\"ltx_td ltx_align_center\">1.60</td>\n<td class=\"ltx_td ltx_align_center\">26.51</td>\n<td class=\"ltx_td ltx_align_center\">2.82</td>\n<td class=\"ltx_td ltx_align_center\">1.93</td>\n<td class=\"ltx_td ltx_align_center\">0.245</td>\n<td class=\"ltx_td ltx_align_center\">1.68</td>\n<td class=\"ltx_td ltx_align_center\">2.58</td>\n<td class=\"ltx_td ltx_align_center\">1.207</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stable Audio&#160;*</td>\n<td class=\"ltx_td ltx_align_center\">11.28</td>\n<td class=\"ltx_td ltx_align_center\">51.67</td>\n<td class=\"ltx_td ltx_align_center\">1.93</td>\n<td class=\"ltx_td ltx_align_center\">1.75</td>\n<td class=\"ltx_td ltx_align_center\">0.318</td>\n<td class=\"ltx_td ltx_align_center\">1.94</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.44</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.821</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">CCTA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.207</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MC-Diffusion</td>\n<td class=\"ltx_td ltx_align_center\">29.07</td>\n<td class=\"ltx_td ltx_align_center\">47.11</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Tango + LControl</td>\n<td class=\"ltx_td ltx_align_center\">21.46</td>\n<td class=\"ltx_td ltx_align_center\">55.15</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.207</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioComposer-Small</td>\n<td class=\"ltx_td ltx_align_center\">43.51</td>\n<td class=\"ltx_td ltx_align_center\">60.83</td>\n<td class=\"ltx_td ltx_align_center\">4.92</td>\n<td class=\"ltx_td ltx_align_center\">2.00</td>\n<td class=\"ltx_td ltx_align_center\">0.261</td>\n<td class=\"ltx_td ltx_align_center\">3.12</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.721</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioComposer-Large</td>\n<td class=\"ltx_td ltx_align_center\">44.40</td>\n<td class=\"ltx_td ltx_align_center\">63.30</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TG-Diff<math alttext=\"~{}^{\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><msup><mi/><mo>&#8225;</mo></msup><annotation encoding=\"application/x-tex\">~{}^{\\ddagger}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.70</td>\n<td class=\"ltx_td ltx_align_center\">60.06</td>\n<td class=\"ltx_td ltx_align_center\">2.66</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.244</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.207</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FreeAudio</td>\n<td class=\"ltx_td ltx_align_center\">44.34</td>\n<td class=\"ltx_td ltx_align_center\">68.50</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.73</span></td>\n<td class=\"ltx_td ltx_align_center\">0.321</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.166</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ControlAudio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">55.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">79.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.325</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.821</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">ControlAudio full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">49.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">71.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.356</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.821</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "temporal↑",
            "full",
            "evaluated",
            "tgdiff‡ddagger",
            "prompts",
            "obj",
            "evaluation",
            "denotes",
            "each",
            "event",
            "clap↑",
            "ourselves",
            "audiocondition",
            "tango",
            "audiocomposerlarge",
            "classes",
            "audio",
            "objective",
            "generation",
            "efficiency",
            "sed",
            "test",
            "trained",
            "ground",
            "truth",
            "covering",
            "fad↓",
            "result",
            "bold",
            "under",
            "ccta",
            "rtf↓",
            "results",
            "underlined",
            "metric",
            "stable",
            "set",
            "audioldm",
            "secondbest",
            "lcontrol",
            "kl↓",
            "model",
            "ovl↑",
            "eb↑",
            "at↑",
            "best",
            "all",
            "large",
            "mcdiffusion",
            "controlaudio",
            "method",
            "freeaudio",
            "subjective",
            "audiocomposersmall",
            "different",
            "temporal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> We compare ControlAudio with several state-of-the-art TTA models, including AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>, AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib26\" title=\"\">2024a</a>)</cite>, Tango&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>)</cite>, and our in-house implementation of Stable Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>. We also include models that incorporate explicit temporal conditioning signals, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>, as well as the training-free baselines TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>. TG-Diff reports both timing and audio quality metrics under a training-free framework but relies on a different sound event detection model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Turpault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib37\" title=\"\">2019</a>)</cite> compared to other baselines. Control-Condition-to-Audio (CCTA) is a baseline variant of MC-Diffusion that uses only control conditions without textual input, while Tango + LControl is an AudioComposer variant built on Tango with language-based temporal control. In terms of efficiency, we measure the real-time factor (RTF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib27\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib28\" title=\"\">c</a>)</cite> for all models on a single NVIDIA A800 GPU. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Progressive Model Training &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ControlAudio achieves competitive or superior temporal alignment compared to existing methods, while significantly improving audio generation quality in both objective and subjective metrics, and does so without introducing additional inference overhead compared to baseline models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective",
                    "generation",
                    "controlaudio",
                    "method",
                    "subjective",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation systems aim at synthesizing high-fidelity audio samples that are consistent with the given natural language description,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping\"&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>); Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib16\" title=\"\">2023</a>); Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>)</cite>.\nRecent efforts are exploring more fine-grained control for TTA systems, which can be categorized into two main classifications.\nThe first group adds precise timing control,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 2-5 seconds\", with innovations spanning conditioning techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite> and training-free latent manipulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>.\nThe second group works on intelligible audio generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, and a man is saying: &#8217;it&#8217;s a very sunny day&#8217;\", by introducing additional modules to encode both audio and speech semantic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>); Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nHowever, as expensive to collect large-scale text-audio datasets with precise timing and speech information, their controllable generation performance at scale remains limited, and none of the prior work explores&#160;<span class=\"ltx_text ltx_font_italic\">timing-controlled and intelligible TTA generation</span>,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 0-5 seconds, and then a man is saying: &#8217;it&#8217;s a very sunny day&#8217;, at 7-10 seconds\", within a unified framework.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "each",
                    "controlaudio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With target distributions predefined by datasets constructed above, we introduce progressive diffusion training, fulfilling high-quality TTA synthesis at pre-training and gradually integrating fine-grained control signals at continual learning stages.\nAt the first stage, we pre-train a diffusion transformer (DiT) in the latent space directly compressed from the waveform space, solely conditioned on text indication, achieving high-fidelity TTA at scale.\nAt the second stage, we fine-tune the latent DiT on both text and timing conditions, enabling the model to precisely control the timing windows of each sound event.\nIn controllable TTA generation, a common issue is the sacrifice of text-conditioned synthesis quality without fine-grained conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>.\nHence, in ControlAudio, we switch the condition between the text condition and the <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math> condition at the second stage, avoiding catastrophic forgetting in progressive training.\nAt the final stage, given the audio generation prior learned in prior stages, we continually train the diffusion model by switching the condition among text, <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math>, and <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math>, achieving high-fidelity audio synthesis conditioned on flexible indication.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "generation",
                    "each",
                    "controlaudio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance on controllable audio generation tasks, significantly outperforming existing methods in both objective and subjective evaluations of temporal accuracy and speech clarity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "objective",
                    "generation",
                    "controlaudio",
                    "subjective",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "prompts",
                    "classes",
                    "mcdiffusion",
                    "generation",
                    "event",
                    "freeaudio",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion-based&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peebles and Xie (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib32\" title=\"\">2023</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib23\" title=\"\">2024</a>)</cite> TTA models are typically trained to learn a conditional reverse of a data-to-noise forward process&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib12\" title=\"\">2020</a>)</cite>, progressively removing noise from an initial random state conditioned on a text prompt over multiple diffusion steps. This framework consists of three main modules: 1) an audio varational autoencoder (VAE), responsible for transforming the audio sample into a compressed latent representation while ensuring the reconstruction quality; 2) a pretrained text encoder, which encodes a text prompt into conditioning embeddings; and 3) a latent diffusion model, which predicts the denoised audio latents conditioned on the text embeddings.\nIn ControlAudio, we employ a DiT-based architecture to ensure scalability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>, conditioned on the text, timing, and phoneme embeddings to generate the latent audio representation directly compressed from the waveform, without cascaded decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "controlaudio",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Classifier-Free Guidance (CFG) <cite class=\"ltx_cite ltx_citemacro_cite\">Ho and Salimans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib13\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib38\" title=\"\">2025a</a>)</cite> emphasizes the guidance of a conditioning signal <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> during sampling. At each sampling step, CFG-guided diffusion models produce two predictions: a conditional estimation <math alttext=\"\\epsilon_{\\theta}(x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(x_{t},c)</annotation></semantics></math> and an unconditional estimation <math alttext=\"\\epsilon_{\\theta}(x_{t},\\emptyset)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(x_{t},\\emptyset)</annotation></semantics></math>. Then the final prediction is obtained by extrapolating these two terms with a guidance scale <math alttext=\"w&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>w</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w&gt;1</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "models",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, current TTA generation quality has been advanced with latent diffusion models, while the quality of controllable generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech control, is still limited.\nAlthough diverse innovations have been proposed, their synthesis quality at scale is still compromised by data scarcity.\nMoreover, previous research rarely achieves versatile TTA generation, namely, integrating additional fine-grained control signals while preserving high-fidelity audio generation solely conditioned on text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a progressive diffusion modeling design covering data construction and representation, model training, and guided sampling to tackle these difficulties, achieving text-guided, timing-indicated, and intelligible audio generation with a single diffusion model.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates our overall progressive strategy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "generation",
                    "covering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Scarcity.</span>\nFor TTA generation, we can collect various publicly available datasets, which comprise millions of weakly-labeled text-audio pairs&#160;(Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS1\" title=\"A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>), supporting high-quality synthesis at scale.\nHowever, these datasets typically contain only high-level textual descriptions, lacking the fine-grained annotations required for controllable synthesis. Specifically, training timing-controlled and intelligible TTA generation requires datasets that combine speech with general audio events under precise timing annotations. Yet, such datasets are rare: existing timing-annotated audio datasets are limited in scale and lack transcriptions for speech segments, while publicly available speech datasets do not have reliable temporal labels. To overcome this limitation, we first construct a multi-source dataset.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "under",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotated Data.</span> Our data annotation pipeline begins with the AudioSet-SL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite> dataset, chosen for its reliable temporal annotations while lacking corresponding speech transcripts. To create the ControlAudio dataset, we first select all clips containing \"human speech\" and then extract a clean speech track from each using a dual-demixing strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite> that leverages both MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite>. The clean track is subsequently segmented into individual events using the original timestamps.\nFinally, each segmented event is transcribed using Gemini 2.5 Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/models/gemini/pro/\" title=\"\">https://deepmind.google/models/gemini/pro/</a></span></span></span>. Further details of this entire pipeline are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS3\" title=\"A.3 Annotated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.\nThis transcription process enriches the dataset&#160;<span class=\"ltx_text ltx_font_italic\">i.e.</span>, expanding condition to <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math> for fine-grained control. For example, a generic annotation like (man speaking, &lt;3.00,5.00&gt;) is transformed into a specific, content-rich event&#160;(man speaking: \"It&#8217;s been raining all day.\", &lt;3.00,5.00&gt;).</p>\n\n",
                "matched_terms": [
                    "all",
                    "each",
                    "controlaudio",
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Simulated Data.</span> To further expand our dataset, we construct a large-scale simulated dataset guided by real-world data distribution. Specifically, we first analyze the AudioSet-SL dataset to derive statistical priors on speech activity patterns, with further details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS4\" title=\"A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>. These distributions guide our synthesis process, which proportionally simulates two main scenarios: single-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">monologue</span>) are created by combining multiple utterances from the same speaker in LibriTTS-R, while multi-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">dialogue</span>) are formed by sampling from different speakers. After composing the speech samples, we simulate a plausible temporal arrangement for the utterances. Finally, the composed speech is mixed with non-speech backgrounds from WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite> at a signal-to-noise ratio sampled from a uniform 2 to 10&#160;dB range&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>. Through this simulation pipeline, we generate an additional 171,246 complex audio scenes, significantly expanding the scale and diversity of our training data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "different",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of encoding diverse condition information, including text, timing, and phoneme features, we propose a unified semantic modeling approach that handles them with a single text encoder in a progressive and coarse-to-fine manner. This approach avoids the complexity of multiple specialized modules&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> by first establishing a robust structural representation, providing a simple yet effective solution for rendering fine-grained content in audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Text and Timing Representation.</span>\nThe foundation of our approach is the Structured Prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>), a novel representation we design to explicitly and unambiguously define the composition of an acoustic scene. The prompt employs a standardized format using special tokens to delimit event descriptions and their precise start-and-end times, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Dataset Construction &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We propose this format to overcome the critical limitations of using free-form natural language for control. Natural language is often ambiguous; for instance, a prompt like \"<span class=\"ltx_text ltx_font_italic\">an alarm sounds from low to high from 1 second to 9 seconds</span>\" creates confusion, as a model must disentangle whether \"<span class=\"ltx_text ltx_font_italic\">from&#8230;to</span>\" refers to a change in pitch or a temporal boundary. Moreover, natural language descriptions become verbose and difficult to parse as scene complexity increases. In contrast, our structured format provides a concise, scalable, and machine-readable representation, forming a robust foundation for generating complex, temporally-aligned audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Phoneme Representation.</span>\nOur approach to synthesizing intelligible speech is built directly upon the temporal foundation provided by the structured prompt. A key insight of our work is that the explicit timing windows (&lt;start,end&gt;) assigned to each speech event inherently define the utterance&#8217;s total duration. This is a significant advantage over standard TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib1\" title=\"\">2024</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib21\" title=\"\">2024a</a>)</cite>, which must employ complex, often error-prone models just to predict the duration of each phoneme or word. By having the duration as a given constraint, our framework can bypass this challenging duration modeling task entirely.</p>\n\n",
                "matched_terms": [
                    "models",
                    "each",
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This simplification makes it natural and highly efficient to use the same, single text encoder to progressively model both the coarse-grained temporal structure and the fine-grained speech content. We therefore represent the speech content at the phoneme level (e.g., \"<span class=\"ltx_text ltx_font_italic\">hello</span>\" <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [HH, AH0, L, OW1]). Phonemes provide a more direct, pronunciation-aware signal than words, reducing ambiguity and improving the acoustic consistency of the generated speech. By augmenting our single encoder&#8217;s vocabulary with these phoneme tokens, it learns to render the precise phonetic sequence within the specified temporal boundaries, naturally inheriting the ability to handle speech duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train our model for multi-condition audio generation, we adopt a progressive three-stage training strategy.\nThis approach allows the model to acquire fine-grained control capability incrementally, where each new stage builds upon and refines the skills learned previously, ensuring a stable and efficient learning process.\nAt each stage, the model is optimized using the conditional diffusion objective&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib12\" title=\"\">2020</a>)</cite>, where a network is trained to predict the noise <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> added on the clean audio latents:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "objective",
                    "generation",
                    "each",
                    "trained",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: TTA Pre-training.</span> We first pre-train a DiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> on large-scale text-audio datasets to learn a robust, general mapping from textual descriptions to audio latent representation, ensuring&#160;<span class=\"ltx_text ltx_font_italic\">high-fidelity text-guided audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Timing-Controlled TTA Fine-tuning.</span> The pre-trained model is then fine-tuned on our dataset of precisely timing-annotated audio, while preserving the training on text condition without timing. This stage specifically optimizes the model to interpret the structured prompt containing both text and timing information, achieving&#160;<span class=\"ltx_text ltx_font_italic\">text-guided and timing-controlled audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "full",
                    "audio",
                    "model",
                    "generation",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our progressive model training incrementally acquires finer-grained capabilities while building upon the foundational skills from previous stages. Notably, we find that the joint optimization at Stage 3 not only unlocks speech intelligibility but also further enhances the model&#8217;s previously learned temporal precision. We attribute these significant improvements to two key factors. The first is the introduction of time-annotated speech data, which provides a richer, more targeted signal for learning the alignment between linguistic content and temporal boundaries. The second is the fine-tuning of the text encoder, which allows it to be jointly optimized with the diffusion backbone; this synergistic training enables both the conditioning (text encoder) and generation (DiT) components to co-adapt to the complex, multi-objective task. This effective co-adaptation is achieved within a simple yet effective framework, where a single text encoder is responsible for processing all conditioning signals: text, timing, and phoneme features.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation",
                    "model",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To optimize our capability to handle both timing and more fine-grained phonetic content, we propose a Progressively Guided Sampling strategy. This approach divides the reverse diffusion process into two phases based on a threshold timestep <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, modulating the conditioning prompt and guidance scale accordingly. Specifically, in the initial sampling phase (<math alttext=\"t\\in[1.0,t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1.0</mn><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in[1.0,t_{1}]</annotation></semantics></math>), we guide the model with a simplified version of our structured prompt that excludes phonetic content <math alttext=\"c_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">c_{1}</annotation></semantics></math>, using a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>). This encourages the model to first establish a plausible temporal structure for all audio events:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "temporal",
                    "model",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This second phase strictly enforces adherence to the phonetic sequence, ensuring the synthesis of highly intelligible speech within the established structure. This coarse-to-fine strategy improves temporal accuracy and speech clarity by decoupling event placement and content rendering.</p>\n\n",
                "matched_terms": [
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audiocondition",
                    "generation",
                    "evaluation",
                    "each",
                    "results",
                    "test",
                    "method",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We conduct a comprehensive evaluation covering three key aspects: temporal control, audio quality, and speech intelligibility. For temporal control, we follow prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> and report two metrics computed by a sound event detection (SED) system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mesaros et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib30\" title=\"\">2016</a>)</cite>: the event-based measures (Eb) and the clip-level macro F1 score (At). For audio quality, we employ a suite of standard metrics, including Fr&#233;chet Audio Distance (FAD), Kullback&#8211;Leibler (KL) divergence, Fr&#233;chet Distance (FD), Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite> and CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite>. For speech intelligibility, we conduct both objective and subjective tests. Objectively, we measure the Word Error Rate (WER) by transcribing generated speech with the Whisper <span class=\"ltx_text ltx_font_italic\">Large-v3</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib33\" title=\"\">2023</a>)</cite>. Subjectively, we conduct Mean Opinion Score (MOS) tests where 20 participants rate three aspects on a five-point scale: Speech Intelligibility, Overall Quality (OVL), and Relevance to the prompt (REL). Further details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3\" title=\"Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "covering",
                    "audio",
                    "model",
                    "objective",
                    "evaluation",
                    "sed",
                    "event",
                    "subjective",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n",
                "matched_terms": [
                    "audioldm",
                    "audio",
                    "all",
                    "generation",
                    "controlaudio",
                    "results",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio",
                    "generation",
                    "under",
                    "controlaudio",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Prompt Design.</span> To isolate and evaluate the effectiveness of our structured prompt design, we conduct a targeted ablation study. For this analysis, we compare a baseline model trained with conventional natural language descriptions against our model trained with structured prompts. Crucially, both models are trained only up to Stage 2 of our progressive curriculum, the phase dedicated specifically to learning timing control. This controlled setting allows us to fairly assess the impact of the prompt format itself. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model trained with structured prompts consistently achieves superior temporal alignment and overall audio quality on the AudioCondition test set. The results suggest that the structured format provides a clearer, unambiguous mapping between events and their time spans, an advantage that becomes particularly pronounced in complex scenes where verbose natural language descriptions can degrade timing accuracy.</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "audiocondition",
                    "audio",
                    "prompts",
                    "model",
                    "results",
                    "test",
                    "trained",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Sampling Strategy.</span> We conduct an analysis to validate our progressive sampling strategy. This coarse-to-fine approach first uses a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>) with a simplified, content-free prompt to establish the temporal structure. It then transitions to a high scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>) with the full, phoneme-inclusive prompt to render intelligible speech. For a total of <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T=100</annotation></semantics></math> sampling steps, this transition occurs at timestep <math alttext=\"t_{1}=88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mn>88</mn></mrow><annotation encoding=\"application/x-tex\">t_{1}=88</annotation></semantics></math>. As visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our analysis of varying <math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math> and <math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math> reveals a clear trade-off: a low initial scale is crucial for overall audio quality, while a high subsequent scale is essential for speech intelligibility. This study empirically identifies the optimal configuration as (<math alttext=\"w_{low}=3,w_{high}=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><mo>=</mo><mn>9</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{low}=3,w_{high}=9</annotation></semantics></math>), confirming the effectiveness of our approach.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "full",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced ControlAudio, which recasts controllable TTA generation as a multi-task learning problem solved via a progressive diffusion modeling strategy. This progressive approach is applied across data construction, model training, and inference, enabling our model to incrementally master fine-grained control from text, timing, and phoneme conditions. Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance in both temporal accuracy and speech clarity. Our work&#8217;s potential for misuse in creating deceptive content or voice impersonations underscores the urgent need for robust detection methods and responsible AI governance.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "controlaudio",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "generation",
                    "controlaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T6\" title=\"Table 6 &#8227; A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a comprehensive summary of all corpora used for pretraining our TTA backbone. To learn a robust mapping between text and audio, we aggregate a diverse mixture of large-scale, publicly available datasets. This includes datasets with descriptive captions, such as the large-scale WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and the widely-used AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>, as well as corpora with high-level event labels, like the massive AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gemmeke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib7\" title=\"\">2017</a>)</cite>. This rich combination of data sources, spanning both detailed descriptions and a wide vocabulary of sound classes, allows the model to learn robust and versatile semantic representations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "classes",
                    "model",
                    "all",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio samples from these sources undergo a standardized preprocessing pipeline. First, all audio is resampled to 16kHz and converted to a mono-channel format. To accommodate the fixed-size input requirement of our diffusion model, all clips are processed into a uniform 10-second duration. Samples shorter than 10 seconds are right-padded with silence, while for samples longer than 10 seconds, a random 10-second segment is cropped.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the timing-control fine-tuning stage, our dataset is constructed based on AudioSet-Strong&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains 1.8M audio clips. This dataset is crucial as it provides dense, frame-level timestamps for 456 sound event classes. However, since AudioSet-Strong only provides categorical labels (e.g., \"Dog\"), not descriptive text, we generate richer captions for each timed event. Inspired by the methodology of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite>, we employ a large language model (LLM) to create a unique textual description for each segmented audio event.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "classes",
                    "model",
                    "large",
                    "each",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical difference in preprocessing this dataset is the handling of audio duration to preserve the integrity of the timestamps. Unlike in the pretraining phase, we do not apply random cropping. Instead, we consistently take the first 10 seconds of each audio clip and subsequently filter the event annotations, retaining only those whose timestamps fall within this 0-10s window. Clips shorter than 10 seconds are right-padded with silence. This deterministic process ensures that the temporal annotations in our final dataset remain perfectly aligned with the corresponding audio segments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each",
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Data Selection from AudioSet-SL.</span> We begin with the AudioSet-SL dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains strong, human-verified temporal annotations for a wide range of sound events. From the full dataset, we first identify and select all 10-second audio clips that contain at least one event labeled as \"Human speech,\" \"Speech,\" or any of their subcategories. This initial filtering yields a subset of 49,950 clips containing speech.</p>\n\n",
                "matched_terms": [
                    "full",
                    "audio",
                    "all",
                    "event",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event-Level Segmentation.</span> The extracted clean speech track is then segmented into individual, non-overlapping speech events. We use the original, human-annotated start and end timestamps provided by AudioSet-SL to perform this segmentation. Each resulting audio segment represents a single, continuous speech utterance from the original recording. This process yields a total of 173,831 individual speech segments, which are then prepared for transcription.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription with Large Language Model.</span> Each of the 173,831 clean, segmented speech events is then sent for transcription. We input the audio segment into the Gemini 2.5 Pro model with a direct prompt to generate a precise textual transcription. To ensure the quality of the final annotations, we explicitly instruct the model to return an empty output if the spoken content in an audio segment is unintelligible or heavily obscured by noise. This step serves as a crucial quality filter.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "each",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This entire pipeline, from segmentation to filtered transcription, results in our final annotated dataset. From the initial pool of segments, a total of 152,070 high-quality, transcribed speech events are retained. Each event in this dataset is characterized by a precise start time, end time, and a verified textual transcription, providing an authentic and challenging data source for training our model on real-world, timed speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "each",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Utterance-per-Clip Distribution:</span> The empirical distribution is characterized by a prominent peak at a single utterance per clip (<math alttext=\"n=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n=1</annotation></semantics></math>), which accounts for 32.20% of all single-speaker scenarios. For <math alttext=\"n&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n&gt;1</annotation></semantics></math>, the frequency of clips generally decreases as the number of utterances increases, exhibiting a long tail. We sample from this distribution to determine the number of utterances in our simulated monologues, with the maximum number of utterances per clip capped at 8 to focus on the most prevalent scenarios. The full distribution is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T7\" title=\"Table 7 &#8227; A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guided Synthesis Pipeline.</span> The synthesis process for each 10-second clip is as follows. First, we determine the scenario type by sampling from the speaker distribution (a 79.1% chance of a single-speaker monologue). Next, we source clean speech utterances with transcripts from the LibriTTS-R dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib20\" title=\"\">2023</a>)</cite>. For a monologue, we sample a number of utterances (determined by the utterance-per-clip distribution, and capped at a maximum of 8) from a single speaker. For a dialogue, we sample utterances from 2 to 4 different speakers, ensuring that no single speaker contributes more than 4 utterances. We then simulate a plausible temporal arrangement for these utterances within the 10-second window. Finally, the composed speech-only track is mixed with a non-speech background audio clip randomly selected from a filtered subset of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite>. The mixing is performed at a signal-to-noise ratio (SNR) randomly sampled from a uniform distribution between 2 and 10&#160;dB.</p>\n\n",
                "matched_terms": [
                    "different",
                    "audio",
                    "each",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the architecture of the base model used for pretraining, before it is fine-tuned into ControlAudio. Our diffusion model is built upon the DiT (Diffusion Transformer) architecture within a latent diffusion modeling (LDM) paradigm. For pretraining, the model is conditioned on three input types: a natural language prompt (<span class=\"ltx_text ltx_font_typewriter\">prompt</span>), the start time (<span class=\"ltx_text ltx_font_typewriter\">seconds_start</span>), and the total duration (<span class=\"ltx_text ltx_font_typewriter\">seconds_total</span>). All conditions are embedded into a 768-dimensional feature space. The prompt is encoded using a pretrained Flan-T5 large model, while <span class=\"ltx_text ltx_font_typewriter\">seconds_start</span> and <span class=\"ltx_text ltx_font_typewriter\">seconds_total</span> are treated as numerical inputs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "controlaudio",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diffusion network backbone is a DiT with 24 layers, 24 attention heads, and a model hidden dimension of 1536&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">2024b</a>)</cite>. The model utilizes both cross-attention for all conditional inputs and global conditioning for duration-related signals. The internal token dimension of the diffusion model is 64, with a conditional token dimension of 768 and a global condition embedding dimension of 1536.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio",
                    "trained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve convergence stability and generation quality, we adopt several common training strategies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>, with configurations specified in our training setup. We apply Exponential Moving Average (EMA) to the model parameters. For optimization, we use the AdamW optimizer with a learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math>, <math alttext=\"(\\beta_{1},\\beta_{2})=(0.9,0.999)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#946;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#946;</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mn>0.9</mn><mo>,</mo><mn>0.999</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1},\\beta_{2})=(0.9,0.999)</annotation></semantics></math>, and a weight decay of <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this final stage, we initialize the model from the Stage 2 checkpoint and unfreeze the Flan-T5 text encoder, enabling joint optimization with the diffusion backbone. The optimization configurations are retained from the previous stages. This joint training is crucial as it allows the text encoder to adapt its representations to the composite nature of our prompt, which includes the structured format, special tokens for timing, and the extended phoneme-level vocabulary for speech. As a result, the model learns a unified representation that maps diverse inputs, such as semantic descriptions, precise temporal spans, and intelligible speech content, to a single, high-quality, timing-controlled audio output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "result",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive objective evaluation to assess our model&#8217;s performance in two key areas: audio quality and semantic alignment with the text prompt.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality.</span> Our primary metric for audio fidelity is the Fr&#233;chet Audio Distance (FAD), which measures the distributional difference between generated and reference audio based on VGGish embeddings. To evaluate the consistency of acoustic event distributions, we also report Kullback-Leibler (KL) divergence computed using the PANNs tagging model. For completeness and comparison with prior works, we include the Inception Score (IS) and Fr&#233;chet Distance (FD) as supplementary metrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "metric",
                    "model",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A higher CLAP score indicates better semantic correspondence in the shared embedding space. All objective metrics are computed using the official AudioLDM evaluation toolkit for consistency.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "audioldm",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our subjective evaluation, we recruited 20 human evaluators to rate generated audio samples on a 5-point Mean Opinion Score (MOS) scale (1-5, with higher scores being better). The evaluation was divided into two distinct tasks, each with specific criteria:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "each",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> In this task, participants were presented with an audio clip and its corresponding timed prompt. They were asked to rate the audio based on the following two aspects:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporal Alignment (Temporal):</span> This measures the accuracy of timestamp adherence. The question asked was: \"How accurately does the timing of the audio events match the given start and end times in the prompt?\"</p>\n\n",
                "matched_terms": [
                    "audio",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> In this task, participants were presented with an audio clip containing speech and the text it was intended to convey. They rated the audio based on the following three aspects:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances have demonstrated the powerful planning and cross-modal reasoning capabilities of large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib40\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib39\" title=\"\">2024</a>)</cite>. We leverage these capabilities by employing an LLM to function as a \"planner\" that automatically converts a free-form natural language caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) into a precise, structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>) for our generative model. This conversion follows a three-stage reasoning process inspired by the Chain-of-Thought (CoT) paradigm, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3.F4\" title=\"Figure 4 &#8227; C.1 Objective Metrics &#8227; Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The process consists of the following steps:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event and Timing Planning.</span> Given the input caption, the LLM first identifies a set of distinct audio events <math alttext=\"\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}</annotation></semantics></math>. For each event, it infers a corresponding set of timing spans <math alttext=\"\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}</annotation></semantics></math>, where <math alttext=\"s_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">s_{ik}</annotation></semantics></math> and <math alttext=\"t_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ik}</annotation></semantics></math> are the start and end times in seconds. This multi-span representation is designed to handle events that occur multiple times.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio",
                    "each",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Content Planning.</span> For any event identified as speech (<math alttext=\"e_{i}\\in\\mathcal{E}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mtext>speech</mtext></msub></mrow><annotation encoding=\"application/x-tex\">e_{i}\\in\\mathcal{E}_{\\text{speech}}</annotation></semantics></math>), the LLM then infers a plausible utterance <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">c_{i}</annotation></semantics></math> that fits the overall context. This step enriches the planned events with specific, intelligible speech content, resulting in a set of intermediate tuples <math alttext=\"(e_{i},\\mathcal{T}_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e_{i},\\mathcal{T}_{i},c_{i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Recaption.</span> Finally, the LLM serializes the extracted information into the final structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>). This process starts with the original caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) and appends a specially formatted string for each planned event, which includes its name, associated time spans, and any inferred speech content.</p>\n\n",
                "matched_terms": [
                    "each",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n",
                "matched_terms": [
                    "different",
                    "models",
                    "results",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To generate textual transcriptions for our segmented speech events, we utilize the Gemini 2.5 Pro model. Each clean audio segment is provided as direct input. We designed a prompt that serves a dual function: it instructs the model to accurately transcribe the spoken content while simultaneously acting as a quality filter. Specifically, the prompt directs the model to return an empty string if the speech in an audio segment is unintelligible or heavily obscured by noise, thereby automatically discarding low-quality samples. This process ensures that only clear, valid audio segments are converted into high-quality audio-text pairs. The full prompt used for this task is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A5.F5\" title=\"Figure 5 &#8227; Appendix E Speech Transcription via ALM &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "full",
                    "each"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Objective and subjective evaluation results on the AC-Filtered.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Subjective</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FAD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">KL&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IS&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CLAP&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Intelligible&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">OVL&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">REL&#8593;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.523</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AudioLDM 2 Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">102.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.078</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceLDM-S</td>\n<td class=\"ltx_td ltx_align_center\">4.46</td>\n<td class=\"ltx_td ltx_align_center\">1.52</td>\n<td class=\"ltx_td ltx_align_center\">47.08</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.479</span></td>\n<td class=\"ltx_td ltx_align_center\">43.21</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n<td class=\"ltx_td ltx_align_center\">2.55</td>\n<td class=\"ltx_td ltx_align_center\">2.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceLDM-M</td>\n<td class=\"ltx_td ltx_align_center\">5.90</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">46.40</span></td>\n<td class=\"ltx_td ltx_align_center\">3.16</td>\n<td class=\"ltx_td ltx_align_center\">0.458</td>\n<td class=\"ltx_td ltx_align_center\">8.84</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceDiT</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.60</span></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.220</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.09</span></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">ControlAudio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">32.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.513</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.82</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluation",
            "clap↑",
            "voiceldmm",
            "voiceldms",
            "objective",
            "truth",
            "fd↓",
            "wer↓",
            "fad↓",
            "is↑",
            "results",
            "speech",
            "acfiltered",
            "audioldm",
            "kl↓",
            "ovl↑",
            "rel↑",
            "voicedit",
            "intelligible↑",
            "controlaudio",
            "method",
            "subjective",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "controlaudio",
                    "method",
                    "speech",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance on controllable audio generation tasks, significantly outperforming existing methods in both objective and subjective evaluations of temporal accuracy and speech clarity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective",
                    "subjective",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicedit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotated Data.</span> Our data annotation pipeline begins with the AudioSet-SL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite> dataset, chosen for its reliable temporal annotations while lacking corresponding speech transcripts. To create the ControlAudio dataset, we first select all clips containing \"human speech\" and then extract a clean speech track from each using a dual-demixing strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite> that leverages both MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite>. The clean track is subsequently segmented into individual events using the original timestamps.\nFinally, each segmented event is transcribed using Gemini 2.5 Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/models/gemini/pro/\" title=\"\">https://deepmind.google/models/gemini/pro/</a></span></span></span>. Further details of this entire pipeline are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS3\" title=\"A.3 Annotated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.\nThis transcription process enriches the dataset&#160;<span class=\"ltx_text ltx_font_italic\">i.e.</span>, expanding condition to <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math> for fine-grained control. For example, a generic annotation like (man speaking, &lt;3.00,5.00&gt;) is transformed into a specific, content-rich event&#160;(man speaking: \"It&#8217;s been raining all day.\", &lt;3.00,5.00&gt;).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "method",
                    "results",
                    "speech",
                    "acfiltered"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We conduct a comprehensive evaluation covering three key aspects: temporal control, audio quality, and speech intelligibility. For temporal control, we follow prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> and report two metrics computed by a sound event detection (SED) system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mesaros et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib30\" title=\"\">2016</a>)</cite>: the event-based measures (Eb) and the clip-level macro F1 score (At). For audio quality, we employ a suite of standard metrics, including Fr&#233;chet Audio Distance (FAD), Kullback&#8211;Leibler (KL) divergence, Fr&#233;chet Distance (FD), Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite> and CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite>. For speech intelligibility, we conduct both objective and subjective tests. Objectively, we measure the Word Error Rate (WER) by transcribing generated speech with the Whisper <span class=\"ltx_text ltx_font_italic\">Large-v3</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib33\" title=\"\">2023</a>)</cite>. Subjectively, we conduct Mean Opinion Score (MOS) tests where 20 participants rate three aspects on a five-point scale: Speech Intelligibility, Overall Quality (OVL), and Relevance to the prompt (REL). Further details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3\" title=\"Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective",
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> We compare ControlAudio with several state-of-the-art TTA models, including AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>, AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib26\" title=\"\">2024a</a>)</cite>, Tango&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>)</cite>, and our in-house implementation of Stable Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>. We also include models that incorporate explicit temporal conditioning signals, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>, as well as the training-free baselines TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>. TG-Diff reports both timing and audio quality metrics under a training-free framework but relies on a different sound event detection model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Turpault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib37\" title=\"\">2019</a>)</cite> compared to other baselines. Control-Condition-to-Audio (CCTA) is a baseline variant of MC-Diffusion that uses only control conditions without textual input, while Tango + LControl is an AudioComposer variant built on Tango with language-based temporal control. In terms of efficiency, we measure the real-time factor (RTF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib27\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib28\" title=\"\">c</a>)</cite> for all models on a single NVIDIA A800 GPU. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Progressive Model Training &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ControlAudio achieves competitive or superior temporal alignment compared to existing methods, while significantly improving audio generation quality in both objective and subjective metrics, and does so without introducing additional inference overhead compared to baseline models.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "audioldm",
                    "controlaudio",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "controlaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced ControlAudio, which recasts controllable TTA generation as a multi-task learning problem solved via a progressive diffusion modeling strategy. This progressive approach is applied across data construction, model training, and inference, enabling our model to incrementally master fine-grained control from text, timing, and phoneme conditions. Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance in both temporal accuracy and speech clarity. Our work&#8217;s potential for misuse in creating deceptive content or voice impersonations underscores the urgent need for robust detection methods and responsible AI governance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "controlaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This entire pipeline, from segmentation to filtered transcription, results in our final annotated dataset. From the initial pool of segments, a total of 152,070 high-quality, transcribed speech events are retained. Each event in this dataset is characterized by a precise start time, end time, and a verified textual transcription, providing an authentic and challenging data source for training our model on real-world, timed speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive objective evaluation to assess our model&#8217;s performance in two key areas: audio quality and semantic alignment with the text prompt.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A higher CLAP score indicates better semantic correspondence in the shared embedding space. All objective metrics are computed using the official AudioLDM evaluation toolkit for consistency.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "audioldm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our subjective evaluation, we recruited 20 human evaluators to rate generated audio samples on a 5-point Mean Opinion Score (MOS) scale (1-5, with higher scores being better). The evaluation was divided into two distinct tasks, each with specific criteria:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "acfiltered"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Objective and subjective evaluation results on the AudioCaps test set.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">FAD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">KL&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">FD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">IS&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">CLAP&#8593;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.525</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AudioGen</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioLDM</td>\n<td class=\"ltx_td ltx_align_center\">4.96</td>\n<td class=\"ltx_td ltx_align_center\">2.17</td>\n<td class=\"ltx_td ltx_align_center\">29.29</td>\n<td class=\"ltx_td ltx_align_center\">8.13</td>\n<td class=\"ltx_td ltx_align_center\">0.373</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioLDM 2</td>\n<td class=\"ltx_td ltx_align_center\">2.12</td>\n<td class=\"ltx_td ltx_align_center\">1.54</td>\n<td class=\"ltx_td ltx_align_center\">33.18</td>\n<td class=\"ltx_td ltx_align_center\">8.29</td>\n<td class=\"ltx_td ltx_align_center\">0.281</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Tango</td>\n<td class=\"ltx_td ltx_align_center\">1.73</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.27</span></td>\n<td class=\"ltx_td ltx_align_center\">24.42</td>\n<td class=\"ltx_td ltx_align_center\">7.70</td>\n<td class=\"ltx_td ltx_align_center\">0.315</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Tango 2</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\">20.66</td>\n<td class=\"ltx_td ltx_align_center\">9.09</td>\n<td class=\"ltx_td ltx_align_center\">0.375</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stable Audio&#160;*</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.52</span></td>\n<td class=\"ltx_td ltx_align_center\">1.51</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">18.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">13.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.538</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AudioComposer-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AudioComposer-L</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\">1.39</td>\n<td class=\"ltx_td ltx_align_center\">19.25</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceLDM-S</td>\n<td class=\"ltx_td ltx_align_center\">13.83</td>\n<td class=\"ltx_td ltx_align_center\">3.36</td>\n<td class=\"ltx_td ltx_align_center\">63.42</td>\n<td class=\"ltx_td ltx_align_center\">4.56</td>\n<td class=\"ltx_td ltx_align_center\">0.217</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceLDM-M</td>\n<td class=\"ltx_td ltx_align_center\">9.70</td>\n<td class=\"ltx_td ltx_align_center\">2.81</td>\n<td class=\"ltx_td ltx_align_center\">55.80</td>\n<td class=\"ltx_td ltx_align_center\">4.60</td>\n<td class=\"ltx_td ltx_align_center\">0.272</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceDiT</td>\n<td class=\"ltx_td ltx_align_center\">3.55</td>\n<td class=\"ltx_td ltx_align_center\">1.87</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.450</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">ControlAudio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">14.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">14.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.535</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "audiocaps",
            "evaluation",
            "clap↑",
            "audiogen",
            "voiceldmm",
            "tango",
            "audio",
            "voiceldms",
            "objective",
            "test",
            "truth",
            "fd↓",
            "fad↓",
            "audiocomposerl",
            "is↑",
            "results",
            "stable",
            "set",
            "audioldm",
            "kl↓",
            "voicedit",
            "audiocomposers",
            "controlaudio",
            "method",
            "subjective",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "subjective",
                    "controlaudio",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With target distributions predefined by datasets constructed above, we introduce progressive diffusion training, fulfilling high-quality TTA synthesis at pre-training and gradually integrating fine-grained control signals at continual learning stages.\nAt the first stage, we pre-train a diffusion transformer (DiT) in the latent space directly compressed from the waveform space, solely conditioned on text indication, achieving high-fidelity TTA at scale.\nAt the second stage, we fine-tune the latent DiT on both text and timing conditions, enabling the model to precisely control the timing windows of each sound event.\nIn controllable TTA generation, a common issue is the sacrifice of text-conditioned synthesis quality without fine-grained conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>.\nHence, in ControlAudio, we switch the condition between the text condition and the <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math> condition at the second stage, avoiding catastrophic forgetting in progressive training.\nAt the final stage, given the audio generation prior learned in prior stages, we continually train the diffusion model by switching the condition among text, <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math>, and <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math>, achieving high-fidelity audio synthesis conditioned on flexible indication.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance on controllable audio generation tasks, significantly outperforming existing methods in both objective and subjective evaluations of temporal accuracy and speech clarity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "subjective",
                    "objective",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "voicedit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion-based&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peebles and Xie (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib32\" title=\"\">2023</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib23\" title=\"\">2024</a>)</cite> TTA models are typically trained to learn a conditional reverse of a data-to-noise forward process&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib12\" title=\"\">2020</a>)</cite>, progressively removing noise from an initial random state conditioned on a text prompt over multiple diffusion steps. This framework consists of three main modules: 1) an audio varational autoencoder (VAE), responsible for transforming the audio sample into a compressed latent representation while ensuring the reconstruction quality; 2) a pretrained text encoder, which encodes a text prompt into conditioning embeddings; and 3) a latent diffusion model, which predicts the denoised audio latents conditioned on the text embeddings.\nIn ControlAudio, we employ a DiT-based architecture to ensure scalability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>, conditioned on the text, timing, and phoneme embeddings to generate the latent audio representation directly compressed from the waveform, without cascaded decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "controlaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train our model for multi-condition audio generation, we adopt a progressive three-stage training strategy.\nThis approach allows the model to acquire fine-grained control capability incrementally, where each new stage builds upon and refines the skills learned previously, ensuring a stable and efficient learning process.\nAt each stage, the model is optimized using the conditional diffusion objective&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib12\" title=\"\">2020</a>)</cite>, where a network is trained to predict the noise <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> added on the clean audio latents:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "objective",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audiocaps",
                    "evaluation",
                    "results",
                    "test",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We conduct a comprehensive evaluation covering three key aspects: temporal control, audio quality, and speech intelligibility. For temporal control, we follow prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> and report two metrics computed by a sound event detection (SED) system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mesaros et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib30\" title=\"\">2016</a>)</cite>: the event-based measures (Eb) and the clip-level macro F1 score (At). For audio quality, we employ a suite of standard metrics, including Fr&#233;chet Audio Distance (FAD), Kullback&#8211;Leibler (KL) divergence, Fr&#233;chet Distance (FD), Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite> and CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite>. For speech intelligibility, we conduct both objective and subjective tests. Objectively, we measure the Word Error Rate (WER) by transcribing generated speech with the Whisper <span class=\"ltx_text ltx_font_italic\">Large-v3</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib33\" title=\"\">2023</a>)</cite>. Subjectively, we conduct Mean Opinion Score (MOS) tests where 20 participants rate three aspects on a five-point scale: Speech Intelligibility, Overall Quality (OVL), and Relevance to the prompt (REL). Further details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3\" title=\"Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "objective",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> We compare ControlAudio with several state-of-the-art TTA models, including AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>, AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib26\" title=\"\">2024a</a>)</cite>, Tango&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>)</cite>, and our in-house implementation of Stable Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>. We also include models that incorporate explicit temporal conditioning signals, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>, as well as the training-free baselines TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>. TG-Diff reports both timing and audio quality metrics under a training-free framework but relies on a different sound event detection model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Turpault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib37\" title=\"\">2019</a>)</cite> compared to other baselines. Control-Condition-to-Audio (CCTA) is a baseline variant of MC-Diffusion that uses only control conditions without textual input, while Tango + LControl is an AudioComposer variant built on Tango with language-based temporal control. In terms of efficiency, we measure the real-time factor (RTF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib27\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib28\" title=\"\">c</a>)</cite> for all models on a single NVIDIA A800 GPU. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Progressive Model Training &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ControlAudio achieves competitive or superior temporal alignment compared to existing methods, while significantly improving audio generation quality in both objective and subjective metrics, and does so without introducing additional inference overhead compared to baseline models.</p>\n\n",
                "matched_terms": [
                    "audioldm",
                    "tango",
                    "audio",
                    "objective",
                    "controlaudio",
                    "subjective",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n",
                "matched_terms": [
                    "audioldm",
                    "voiceldmm",
                    "audio",
                    "voiceldms",
                    "voicedit",
                    "controlaudio",
                    "results",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Prompt Design.</span> To isolate and evaluate the effectiveness of our structured prompt design, we conduct a targeted ablation study. For this analysis, we compare a baseline model trained with conventional natural language descriptions against our model trained with structured prompts. Crucially, both models are trained only up to Stage 2 of our progressive curriculum, the phase dedicated specifically to learning timing control. This controlled setting allows us to fairly assess the impact of the prompt format itself. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model trained with structured prompts consistently achieves superior temporal alignment and overall audio quality on the AudioCondition test set. The results suggest that the structured format provides a clearer, unambiguous mapping between events and their time spans, an advantage that becomes particularly pronounced in complex scenes where verbose natural language descriptions can degrade timing accuracy.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "controlaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T6\" title=\"Table 6 &#8227; A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a comprehensive summary of all corpora used for pretraining our TTA backbone. To learn a robust mapping between text and audio, we aggregate a diverse mixture of large-scale, publicly available datasets. This includes datasets with descriptive captions, such as the large-scale WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and the widely-used AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>, as well as corpora with high-level event labels, like the massive AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gemmeke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib7\" title=\"\">2017</a>)</cite>. This rich combination of data sources, spanning both detailed descriptions and a wide vocabulary of sound classes, allows the model to learn robust and versatile semantic representations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive objective evaluation to assess our model&#8217;s performance in two key areas: audio quality and semantic alignment with the text prompt.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A higher CLAP score indicates better semantic correspondence in the shared embedding space. All objective metrics are computed using the official AudioLDM evaluation toolkit for consistency.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "audioldm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our subjective evaluation, we recruited 20 human evaluators to rate generated audio samples on a 5-point Mean Opinion Score (MOS) scale (1-5, with higher scores being better). The evaluation was divided into two distinct tasks, each with specific criteria:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event and Timing Planning.</span> Given the input caption, the LLM first identifies a set of distinct audio events <math alttext=\"\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}</annotation></semantics></math>. For each event, it infers a corresponding set of timing spans <math alttext=\"\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}</annotation></semantics></math>, where <math alttext=\"s_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">s_{ik}</annotation></semantics></math> and <math alttext=\"t_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ik}</annotation></semantics></math> are the start and end times in seconds. This multi-span representation is designed to handle events that occur multiple times.</p>\n\n",
                "matched_terms": [
                    "set",
                    "audio"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Comparison of prompt formats on the AudioCondition test set. We compare Natural Language (NL) with our Structured Prompt (SP).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Format</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Eb&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">At&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">FAD&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">KL&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">CLAP&#8593;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">NL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.245</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SP</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">51.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.81</span></td>\n<td class=\"ltx_td ltx_align_center\">3.61</td>\n<td class=\"ltx_td ltx_align_center\">2.05</td>\n<td class=\"ltx_td ltx_align_center\">0.293</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">NL full</td>\n<td class=\"ltx_td ltx_align_center\">40.79</td>\n<td class=\"ltx_td ltx_align_center\">61.06</td>\n<td class=\"ltx_td ltx_align_center\">1.03</td>\n<td class=\"ltx_td ltx_align_center\">1.36</td>\n<td class=\"ltx_td ltx_align_center\">0.376</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">SP full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.419</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "format",
            "full",
            "compare",
            "audiocondition",
            "formats",
            "natural",
            "language",
            "structured",
            "fad↓",
            "prompt",
            "kl↓",
            "eb↑",
            "at↑",
            "test",
            "clap↑",
            "comparison",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Prompt Design.</span> To isolate and evaluate the effectiveness of our structured prompt design, we conduct a targeted ablation study. For this analysis, we compare a baseline model trained with conventional natural language descriptions against our model trained with structured prompts. Crucially, both models are trained only up to Stage 2 of our progressive curriculum, the phase dedicated specifically to learning timing control. This controlled setting allows us to fairly assess the impact of the prompt format itself. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model trained with structured prompts consistently achieves superior temporal alignment and overall audio quality on the AudioCondition test set. The results suggest that the structured format provides a clearer, unambiguous mapping between events and their time spans, an advantage that becomes particularly pronounced in complex scenes where verbose natural language descriptions can degrade timing accuracy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation systems aim at synthesizing high-fidelity audio samples that are consistent with the given natural language description,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping\"&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>); Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib16\" title=\"\">2023</a>); Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>)</cite>.\nRecent efforts are exploring more fine-grained control for TTA systems, which can be categorized into two main classifications.\nThe first group adds precise timing control,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 2-5 seconds\", with innovations spanning conditioning techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite> and training-free latent manipulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>.\nThe second group works on intelligible audio generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, and a man is saying: &#8217;it&#8217;s a very sunny day&#8217;\", by introducing additional modules to encode both audio and speech semantic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>); Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nHowever, as expensive to collect large-scale text-audio datasets with precise timing and speech information, their controllable generation performance at scale remains limited, and none of the prior work explores&#160;<span class=\"ltx_text ltx_font_italic\">timing-controlled and intelligible TTA generation</span>,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 0-5 seconds, and then a man is saying: &#8217;it&#8217;s a very sunny day&#8217;, at 7-10 seconds\", within a unified framework.</p>\n\n",
                "matched_terms": [
                    "language",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "language",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Text and Timing Representation.</span>\nThe foundation of our approach is the Structured Prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>), a novel representation we design to explicitly and unambiguously define the composition of an acoustic scene. The prompt employs a standardized format using special tokens to delimit event descriptions and their precise start-and-end times, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Dataset Construction &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We propose this format to overcome the critical limitations of using free-form natural language for control. Natural language is often ambiguous; for instance, a prompt like \"<span class=\"ltx_text ltx_font_italic\">an alarm sounds from low to high from 1 second to 9 seconds</span>\" creates confusion, as a model must disentangle whether \"<span class=\"ltx_text ltx_font_italic\">from&#8230;to</span>\" refers to a change in pitch or a temporal boundary. Moreover, natural language descriptions become verbose and difficult to parse as scene complexity increases. In contrast, our structured format provides a concise, scalable, and machine-readable representation, forming a robust foundation for generating complex, temporally-aligned audio.</p>\n\n",
                "matched_terms": [
                    "format",
                    "natural",
                    "structured",
                    "language",
                    "prompt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Phoneme Representation.</span>\nOur approach to synthesizing intelligible speech is built directly upon the temporal foundation provided by the structured prompt. A key insight of our work is that the explicit timing windows (&lt;start,end&gt;) assigned to each speech event inherently define the utterance&#8217;s total duration. This is a significant advantage over standard TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib1\" title=\"\">2024</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib21\" title=\"\">2024a</a>)</cite>, which must employ complex, often error-prone models just to predict the duration of each phoneme or word. By having the duration as a given constraint, our framework can bypass this challenging duration modeling task entirely.</p>\n\n",
                "matched_terms": [
                    "our",
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This simplification makes it natural and highly efficient to use the same, single text encoder to progressively model both the coarse-grained temporal structure and the fine-grained speech content. We therefore represent the speech content at the phoneme level (e.g., \"<span class=\"ltx_text ltx_font_italic\">hello</span>\" <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [HH, AH0, L, OW1]). Phonemes provide a more direct, pronunciation-aware signal than words, reducing ambiguity and improving the acoustic consistency of the generated speech. By augmenting our single encoder&#8217;s vocabulary with these phoneme tokens, it learns to render the precise phonetic sequence within the specified temporal boundaries, naturally inheriting the ability to handle speech duration.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">z_{t}</annotation></semantics></math> is the noisy latent at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"\\epsilon_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}</annotation></semantics></math> is the denoising DiT, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the condition signal, and <math alttext=\"\\tau_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math> is the text encoder. The core of our progressive strategy lies in how the conditioning signal <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is structured and utilized across the training stages.</p>\n\n",
                "matched_terms": [
                    "our",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Timing-Controlled TTA Fine-tuning.</span> The pre-trained model is then fine-tuned on our dataset of precisely timing-annotated audio, while preserving the training on text condition without timing. This stage specifically optimizes the model to interpret the structured prompt containing both text and timing information, achieving&#160;<span class=\"ltx_text ltx_font_italic\">text-guided and timing-controlled audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "full",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To optimize our capability to handle both timing and more fine-grained phonetic content, we propose a Progressively Guided Sampling strategy. This approach divides the reverse diffusion process into two phases based on a threshold timestep <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, modulating the conditioning prompt and guidance scale accordingly. Specifically, in the initial sampling phase (<math alttext=\"t\\in[1.0,t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1.0</mn><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in[1.0,t_{1}]</annotation></semantics></math>), we guide the model with a simplified version of our structured prompt that excludes phonetic content <math alttext=\"c_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">c_{1}</annotation></semantics></math>, using a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>). This encourages the model to first establish a plausible temporal structure for all audio events:</p>\n\n",
                "matched_terms": [
                    "our",
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remainder of the sampling process (<math alttext=\"t\\in(t_{1},0.0]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mn>0.0</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in(t_{1},0.0]</annotation></semantics></math>), we switch to the complete, phoneme-inclusive structured prompt <math alttext=\"c_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">c_{2}</annotation></semantics></math> and a higher guidance scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our",
                    "audiocondition",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> We compare ControlAudio with several state-of-the-art TTA models, including AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>, AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib26\" title=\"\">2024a</a>)</cite>, Tango&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>)</cite>, and our in-house implementation of Stable Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>. We also include models that incorporate explicit temporal conditioning signals, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>, as well as the training-free baselines TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>. TG-Diff reports both timing and audio quality metrics under a training-free framework but relies on a different sound event detection model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Turpault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib37\" title=\"\">2019</a>)</cite> compared to other baselines. Control-Condition-to-Audio (CCTA) is a baseline variant of MC-Diffusion that uses only control conditions without textual input, while Tango + LControl is an AudioComposer variant built on Tango with language-based temporal control. In terms of efficiency, we measure the real-time factor (RTF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib27\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib28\" title=\"\">c</a>)</cite> for all models on a single NVIDIA A800 GPU. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Progressive Model Training &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ControlAudio achieves competitive or superior temporal alignment compared to existing methods, while significantly improving audio generation quality in both objective and subjective metrics, and does so without introducing additional inference overhead compared to baseline models.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n",
                "matched_terms": [
                    "set",
                    "natural",
                    "structured",
                    "language",
                    "prompt",
                    "test",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "compare",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Sampling Strategy.</span> We conduct an analysis to validate our progressive sampling strategy. This coarse-to-fine approach first uses a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>) with a simplified, content-free prompt to establish the temporal structure. It then transitions to a high scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>) with the full, phoneme-inclusive prompt to render intelligible speech. For a total of <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T=100</annotation></semantics></math> sampling steps, this transition occurs at timestep <math alttext=\"t_{1}=88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mn>88</mn></mrow><annotation encoding=\"application/x-tex\">t_{1}=88</annotation></semantics></math>. As visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our analysis of varying <math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math> and <math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math> reveals a clear trade-off: a low initial scale is crucial for overall audio quality, while a high subsequent scale is essential for speech intelligibility. This study empirically identifies the optimal configuration as (<math alttext=\"w_{low}=3,w_{high}=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><mo>=</mo><mn>9</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{low}=3,w_{high}=9</annotation></semantics></math>), confirming the effectiveness of our approach.</p>\n\n",
                "matched_terms": [
                    "full",
                    "prompt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio samples from these sources undergo a standardized preprocessing pipeline. First, all audio is resampled to 16kHz and converted to a mono-channel format. To accommodate the fixed-size input requirement of our diffusion model, all clips are processed into a uniform 10-second duration. Samples shorter than 10 seconds are right-padded with silence, while for samples longer than 10 seconds, a random 10-second segment is cropped.</p>\n\n",
                "matched_terms": [
                    "format",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the timing-control fine-tuning stage, our dataset is constructed based on AudioSet-Strong&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains 1.8M audio clips. This dataset is crucial as it provides dense, frame-level timestamps for 456 sound event classes. However, since AudioSet-Strong only provides categorical labels (e.g., \"Dog\"), not descriptive text, we generate richer captions for each timed event. Inspired by the methodology of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite>, we employ a large language model (LLM) to create a unique textual description for each segmented audio event.</p>\n\n",
                "matched_terms": [
                    "language",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription with Large Language Model.</span> Each of the 173,831 clean, segmented speech events is then sent for transcription. We input the audio segment into the Gemini 2.5 Pro model with a direct prompt to generate a precise textual transcription. To ensure the quality of the final annotations, we explicitly instruct the model to return an empty output if the spoken content in an audio segment is unintelligible or heavily obscured by noise. This step serves as a crucial quality filter.</p>\n\n",
                "matched_terms": [
                    "language",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Utterance-per-Clip Distribution:</span> The empirical distribution is characterized by a prominent peak at a single utterance per clip (<math alttext=\"n=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n=1</annotation></semantics></math>), which accounts for 32.20% of all single-speaker scenarios. For <math alttext=\"n&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n&gt;1</annotation></semantics></math>, the frequency of clips generally decreases as the number of utterances increases, exhibiting a long tail. We sample from this distribution to determine the number of utterances in our simulated monologues, with the maximum number of utterances per clip capped at 8 to focus on the most prevalent scenarios. The full distribution is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T7\" title=\"Table 7 &#8227; A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "full",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the architecture of the base model used for pretraining, before it is fine-tuned into ControlAudio. Our diffusion model is built upon the DiT (Diffusion Transformer) architecture within a latent diffusion modeling (LDM) paradigm. For pretraining, the model is conditioned on three input types: a natural language prompt (<span class=\"ltx_text ltx_font_typewriter\">prompt</span>), the start time (<span class=\"ltx_text ltx_font_typewriter\">seconds_start</span>), and the total duration (<span class=\"ltx_text ltx_font_typewriter\">seconds_total</span>). All conditions are embedded into a 768-dimensional feature space. The prompt is encoded using a pretrained Flan-T5 large model, while <span class=\"ltx_text ltx_font_typewriter\">seconds_start</span> and <span class=\"ltx_text ltx_font_typewriter\">seconds_total</span> are treated as numerical inputs.</p>\n\n",
                "matched_terms": [
                    "language",
                    "prompt",
                    "natural",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this final stage, we initialize the model from the Stage 2 checkpoint and unfreeze the Flan-T5 text encoder, enabling joint optimization with the diffusion backbone. The optimization configurations are retained from the previous stages. This joint training is crucial as it allows the text encoder to adapt its representations to the composite nature of our prompt, which includes the structured format, special tokens for timing, and the extended phoneme-level vocabulary for speech. As a result, the model learns a unified representation that maps diverse inputs, such as semantic descriptions, precise temporal spans, and intelligible speech content, to a single, high-quality, timing-controlled audio output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "format",
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive objective evaluation to assess our model&#8217;s performance in two key areas: audio quality and semantic alignment with the text prompt.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality.</span> Our primary metric for audio fidelity is the Fr&#233;chet Audio Distance (FAD), which measures the distributional difference between generated and reference audio based on VGGish embeddings. To evaluate the consistency of acoustic event distributions, we also report Kullback-Leibler (KL) divergence computed using the PANNs tagging model. For completeness and comparison with prior works, we include the Inception Score (IS) and Fr&#233;chet Distance (FD) as supplementary metrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances have demonstrated the powerful planning and cross-modal reasoning capabilities of large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib40\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib39\" title=\"\">2024</a>)</cite>. We leverage these capabilities by employing an LLM to function as a \"planner\" that automatically converts a free-form natural language caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) into a precise, structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>) for our generative model. This conversion follows a three-stage reasoning process inspired by the Chain-of-Thought (CoT) paradigm, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3.F4\" title=\"Figure 4 &#8227; C.1 Objective Metrics &#8227; Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The process consists of the following steps:</p>\n\n",
                "matched_terms": [
                    "natural",
                    "structured",
                    "language",
                    "prompt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Recaption.</span> Finally, the LLM serializes the extracted information into the final structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>). This process starts with the original caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) and appends a specially formatted string for each planned event, which includes its name, associated time spans, and any inferred speech content.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n",
                "matched_terms": [
                    "our",
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To generate textual transcriptions for our segmented speech events, we utilize the Gemini 2.5 Pro model. Each clean audio segment is provided as direct input. We designed a prompt that serves a dual function: it instructs the model to accurately transcribe the spoken content while simultaneously acting as a quality filter. Specifically, the prompt directs the model to return an empty string if the speech in an audio segment is unintelligible or heavily obscured by noise, thereby automatically discarding low-quality samples. This process ensures that only clear, valid audio segments are converted into high-quality audio-text pairs. The full prompt used for this task is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A5.F5\" title=\"Figure 5 &#8227; Appendix E Speech Transcription via ALM &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "full",
                    "prompt",
                    "our"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Comparison of vocabulary extension strategies for intelligible speech synthesis on LibriTTS-R and LibriSpeech test sets.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_tt\" rowspan=\"2\">Token Type</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">LibriTTS-R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">LibriSpeech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">UT-M&#8593;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER&#8595;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">UT-M&#8593;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VoiceLDM-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceLDM-M</td>\n<td class=\"ltx_td ltx_align_center\">4.98</td>\n<td class=\"ltx_td ltx_align_center\">2.83</td>\n<td class=\"ltx_td ltx_align_center\">9.76</td>\n<td class=\"ltx_td ltx_align_center\">2.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Word</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BPE</td>\n<td class=\"ltx_td ltx_align_center\">7.53</td>\n<td class=\"ltx_td ltx_align_center\">4.15</td>\n<td class=\"ltx_td ltx_align_center\">5.04</td>\n<td class=\"ltx_td ltx_align_center\">4.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Phoneme</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.22</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "word",
            "librispeech",
            "comparison",
            "voiceldmm",
            "synthesis",
            "strategies",
            "token",
            "voiceldms",
            "test",
            "truth",
            "phoneme",
            "wer↓",
            "sets",
            "bpe",
            "speech",
            "vocabulary",
            "librittsr",
            "intelligible",
            "utm↑",
            "extension",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "intelligible",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation systems aim at synthesizing high-fidelity audio samples that are consistent with the given natural language description,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping\"&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>); Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib16\" title=\"\">2023</a>); Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>)</cite>.\nRecent efforts are exploring more fine-grained control for TTA systems, which can be categorized into two main classifications.\nThe first group adds precise timing control,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 2-5 seconds\", with innovations spanning conditioning techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite> and training-free latent manipulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>.\nThe second group works on intelligible audio generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, and a man is saying: &#8217;it&#8217;s a very sunny day&#8217;\", by introducing additional modules to encode both audio and speech semantic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>); Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nHowever, as expensive to collect large-scale text-audio datasets with precise timing and speech information, their controllable generation performance at scale remains limited, and none of the prior work explores&#160;<span class=\"ltx_text ltx_font_italic\">timing-controlled and intelligible TTA generation</span>,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 0-5 seconds, and then a man is saying: &#8217;it&#8217;s a very sunny day&#8217;, at 7-10 seconds\", within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "vocabulary",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "intelligible",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "strategies",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent cross-modal generation tasks, such as video or avatar generation conditioned on diverse control signals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib24\" title=\"\">2025</a>); Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib15\" title=\"\">2025</a>)</cite>, progressive modeling has proven effective in handling multi-condition video generation.\nHowever, its advantages have not been extended to controllable TTA generation, where precise timing control and intelligible speech represent critical requirements but remain unresolved.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, current TTA generation quality has been advanced with latent diffusion models, while the quality of controllable generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech control, is still limited.\nAlthough diverse innovations have been proposed, their synthesis quality at scale is still compromised by data scarcity.\nMoreover, previous research rarely achieves versatile TTA generation, namely, integrating additional fine-grained control signals while preserving high-fidelity audio generation solely conditioned on text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Scarcity.</span>\nFor TTA generation, we can collect various publicly available datasets, which comprise millions of weakly-labeled text-audio pairs&#160;(Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS1\" title=\"A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>), supporting high-quality synthesis at scale.\nHowever, these datasets typically contain only high-level textual descriptions, lacking the fine-grained annotations required for controllable synthesis. Specifically, training timing-controlled and intelligible TTA generation requires datasets that combine speech with general audio events under precise timing annotations. Yet, such datasets are rare: existing timing-annotated audio datasets are limited in scale and lack transcriptions for speech segments, while publicly available speech datasets do not have reliable temporal labels. To overcome this limitation, we first construct a multi-source dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Simulated Data.</span> To further expand our dataset, we construct a large-scale simulated dataset guided by real-world data distribution. Specifically, we first analyze the AudioSet-SL dataset to derive statistical priors on speech activity patterns, with further details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS4\" title=\"A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>. These distributions guide our synthesis process, which proportionally simulates two main scenarios: single-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">monologue</span>) are created by combining multiple utterances from the same speaker in LibriTTS-R, while multi-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">dialogue</span>) are formed by sampling from different speakers. After composing the speech samples, we simulate a plausible temporal arrangement for the utterances. Finally, the composed speech is mixed with non-speech backgrounds from WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite> at a signal-to-noise ratio sampled from a uniform 2 to 10&#160;dB range&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>. Through this simulation pipeline, we generate an additional 171,246 complex audio scenes, significantly expanding the scale and diversity of our training data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "librittsr",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Phoneme Representation.</span>\nOur approach to synthesizing intelligible speech is built directly upon the temporal foundation provided by the structured prompt. A key insight of our work is that the explicit timing windows (&lt;start,end&gt;) assigned to each speech event inherently define the utterance&#8217;s total duration. This is a significant advantage over standard TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib1\" title=\"\">2024</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib21\" title=\"\">2024a</a>)</cite>, which must employ complex, often error-prone models just to predict the duration of each phoneme or word. By having the duration as a given constraint, our framework can bypass this challenging duration modeling task entirely.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "intelligible",
                    "speech",
                    "word"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This simplification makes it natural and highly efficient to use the same, single text encoder to progressively model both the coarse-grained temporal structure and the fine-grained speech content. We therefore represent the speech content at the phoneme level (e.g., \"<span class=\"ltx_text ltx_font_italic\">hello</span>\" <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [HH, AH0, L, OW1]). Phonemes provide a more direct, pronunciation-aware signal than words, reducing ambiguity and improving the acoustic consistency of the generated speech. By augmenting our single encoder&#8217;s vocabulary with these phoneme tokens, it learns to render the precise phonetic sequence within the specified temporal boundaries, naturally inheriting the ability to handle speech duration.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "vocabulary",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our progressive model training incrementally acquires finer-grained capabilities while building upon the foundational skills from previous stages. Notably, we find that the joint optimization at Stage 3 not only unlocks speech intelligibility but also further enhances the model&#8217;s previously learned temporal precision. We attribute these significant improvements to two key factors. The first is the introduction of time-annotated speech data, which provides a richer, more targeted signal for learning the alignment between linguistic content and temporal boundaries. The second is the fine-tuning of the text encoder, which allows it to be jointly optimized with the diffusion backbone; this synergistic training enables both the conditioning (text encoder) and generation (DiT) components to co-adapt to the complex, multi-objective task. This effective co-adaptation is achieved within a simple yet effective framework, where a single text encoder is responsible for processing all conditioning signals: text, timing, and phoneme features.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This second phase strictly enforces adherence to the phonetic sequence, ensuring the synthesis of highly intelligible speech within the established structure. This coarse-to-fine strategy improves temporal accuracy and speech clarity by decoupling event placement and content rendering.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "intelligible",
                    "librispeech",
                    "test",
                    "speech",
                    "librittsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We conduct a comprehensive evaluation covering three key aspects: temporal control, audio quality, and speech intelligibility. For temporal control, we follow prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> and report two metrics computed by a sound event detection (SED) system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mesaros et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib30\" title=\"\">2016</a>)</cite>: the event-based measures (Eb) and the clip-level macro F1 score (At). For audio quality, we employ a suite of standard metrics, including Fr&#233;chet Audio Distance (FAD), Kullback&#8211;Leibler (KL) divergence, Fr&#233;chet Distance (FD), Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite> and CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite>. For speech intelligibility, we conduct both objective and subjective tests. Objectively, we measure the Word Error Rate (WER) by transcribing generated speech with the Whisper <span class=\"ltx_text ltx_font_italic\">Large-v3</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib33\" title=\"\">2023</a>)</cite>. Subjectively, we conduct Mean Opinion Score (MOS) tests where 20 participants rate three aspects on a five-point scale: Speech Intelligibility, Overall Quality (OVL), and Relevance to the prompt (REL). Further details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3\" title=\"Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "word"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n",
                "matched_terms": [
                    "intelligible",
                    "voiceldmm",
                    "voiceldms",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n",
                "matched_terms": [
                    "intelligible",
                    "extension",
                    "test",
                    "speech",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Sampling Strategy.</span> We conduct an analysis to validate our progressive sampling strategy. This coarse-to-fine approach first uses a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>) with a simplified, content-free prompt to establish the temporal structure. It then transitions to a high scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>) with the full, phoneme-inclusive prompt to render intelligible speech. For a total of <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T=100</annotation></semantics></math> sampling steps, this transition occurs at timestep <math alttext=\"t_{1}=88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mn>88</mn></mrow><annotation encoding=\"application/x-tex\">t_{1}=88</annotation></semantics></math>. As visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our analysis of varying <math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math> and <math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math> reveals a clear trade-off: a low initial scale is crucial for overall audio quality, while a high subsequent scale is essential for speech intelligibility. This study empirically identifies the optimal configuration as (<math alttext=\"w_{low}=3,w_{high}=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><mo>=</mo><mn>9</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{low}=3,w_{high}=9</annotation></semantics></math>), confirming the effectiveness of our approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced ControlAudio, which recasts controllable TTA generation as a multi-task learning problem solved via a progressive diffusion modeling strategy. This progressive approach is applied across data construction, model training, and inference, enabling our model to incrementally master fine-grained control from text, timing, and phoneme conditions. Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance in both temporal accuracy and speech clarity. Our work&#8217;s potential for misuse in creating deceptive content or voice impersonations underscores the urgent need for robust detection methods and responsible AI governance.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-Quality Speech Track Extraction.</span> To obtain high-quality and reliable speech stems, we employ a dual-demixing comparison strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite>. This strategy involves comparing the separation outputs from MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite> to filter for quality and extract a clean speech signal for the subsequent processing steps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guided Synthesis Pipeline.</span> The synthesis process for each 10-second clip is as follows. First, we determine the scenario type by sampling from the speaker distribution (a 79.1% chance of a single-speaker monologue). Next, we source clean speech utterances with transcripts from the LibriTTS-R dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib20\" title=\"\">2023</a>)</cite>. For a monologue, we sample a number of utterances (determined by the utterance-per-clip distribution, and capped at a maximum of 8) from a single speaker. For a dialogue, we sample utterances from 2 to 4 different speakers, ensuring that no single speaker contributes more than 4 utterances. We then simulate a plausible temporal arrangement for these utterances within the 10-second window. Finally, the composed speech-only track is mixed with a non-speech background audio clip randomly selected from a filtered subset of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite>. The mixing is performed at a signal-to-noise ratio (SNR) randomly sampled from a uniform distribution between 2 and 10&#160;dB.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "synthesis",
                    "type",
                    "librittsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this final stage, we initialize the model from the Stage 2 checkpoint and unfreeze the Flan-T5 text encoder, enabling joint optimization with the diffusion backbone. The optimization configurations are retained from the previous stages. This joint training is crucial as it allows the text encoder to adapt its representations to the composite nature of our prompt, which includes the structured format, special tokens for timing, and the extended phoneme-level vocabulary for speech. As a result, the model learns a unified representation that maps diverse inputs, such as semantic descriptions, precise temporal spans, and intelligible speech content, to a single, high-quality, timing-controlled audio output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vocabulary",
                    "intelligible"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> In this task, participants were presented with an audio clip containing speech and the text it was intended to convey. They rated the audio based on the following three aspects:</p>\n\n",
                "matched_terms": [
                    "intelligible",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Intelligibility (Intelligible):</span> This measures the clarity of the spoken content. The question asked was: \"How clear and understandable is the spoken content in the audio?\"</p>\n\n",
                "matched_terms": [
                    "intelligible",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Content Planning.</span> For any event identified as speech (<math alttext=\"e_{i}\\in\\mathcal{E}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mtext>speech</mtext></msub></mrow><annotation encoding=\"application/x-tex\">e_{i}\\in\\mathcal{E}_{\\text{speech}}</annotation></semantics></math>), the LLM then infers a plausible utterance <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">c_{i}</annotation></semantics></math> that fits the overall context. This step enriches the planned events with specific, intelligible speech content, resulting in a set of intermediate tuples <math alttext=\"(e_{i},\\mathcal{T}_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e_{i},\\mathcal{T}_{i},c_{i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligible"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Details about audio-text datasets we use.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">Hours</span>(h)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Number</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AudioCaps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">109</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WavCaps</td>\n<td class=\"ltx_td ltx_align_center\">7090</td>\n<td class=\"ltx_td ltx_align_center\">400K</td>\n<td class=\"ltx_td ltx_align_center\">caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Clotho v2</td>\n<td class=\"ltx_td ltx_align_center\">152</td>\n<td class=\"ltx_td ltx_align_center\">7k</td>\n<td class=\"ltx_td ltx_align_center\">caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AudioSet</td>\n<td class=\"ltx_td ltx_align_center\">5800</td>\n<td class=\"ltx_td ltx_align_center\">2M</td>\n<td class=\"ltx_td ltx_align_center\">label</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">FSD50k</td>\n<td class=\"ltx_td ltx_align_center\">108</td>\n<td class=\"ltx_td ltx_align_center\">51K</td>\n<td class=\"ltx_td ltx_align_center\">label</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ESC-50</td>\n<td class=\"ltx_td ltx_align_center\">2.8</td>\n<td class=\"ltx_td ltx_align_center\">2K</td>\n<td class=\"ltx_td ltx_align_center\">label</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VGG-Sound</td>\n<td class=\"ltx_td ltx_align_center\">550</td>\n<td class=\"ltx_td ltx_align_center\">210k</td>\n<td class=\"ltx_td ltx_align_center\">label</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MTT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MSD</td>\n<td class=\"ltx_td ltx_align_center\">7333</td>\n<td class=\"ltx_td ltx_align_center\">880K</td>\n<td class=\"ltx_td ltx_align_center\">caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">FMA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">900</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">11K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">caption</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "audiocaps",
            "11k",
            "label",
            "mtt",
            "210k",
            "details",
            "msd",
            "fma",
            "caption",
            "fsd50k",
            "vggsound",
            "about",
            "text",
            "hoursh",
            "880k",
            "clotho",
            "51k",
            "audioset",
            "number",
            "datasets",
            "audiotext",
            "400k",
            "wavcaps",
            "esc50",
            "use",
            "44k",
            "24k",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T6\" title=\"Table 6 &#8227; A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a comprehensive summary of all corpora used for pretraining our TTA backbone. To learn a robust mapping between text and audio, we aggregate a diverse mixture of large-scale, publicly available datasets. This includes datasets with descriptive captions, such as the large-scale WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and the widely-used AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>, as well as corpora with high-level event labels, like the massive AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gemmeke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib7\" title=\"\">2017</a>)</cite>. This rich combination of data sources, spanning both detailed descriptions and a wide vocabulary of sound classes, allows the model to learn robust and versatile semantic representations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With target distributions predefined by datasets constructed above, we introduce progressive diffusion training, fulfilling high-quality TTA synthesis at pre-training and gradually integrating fine-grained control signals at continual learning stages.\nAt the first stage, we pre-train a diffusion transformer (DiT) in the latent space directly compressed from the waveform space, solely conditioned on text indication, achieving high-fidelity TTA at scale.\nAt the second stage, we fine-tune the latent DiT on both text and timing conditions, enabling the model to precisely control the timing windows of each sound event.\nIn controllable TTA generation, a common issue is the sacrifice of text-conditioned synthesis quality without fine-grained conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>.\nHence, in ControlAudio, we switch the condition between the text condition and the <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math> condition at the second stage, avoiding catastrophic forgetting in progressive training.\nAt the final stage, given the audio generation prior learned in prior stages, we continually train the diffusion model by switching the condition among text, <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math>, and <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math>, achieving high-fidelity audio synthesis conditioned on flexible indication.</p>\n\n",
                "matched_terms": [
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Scarcity.</span>\nFor TTA generation, we can collect various publicly available datasets, which comprise millions of weakly-labeled text-audio pairs&#160;(Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS1\" title=\"A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>), supporting high-quality synthesis at scale.\nHowever, these datasets typically contain only high-level textual descriptions, lacking the fine-grained annotations required for controllable synthesis. Specifically, training timing-controlled and intelligible TTA generation requires datasets that combine speech with general audio events under precise timing annotations. Yet, such datasets are rare: existing timing-annotated audio datasets are limited in scale and lack transcriptions for speech segments, while publicly available speech datasets do not have reliable temporal labels. To overcome this limitation, we first construct a multi-source dataset.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotated Data.</span> Our data annotation pipeline begins with the AudioSet-SL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite> dataset, chosen for its reliable temporal annotations while lacking corresponding speech transcripts. To create the ControlAudio dataset, we first select all clips containing \"human speech\" and then extract a clean speech track from each using a dual-demixing strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite> that leverages both MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite>. The clean track is subsequently segmented into individual events using the original timestamps.\nFinally, each segmented event is transcribed using Gemini 2.5 Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/models/gemini/pro/\" title=\"\">https://deepmind.google/models/gemini/pro/</a></span></span></span>. Further details of this entire pipeline are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS3\" title=\"A.3 Annotated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.\nThis transcription process enriches the dataset&#160;<span class=\"ltx_text ltx_font_italic\">i.e.</span>, expanding condition to <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math> for fine-grained control. For example, a generic annotation like (man speaking, &lt;3.00,5.00&gt;) is transformed into a specific, content-rich event&#160;(man speaking: \"It&#8217;s been raining all day.\", &lt;3.00,5.00&gt;).</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Simulated Data.</span> To further expand our dataset, we construct a large-scale simulated dataset guided by real-world data distribution. Specifically, we first analyze the AudioSet-SL dataset to derive statistical priors on speech activity patterns, with further details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS4\" title=\"A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>. These distributions guide our synthesis process, which proportionally simulates two main scenarios: single-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">monologue</span>) are created by combining multiple utterances from the same speaker in LibriTTS-R, while multi-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">dialogue</span>) are formed by sampling from different speakers. After composing the speech samples, we simulate a plausible temporal arrangement for the utterances. Finally, the composed speech is mixed with non-speech backgrounds from WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite> at a signal-to-noise ratio sampled from a uniform 2 to 10&#160;dB range&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>. Through this simulation pipeline, we generate an additional 171,246 complex audio scenes, significantly expanding the scale and diversity of our training data.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "details",
                    "vggsound",
                    "wavcaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This simplification makes it natural and highly efficient to use the same, single text encoder to progressively model both the coarse-grained temporal structure and the fine-grained speech content. We therefore represent the speech content at the phoneme level (e.g., \"<span class=\"ltx_text ltx_font_italic\">hello</span>\" <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [HH, AH0, L, OW1]). Phonemes provide a more direct, pronunciation-aware signal than words, reducing ambiguity and improving the acoustic consistency of the generated speech. By augmenting our single encoder&#8217;s vocabulary with these phoneme tokens, it learns to render the precise phonetic sequence within the specified temporal boundaries, naturally inheriting the ability to handle speech duration.</p>\n\n",
                "matched_terms": [
                    "use",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Timing-Controlled TTA Fine-tuning.</span> The pre-trained model is then fine-tuned on our dataset of precisely timing-annotated audio, while preserving the training on text condition without timing. This stage specifically optimizes the model to interpret the structured prompt containing both text and timing information, achieving&#160;<span class=\"ltx_text ltx_font_italic\">text-guided and timing-controlled audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "use",
                    "datasets",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "caption",
                    "details",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the timing-control fine-tuning stage, our dataset is constructed based on AudioSet-Strong&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains 1.8M audio clips. This dataset is crucial as it provides dense, frame-level timestamps for 456 sound event classes. However, since AudioSet-Strong only provides categorical labels (e.g., \"Dog\"), not descriptive text, we generate richer captions for each timed event. Inspired by the methodology of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite>, we employ a large language model (LLM) to create a unique textual description for each segmented audio event.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text",
                    "wavcaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guided Synthesis Pipeline.</span> The synthesis process for each 10-second clip is as follows. First, we determine the scenario type by sampling from the speaker distribution (a 79.1% chance of a single-speaker monologue). Next, we source clean speech utterances with transcripts from the LibriTTS-R dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib20\" title=\"\">2023</a>)</cite>. For a monologue, we sample a number of utterances (determined by the utterance-per-clip distribution, and capped at a maximum of 8) from a single speaker. For a dialogue, we sample utterances from 2 to 4 different speakers, ensuring that no single speaker contributes more than 4 utterances. We then simulate a plausible temporal arrangement for these utterances within the 10-second window. Finally, the composed speech-only track is mixed with a non-speech background audio clip randomly selected from a filtered subset of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite>. The mixing is performed at a signal-to-noise ratio (SNR) randomly sampled from a uniform distribution between 2 and 10&#160;dB.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "number",
                    "vggsound",
                    "wavcaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "use",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Alignment.</span> To measure the alignment between the generated audio and its corresponding text prompt, we use the LAION-CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite> score. This score is defined as the cosine similarity between the CLAP embeddings of the generated audio <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the text prompt <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "use",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "about",
                    "text"
                ]
            }
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Empirical distribution of the number of utterances per 10-second single-speaker clip, analyzed from AudioSet-SL. This distribution guides the synthesis of our simulated monologue data.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Events (<math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Number</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Percentage (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12,723</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">6,462</td>\n<td class=\"ltx_td ltx_align_center\">16.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">6,284</td>\n<td class=\"ltx_td ltx_align_center\">15.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_center\">5,720</td>\n<td class=\"ltx_td ltx_align_center\">14.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">5</td>\n<td class=\"ltx_td ltx_align_center\">4,201</td>\n<td class=\"ltx_td ltx_align_center\">10.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_align_center\">2,328</td>\n<td class=\"ltx_td ltx_align_center\">5.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">7</td>\n<td class=\"ltx_td ltx_align_center\">1,047</td>\n<td class=\"ltx_td ltx_align_center\">2.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">456</td>\n<td class=\"ltx_td ltx_align_center\">1.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">150</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">67</td>\n<td class=\"ltx_td ltx_align_center\">0.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">11</td>\n<td class=\"ltx_td ltx_align_center\">41</td>\n<td class=\"ltx_td ltx_align_center\">0.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"n&gt;12\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">n&gt;12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">39,509</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">100.00</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "events",
            "monologue",
            "our",
            "clip",
            "audiosetsl",
            "guides",
            "synthesis",
            "from",
            "percentage",
            "distribution",
            "analyzed",
            "10second",
            "simulated",
            "number",
            "singlespeaker",
            "total",
            "empirical",
            "data",
            "utterances",
            "n12n12"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Utterance-per-Clip Distribution:</span> The empirical distribution is characterized by a prominent peak at a single utterance per clip (<math alttext=\"n=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n=1</annotation></semantics></math>), which accounts for 32.20% of all single-speaker scenarios. For <math alttext=\"n&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n&gt;1</annotation></semantics></math>, the frequency of clips generally decreases as the number of utterances increases, exhibiting a long tail. We sample from this distribution to determine the number of utterances in our simulated monologues, with the maximum number of utterances per clip capped at 8 to focus on the most prevalent scenarios. The full distribution is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T7\" title=\"Table 7 &#8227; A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "data",
                    "distribution",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With target distributions predefined by datasets constructed above, we introduce progressive diffusion training, fulfilling high-quality TTA synthesis at pre-training and gradually integrating fine-grained control signals at continual learning stages.\nAt the first stage, we pre-train a diffusion transformer (DiT) in the latent space directly compressed from the waveform space, solely conditioned on text indication, achieving high-fidelity TTA at scale.\nAt the second stage, we fine-tune the latent DiT on both text and timing conditions, enabling the model to precisely control the timing windows of each sound event.\nIn controllable TTA generation, a common issue is the sacrifice of text-conditioned synthesis quality without fine-grained conditions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>.\nHence, in ControlAudio, we switch the condition between the text condition and the <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math> condition at the second stage, avoiding catastrophic forgetting in progressive training.\nAt the final stage, given the audio generation prior learned in prior stages, we continually train the diffusion model by switching the condition among text, <math alttext=\"\\langle\\text{text, timing}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing}\\rangle</annotation></semantics></math>, and <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math>, achieving high-fidelity audio synthesis conditioned on flexible indication.</p>\n\n",
                "matched_terms": [
                    "from",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "guides",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "singlespeaker",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, current TTA generation quality has been advanced with latent diffusion models, while the quality of controllable generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech control, is still limited.\nAlthough diverse innovations have been proposed, their synthesis quality at scale is still compromised by data scarcity.\nMoreover, previous research rarely achieves versatile TTA generation, namely, integrating additional fine-grained control signals while preserving high-fidelity audio generation solely conditioned on text.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a progressive diffusion modeling design covering data construction and representation, model training, and guided sampling to tackle these difficulties, achieving text-guided, timing-indicated, and intelligible audio generation with a single diffusion model.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates our overall progressive strategy.</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Scarcity.</span>\nFor TTA generation, we can collect various publicly available datasets, which comprise millions of weakly-labeled text-audio pairs&#160;(Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS1\" title=\"A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>), supporting high-quality synthesis at scale.\nHowever, these datasets typically contain only high-level textual descriptions, lacking the fine-grained annotations required for controllable synthesis. Specifically, training timing-controlled and intelligible TTA generation requires datasets that combine speech with general audio events under precise timing annotations. Yet, such datasets are rare: existing timing-annotated audio datasets are limited in scale and lack transcriptions for speech segments, while publicly available speech datasets do not have reliable temporal labels. To overcome this limitation, we first construct a multi-source dataset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthesis",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotated Data.</span> Our data annotation pipeline begins with the AudioSet-SL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite> dataset, chosen for its reliable temporal annotations while lacking corresponding speech transcripts. To create the ControlAudio dataset, we first select all clips containing \"human speech\" and then extract a clean speech track from each using a dual-demixing strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite> that leverages both MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite>. The clean track is subsequently segmented into individual events using the original timestamps.\nFinally, each segmented event is transcribed using Gemini 2.5 Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/models/gemini/pro/\" title=\"\">https://deepmind.google/models/gemini/pro/</a></span></span></span>. Further details of this entire pipeline are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS3\" title=\"A.3 Annotated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.\nThis transcription process enriches the dataset&#160;<span class=\"ltx_text ltx_font_italic\">i.e.</span>, expanding condition to <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math> for fine-grained control. For example, a generic annotation like (man speaking, &lt;3.00,5.00&gt;) is transformed into a specific, content-rich event&#160;(man speaking: \"It&#8217;s been raining all day.\", &lt;3.00,5.00&gt;).</p>\n\n",
                "matched_terms": [
                    "our",
                    "events",
                    "data",
                    "from",
                    "audiosetsl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Simulated Data.</span> To further expand our dataset, we construct a large-scale simulated dataset guided by real-world data distribution. Specifically, we first analyze the AudioSet-SL dataset to derive statistical priors on speech activity patterns, with further details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS4\" title=\"A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>. These distributions guide our synthesis process, which proportionally simulates two main scenarios: single-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">monologue</span>) are created by combining multiple utterances from the same speaker in LibriTTS-R, while multi-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">dialogue</span>) are formed by sampling from different speakers. After composing the speech samples, we simulate a plausible temporal arrangement for the utterances. Finally, the composed speech is mixed with non-speech backgrounds from WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite> at a signal-to-noise ratio sampled from a uniform 2 to 10&#160;dB range&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>. Through this simulation pipeline, we generate an additional 171,246 complex audio scenes, significantly expanding the scale and diversity of our training data.</p>\n\n",
                "matched_terms": [
                    "our",
                    "synthesis",
                    "singlespeaker",
                    "monologue",
                    "data",
                    "from",
                    "simulated",
                    "utterances",
                    "distribution",
                    "audiosetsl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Text and Timing Representation.</span>\nThe foundation of our approach is the Structured Prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>), a novel representation we design to explicitly and unambiguously define the composition of an acoustic scene. The prompt employs a standardized format using special tokens to delimit event descriptions and their precise start-and-end times, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Dataset Construction &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We propose this format to overcome the critical limitations of using free-form natural language for control. Natural language is often ambiguous; for instance, a prompt like \"<span class=\"ltx_text ltx_font_italic\">an alarm sounds from low to high from 1 second to 9 seconds</span>\" creates confusion, as a model must disentangle whether \"<span class=\"ltx_text ltx_font_italic\">from&#8230;to</span>\" refers to a change in pitch or a temporal boundary. Moreover, natural language descriptions become verbose and difficult to parse as scene complexity increases. In contrast, our structured format provides a concise, scalable, and machine-readable representation, forming a robust foundation for generating complex, temporally-aligned audio.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Phoneme Representation.</span>\nOur approach to synthesizing intelligible speech is built directly upon the temporal foundation provided by the structured prompt. A key insight of our work is that the explicit timing windows (&lt;start,end&gt;) assigned to each speech event inherently define the utterance&#8217;s total duration. This is a significant advantage over standard TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib1\" title=\"\">2024</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib21\" title=\"\">2024a</a>)</cite>, which must employ complex, often error-prone models just to predict the duration of each phoneme or word. By having the duration as a given constraint, our framework can bypass this challenging duration modeling task entirely.</p>\n\n",
                "matched_terms": [
                    "total",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "simulated",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our progressive model training incrementally acquires finer-grained capabilities while building upon the foundational skills from previous stages. Notably, we find that the joint optimization at Stage 3 not only unlocks speech intelligibility but also further enhances the model&#8217;s previously learned temporal precision. We attribute these significant improvements to two key factors. The first is the introduction of time-annotated speech data, which provides a richer, more targeted signal for learning the alignment between linguistic content and temporal boundaries. The second is the fine-tuning of the text encoder, which allows it to be jointly optimized with the diffusion backbone; this synergistic training enables both the conditioning (text encoder) and generation (DiT) components to co-adapt to the complex, multi-objective task. This effective co-adaptation is achieved within a simple yet effective framework, where a single text encoder is responsible for processing all conditioning signals: text, timing, and phoneme features.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To optimize our capability to handle both timing and more fine-grained phonetic content, we propose a Progressively Guided Sampling strategy. This approach divides the reverse diffusion process into two phases based on a threshold timestep <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, modulating the conditioning prompt and guidance scale accordingly. Specifically, in the initial sampling phase (<math alttext=\"t\\in[1.0,t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1.0</mn><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in[1.0,t_{1}]</annotation></semantics></math>), we guide the model with a simplified version of our structured prompt that excludes phonetic content <math alttext=\"c_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">c_{1}</annotation></semantics></math>, using a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>). This encourages the model to first establish a plausible temporal structure for all audio events:</p>\n\n",
                "matched_terms": [
                    "events",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Prompt Design.</span> To isolate and evaluate the effectiveness of our structured prompt design, we conduct a targeted ablation study. For this analysis, we compare a baseline model trained with conventional natural language descriptions against our model trained with structured prompts. Crucially, both models are trained only up to Stage 2 of our progressive curriculum, the phase dedicated specifically to learning timing control. This controlled setting allows us to fairly assess the impact of the prompt format itself. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model trained with structured prompts consistently achieves superior temporal alignment and overall audio quality on the AudioCondition test set. The results suggest that the structured format provides a clearer, unambiguous mapping between events and their time spans, an advantage that becomes particularly pronounced in complex scenes where verbose natural language descriptions can degrade timing accuracy.</p>\n\n",
                "matched_terms": [
                    "events",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Sampling Strategy.</span> We conduct an analysis to validate our progressive sampling strategy. This coarse-to-fine approach first uses a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>) with a simplified, content-free prompt to establish the temporal structure. It then transitions to a high scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>) with the full, phoneme-inclusive prompt to render intelligible speech. For a total of <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T=100</annotation></semantics></math> sampling steps, this transition occurs at timestep <math alttext=\"t_{1}=88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mn>88</mn></mrow><annotation encoding=\"application/x-tex\">t_{1}=88</annotation></semantics></math>. As visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our analysis of varying <math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math> and <math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math> reveals a clear trade-off: a low initial scale is crucial for overall audio quality, while a high subsequent scale is essential for speech intelligibility. This study empirically identifies the optimal configuration as (<math alttext=\"w_{low}=3,w_{high}=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><mo>=</mo><mn>9</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{low}=3,w_{high}=9</annotation></semantics></math>), confirming the effectiveness of our approach.</p>\n\n",
                "matched_terms": [
                    "total",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced ControlAudio, which recasts controllable TTA generation as a multi-task learning problem solved via a progressive diffusion modeling strategy. This progressive approach is applied across data construction, model training, and inference, enabling our model to incrementally master fine-grained control from text, timing, and phoneme conditions. Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance in both temporal accuracy and speech clarity. Our work&#8217;s potential for misuse in creating deceptive content or voice impersonations underscores the urgent need for robust detection methods and responsible AI governance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "simulated",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T6\" title=\"Table 6 &#8227; A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a comprehensive summary of all corpora used for pretraining our TTA backbone. To learn a robust mapping between text and audio, we aggregate a diverse mixture of large-scale, publicly available datasets. This includes datasets with descriptive captions, such as the large-scale WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and the widely-used AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>, as well as corpora with high-level event labels, like the massive AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gemmeke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib7\" title=\"\">2017</a>)</cite>. This rich combination of data sources, spanning both detailed descriptions and a wide vocabulary of sound classes, allows the model to learn robust and versatile semantic representations.</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio samples from these sources undergo a standardized preprocessing pipeline. First, all audio is resampled to 16kHz and converted to a mono-channel format. To accommodate the fixed-size input requirement of our diffusion model, all clips are processed into a uniform 10-second duration. Samples shorter than 10 seconds are right-padded with silence, while for samples longer than 10 seconds, a random 10-second segment is cropped.</p>\n\n",
                "matched_terms": [
                    "10second",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical difference in preprocessing this dataset is the handling of audio duration to preserve the integrity of the timestamps. Unlike in the pretraining phase, we do not apply random cropping. Instead, we consistently take the first 10 seconds of each audio clip and subsequently filter the event annotations, retaining only those whose timestamps fall within this 0-10s window. Clips shorter than 10 seconds are right-padded with silence. This deterministic process ensures that the temporal annotations in our final dataset remain perfectly aligned with the corresponding audio segments.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed, step-by-step description of the pipeline used to create our annotated dataset of real-world speech events with both precise temporal boundaries and textual transcriptions. The process is as follows:</p>\n\n",
                "matched_terms": [
                    "events",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Data Selection from AudioSet-SL.</span> We begin with the AudioSet-SL dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains strong, human-verified temporal annotations for a wide range of sound events. From the full dataset, we first identify and select all 10-second audio clips that contain at least one event labeled as \"Human speech,\" \"Speech,\" or any of their subcategories. This initial filtering yields a subset of 49,950 clips containing speech.</p>\n\n",
                "matched_terms": [
                    "10second",
                    "events",
                    "data",
                    "from",
                    "audiosetsl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event-Level Segmentation.</span> The extracted clean speech track is then segmented into individual, non-overlapping speech events. We use the original, human-annotated start and end timestamps provided by AudioSet-SL to perform this segmentation. Each resulting audio segment represents a single, continuous speech utterance from the original recording. This process yields a total of 173,831 individual speech segments, which are then prepared for transcription.</p>\n\n",
                "matched_terms": [
                    "total",
                    "from",
                    "events",
                    "audiosetsl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This entire pipeline, from segmentation to filtered transcription, results in our final annotated dataset. From the initial pool of segments, a total of 152,070 high-quality, transcribed speech events are retained. Each event in this dataset is characterized by a precise start time, end time, and a verified textual transcription, providing an authentic and challenging data source for training our model on real-world, timed speech.</p>\n\n",
                "matched_terms": [
                    "events",
                    "total",
                    "data",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the annotated real-world data, we developed a pipeline to construct a large-scale simulated dataset. The goal of this pipeline is to generate realistic, complex audio scenes with precise timing and transcription information, guided by the statistical patterns observed in a real-world dataset. The process consists of two main stages: deriving statistical priors and the guided synthesis itself.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthesis",
                    "simulated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deriving Statistical Priors from AudioSet-SL.</span> To ensure our simulated data reflects real-world patterns of speech activity, we first perform a statistical analysis on the speech-containing clips within AudioSet-SL. We identify two key distributions:</p>\n\n",
                "matched_terms": [
                    "our",
                    "data",
                    "from",
                    "simulated",
                    "audiosetsl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Distribution:</span> We find that approximately 79.1% of clips (39,509 out of 49,950) feature a single speaker, while 20.9% feature multiple speakers. This ratio guides the proportion of monologue vs. dialogue scenarios in our simulation.</p>\n\n",
                "matched_terms": [
                    "monologue",
                    "guides",
                    "distribution",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guided Synthesis Pipeline.</span> The synthesis process for each 10-second clip is as follows. First, we determine the scenario type by sampling from the speaker distribution (a 79.1% chance of a single-speaker monologue). Next, we source clean speech utterances with transcripts from the LibriTTS-R dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib20\" title=\"\">2023</a>)</cite>. For a monologue, we sample a number of utterances (determined by the utterance-per-clip distribution, and capped at a maximum of 8) from a single speaker. For a dialogue, we sample utterances from 2 to 4 different speakers, ensuring that no single speaker contributes more than 4 utterances. We then simulate a plausible temporal arrangement for these utterances within the 10-second window. Finally, the composed speech-only track is mixed with a non-speech background audio clip randomly selected from a filtered subset of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite>. The mixing is performed at a signal-to-noise ratio (SNR) randomly sampled from a uniform distribution between 2 and 10&#160;dB.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "10second",
                    "synthesis",
                    "singlespeaker",
                    "monologue",
                    "from",
                    "utterances",
                    "distribution",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the architecture of the base model used for pretraining, before it is fine-tuned into ControlAudio. Our diffusion model is built upon the DiT (Diffusion Transformer) architecture within a latent diffusion modeling (LDM) paradigm. For pretraining, the model is conditioned on three input types: a natural language prompt (<span class=\"ltx_text ltx_font_typewriter\">prompt</span>), the start time (<span class=\"ltx_text ltx_font_typewriter\">seconds_start</span>), and the total duration (<span class=\"ltx_text ltx_font_typewriter\">seconds_total</span>). All conditions are embedded into a 768-dimensional feature space. The prompt is encoded using a pretrained Flan-T5 large model, while <span class=\"ltx_text ltx_font_typewriter\">seconds_start</span> and <span class=\"ltx_text ltx_font_typewriter\">seconds_total</span> are treated as numerical inputs.</p>\n\n",
                "matched_terms": [
                    "total",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this final stage, we initialize the model from the Stage 2 checkpoint and unfreeze the Flan-T5 text encoder, enabling joint optimization with the diffusion backbone. The optimization configurations are retained from the previous stages. This joint training is crucial as it allows the text encoder to adapt its representations to the composite nature of our prompt, which includes the structured format, special tokens for timing, and the extended phoneme-level vocabulary for speech. As a result, the model learns a unified representation that maps diverse inputs, such as semantic descriptions, precise temporal spans, and intelligible speech content, to a single, high-quality, timing-controlled audio output.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n",
                "matched_terms": [
                    "utterances",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To generate textual transcriptions for our segmented speech events, we utilize the Gemini 2.5 Pro model. Each clean audio segment is provided as direct input. We designed a prompt that serves a dual function: it instructs the model to accurately transcribe the spoken content while simultaneously acting as a quality filter. Specifically, the prompt directs the model to return an empty string if the speech in an audio segment is unintelligible or heavily obscured by noise, thereby automatically discarding low-quality samples. This process ensures that only clear, valid audio segments are converted into high-quality audio-text pairs. The full prompt used for this task is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A5.F5\" title=\"Figure 5 &#8227; Appendix E Speech Transcription via ALM &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "events",
                    "our"
                ]
            }
        ]
    },
    "A4.T8": {
        "caption": "Table 8: Examples of LLM-based planning results, converting natural language inputs (caption and speech text) into structured prompts.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Input</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Generated Structured Prompt (Output)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> She is talking in the park. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"Good morning! How are you feeling today?\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">She is talking in the park. @{park ambient sounds. &amp; &lt;0.00, 10.00&gt;}@{Female speech, woman speaking. &amp; &lt;1.50, 6.00&gt; \"Good morning! How are you feeling today?\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> A child yelling as a young boy talks during several slaps on a hard surface \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"Say yeah, baby. Say yeah, baby. Are you over tired?\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">A child yelling as a young boy talks during several slaps on a hard surface. @{Young boy speaking &amp; &lt;1.50,8.00&gt; \"Say yeah, baby. Say yeah, baby. Are you over tired?\"} @{Child yelling &amp; &lt;2.00,6.00&gt;} @{slaps on a hard surface &amp; &lt;2.50,3.00&gt; &lt;5.00,5.50&gt;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> A female speaking with some rustling followed by another female speaking \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"The IT services at the King&#8217;s University College are proud to announce that we have launched\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">A female speaking with some rustling followed by another female speaking. @{Female speech, woman speaking &amp; &lt;0.50,6.00&gt; \"The IT services at the King&#8217;s University College are proud to announce that\"} @{rustling &amp; &lt;1.00,5.00&gt;} @{Female speech, woman speaking &amp; &lt;6.50,8.00&gt; \"we have launched\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> A duck quacks followed by a man talking while birds chirp in the distance \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"Mama Mama snow mama come over here, baby\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">A duck quacks followed by a man talking while birds chirp in the distance. @{duck quack &amp; &lt;0.50,1.50&gt;} @{Man speaking &amp; &lt;2.00,7.50&gt; \"Mama Mama snow mama come over here, baby\"} @{birds chirping in the distance &amp; &lt;2.50,4.00&gt; &lt;5.50,7.00&gt;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> Two men speaking with loud insects buzzing \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"I&#8217;ve got gloves covered in mid repellent. Still fishing.\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Two men speaking with loud insects buzzing. @{Man speaking &amp; &lt;1.00,4.50&gt; \"I&#8217;ve got gloves covered in mid repellent.\"} @{Man speaking &amp; &lt;5.00,6.50&gt; \"Still fishing.\"} @{loud insects buzzing &amp; &lt;0.00,10.00&gt;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> A man speaking as a stream of water splashes and flows while music faintly plays in the distance \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"in the amateur show tonight then tomorrow on Saturday the broadcasters and the other amateur cast will be going out hope to do well there get some good footage hope you enjoy\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">A man speaking as a stream of water splashes and flows while music faintly plays in the distance. @{Man speaking &amp; &lt;0.50,9.50&gt; \"in the amateur show tonight then tomorrow on Saturday the broadcasters and the other amateur cast will be going out hope to do well there get some good footage hope you enjoy\"} @{water splashing and flowing &amp; &lt;0.00,10.00&gt;} @{faint music in the distance &amp; &lt;0.00,10.00&gt;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> People are giggling, and a man speaks \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> (None)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">People are giggling, and a man speaks. @{people giggling &amp; &lt;1.00,5.00&gt;} @{Man speaking &amp; &lt;2.50,4.50&gt; \"What&#8217;s so funny?\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Caption:</span> (None) \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Text:</span> \"Some people talk about fucking the heads, but the way I do it, I just put my finger down there and pull it out.\"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">A person is giving instructions or explaining a procedure. @{Man speaking &amp; &lt;1.00,9.00&gt; \"Some people talk about fucking the heads, but the way I do it, I just put my finger down there and pull it out.\"}</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "stream",
            "say",
            "services",
            "structured",
            "prompts",
            "footage",
            "get",
            "good",
            "way",
            "followed",
            "mama",
            "several",
            "distance",
            "fucking",
            "splashes",
            "how",
            "over",
            "yelling",
            "language",
            "well",
            "saturday",
            "duck",
            "results",
            "talking",
            "planning",
            "quacks",
            "morning",
            "explaining",
            "broadcasters",
            "child",
            "cast",
            "i’ve",
            "none",
            "people",
            "woman",
            "pull",
            "other",
            "ambient",
            "hard",
            "fishing",
            "person",
            "natural",
            "you",
            "proud",
            "come",
            "still",
            "rustling",
            "college",
            "converting",
            "man",
            "speaking",
            "examples",
            "water",
            "talks",
            "put",
            "young",
            "input",
            "about",
            "text",
            "feeling",
            "faintly",
            "men",
            "giggling",
            "will",
            "some",
            "got",
            "snow",
            "insects",
            "tired",
            "launched",
            "there",
            "yeah",
            "female",
            "chirping",
            "output",
            "down",
            "king’s",
            "procedure",
            "amateur",
            "splashing",
            "funny",
            "another",
            "instructions",
            "heads",
            "announce",
            "hope",
            "speaks",
            "university",
            "sounds",
            "finger",
            "today",
            "into",
            "surface",
            "plays",
            "chirp",
            "flows",
            "speech",
            "slaps",
            "here",
            "giving",
            "prompt",
            "tomorrow",
            "faint",
            "1000female",
            "what’s",
            "while",
            "park",
            "inputs",
            "out",
            "going",
            "have",
            "covered",
            "just",
            "repellent",
            "two",
            "caption",
            "she",
            "boy",
            "talk",
            "loud",
            "buzzing",
            "flowing",
            "show",
            "gloves",
            "birds",
            "baby",
            "mid",
            "then",
            "during",
            "llmbased",
            "generated",
            "music",
            "tonight",
            "enjoy",
            "quack"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To qualitatively assess the effectiveness of our LLM-based prompt planner, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.T8\" title=\"Table 8 &#8227; D.1 Chain-of-Thought for Prompt Planning &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents several example results on samples from the AC-Filtered dataset. The table illustrates the planner&#8217;s capability to parse complex, free-form captions (with or without associated speech text) and convert them into the precise, machine-readable structured prompts that our framework requires. This planning process is particularly crucial for enabling complex, multi-speaker scenarios. For instance, the planner can generate prompts that assign different utterances to distinct speakers at specified times. This capability stands in sharp contrast to speech-oriented models like VoiceLDM, which, even when given a descriptive prompt about a conversation, can only render the entire speech content as a single utterance from one voice. This ability to plan and generate true dialogues is a key advantage of our approach for creating realistic acoustic scenes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation with fine-grained control signals, <span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at:&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://control-audio.github.io/Control-Audio/\" title=\"\">https://control-audio.github.io/Control-Audio/</a>.</p>\n\n",
                "matched_terms": [
                    "then",
                    "still",
                    "show",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) generation systems aim at synthesizing high-fidelity audio samples that are consistent with the given natural language description,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping\"&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>); Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib16\" title=\"\">2023</a>); Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>)</cite>.\nRecent efforts are exploring more fine-grained control for TTA systems, which can be categorized into two main classifications.\nThe first group adds precise timing control,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 2-5 seconds\", with innovations spanning conditioning techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite> and training-free latent manipulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>.\nThe second group works on intelligible audio generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, and a man is saying: &#8217;it&#8217;s a very sunny day&#8217;\", by introducing additional modules to encode both audio and speech semantic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>); Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nHowever, as expensive to collect large-scale text-audio datasets with precise timing and speech information, their controllable generation performance at scale remains limited, and none of the prior work explores&#160;<span class=\"ltx_text ltx_font_italic\">timing-controlled and intelligible TTA generation</span>,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, \"A bird is chirping, at 0-5 seconds, and then a man is saying: &#8217;it&#8217;s a very sunny day&#8217;, at 7-10 seconds\", within a unified framework.</p>\n\n",
                "matched_terms": [
                    "then",
                    "natural",
                    "into",
                    "chirping",
                    "language",
                    "none",
                    "man",
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose ControlAudio, a progressive diffusion modeling approach to progressively capture the target distribution conditioned on fine-grained information, <math alttext=\"(\\text{text, timing, phoneme})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{text, timing, phoneme})</annotation></semantics></math>, enabling controllable TTA generation at scale.\nOur designs cover data construction and representation, model training, as well as guided sampling, each of which progressively integrates more fine-grained condition information, thereby expanding controllability at scale.\nIn data construction, we collect large-scale <math alttext=\"\\langle\\text{text, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, audio}\\rangle</annotation></semantics></math> pairs, and then construct more expensive datasets, <math alttext=\"\\langle\\text{text, timing, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, audio}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\text{text, timing, phoneme, audio}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme, audio</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme, audio}\\rangle</annotation></semantics></math>, with both annotation and simulation methods, predefining the target distribution of each training stage.\nFor the representation of text and timing information, we develop a structural prompt, enabling a pre-trained text encoder to precisely encode them without fine-tuning.\nGiven the timing indication, namely the duration of the speech event, we naturally extend the vocabulary of the same encoder with phoneme tokens, realizing unified semantic modeling for text, timing, and phoneme features with a single text encoder.</p>\n\n",
                "matched_terms": [
                    "then",
                    "prompt",
                    "well",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In generation, diffusion models demonstrate a coarse-to-fine sampling nature. Along the entire trajectory, they generate large-scale features at the early stage and synthesize fine-grained details in the following steps, iteratively refining the generation results.\nIn controllable TTA systems, condition signals show diverse control granularity as well.\nHence, for timing-controlled and intelligible audio generation, we design progressively guided sampling, where the timing condition first guides the sampling to indicate the timing windows as large-scale features and then the phoneme condition is introduced to indicate the speech content as small-scale features.\nIn comparison with a fixed guidance signal, our method gradually emphasizes more fine-grained condition information, inherently aligned with the diffusion sampling process.</p>\n\n",
                "matched_terms": [
                    "then",
                    "well",
                    "show",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have aimed to add temporal control to TTA models, primarily through two strategies. Training-based methods, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>)</cite>, condition on predefined event classes, limiting their expressiveness for open-domain prompts. While AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> uses natural language, it struggles with ambiguity in complex event descriptions. Conversely, training-free approaches like TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite> enforce alignment during inference, but often incur high computational costs and fail in dense scenarios. A parallel challenge is the generation of intelligible speech. Most existing TTA models render speech as vague vocalizations. While models like VoiceLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> and VoiceDiT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite> can synthesize high-quality speech in context, they operate as specialized TTS systems and lack control over general audio events. Furthermore, prior work, including specialized models for controllable dialogue like CoVoMix2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib45\" title=\"\">2025</a>)</cite>, has largely focused on single-speaker or speech-only scenarios. This work is the first to address these dual challenges, proposing a unified framework for the timing-controlled, joint generation of both general audio events and intelligible multi-speaker dialogue.</p>\n\n",
                "matched_terms": [
                    "over",
                    "while",
                    "natural",
                    "during",
                    "prompts",
                    "language",
                    "have",
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent cross-modal generation tasks, such as video or avatar generation conditioned on diverse control signals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib24\" title=\"\">2025</a>); Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib15\" title=\"\">2025</a>)</cite>, progressive modeling has proven effective in handling multi-condition video generation.\nHowever, its advantages have not been extended to controllable TTA generation, where precise timing control and intelligible speech represent critical requirements but remain unresolved.</p>\n\n",
                "matched_terms": [
                    "have",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion-based&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peebles and Xie (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib32\" title=\"\">2023</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib23\" title=\"\">2024</a>)</cite> TTA models are typically trained to learn a conditional reverse of a data-to-noise forward process&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib12\" title=\"\">2020</a>)</cite>, progressively removing noise from an initial random state conditioned on a text prompt over multiple diffusion steps. This framework consists of three main modules: 1) an audio varational autoencoder (VAE), responsible for transforming the audio sample into a compressed latent representation while ensuring the reconstruction quality; 2) a pretrained text encoder, which encodes a text prompt into conditioning embeddings; and 3) a latent diffusion model, which predicts the denoised audio latents conditioned on the text embeddings.\nIn ControlAudio, we employ a DiT-based architecture to ensure scalability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>, conditioned on the text, timing, and phoneme embeddings to generate the latent audio representation directly compressed from the waveform, without cascaded decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>); Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib44\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "while",
                    "into",
                    "prompt",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Classifier-Free Guidance (CFG) <cite class=\"ltx_cite ltx_citemacro_cite\">Ho and Salimans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib13\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib38\" title=\"\">2025a</a>)</cite> emphasizes the guidance of a conditioning signal <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> during sampling. At each sampling step, CFG-guided diffusion models produce two predictions: a conditional estimation <math alttext=\"\\epsilon_{\\theta}(x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(x_{t},c)</annotation></semantics></math> and an unconditional estimation <math alttext=\"\\epsilon_{\\theta}(x_{t},\\emptyset)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}(x_{t},\\emptyset)</annotation></semantics></math>. Then the final prediction is obtained by extrapolating these two terms with a guidance scale <math alttext=\"w&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>w</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w&gt;1</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "then",
                    "during",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, current TTA generation quality has been advanced with latent diffusion models, while the quality of controllable generation,&#160;<span class=\"ltx_text ltx_font_italic\">e.g.</span>, precise timing control or intelligible speech control, is still limited.\nAlthough diverse innovations have been proposed, their synthesis quality at scale is still compromised by data scarcity.\nMoreover, previous research rarely achieves versatile TTA generation, namely, integrating additional fine-grained control signals while preserving high-fidelity audio generation solely conditioned on text.</p>\n\n",
                "matched_terms": [
                    "while",
                    "still",
                    "have",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Scarcity.</span>\nFor TTA generation, we can collect various publicly available datasets, which comprise millions of weakly-labeled text-audio pairs&#160;(Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS1\" title=\"A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>), supporting high-quality synthesis at scale.\nHowever, these datasets typically contain only high-level textual descriptions, lacking the fine-grained annotations required for controllable synthesis. Specifically, training timing-controlled and intelligible TTA generation requires datasets that combine speech with general audio events under precise timing annotations. Yet, such datasets are rare: existing timing-annotated audio datasets are limited in scale and lack transcriptions for speech segments, while publicly available speech datasets do not have reliable temporal labels. To overcome this limitation, we first construct a multi-source dataset.</p>\n\n",
                "matched_terms": [
                    "have",
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotated Data.</span> Our data annotation pipeline begins with the AudioSet-SL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite> dataset, chosen for its reliable temporal annotations while lacking corresponding speech transcripts. To create the ControlAudio dataset, we first select all clips containing \"human speech\" and then extract a clean speech track from each using a dual-demixing strategy inspired by MTV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib42\" title=\"\">2025</a>)</cite> that leverages both MVSEP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib36\" title=\"\">Solovyev </a></cite> and Spleeter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hennequin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib10\" title=\"\">2020</a>)</cite>. The clean track is subsequently segmented into individual events using the original timestamps.\nFinally, each segmented event is transcribed using Gemini 2.5 Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/models/gemini/pro/\" title=\"\">https://deepmind.google/models/gemini/pro/</a></span></span></span>. Further details of this entire pipeline are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS3\" title=\"A.3 Annotated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.\nThis transcription process enriches the dataset&#160;<span class=\"ltx_text ltx_font_italic\">i.e.</span>, expanding condition to <math alttext=\"\\langle\\text{text, timing, phoneme}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>text, timing, phoneme</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{text, timing, phoneme}\\rangle</annotation></semantics></math> for fine-grained control. For example, a generic annotation like (man speaking, &lt;3.00,5.00&gt;) is transformed into a specific, content-rich event&#160;(man speaking: \"It&#8217;s been raining all day.\", &lt;3.00,5.00&gt;).</p>\n\n",
                "matched_terms": [
                    "then",
                    "while",
                    "into",
                    "man",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Simulated Data.</span> To further expand our dataset, we construct a large-scale simulated dataset guided by real-world data distribution. Specifically, we first analyze the AudioSet-SL dataset to derive statistical priors on speech activity patterns, with further details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.SS4\" title=\"A.4 Simulated Data Pipeline &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>. These distributions guide our synthesis process, which proportionally simulates two main scenarios: single-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">monologue</span>) are created by combining multiple utterances from the same speaker in LibriTTS-R, while multi-speaker scenarios (<span class=\"ltx_text ltx_font_italic\">dialogue</span>) are formed by sampling from different speakers. After composing the speech samples, we simulate a plausible temporal arrangement for the utterances. Finally, the composed speech is mixed with non-speech backgrounds from WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite> at a signal-to-noise ratio sampled from a uniform 2 to 10&#160;dB range&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib18\" title=\"\">2025</a>)</cite>. Through this simulation pipeline, we generate an additional 171,246 complex audio scenes, significantly expanding the scale and diversity of our training data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Text and Timing Representation.</span>\nThe foundation of our approach is the Structured Prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>), a novel representation we design to explicitly and unambiguously define the composition of an acoustic scene. The prompt employs a standardized format using special tokens to delimit event descriptions and their precise start-and-end times, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Dataset Construction &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We propose this format to overcome the critical limitations of using free-form natural language for control. Natural language is often ambiguous; for instance, a prompt like \"<span class=\"ltx_text ltx_font_italic\">an alarm sounds from low to high from 1 second to 9 seconds</span>\" creates confusion, as a model must disentangle whether \"<span class=\"ltx_text ltx_font_italic\">from&#8230;to</span>\" refers to a change in pitch or a temporal boundary. Moreover, natural language descriptions become verbose and difficult to parse as scene complexity increases. In contrast, our structured format provides a concise, scalable, and machine-readable representation, forming a robust foundation for generating complex, temporally-aligned audio.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "structured",
                    "language",
                    "prompt",
                    "sounds",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prompt for Phoneme Representation.</span>\nOur approach to synthesizing intelligible speech is built directly upon the temporal foundation provided by the structured prompt. A key insight of our work is that the explicit timing windows (&lt;start,end&gt;) assigned to each speech event inherently define the utterance&#8217;s total duration. This is a significant advantage over standard TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib1\" title=\"\">2024</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib21\" title=\"\">2024a</a>)</cite>, which must employ complex, often error-prone models just to predict the duration of each phoneme or word. By having the duration as a given constraint, our framework can bypass this challenging duration modeling task entirely.</p>\n\n",
                "matched_terms": [
                    "over",
                    "structured",
                    "prompt",
                    "speech",
                    "just"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This simplification makes it natural and highly efficient to use the same, single text encoder to progressively model both the coarse-grained temporal structure and the fine-grained speech content. We therefore represent the speech content at the phoneme level (e.g., \"<span class=\"ltx_text ltx_font_italic\">hello</span>\" <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [HH, AH0, L, OW1]). Phonemes provide a more direct, pronunciation-aware signal than words, reducing ambiguity and improving the acoustic consistency of the generated speech. By augmenting our single encoder&#8217;s vocabulary with these phoneme tokens, it learns to render the precise phonetic sequence within the specified temporal boundaries, naturally inheriting the ability to handle speech duration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "natural",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">z_{t}</annotation></semantics></math> is the noisy latent at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"\\epsilon_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#1013;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\epsilon_{\\theta}</annotation></semantics></math> is the denoising DiT, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the condition signal, and <math alttext=\"\\tau_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math> is the text encoder. The core of our progressive strategy lies in how the conditioning signal <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is structured and utilized across the training stages.</p>\n\n",
                "matched_terms": [
                    "text",
                    "how",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Timing-Controlled TTA Fine-tuning.</span> The pre-trained model is then fine-tuned on our dataset of precisely timing-annotated audio, while preserving the training on text condition without timing. This stage specifically optimizes the model to interpret the structured prompt containing both text and timing information, achieving&#160;<span class=\"ltx_text ltx_font_italic\">text-guided and timing-controlled audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "then",
                    "while",
                    "structured",
                    "prompt",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Timing-Controlled and Intelligible TTA Joint Training.</span>\nAt the final stage, we unfreeze the text encoder to enable joint optimization for both timing control and speech intelligibility. The model is then trained on our full multi-source dataset, which is a comprehensive mixture of our timing-annotated real-world audio and the large-scale simulated data. This final training phase optimizes the model to jointly generate timing-controlled audio and speech samples in a coherent and realistic manner, addressing&#160;<span class=\"ltx_text ltx_font_italic\">text-guided, timing-controlled, and intelligible audio generation</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "then",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our progressive model training incrementally acquires finer-grained capabilities while building upon the foundational skills from previous stages. Notably, we find that the joint optimization at Stage 3 not only unlocks speech intelligibility but also further enhances the model&#8217;s previously learned temporal precision. We attribute these significant improvements to two key factors. The first is the introduction of time-annotated speech data, which provides a richer, more targeted signal for learning the alignment between linguistic content and temporal boundaries. The second is the fine-tuning of the text encoder, which allows it to be jointly optimized with the diffusion backbone; this synergistic training enables both the conditioning (text encoder) and generation (DiT) components to co-adapt to the complex, multi-objective task. This effective co-adaptation is achieved within a simple yet effective framework, where a single text encoder is responsible for processing all conditioning signals: text, timing, and phoneme features.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while",
                    "text",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To optimize our capability to handle both timing and more fine-grained phonetic content, we propose a Progressively Guided Sampling strategy. This approach divides the reverse diffusion process into two phases based on a threshold timestep <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, modulating the conditioning prompt and guidance scale accordingly. Specifically, in the initial sampling phase (<math alttext=\"t\\in[1.0,t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1.0</mn><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in[1.0,t_{1}]</annotation></semantics></math>), we guide the model with a simplified version of our structured prompt that excludes phonetic content <math alttext=\"c_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">c_{1}</annotation></semantics></math>, using a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>). This encourages the model to first establish a plausible temporal structure for all audio events:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "into",
                    "two",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remainder of the sampling process (<math alttext=\"t\\in(t_{1},0.0]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mn>0.0</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in(t_{1},0.0]</annotation></semantics></math>), we switch to the complete, phoneme-inclusive structured prompt <math alttext=\"c_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">c_{2}</annotation></semantics></math> and a higher guidance scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Datasets.</span> To objectively evaluate our method, we utilize several established datasets, each targeting a specific capability. For timing-controllable generation, we use the publicly available test split from AudioCondition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite>, whose fine-grained temporal annotations are ideal for this task.\nFor intelligible speech generation, we use the AC-Filtered&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib22\" title=\"\">2024b</a>)</cite> dataset.\nFor evaluating general TTA performance, we report results on the AudioCaps test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>. In addition, we include the LibriTTS-R and LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib31\" title=\"\">2015</a>)</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span> splits for specific ablation studies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "several"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We conduct a comprehensive evaluation covering three key aspects: temporal control, audio quality, and speech intelligibility. For temporal control, we follow prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite> and report two metrics computed by a sound event detection (SED) system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mesaros et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib30\" title=\"\">2016</a>)</cite>: the event-based measures (Eb) and the clip-level macro F1 score (At). For audio quality, we employ a suite of standard metrics, including Fr&#233;chet Audio Distance (FAD), Kullback&#8211;Leibler (KL) divergence, Fr&#233;chet Distance (FD), Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite> and CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite>. For speech intelligibility, we conduct both objective and subjective tests. Objectively, we measure the Word Error Rate (WER) by transcribing generated speech with the Whisper <span class=\"ltx_text ltx_font_italic\">Large-v3</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib33\" title=\"\">2023</a>)</cite>. Subjectively, we conduct Mean Opinion Score (MOS) tests where 20 participants rate three aspects on a five-point scale: Speech Intelligibility, Overall Quality (OVL), and Relevance to the prompt (REL). Further details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3\" title=\"Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "distance",
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> We compare ControlAudio with several state-of-the-art TTA models, including AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>, AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib26\" title=\"\">2024a</a>)</cite>, Tango&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ghosal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib8\" title=\"\">2023</a>)</cite>, and our in-house implementation of Stable Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib4\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite>. We also include models that incorporate explicit temporal conditioning signals, such as MC-Diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib9\" title=\"\">2024</a>)</cite> and AudioComposer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib41\" title=\"\">2025c</a>)</cite>, as well as the training-free baselines TG-Diff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib3\" title=\"\">2024</a>)</cite> and FreeAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib17\" title=\"\">2025</a>)</cite>. TG-Diff reports both timing and audio quality metrics under a training-free framework but relies on a different sound event detection model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Turpault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib37\" title=\"\">2019</a>)</cite> compared to other baselines. Control-Condition-to-Audio (CCTA) is a baseline variant of MC-Diffusion that uses only control conditions without textual input, while Tango + LControl is an AudioComposer variant built on Tango with language-based temporal control. In terms of efficiency, we measure the real-time factor (RTF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib27\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib28\" title=\"\">c</a>)</cite> for all models on a single NVIDIA A800 GPU. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Progressive Model Training &#8227; 4 ControlAudio &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ControlAudio achieves competitive or superior temporal alignment compared to existing methods, while significantly improving audio generation quality in both objective and subjective metrics, and does so without introducing additional inference overhead compared to baseline models.</p>\n\n",
                "matched_terms": [
                    "while",
                    "well",
                    "several",
                    "other",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> We further assess the ability of ControlAudio to generate intelligible speech on the AC-Filtered, comparing it with speech-oriented baselines including AudioLDM 2 Speech, VoiceLDM-S, VoiceLDM-M, and VoiceDiT. To evaluate these baselines lacking native timing support, we first use an LLM to predict a plausible time window from the caption, with further details in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A4.SS2\" title=\"D.2 Planning Results for AC-Filtered &#8227; Appendix D LLM Planning &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>. For this comparison, we use the publicly available checkpoints of VoiceLDM, while directly reporting the results presented in the original VoiceDiT paper. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Experiment Setting &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, ControlAudio achieves lower WER and superior audio quality metrics compared to all baselines. Subjective evaluations also indicate improvements in speech intelligibility, overall audio quality, and text relevance, demonstrating that ControlAudio can generate clearer and more faithful speech segments while preserving general audio fidelity.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "while",
                    "results",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Audio Generation.</span> To verify that introducing timing and speech content control does not compromise general TTA generation capabilities, we evaluate ControlAudio on the AudioCaps test set under standard natural language captions. Unlike prior controllable generation approaches that often sacrifice audio quality for control precision, ControlAudio maintains high generative performance while providing fine-grained controllability. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ControlAudio achieves competitive or superior results across multiple audio quality metrics compared to state-of-the-art baselines. These findings demonstrate that our structured prompt conditioning and vocabulary extension can be seamlessly integrated into a T2A system, enabling precise timing and intelligible speech control without degrading semantic alignment or acoustic fidelity.</p>\n\n",
                "matched_terms": [
                    "while",
                    "natural",
                    "into",
                    "structured",
                    "language",
                    "prompt",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Prompt Design.</span> To isolate and evaluate the effectiveness of our structured prompt design, we conduct a targeted ablation study. For this analysis, we compare a baseline model trained with conventional natural language descriptions against our model trained with structured prompts. Crucially, both models are trained only up to Stage 2 of our progressive curriculum, the phase dedicated specifically to learning timing control. This controlled setting allows us to fairly assess the impact of the prompt format itself. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T4\" title=\"Table 4 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model trained with structured prompts consistently achieves superior temporal alignment and overall audio quality on the AudioCondition test set. The results suggest that the structured format provides a clearer, unambiguous mapping between events and their time spans, an advantage that becomes particularly pronounced in complex scenes where verbose natural language descriptions can degrade timing accuracy.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "structured",
                    "language",
                    "prompts",
                    "prompt",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation of Vocabulary Granularity.</span> To determine the optimal vocabulary granularity for intelligible speech, we conduct an ablation study on the LibriTTS-R and LibriSpeech test-clean datasets. We first compare three variants of our model, differentiated by their vocabulary: word-level, sub-word (BPE), and phoneme-level. For broader context, we also report results from the strong VoiceLDM baselines. Evaluation metrics include WER for intelligibility and UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Saeki et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib34\" title=\"\">2022</a>)</cite>(UT-M) for speech naturalness. For WER calculation on LibriSpeech, we adopt a HuBERT-based ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib14\" title=\"\">2021</a>)</cite>, following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib35\" title=\"\">2023</a>)</cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.T5\" title=\"Table 5 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the phoneme-level model consistently and significantly outperforms the other granularities, achieving the lowest WER and highest UTMOS scores. These findings confirm that phonemes provide a more direct representation of spoken content, facilitating a tighter alignment between the prompt and the acoustic output, which ultimately translates into substantially clearer and more intelligible speech.</p>\n\n",
                "matched_terms": [
                    "into",
                    "output",
                    "prompt",
                    "other",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Sampling Strategy.</span> We conduct an analysis to validate our progressive sampling strategy. This coarse-to-fine approach first uses a low guidance scale (<math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math>) with a simplified, content-free prompt to establish the temporal structure. It then transitions to a high scale (<math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math>) with the full, phoneme-inclusive prompt to render intelligible speech. For a total of <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T=100</annotation></semantics></math> sampling steps, this transition occurs at timestep <math alttext=\"t_{1}=88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mn>88</mn></mrow><annotation encoding=\"application/x-tex\">t_{1}=88</annotation></semantics></math>. As visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Ablation Study &#8227; 5 Experiments &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our analysis of varying <math alttext=\"w_{low}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{low}</annotation></semantics></math> and <math alttext=\"w_{high}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{high}</annotation></semantics></math> reveals a clear trade-off: a low initial scale is crucial for overall audio quality, while a high subsequent scale is essential for speech intelligibility. This study empirically identifies the optimal configuration as (<math alttext=\"w_{low}=3,w_{high}=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><mo>=</mo><mn>9</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{low}=3,w_{high}=9</annotation></semantics></math>), confirming the effectiveness of our approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "then",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced ControlAudio, which recasts controllable TTA generation as a multi-task learning problem solved via a progressive diffusion modeling strategy. This progressive approach is applied across data construction, model training, and inference, enabling our model to incrementally master fine-grained control from text, timing, and phoneme conditions. Extensive experiments demonstrate that ControlAudio achieves state-of-the-art performance in both temporal accuracy and speech clarity. Our work&#8217;s potential for misuse in creating deceptive content or voice impersonations underscores the urgent need for robust detection methods and responsible AI governance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its promising results, our work has several limitations. First, while ControlAudio pioneers the generation of intelligible speech within a timing-controlled TTA framework, its control is primarily limited to the speech content. The framework currently lacks explicit mechanisms to manipulate crucial stylistic attributes such as emotion, prosody, or speaker identity. Second, a fundamental tension between generating high-quality general audio versus intelligible speech persists. Although our model unifies these tasks, we observe a potential trade-off where heavily optimizing for one modality can slightly impact the fidelity of the other in complex, co-occurring scenes. Finally, the performance of our model is inherently constrained by the availability of large-scale, richly annotated audio-speech datasets, which remain scarce. Our reliance on a combination of existing annotated data and simulated data, while effective, suggests that performance could be further enhanced with the advent of more comprehensive and higher-quality training corpora in the future.</p>\n\n",
                "matched_terms": [
                    "while",
                    "several",
                    "other",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A1.T6\" title=\"Table 6 &#8227; A.1 Pretraining Datasets and Preprocessing &#8227; Appendix A Training Datasets &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a comprehensive summary of all corpora used for pretraining our TTA backbone. To learn a robust mapping between text and audio, we aggregate a diverse mixture of large-scale, publicly available datasets. This includes datasets with descriptive captions, such as the large-scale WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and the widely-used AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib19\" title=\"\">2019</a>)</cite>, as well as corpora with high-level event labels, like the massive AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Gemmeke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib7\" title=\"\">2017</a>)</cite>. This rich combination of data sources, spanning both detailed descriptions and a wide vocabulary of sound classes, allows the model to learn robust and versatile semantic representations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "well"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio samples from these sources undergo a standardized preprocessing pipeline. First, all audio is resampled to 16kHz and converted to a mono-channel format. To accommodate the fixed-size input requirement of our diffusion model, all clips are processed into a uniform 10-second duration. Samples shorter than 10 seconds are right-padded with silence, while for samples longer than 10 seconds, a random 10-second segment is cropped.</p>\n\n",
                "matched_terms": [
                    "input",
                    "while",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the timing-control fine-tuning stage, our dataset is constructed based on AudioSet-Strong&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib11\" title=\"\">2021</a>)</cite>, which contains 1.8M audio clips. This dataset is crucial as it provides dense, frame-level timestamps for 456 sound event classes. However, since AudioSet-Strong only provides categorical labels (e.g., \"Dog\"), not descriptive text, we generate richer captions for each timed event. Inspired by the methodology of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite>, we employ a large language model (LLM) to create a unique textual description for each segmented audio event.</p>\n\n",
                "matched_terms": [
                    "language",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event-Level Segmentation.</span> The extracted clean speech track is then segmented into individual, non-overlapping speech events. We use the original, human-annotated start and end timestamps provided by AudioSet-SL to perform this segmentation. Each resulting audio segment represents a single, continuous speech utterance from the original recording. This process yields a total of 173,831 individual speech segments, which are then prepared for transcription.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "then",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription with Large Language Model.</span> Each of the 173,831 clean, segmented speech events is then sent for transcription. We input the audio segment into the Gemini 2.5 Pro model with a direct prompt to generate a precise textual transcription. To ensure the quality of the final annotations, we explicitly instruct the model to return an empty output if the spoken content in an audio segment is unintelligible or heavily obscured by noise. This step serves as a crucial quality filter.</p>\n\n",
                "matched_terms": [
                    "then",
                    "into",
                    "language",
                    "output",
                    "prompt",
                    "speech",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This entire pipeline, from segmentation to filtered transcription, results in our final annotated dataset. From the initial pool of segments, a total of 152,070 high-quality, transcribed speech events are retained. Each event in this dataset is characterized by a precise start time, end time, and a verified textual transcription, providing an authentic and challenging data source for training our model on real-world, timed speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deriving Statistical Priors from AudioSet-SL.</span> To ensure our simulated data reflects real-world patterns of speech activity, we first perform a statistical analysis on the speech-containing clips within AudioSet-SL. We identify two key distributions:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Distribution:</span> We find that approximately 79.1% of clips (39,509 out of 49,950) feature a single speaker, while 20.9% feature multiple speakers. This ratio guides the proportion of monologue vs. dialogue scenarios in our simulation.</p>\n\n",
                "matched_terms": [
                    "out",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guided Synthesis Pipeline.</span> The synthesis process for each 10-second clip is as follows. First, we determine the scenario type by sampling from the speaker distribution (a 79.1% chance of a single-speaker monologue). Next, we source clean speech utterances with transcripts from the LibriTTS-R dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib20\" title=\"\">2023</a>)</cite>. For a monologue, we sample a number of utterances (determined by the utterance-per-clip distribution, and capped at a maximum of 8) from a single speaker. For a dialogue, we sample utterances from 2 to 4 different speakers, ensuring that no single speaker contributes more than 4 utterances. We then simulate a plausible temporal arrangement for these utterances within the 10-second window. Finally, the composed speech-only track is mixed with a non-speech background audio clip randomly selected from a filtered subset of WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib29\" title=\"\">2024</a>)</cite> and VGG-Sound&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib2\" title=\"\">2020</a>)</cite>. The mixing is performed at a signal-to-noise ratio (SNR) randomly sampled from a uniform distribution between 2 and 10&#160;dB.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "then"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the architecture of the base model used for pretraining, before it is fine-tuned into ControlAudio. Our diffusion model is built upon the DiT (Diffusion Transformer) architecture within a latent diffusion modeling (LDM) paradigm. For pretraining, the model is conditioned on three input types: a natural language prompt (<span class=\"ltx_text ltx_font_typewriter\">prompt</span>), the start time (<span class=\"ltx_text ltx_font_typewriter\">seconds_start</span>), and the total duration (<span class=\"ltx_text ltx_font_typewriter\">seconds_total</span>). All conditions are embedded into a 768-dimensional feature space. The prompt is encoded using a pretrained Flan-T5 large model, while <span class=\"ltx_text ltx_font_typewriter\">seconds_start</span> and <span class=\"ltx_text ltx_font_typewriter\">seconds_total</span> are treated as numerical inputs.</p>\n\n",
                "matched_terms": [
                    "while",
                    "natural",
                    "into",
                    "inputs",
                    "language",
                    "prompt",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diffusion network backbone is a DiT with 24 layers, 24 attention heads, and a model hidden dimension of 1536&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib5\" title=\"\">2024b</a>)</cite>. The model utilizes both cross-attention for all conditional inputs and global conditioning for duration-related signals. The internal token dimension of the diffusion model is 64, with a conditional token dimension of 768 and a global condition embedding dimension of 1536.</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "heads"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our audio autoencoder is a variational autoencoder (VAE) based on the Descript Audio VAE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib6\" title=\"\">2025</a>)</cite> framework, operating at a 16kHz sampling rate. The model is trained from scratch on the audio portions of large-scale public datasets to learn a compact audio representation. The encoder is configured with a model dimension (<span class=\"ltx_text ltx_font_typewriter\">d_model</span>) of 128 and uses strides of [4, 4, 4, 10], resulting in an overall downsampling ratio of 640. The encoder maps the input waveform into a final 64-dimensional latent representation, which is then used by the decoder for reconstruction. The model&#8217;s input/output channels (<span class=\"ltx_text ltx_font_typewriter\">io_channels</span>) are set to 1 for mono audio. We use Snake activation throughout the network and omit the final tanh activation in the decoder.</p>\n\n",
                "matched_terms": [
                    "input",
                    "then",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our learning rate schedule consists of two phases. For the first 99% of training iterations, the learning rate is held constant at its initial value, <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>. For the final 1% of iterations, it then decays following the InverseLR formula:</p>\n\n",
                "matched_terms": [
                    "then",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this final stage, we initialize the model from the Stage 2 checkpoint and unfreeze the Flan-T5 text encoder, enabling joint optimization with the diffusion backbone. The optimization configurations are retained from the previous stages. This joint training is crucial as it allows the text encoder to adapt its representations to the composite nature of our prompt, which includes the structured format, special tokens for timing, and the extended phoneme-level vocabulary for speech. As a result, the model learns a unified representation that maps diverse inputs, such as semantic descriptions, precise temporal spans, and intelligible speech content, to a single, high-quality, timing-controlled audio output.</p>\n\n",
                "matched_terms": [
                    "structured",
                    "inputs",
                    "output",
                    "prompt",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive objective evaluation to assess our model&#8217;s performance in two key areas: audio quality and semantic alignment with the text prompt.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality.</span> Our primary metric for audio fidelity is the Fr&#233;chet Audio Distance (FAD), which measures the distributional difference between generated and reference audio based on VGGish embeddings. To evaluate the consistency of acoustic event distributions, we also report Kullback-Leibler (KL) divergence computed using the PANNs tagging model. For completeness and comparison with prior works, we include the Inception Score (IS) and Fr&#233;chet Distance (FD) as supplementary metrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Alignment.</span> To measure the alignment between the generated audio and its corresponding text prompt, we use the LAION-CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib43\" title=\"\">2023</a>)</cite> score. This score is defined as the cosine similarity between the CLAP embeddings of the generated audio <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the text prompt <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our subjective evaluation, we recruited 20 human evaluators to rate generated audio samples on a 5-point Mean Opinion Score (MOS) scale (1-5, with higher scores being better). The evaluation was divided into two distinct tasks, each with specific criteria:</p>\n\n",
                "matched_terms": [
                    "into",
                    "two",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timing-Controlled Audio Generation.</span> In this task, participants were presented with an audio clip and its corresponding timed prompt. They were asked to rate the audio based on the following two aspects:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporal Alignment (Temporal):</span> This measures the accuracy of timestamp adherence. The question asked was: \"How accurately does the timing of the audio events match the given start and end times in the prompt?\"</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality (OVL):</span> This assesses the perceptual quality of the audio clip itself. The question asked was: \"Ignoring the prompt, how would you rate the overall quality and realism of the audio clip?\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "prompt",
                    "you"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intelligible Audio Generation.</span> In this task, participants were presented with an audio clip containing speech and the text it was intended to convey. They rated the audio based on the following three aspects:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Intelligibility (Intelligible):</span> This measures the clarity of the spoken content. The question asked was: \"How clear and understandable is the spoken content in the audio?\"</p>\n\n",
                "matched_terms": [
                    "speech",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality (OVL):</span> This assesses the quality of the entire acoustic scene. The question asked was: \"How would you rate the overall audio quality, including both the speech and any background sounds?\"</p>\n\n",
                "matched_terms": [
                    "speech",
                    "how",
                    "sounds",
                    "you"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relevance (REL):</span> This measures the semantic correspondence between the audio and the text. The question asked was: \"How well does the generated audio, as a whole, match the text description?\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "text",
                    "well",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances have demonstrated the powerful planning and cross-modal reasoning capabilities of large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib40\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#bib.bib39\" title=\"\">2024</a>)</cite>. We leverage these capabilities by employing an LLM to function as a \"planner\" that automatically converts a free-form natural language caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) into a precise, structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>) for our generative model. This conversion follows a three-stage reasoning process inspired by the Chain-of-Thought (CoT) paradigm, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A3.F4\" title=\"Figure 4 &#8227; C.1 Objective Metrics &#8227; Appendix C Evaluation &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The process consists of the following steps:</p>\n\n",
                "matched_terms": [
                    "caption",
                    "into",
                    "natural",
                    "structured",
                    "language",
                    "prompt",
                    "have",
                    "planning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Event and Timing Planning.</span> Given the input caption, the LLM first identifies a set of distinct audio events <math alttext=\"\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}</annotation></semantics></math>. For each event, it infers a corresponding set of timing spans <math alttext=\"\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}=\\{(s_{i1},t_{i1}),\\dots\\}</annotation></semantics></math>, where <math alttext=\"s_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">s_{ik}</annotation></semantics></math> and <math alttext=\"t_{ik}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ik}</annotation></semantics></math> are the start and end times in seconds. This multi-span representation is designed to handle events that occur multiple times.</p>\n\n",
                "matched_terms": [
                    "input",
                    "planning",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Content Planning.</span> For any event identified as speech (<math alttext=\"e_{i}\\in\\mathcal{E}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mtext>speech</mtext></msub></mrow><annotation encoding=\"application/x-tex\">e_{i}\\in\\mathcal{E}_{\\text{speech}}</annotation></semantics></math>), the LLM then infers a plausible utterance <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">c_{i}</annotation></semantics></math> that fits the overall context. This step enriches the planned events with specific, intelligible speech content, resulting in a set of intermediate tuples <math alttext=\"(e_{i},\\mathcal{T}_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e_{i},\\mathcal{T}_{i},c_{i})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "planning",
                    "then"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Recaption.</span> Finally, the LLM serializes the extracted information into the final structured prompt (<math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math>). This process starts with the original caption (<math alttext=\"y_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">y_{c}</annotation></semantics></math>) and appends a specially formatted string for each planned event, which includes its name, associated time spans, and any inferred speech content.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "into",
                    "structured",
                    "prompt",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To generate textual transcriptions for our segmented speech events, we utilize the Gemini 2.5 Pro model. Each clean audio segment is provided as direct input. We designed a prompt that serves a dual function: it instructs the model to accurately transcribe the spoken content while simultaneously acting as a quality filter. Specifically, the prompt directs the model to return an empty string if the speech in an audio segment is unintelligible or heavily obscured by noise, thereby automatically discarding low-quality samples. This process ensures that only clear, valid audio segments are converted into high-quality audio-text pairs. The full prompt used for this task is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08878v1#A5.F5\" title=\"Figure 5 &#8227; Appendix E Speech Transcription via ALM &#8227; ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "into",
                    "prompt",
                    "speech",
                    "input"
                ]
            }
        ]
    }
}