{
    "S2.T1": {
        "source_file": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets",
        "caption": "Table 1: Number of people can retrieve the result at top-1 with different α\\alpha thresholds and model backbones on the person-matching task.",
        "body": "α\\alpha\n\n\n0.1\n\n\n\n\n0.2\n\n\n\n\n0.3\n\n\n\n\n0.4\n\n\n\n\n0.5\n\n\n\n\n0.6\n\n\n\n\n0.7\n\n\n\n\n0.8\n\n\n\n\n0.9\n\n\n\n\nNumber of People\n\n\n34\n\n\n\n\n35\n\n\n\n\n31\n\n\n\n\n40\n\n\n\n\n39\n\n\n\n\n38\n\n\n\n\n43\n\n\n\n\n29\n\n\n\n\n38\n\n\n\n\n\n\nCLIP Sig400M\n38\n\n\nCLIP ViT-5B\n33",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.1</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.2</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.3</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.4</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.5</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.6</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.7</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.8</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.9</span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Number of People</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">34</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">35</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">31</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">40</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">38</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">29</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">38</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">CLIP Sig400M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"9\">38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">CLIP ViT-5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"9\">33</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "people",
            "result",
            "thresholds",
            "top1",
            "sig400m",
            "number",
            "task",
            "clip",
            "vit5b",
            "αalpha",
            "different",
            "personmatching",
            "backbones",
            "model",
            "retrieve"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The value of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is fixed equal to 0.7, which is empirically chosen based on extensive internal experimentation and a small-scale user study (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12255v1#S2.T1\" title=\"Table 1 &#8227; 2.3 Textual Search &#8227; 2 Key functions upgrade &#8227; Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) conducted with 50 participants interacting with our system to find best result for a text-image retrieval question. This adaptive weighting allows <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span> to capture semantic nuances more robustly than using either model in isolation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In our previous system, text-to-image retrieval relied on a single CLIP-based model, which often required a trade-off between efficiency and accuracy. In <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span>, we enhance this module by employing two state-of-the-art CLIP variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clipbench</span>]</cite>: <span class=\"ltx_text ltx_font_italic\">CLIP-Sig400M</span> and <span class=\"ltx_text ltx_font_italic\">CLIP-ViT-5B</span>. Both models are widely recognized for their balanced performance in terms of inference speed and retrieval accuracy, making them well-suited for large-scale interactive search scenarios.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "model"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets",
        "caption": "Table 2: Summary performance per model.",
        "body": "Model\nCount Acc.\nImg. Ans. Acc.\nVideo Anss Acc.\nAvg. Time (s)\n\n\n\n\n\nInternVL-1B-Seq [intervl]\n\n0.72\n0.68\n0.67\n2.52\n\n\n\nInternVL-1B-ffn6 [intervl]\n\n0.79\n0.74\n0.71\n2.48\n\n\n\nInternVL-1B-ffn6-Seq [intervl]\n\n0.84\n0.79\n0.75\n4.80\n\n\n\nLLaVA-0.5B-ffn6 [llava]\n\n0.78\n0.63\n0.73\n10.50\n\n\n\nSmolVLM-0.5B-ffn6 [smovlm]\n\n0.83\n0.52\n0.65\n1.07",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:70%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:70%;\">Count Acc.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:70%;\">Img. Ans. Acc.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:70%;\">Video Anss Acc.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:70%;\">Avg. Time (s)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">InternVL-1B-Seq&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">intervl</span><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.52</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">InternVL-1B-ffn6&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">intervl</span><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">InternVL-1B-ffn6-Seq&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">intervl</span><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">LLaVA-0.5B-ffn6&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llava</span><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">10.50</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SmolVLM-0.5B-ffn6&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">smovlm</span><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">1.07</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "internvl1bffn6",
            "summary",
            "ans",
            "smovlm",
            "count",
            "time",
            "avg",
            "video",
            "internvl1bseq",
            "llava05bffn6",
            "performance",
            "intervl",
            "anss",
            "llava",
            "img",
            "acc",
            "internvl1bffn6seq",
            "smolvlm05bffn6",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To systematically choose the right lightweight models, we constructed a benchmark dataset of 200 video&#8211;question pairs from previous queries of VBS, categorized into three main groups: (1) Counting (<span class=\"ltx_text ltx_font_italic\">How many completed shoes in the image?</span>), (2) Image information extraction (<span class=\"ltx_text ltx_font_italic\">What is on the street?</span>), and (3) Video information extraction (<span class=\"ltx_text ltx_font_italic\">What color is the phone the woman is using?</span>). The dataset requires models to provide an image/video. We then tested several compact VLMs (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p2.m1\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>1B parameters) on this dataset, with the aim of identifying the best compromise between accuracy and inference speed. The detailed results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12255v1#S2.T2\" title=\"Table 2 &#8227; 2.5 Question Answering &#8227; 2 Key functions upgrade &#8227; Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span>, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, Optical Character Recognition (OCR) is powered by Vintern-1B-v3.5 for robust multilingual text recognition, and Automatic Speech Recognition (ASR) employs faster-whisper for real-time transcription. For question answering, lightweight vision&#8211;language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span> introduces a redesigned UI/UX with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span> as a competitive and user-friendly system for large-scale video search.</p>\n\n",
                "matched_terms": [
                    "time",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our previous system, text-to-image retrieval relied on a single CLIP-based model, which often required a trade-off between efficiency and accuracy. In <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span>, we enhance this module by employing two state-of-the-art CLIP variants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clipbench</span>]</cite>: <span class=\"ltx_text ltx_font_italic\">CLIP-Sig400M</span> and <span class=\"ltx_text ltx_font_italic\">CLIP-ViT-5B</span>. Both models are widely recognized for their balanced performance in terms of inference speed and retrieval accuracy, making them well-suited for large-scale interactive search scenarios.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_typewriter\">Fusionista2.0</span>, we replace the previous PaddleOCR pipeline with the <span class=\"ltx_text ltx_font_italic\">Vintern-1B-v3.5</span> model &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vintern</span>]</cite>. This model, fine-tuned from InternVL2.5-1B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">intervl</span>]</cite>, leverages low-resource languages datasets such as Viet-ShareGPT-4o-Text-VQA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vintern</span>]</cite> and over one million OCR samples, achieving strong performance in many languages. Vintern-1B-v3.5 not only delivers high accuracy in scene text detection and recognition but also benefits from enhanced reasoning and general knowledge, enabling it to infer characters that are blurred or occluded.</p>\n\n",
                "matched_terms": [
                    "intervl",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among all candidates, the <span class=\"ltx_text ltx_font_italic\">InternVL-1B-ffn6-Seq</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">intervl</span>]</cite> model demonstrated the most consistent performance across the three categories. It achieved an average inference time of less than 5 seconds per query over the entire dataset, while maintaining high accuracy in counting and frame-level information extraction tasks. This balance of speed and reliability makes it the most suitable choice for our system, ensuring accurate answers within the strict time limits of VBS.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "intervl",
                    "model",
                    "internvl1bffn6seq",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our reranking approach is inspired by the methodology outlined in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aaaivqa4cir</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">summary</span>]</cite>, which utilizes a post-processing strategy that can be effortlessly incorporated into existing image retrieval systems. However, we extend this concept by designing a fully integrated AI-based VQA pipeline. In this pipeline, we employ GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai2024gpt4ocard</span>]</cite> to formulate three yes-no questions based on the initial user query. These questions are crafted to explore the details of objects mentioned in the query, such as \"<span class=\"ltx_text ltx_font_italic\">Is there a dog in the scene?</span>\" or \"<span class=\"ltx_text ltx_font_italic\">Is the dog colored yellow?</span>\" Subsequently, both the questions and the associated images are evaluated by a vision language model to generate accurate responses. To enhance flexibility, we test multiple vision language models, including VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/emnlp/ZhangLB23_video_llama</span>]</cite> and BLIP-2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">10.5555/3618408.3619222</span>]</cite>, leveraging the VLLMs<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2023efficient_vllm_lib</span>]</cite> library to enable API-driven interactions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "summary"
                ]
            }
        ]
    }
}