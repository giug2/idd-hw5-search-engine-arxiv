{
    "S3.T1": {
        "source_file": "Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS",
        "caption": "Table 1: Performance comparison on Seed-TTS test sets. #Tr. Data: total hours of training data. SFT: fine-tuning stage on speaker-related, task-specific data. Bold results indicate the best performance of our models. ∗The training set of Seed-TTS is orders of magnitude larger than the previous largest TTS dataset (>>100k hours), but the exact number is not revealed.",
        "body": "System\n#Tr. Data\nSFT\ntest-zh\ntest-en\n\n\nCER (%) ↓\\downarrow\nSim-WavLM ↑\\uparrow\nSim-ERes2Net ↑\\uparrow\nWER (%) ↓\\downarrow\nSim-WavLM ↑\\uparrow\nSim-ERes2Net ↑\\uparrow\n\n\nHuman\n-\n-\n1.26\n0.755\n0.775\n2.14\n0.734\n0.742\n\n\n\nFireRedTTS [arxiv2024-guohanhan-fireredtts]\n\n150k\n✓\n1.51\n0.635\n0.653\n3.82\n0.460\n0.526\n\n\n\nCosyVoice 2 [arxiv2025-duzhihao-cosyvoice2]\n\n200k\n✓\n1.45\n0.748\n0.806\n2.57\n0.652\n0.736\n\n\n\nSeed-TTS [arxiv2024-anastassiou-seedtts]\n\n*\n✓\n1.12\n0.796\n-\n2.25\n0.762\n-\n\n\nCosyVoice 2\n100k\n✘\n1.31\n0.714\n0.761\n2.48\n0.606\n0.677\n\n\nCosyVoice 2 + TLA-SA\n100k\n✘\n1.34\n0.736\n0.775\n2.57\n0.644\n0.705",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">#Tr. Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SFT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">test-zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">test-en</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.755</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.775</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.734</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.742</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FireRedTTS</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-guohanhan-fireredtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">150k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.635</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.653</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.460</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.526</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice 2</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.748</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.806</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.652</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.736</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.796</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.762</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.714</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.761</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.606</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.677</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice 2 + TLA-SA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">100k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.736</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.775</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.644</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.705</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "previous",
            "sets",
            "training",
            "cosyvoice",
            "seedtts",
            "↓downarrow",
            "testzh",
            "hours",
            "speakerrelated",
            "finetuning",
            "magnitude",
            "our",
            "largest",
            "test",
            "∗the",
            "arxiv2025duzhihaocosyvoice2",
            "arxiv2024guohanhanfireredtts",
            "tts",
            "larger",
            "not",
            "indicate",
            "wer",
            "human",
            "results",
            "100k",
            "system",
            "stage",
            "exact",
            "simwavlm",
            "simeres2net",
            "revealed",
            "dataset",
            "than",
            "tlasa",
            "bold",
            "performance",
            "number",
            "testen",
            "set",
            "cer",
            "↑uparrow",
            "200k",
            "orders",
            "150k",
            "taskspecific",
            "models",
            "total",
            "sft",
            "best",
            "data",
            "fireredtts",
            "comparison",
            "arxiv2024anastassiouseedtts"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S3.T1\" style=\"font-size:90%;\" title=\"In 3.3 Evaluation metrics &#8227; 3 Experimental Setups &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that TLA-SA improves speaker similarity by 3.0% (WavLM) and 2.1% (ERes2Net) on average compared to our model without TLA-SA, with negligible impact on CER/WER. These results confirm that TLA-SA provides an effective yet simple approach to improving speaker modeling during pre-training, consistent with our findings in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that the model with TLA-SA shows stronger speaker modeling capability.\nFree of speaker SFT stage, our CosyVoice 2 significantly outperforms FireRedTTS, which not only incorporates 50% additional data but also a SFT stage, demonstrating that TLA-SA offers a simple yet effective method to improve speaker modeling ability.\nMoreover, the TLA-SA boosted model achieves comparable performance to the official CosyVoice 2 with 50.0%, with a gap of only 2.1% on average against the model trained with </span>\n  <math alttext=\"2\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> data and SFT.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose <span class=\"ltx_text ltx_font_italic\">Time-Layer Adaptive Speaker Alignment</span> (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "models",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Driven by the advances in large-scale datasets and model scaling, modern Text-to-Speech (TTS) systems have achieved remarkable zero-shot generation, enabling them to synthesize speech for unseen speakers without fine-tuning. These models excel at capturing speaker characteristics, such as timbre and prosody, thereby enhancing their utility in real-world scenarios.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most state-of-the-art FM-based TTS models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are based on Flow-Matching (FM) due to its strong modeling power and generalization capability.\nThe use of FM modules in mainstream TTS systems falls into two paradigms:\n(1) conditioned on upstream intermediate representations (e.g., LM-generated tokens) to learn token-to-spectrogram mappings;\n(2) conditioned only on text (with optional prompts) to directly generate spectrograms.\nIn the first type, the FM module takes input from a speech-text LM, such as&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> integrates a text-speech LM with an FM decoder, achieving high naturalness and quality with low inference latency. IndexTTS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhances speech duration control and disentangles speaker identity from emotion, enabling precise control over duration, timbre, and affect. Minimax&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a learnable speaker encoder and an auxiliary FM module, yielding strong zero-shot generalization and generation quality.\nIn the second type, the FM TTS model generates Mel spectrogram conditioned on text and speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without a LM. VoiceFlow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces a rectified FM algorithm that enhances generation efficiency. E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enables end-to-end FM-based TTS by padding blank tokens during generation. F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves E2-TTS by incorporating an additional text encoder and a tailored sampling strategy, achieving strong performance with high real-time efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "arxiv2025duzhihaocosyvoice2",
                    "models",
                    "tts",
                    "arxiv2024anastassiouseedtts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite the success of FM-based frameworks, their training objective is limited: the standard loss implicitly models the high-dimensional speech distribution without explicitly enforcing perceptual attributes such as speaker identity. This often results in insufficient refinement of fine-grained features in zero-shot scenarios, e.g., speaker similarity. Auxiliary supervision is a promising direction to explicitly guide such attributes.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iclr2025-yusihyun-repa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-tri-taro</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that external representation alignment applied to intermediate FM features can accelerate convergence and improve performance. In the context of TTS, previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aligns text and speech modalities to increase efficiency.\nHowever, two questions remain underexplored: (1) how speaker information distributes across denoising steps and hierarchical layers in FM-based TTS, and (2) how to best design and integrate speaker supervision to leverage this structure.\nWe first analyze speaker information flow in FM-based models and find that it varies significantly across layers and timesteps, motivating adaptive alignment. We thus propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which dynamically aligns latent FM representations with speaker embeddings from a pre-trained encoder, conditioned on both timestep and depth.\nOur contributions are threefold:</span>\n</p>\n\n",
                "matched_terms": [
                    "previous",
                    "training",
                    "models",
                    "tts",
                    "best",
                    "tlasa",
                    "performance",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TLA-SA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss to enhance speaker consistency for zero-shot TTS. The proposed TLA-SA aligns intermediate representation of FM with a pre-trained encoder and integrates temporal and hierarchical information of FM, and thus provides a dynamic speaker supervision to TM-based TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present an analytical study revealing that speaker information in FM-based TTS is unevenly distributed across denoising steps and model layers, highlighting the necessity of time-layer adaptive alignment and providing insight for speaker modeling in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Experimental results confirm that the proposed TLA-SA loss consistently improves speaker modeling across large-scale and small-scale dataset sizes and mainstream model architectures. We will open-source our large-scale dataset</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The dataset is currently being organized and will be open-sourced soon.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "results",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM) zero-shot TTS models reconstruct Mel spectrograms through a mask prediction strategy, as shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S1.F1\" style=\"font-size:90%;\" title=\"In 1 Introduction &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During training, the FM model learns a flow </span>\n  <math alttext=\"\\phi_{t}(\\mathbf{x}_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119857;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\phi_{t}(\\mathbf{x}_{t},c)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that transforms a noise distribution </span>\n  <math alttext=\"\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119857;</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119977;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">&#120782;</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">&#119816;</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into a data distribution </span>\n  <math alttext=\"\\mathbf{x}_{1}\\sim q(\\mathbf{x})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119857;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">q</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">&#119857;</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1}\\sim q(\\mathbf{x})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, conditioned on the masked input </span>\n  <math alttext=\"\\mathbf{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and speaker embedding </span>\n  <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">c</mi>\n      <annotation encoding=\"application/x-tex\">c</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Given a vector field parameterized by </span>\n  <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#952;</mi>\n      <annotation encoding=\"application/x-tex\">\\theta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Conditional Flow Matching (CFM) loss minimizes the </span>\n  <math alttext=\"\\ell_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\ell_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> distance along the optimal transport path:</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the distribution of speaker information in the FM-based TTS model, we adopt the training-free Centered Kernel Nearest-Neighbor Alignment (CKNNA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv-huh-platonic</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> criterion, which quantifies the similarity between two sets of embeddings.\nWe first examine whether CKNNA faithfully reflects the speaker modeling quality of TTS models.\nFor each baseline checkpoint at different training steps, we first assess speaker similarity using a pre-trained speaker encoder for </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> synthesized utterances from one of our test sets.\nSimultaneously, we extract intermediate speaker representations from each layer of the checkpoint. Specifically, given the intermediate representation </span>\n  <math alttext=\"E_{i}\\in\\mathbb{R}^{B\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E_{i}\\in\\mathbb{R}^{B\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer of the FM model with </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layers, temporal mean pooling is applied to derive the speaker representation:</span>\n</p>\n\n",
                "matched_terms": [
                    "sets",
                    "training",
                    "test",
                    "models",
                    "tts",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We extract speaker embeddings </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from corresponding </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> prompt speech in a test set with 37 distinct speakers and 500 utterances with the same encoder.\nWe utilize CKNNA score of </span>\n  <math alttext=\"E_{i}^{\\text{Speaker}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{i}^{\\text{Speaker}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E^{\\text{SA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to reflect the distribution of speaker information at </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer.\nAs shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CKNNA exhibits a strong correlation with speaker similarity performance, validating its efficacy for analyzing speaker information.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyze the distribution of speaker information in the baseline FM model using CKNNA and observe significant non-uniformity across denoising times and layers&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAlong the temporal dimension, we observe that speaker information is predominantly encoded during the initial denoising steps (t&#8594;0), where noise levels are highest. This observation is consistent with the understanding that the early stages of the diffusion prioritizes establishing global characteristics like speaker identity.\nAs for hierarchical dimension, outputs of layers differ in their capacity to represent speaker information, with the final layers being particularly weak.\nResults indicate that a fixed and uniform supervision strategy is suboptimal, as it fails to adapt to these dynamics. This motivates an adaptive supervision scheme that accounts for both timestep and layer when enhancing speaker modeling.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which facilitates both temporal and hierarchical information to boost speaker modeling in FM-based TTS.\nThe key idea is to align intermediate representations of the FM module with embeddings extracted from a pre-trained speaker encoder.\nThe speaker alignment loss at layer </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, denoted by </span>\n  <math alttext=\"\\mathcal{L}^{\\text{SA}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\text{SA}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, minimizes the discrepancy between </span>\n  <math alttext=\"E^{\\text{Speaker}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{Speaker}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as mentioned in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the speaker alignment embedding </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a pre-trained speaker encoder:</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We set </span>\n  <math alttext=\"\\alpha=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m15\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.01</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> empirically and add an entropy penalty regularization term </span>\n  <math alttext=\"\\mathcal{L}^{\\text{Reg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m16\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">Reg</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\text{Reg}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">neurips2020-zhaoshanshan-entropy_reg</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to stabilize weight learning. The overall training objective is given by:</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TLASA-100k</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: An industrial-scale training dataset comprising 100k hours of open-source audio, including 60k hours of Mandarin and 40k hours of English speech. This dataset enables a fair comparison with industrial baselines, and will be publicly released on Hugging Face</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The dataset is currently being organized and will be open-sourced soon.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset",
                    "hours",
                    "comparison",
                    "100k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A widely used zero-shot TTS evaluation set in Mandarin and English. For models trained on large-scale data, we evaluate on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> subsets, containing 2,020 and 1,088 prompt-text pairs, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "seedtts",
                    "arxiv2024anastassiouseedtts",
                    "testzh",
                    "data",
                    "testen",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">libritts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A small-scale high-quality English TTS dataset for research, containing approximately 585 hours of training data. We follow the evaluation protocol of CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UniCATS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aaai24-duchenpeng-unicats</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using 500 utterances from the test-clean split with 37 non-overlapping speakers. We also conduct CKNNA-based analysis on the LibriTTS test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "test",
                    "cosyvoice",
                    "arxiv2025duzhihaocosyvoice2",
                    "tts",
                    "dataset",
                    "hours",
                    "data",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Paradigm.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To evaluate the effectiveness and generalization of TLA-SA, we experiment with two FM-based TTS architectures. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-based FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Following CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model comprises a speech-text language model and a Transformer U-Net CFM decoder with 2 down-sampling layers, 12 middle layers, and 2 up-sampling layers. TLA-SA supervision is applied to the middle layers.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-free FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We utilize an FM-based TTS model with a transformer-based text encoder and 9 MMDiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icml2024-esser-mmdit</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> decoder layers, which is similar to&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "cosyvoice",
                    "arxiv2025duzhihaocosyvoice2",
                    "tts",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configuration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> All models are trained from scratch on utterances with 24kHz sampling rate. A cosine learning rate scheduler with a 0.25-epoch warm-up is used, and optimization is performed with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2017-loshchilov-adamw</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For TLASA-100k, models are trained for 156k steps with batch size 16 and gradient accumulation of 4. The learning rate decays from 8e-4 to 1e-6. For LibriTTS, models are trained for 125k steps with batch size 16, no gradient accumulation, and a learning rate decaying from 2e-4 to 1e-4. Unless specified, we use a WavLM-based&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">taslp2022-chensanyuan-wavlm_sv</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model fine-tuned on speaker verification task for the TLA-SA loss.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER/CER.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate Word Error Rate (WER) for English test sets and Character Error Rate (CER) for Mandarin test sets, using the speech recognition models provided by Seed-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker Similarity (SS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We measure the similarity between generated speech and the prompt speech using the cosine similarity of their embeddings. Two pre-trained models are used: a speaker-related fine-tuned WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the ERes2Net&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2025-chenyafeng-3dspeaker</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "simwavlm",
                    "sets",
                    "test",
                    "cosyvoice",
                    "arxiv2025duzhihaocosyvoice2",
                    "simeres2net",
                    "models",
                    "seedtts",
                    "wer",
                    "speakerrelated",
                    "cer",
                    "arxiv2024anastassiouseedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F4\" style=\"font-size:90%;\" title=\"In 4.1 TLA-SA on large-scale dataset &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA improves speaker similarity by 2.2% on Sim-WavLM, enhancing speaker modeling in FM-based TTS.\nFurthermore, it accelerates convergence by </span>\n  <math alttext=\"2.9\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.9</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.9\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the large-scale TLASA-100k dataset, demon strating its efficacy in guiding speaker representation learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts",
                    "dataset",
                    "simwavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on the high-quality LibriTTS dataset, which consists of only 0.6% of TLASA-100k. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying TLA-SA consistently improves speaker similarity by 2.8% and 2.5% (0.538 vs. 0.510, 0.596 vs. 0.571) on WavLM and ERes2Net, using supervision from a single pre-trained speaker encoder. These results indicate that TLA-SA enhances general speaker modeling rather than overfitting to specific evaluation models.\nMoreover, gains from layer and time adaptation underscore the importance of jointly leveraging temporal and hierarchical information for effective speaker alignment in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "dataset",
                    "than",
                    "indicate",
                    "tlasa",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the generalization capability of our approach across FM-based TTS architectures, we evaluate it on models employing a decoder-only LM (e.g., CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and those without a LM (e.g., E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Performance on different training paradigms &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed TLA-SA consistently enhances speaker modeling performance in both settings, indicating a strong cross-architecture generalization. Furthermore, the proposed TLA-SA gains an absolute improvement on the LM-free architecture (6% vs. 2.8% on WavLM and 7.1% vs. 2.5% on ERes2Net), demonstrating that TLA-SA effectively assists the speaker modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "arxiv2025duzhihaocosyvoice2",
                    "models",
                    "tts",
                    "tlasa",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we search for the optimal speaker encoder of TLA-SA using LibriTTS. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Ablation on SA encoder &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CAM++ and ERes2Net outperform the WavLM-based model while using </span>\n  <math alttext=\"42.5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">42.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">42.5\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"46.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">46.4</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">46.4\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> fewer parameters, indicating that the WavLM-based model contains less speaker information even with fine-tuning on speaker verification.\nMoreover, TLA-SA with CAM++ achieves comparable or superior performance to WavLM and ERes2Net (0.544 vs. 0.538, 0.609 vs. 0.606), highlighting that TLA-SA enhances general speaker modeling in FM-based TTS rather than merely transferring information from a specific speaker encoder.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "than",
                    "tlasa",
                    "performance",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TLA-SA, a loss that leverages temporal and hierarchical speaker alignment to enhance speaker consistency in zero-shot FM-based TTS systems. By introducing joint alignment across time steps and model layers, TLA-SA effectively strengthens speaker modeling. TLA-SA improves speaker similarity on large- and small-scale datasets, enabling models to achieve comparable or superior performance without speaker-specific fine-tuning, regardless of the training data size. Furthermore, TLA-SA yields consistent improvement on both FM TTS models with and without a cascaded LM respectively, demonstrating strong generalization across architectures. Future work will extend TLA-SA to broader FM variants and explore additional alignment signals to further enhance TTS performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "models",
                    "tts",
                    "tlasa",
                    "data",
                    "performance",
                    "finetuning"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS",
        "caption": "Table 2: Effectiveness of different aspects of speaker alignment supervision. We evaluate performance on the LibriTTS dataset.",
        "body": "Layer\nTime\nSim-WavLM ↑\\uparrow\nSim-ERes2Net ↑\\uparrow\n\n\n✘\n✘\n0.510\n0.571\n\n\n\n\n✓\n✘\n0.525\n0.582\n\n\n✓\n✓\n0.538\n0.596",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Layer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Time</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.510</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.571</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.525</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.582</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.538</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.596</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluate",
            "time",
            "supervision",
            "simwavlm",
            "simeres2net",
            "performance",
            "↑uparrow",
            "different",
            "dataset",
            "libritts",
            "aspects",
            "alignment",
            "layer",
            "speaker",
            "effectiveness"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on the high-quality LibriTTS dataset, which consists of only 0.6% of TLASA-100k. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying TLA-SA consistently improves speaker similarity by 2.8% and 2.5% (0.538 vs. 0.510, 0.596 vs. 0.571) on WavLM and ERes2Net, using supervision from a single pre-trained speaker encoder. These results indicate that TLA-SA enhances general speaker modeling rather than overfitting to specific evaluation models.\nMoreover, gains from layer and time adaptation underscore the importance of jointly leveraging temporal and hierarchical information for effective speaker alignment in FM-based TTS models.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose <span class=\"ltx_text ltx_font_italic\">Time-Layer Adaptive Speaker Alignment</span> (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "time",
                    "supervision",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nText-to-speech, flow matching, time and layer adaptation, speaker alignment</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "time",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most state-of-the-art FM-based TTS models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are based on Flow-Matching (FM) due to its strong modeling power and generalization capability.\nThe use of FM modules in mainstream TTS systems falls into two paradigms:\n(1) conditioned on upstream intermediate representations (e.g., LM-generated tokens) to learn token-to-spectrogram mappings;\n(2) conditioned only on text (with optional prompts) to directly generate spectrograms.\nIn the first type, the FM module takes input from a speech-text LM, such as&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> integrates a text-speech LM with an FM decoder, achieving high naturalness and quality with low inference latency. IndexTTS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhances speech duration control and disentangles speaker identity from emotion, enabling precise control over duration, timbre, and affect. Minimax&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a learnable speaker encoder and an auxiliary FM module, yielding strong zero-shot generalization and generation quality.\nIn the second type, the FM TTS model generates Mel spectrogram conditioned on text and speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without a LM. VoiceFlow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces a rectified FM algorithm that enhances generation efficiency. E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enables end-to-end FM-based TTS by padding blank tokens during generation. F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves E2-TTS by incorporating an additional text encoder and a tailored sampling strategy, achieving strong performance with high real-time efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite the success of FM-based frameworks, their training objective is limited: the standard loss implicitly models the high-dimensional speech distribution without explicitly enforcing perceptual attributes such as speaker identity. This often results in insufficient refinement of fine-grained features in zero-shot scenarios, e.g., speaker similarity. Auxiliary supervision is a promising direction to explicitly guide such attributes.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iclr2025-yusihyun-repa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-tri-taro</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that external representation alignment applied to intermediate FM features can accelerate convergence and improve performance. In the context of TTS, previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aligns text and speech modalities to increase efficiency.\nHowever, two questions remain underexplored: (1) how speaker information distributes across denoising steps and hierarchical layers in FM-based TTS, and (2) how to best design and integrate speaker supervision to leverage this structure.\nWe first analyze speaker information flow in FM-based models and find that it varies significantly across layers and timesteps, motivating adaptive alignment. We thus propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which dynamically aligns latent FM representations with speaker embeddings from a pre-trained encoder, conditioned on both timestep and depth.\nOur contributions are threefold:</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance",
                    "supervision",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TLA-SA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss to enhance speaker consistency for zero-shot TTS. The proposed TLA-SA aligns intermediate representation of FM with a pre-trained encoder and integrates temporal and hierarchical information of FM, and thus provides a dynamic speaker supervision to TM-based TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "supervision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present an analytical study revealing that speaker information in FM-based TTS is unevenly distributed across denoising steps and model layers, highlighting the necessity of time-layer adaptive alignment and providing insight for speaker modeling in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Experimental results confirm that the proposed TLA-SA loss consistently improves speaker modeling across large-scale and small-scale dataset sizes and mainstream model architectures. We will open-source our large-scale dataset</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The dataset is currently being organized and will be open-sourced soon.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the distribution of speaker information in the FM-based TTS model, we adopt the training-free Centered Kernel Nearest-Neighbor Alignment (CKNNA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv-huh-platonic</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> criterion, which quantifies the similarity between two sets of embeddings.\nWe first examine whether CKNNA faithfully reflects the speaker modeling quality of TTS models.\nFor each baseline checkpoint at different training steps, we first assess speaker similarity using a pre-trained speaker encoder for </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> synthesized utterances from one of our test sets.\nSimultaneously, we extract intermediate speaker representations from each layer of the checkpoint. Specifically, given the intermediate representation </span>\n  <math alttext=\"E_{i}\\in\\mathbb{R}^{B\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E_{i}\\in\\mathbb{R}^{B\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer of the FM model with </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layers, temporal mean pooling is applied to derive the speaker representation:</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "different",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We extract speaker embeddings </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from corresponding </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> prompt speech in a test set with 37 distinct speakers and 500 utterances with the same encoder.\nWe utilize CKNNA score of </span>\n  <math alttext=\"E_{i}^{\\text{Speaker}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{i}^{\\text{Speaker}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E^{\\text{SA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to reflect the distribution of speaker information at </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer.\nAs shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CKNNA exhibits a strong correlation with speaker similarity performance, validating its efficacy for analyzing speaker information.</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "performance",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyze the distribution of speaker information in the baseline FM model using CKNNA and observe significant non-uniformity across denoising times and layers&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAlong the temporal dimension, we observe that speaker information is predominantly encoded during the initial denoising steps (t&#8594;0), where noise levels are highest. This observation is consistent with the understanding that the early stages of the diffusion prioritizes establishing global characteristics like speaker identity.\nAs for hierarchical dimension, outputs of layers differ in their capacity to represent speaker information, with the final layers being particularly weak.\nResults indicate that a fixed and uniform supervision strategy is suboptimal, as it fails to adapt to these dynamics. This motivates an adaptive supervision scheme that accounts for both timestep and layer when enhancing speaker modeling.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "supervision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which facilitates both temporal and hierarchical information to boost speaker modeling in FM-based TTS.\nThe key idea is to align intermediate representations of the FM module with embeddings extracted from a pre-trained speaker encoder.\nThe speaker alignment loss at layer </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, denoted by </span>\n  <math alttext=\"\\mathcal{L}^{\\text{SA}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\text{SA}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, minimizes the discrepancy between </span>\n  <math alttext=\"E^{\\text{Speaker}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{Speaker}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as mentioned in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the speaker alignment embedding </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a pre-trained speaker encoder:</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathcal{D}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo mathsize=\"0.900em\" rspace=\"0em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}(\\cdot,\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a distance function and </span>\n  <math alttext=\"\\text{MLP}^{\\text{Layer}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mtext mathsize=\"0.900em\">MLP</mtext>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Layer</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\text{MLP}^{\\text{Layer}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is one of </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layer adapters that projects </span>\n  <math alttext=\"E^{\\text{Speaker}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{Speaker}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into a shared latent space with </span>\n  <math alttext=\"E^{\\text{SA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo integrate temporal information into the hierarchical speaker alignment process, we employ the denoising time-step to predict a list of weights </span>\n  <math alttext=\"\\mathbf{w}\\in\\mathbb{R}^{B\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119856;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{w}\\in\\mathbb{R}^{B\\times N}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which combine layer-wise SA losses across </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> blocks. At denoising time </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an embedding layer </span>\n  <math alttext=\"\\text{Emb}^{\\text{Time}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m13\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mtext mathsize=\"0.900em\">Emb</mtext>\n        <mtext mathsize=\"0.900em\">Time</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\text{Emb}^{\\text{Time}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs the time embedding, followed by the time adapter </span>\n  <math alttext=\"\\text{MLP}^{\\text{Time}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m14\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mtext mathsize=\"0.900em\">MLP</mtext>\n        <mtext mathsize=\"0.900em\">Time</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\text{MLP}^{\\text{Time}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produces these weights, through a softmax over the layer dimension:</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "time",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct experiments on the following datasets to evaluate performance under both large- and small-scale conditions:</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">libritts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A small-scale high-quality English TTS dataset for research, containing approximately 585 hours of training data. We follow the evaluation protocol of CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UniCATS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aaai24-duchenpeng-unicats</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using 500 utterances from the test-clean split with 37 non-overlapping speakers. We also conduct CKNNA-based analysis on the LibriTTS test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Paradigm.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To evaluate the effectiveness and generalization of TLA-SA, we experiment with two FM-based TTS architectures. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-based FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Following CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model comprises a speech-text language model and a Transformer U-Net CFM decoder with 2 down-sampling layers, 12 middle layers, and 2 up-sampling layers. TLA-SA supervision is applied to the middle layers.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-free FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We utilize an FM-based TTS model with a transformer-based text encoder and 9 MMDiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icml2024-esser-mmdit</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> decoder layers, which is similar to&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "supervision",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configuration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> All models are trained from scratch on utterances with 24kHz sampling rate. A cosine learning rate scheduler with a 0.25-epoch warm-up is used, and optimization is performed with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2017-loshchilov-adamw</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For TLASA-100k, models are trained for 156k steps with batch size 16 and gradient accumulation of 4. The learning rate decays from 8e-4 to 1e-6. For LibriTTS, models are trained for 125k steps with batch size 16, no gradient accumulation, and a learning rate decaying from 2e-4 to 1e-4. Unless specified, we use a WavLM-based&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">taslp2022-chensanyuan-wavlm_sv</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model fine-tuned on speaker verification task for the TLA-SA loss.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER/CER.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate Word Error Rate (WER) for English test sets and Character Error Rate (CER) for Mandarin test sets, using the speech recognition models provided by Seed-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker Similarity (SS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We measure the similarity between generated speech and the prompt speech using the cosine similarity of their embeddings. Two pre-trained models are used: a speaker-related fine-tuned WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the ERes2Net&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2025-chenyafeng-3dspeaker</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "simeres2net",
                    "speaker",
                    "evaluate",
                    "simwavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F4\" style=\"font-size:90%;\" title=\"In 4.1 TLA-SA on large-scale dataset &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA improves speaker similarity by 2.2% on Sim-WavLM, enhancing speaker modeling in FM-based TTS.\nFurthermore, it accelerates convergence by </span>\n  <math alttext=\"2.9\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.9</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.9\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the large-scale TLASA-100k dataset, demon strating its efficacy in guiding speaker representation learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "simwavlm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S3.T1\" style=\"font-size:90%;\" title=\"In 3.3 Evaluation metrics &#8227; 3 Experimental Setups &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that TLA-SA improves speaker similarity by 3.0% (WavLM) and 2.1% (ERes2Net) on average compared to our model without TLA-SA, with negligible impact on CER/WER. These results confirm that TLA-SA provides an effective yet simple approach to improving speaker modeling during pre-training, consistent with our findings in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that the model with TLA-SA shows stronger speaker modeling capability.\nFree of speaker SFT stage, our CosyVoice 2 significantly outperforms FireRedTTS, which not only incorporates 50% additional data but also a SFT stage, demonstrating that TLA-SA offers a simple yet effective method to improve speaker modeling ability.\nMoreover, the TLA-SA boosted model achieves comparable performance to the official CosyVoice 2 with 50.0%, with a gap of only 2.1% on average against the model trained with </span>\n  <math alttext=\"2\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> data and SFT.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, we visualize the adaptive weight </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> across time steps and model layers. As shown in &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F5\" style=\"font-size:90%;\" title=\"In 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA loss applies selective supervision strength along both time and layer axes, effectively incorporating temporal and hierarchical information, which is consistent with observations in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSpecifically, higher weights are assigned to shallow layers and later time steps, suggesting their greater importance in speaker modeling and the need for stronger speaker-specific supervision.</span>\n</p>\n\n",
                "matched_terms": [
                    "layer",
                    "speaker",
                    "time",
                    "supervision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the generalization capability of our approach across FM-based TTS architectures, we evaluate it on models employing a decoder-only LM (e.g., CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and those without a LM (e.g., E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Performance on different training paradigms &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed TLA-SA consistently enhances speaker modeling performance in both settings, indicating a strong cross-architecture generalization. Furthermore, the proposed TLA-SA gains an absolute improvement on the LM-free architecture (6% vs. 2.8% on WavLM and 7.1% vs. 2.5% on ERes2Net), demonstrating that TLA-SA effectively assists the speaker modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we search for the optimal speaker encoder of TLA-SA using LibriTTS. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Ablation on SA encoder &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CAM++ and ERes2Net outperform the WavLM-based model while using </span>\n  <math alttext=\"42.5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">42.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">42.5\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"46.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">46.4</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">46.4\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> fewer parameters, indicating that the WavLM-based model contains less speaker information even with fine-tuning on speaker verification.\nMoreover, TLA-SA with CAM++ achieves comparable or superior performance to WavLM and ERes2Net (0.544 vs. 0.538, 0.609 vs. 0.606), highlighting that TLA-SA enhances general speaker modeling in FM-based TTS rather than merely transferring information from a specific speaker encoder.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TLA-SA, a loss that leverages temporal and hierarchical speaker alignment to enhance speaker consistency in zero-shot FM-based TTS systems. By introducing joint alignment across time steps and model layers, TLA-SA effectively strengthens speaker modeling. TLA-SA improves speaker similarity on large- and small-scale datasets, enabling models to achieve comparable or superior performance without speaker-specific fine-tuning, regardless of the training data size. Furthermore, TLA-SA yields consistent improvement on both FM TTS models with and without a cascaded LM respectively, demonstrating strong generalization across architectures. Future work will extend TLA-SA to broader FM variants and explore additional alignment signals to further enhance TTS performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "performance",
                    "time",
                    "alignment"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS",
        "caption": "Table 3: Generalization of TLA-SA across FM-based TTS systems on LibriTTS under LM-based and LM-free training paradigms.",
        "body": "Training Paradigm\nTLA-SA\nSim-WavLM ↑\\uparrow\nSim-ERes2Net ↑\\uparrow\n\n\n\n\nLM-based\n✘\n0.510\n0.571\n\n\n✓\n0.538\n0.596\n\n\nLM-free\n✘\n0.398\n0.500\n\n\n✓\n0.458\n0.571",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Paradigm</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TLA-SA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">LM-based</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.510</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.571</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.538</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.596</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">LM-free</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10008;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.398</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.500</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.458</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.571</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "simwavlm",
            "fmbased",
            "training",
            "across",
            "simeres2net",
            "lmbased",
            "under",
            "tts",
            "generalization",
            "paradigm",
            "libritts",
            "paradigms",
            "lmfree",
            "tlasa",
            "systems",
            "↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the generalization capability of our approach across FM-based TTS architectures, we evaluate it on models employing a decoder-only LM (e.g., CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and those without a LM (e.g., E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Performance on different training paradigms &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed TLA-SA consistently enhances speaker modeling performance in both settings, indicating a strong cross-architecture generalization. Furthermore, the proposed TLA-SA gains an absolute improvement on the LM-free architecture (6% vs. 2.8% on WavLM and 7.1% vs. 2.5% on ERes2Net), demonstrating that TLA-SA effectively assists the speaker modeling.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose <span class=\"ltx_text ltx_font_italic\">Time-Layer Adaptive Speaker Alignment</span> (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</span>\n</p>\n\n",
                "matched_terms": [
                    "fmbased",
                    "across",
                    "tts",
                    "generalization",
                    "tlasa",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Driven by the advances in large-scale datasets and model scaling, modern Text-to-Speech (TTS) systems have achieved remarkable zero-shot generation, enabling them to synthesize speech for unseen speakers without fine-tuning. These models excel at capturing speaker characteristics, such as timbre and prosody, thereby enhancing their utility in real-world scenarios.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most state-of-the-art FM-based TTS models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are based on Flow-Matching (FM) due to its strong modeling power and generalization capability.\nThe use of FM modules in mainstream TTS systems falls into two paradigms:\n(1) conditioned on upstream intermediate representations (e.g., LM-generated tokens) to learn token-to-spectrogram mappings;\n(2) conditioned only on text (with optional prompts) to directly generate spectrograms.\nIn the first type, the FM module takes input from a speech-text LM, such as&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> integrates a text-speech LM with an FM decoder, achieving high naturalness and quality with low inference latency. IndexTTS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhances speech duration control and disentangles speaker identity from emotion, enabling precise control over duration, timbre, and affect. Minimax&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a learnable speaker encoder and an auxiliary FM module, yielding strong zero-shot generalization and generation quality.\nIn the second type, the FM TTS model generates Mel spectrogram conditioned on text and speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without a LM. VoiceFlow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces a rectified FM algorithm that enhances generation efficiency. E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enables end-to-end FM-based TTS by padding blank tokens during generation. F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves E2-TTS by incorporating an additional text encoder and a tailored sampling strategy, achieving strong performance with high real-time efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "fmbased",
                    "tts",
                    "generalization",
                    "paradigms",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite the success of FM-based frameworks, their training objective is limited: the standard loss implicitly models the high-dimensional speech distribution without explicitly enforcing perceptual attributes such as speaker identity. This often results in insufficient refinement of fine-grained features in zero-shot scenarios, e.g., speaker similarity. Auxiliary supervision is a promising direction to explicitly guide such attributes.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iclr2025-yusihyun-repa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-tri-taro</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that external representation alignment applied to intermediate FM features can accelerate convergence and improve performance. In the context of TTS, previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aligns text and speech modalities to increase efficiency.\nHowever, two questions remain underexplored: (1) how speaker information distributes across denoising steps and hierarchical layers in FM-based TTS, and (2) how to best design and integrate speaker supervision to leverage this structure.\nWe first analyze speaker information flow in FM-based models and find that it varies significantly across layers and timesteps, motivating adaptive alignment. We thus propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which dynamically aligns latent FM representations with speaker embeddings from a pre-trained encoder, conditioned on both timestep and depth.\nOur contributions are threefold:</span>\n</p>\n\n",
                "matched_terms": [
                    "fmbased",
                    "training",
                    "across",
                    "tts",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TLA-SA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss to enhance speaker consistency for zero-shot TTS. The proposed TLA-SA aligns intermediate representation of FM with a pre-trained encoder and integrates temporal and hierarchical information of FM, and thus provides a dynamic speaker supervision to TM-based TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present an analytical study revealing that speaker information in FM-based TTS is unevenly distributed across denoising steps and model layers, highlighting the necessity of time-layer adaptive alignment and providing insight for speaker modeling in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "fmbased",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Experimental results confirm that the proposed TLA-SA loss consistently improves speaker modeling across large-scale and small-scale dataset sizes and mainstream model architectures. We will open-source our large-scale dataset</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The dataset is currently being organized and will be open-sourced soon.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM) zero-shot TTS models reconstruct Mel spectrograms through a mask prediction strategy, as shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S1.F1\" style=\"font-size:90%;\" title=\"In 1 Introduction &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During training, the FM model learns a flow </span>\n  <math alttext=\"\\phi_{t}(\\mathbf{x}_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119857;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\phi_{t}(\\mathbf{x}_{t},c)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that transforms a noise distribution </span>\n  <math alttext=\"\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119857;</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119977;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">&#120782;</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">&#119816;</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into a data distribution </span>\n  <math alttext=\"\\mathbf{x}_{1}\\sim q(\\mathbf{x})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119857;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">q</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">&#119857;</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1}\\sim q(\\mathbf{x})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, conditioned on the masked input </span>\n  <math alttext=\"\\mathbf{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and speaker embedding </span>\n  <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">c</mi>\n      <annotation encoding=\"application/x-tex\">c</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Given a vector field parameterized by </span>\n  <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#952;</mi>\n      <annotation encoding=\"application/x-tex\">\\theta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Conditional Flow Matching (CFM) loss minimizes the </span>\n  <math alttext=\"\\ell_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\ell_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> distance along the optimal transport path:</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This formulation enables zero-shot generalization, allowing the model to synthesize speech for speakers specified by </span>\n  <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">c</mi>\n      <annotation encoding=\"application/x-tex\">c</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, even if they are unseen during training.</span>\n</p>\n\n",
                "matched_terms": [
                    "generalization",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the distribution of speaker information in the FM-based TTS model, we adopt the training-free Centered Kernel Nearest-Neighbor Alignment (CKNNA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv-huh-platonic</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> criterion, which quantifies the similarity between two sets of embeddings.\nWe first examine whether CKNNA faithfully reflects the speaker modeling quality of TTS models.\nFor each baseline checkpoint at different training steps, we first assess speaker similarity using a pre-trained speaker encoder for </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> synthesized utterances from one of our test sets.\nSimultaneously, we extract intermediate speaker representations from each layer of the checkpoint. Specifically, given the intermediate representation </span>\n  <math alttext=\"E_{i}\\in\\mathbb{R}^{B\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E_{i}\\in\\mathbb{R}^{B\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer of the FM model with </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layers, temporal mean pooling is applied to derive the speaker representation:</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "fmbased",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which facilitates both temporal and hierarchical information to boost speaker modeling in FM-based TTS.\nThe key idea is to align intermediate representations of the FM module with embeddings extracted from a pre-trained speaker encoder.\nThe speaker alignment loss at layer </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, denoted by </span>\n  <math alttext=\"\\mathcal{L}^{\\text{SA}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\text{SA}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, minimizes the discrepancy between </span>\n  <math alttext=\"E^{\\text{Speaker}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{Speaker}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as mentioned in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the speaker alignment embedding </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a pre-trained speaker encoder:</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts",
                    "fmbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">libritts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A small-scale high-quality English TTS dataset for research, containing approximately 585 hours of training data. We follow the evaluation protocol of CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UniCATS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aaai24-duchenpeng-unicats</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using 500 utterances from the test-clean split with 37 non-overlapping speakers. We also conduct CKNNA-based analysis on the LibriTTS test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "training",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Paradigm.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To evaluate the effectiveness and generalization of TLA-SA, we experiment with two FM-based TTS architectures. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-based FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Following CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model comprises a speech-text language model and a Transformer U-Net CFM decoder with 2 down-sampling layers, 12 middle layers, and 2 up-sampling layers. TLA-SA supervision is applied to the middle layers.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-free FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We utilize an FM-based TTS model with a transformer-based text encoder and 9 MMDiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icml2024-esser-mmdit</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> decoder layers, which is similar to&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "fmbased",
                    "training",
                    "lmbased",
                    "tts",
                    "generalization",
                    "paradigm",
                    "lmfree",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configuration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> All models are trained from scratch on utterances with 24kHz sampling rate. A cosine learning rate scheduler with a 0.25-epoch warm-up is used, and optimization is performed with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2017-loshchilov-adamw</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For TLASA-100k, models are trained for 156k steps with batch size 16 and gradient accumulation of 4. The learning rate decays from 8e-4 to 1e-6. For LibriTTS, models are trained for 125k steps with batch size 16, no gradient accumulation, and a learning rate decaying from 2e-4 to 1e-4. Unless specified, we use a WavLM-based&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">taslp2022-chensanyuan-wavlm_sv</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model fine-tuned on speaker verification task for the TLA-SA loss.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "training",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER/CER.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate Word Error Rate (WER) for English test sets and Character Error Rate (CER) for Mandarin test sets, using the speech recognition models provided by Seed-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker Similarity (SS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We measure the similarity between generated speech and the prompt speech using the cosine similarity of their embeddings. Two pre-trained models are used: a speaker-related fine-tuned WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the ERes2Net&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2025-chenyafeng-3dspeaker</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "simwavlm",
                    "simeres2net"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F4\" style=\"font-size:90%;\" title=\"In 4.1 TLA-SA on large-scale dataset &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA improves speaker similarity by 2.2% on Sim-WavLM, enhancing speaker modeling in FM-based TTS.\nFurthermore, it accelerates convergence by </span>\n  <math alttext=\"2.9\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.9</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.9\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the large-scale TLASA-100k dataset, demon strating its efficacy in guiding speaker representation learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts",
                    "simwavlm",
                    "fmbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on the high-quality LibriTTS dataset, which consists of only 0.6% of TLASA-100k. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying TLA-SA consistently improves speaker similarity by 2.8% and 2.5% (0.538 vs. 0.510, 0.596 vs. 0.571) on WavLM and ERes2Net, using supervision from a single pre-trained speaker encoder. These results indicate that TLA-SA enhances general speaker modeling rather than overfitting to specific evaluation models.\nMoreover, gains from layer and time adaptation underscore the importance of jointly leveraging temporal and hierarchical information for effective speaker alignment in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts",
                    "fmbased",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, we visualize the adaptive weight </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> across time steps and model layers. As shown in &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F5\" style=\"font-size:90%;\" title=\"In 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA loss applies selective supervision strength along both time and layer axes, effectively incorporating temporal and hierarchical information, which is consistent with observations in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSpecifically, higher weights are assigned to shallow layers and later time steps, suggesting their greater importance in speaker modeling and the need for stronger speaker-specific supervision.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we search for the optimal speaker encoder of TLA-SA using LibriTTS. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Ablation on SA encoder &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CAM++ and ERes2Net outperform the WavLM-based model while using </span>\n  <math alttext=\"42.5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">42.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">42.5\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"46.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">46.4</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">46.4\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> fewer parameters, indicating that the WavLM-based model contains less speaker information even with fine-tuning on speaker verification.\nMoreover, TLA-SA with CAM++ achieves comparable or superior performance to WavLM and ERes2Net (0.544 vs. 0.538, 0.609 vs. 0.606), highlighting that TLA-SA enhances general speaker modeling in FM-based TTS rather than merely transferring information from a specific speaker encoder.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "tts",
                    "fmbased",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TLA-SA, a loss that leverages temporal and hierarchical speaker alignment to enhance speaker consistency in zero-shot FM-based TTS systems. By introducing joint alignment across time steps and model layers, TLA-SA effectively strengthens speaker modeling. TLA-SA improves speaker similarity on large- and small-scale datasets, enabling models to achieve comparable or superior performance without speaker-specific fine-tuning, regardless of the training data size. Furthermore, TLA-SA yields consistent improvement on both FM TTS models with and without a cascaded LM respectively, demonstrating strong generalization across architectures. Future work will extend TLA-SA to broader FM variants and explore additional alignment signals to further enhance TTS performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "fmbased",
                    "training",
                    "across",
                    "tts",
                    "generalization",
                    "tlasa",
                    "systems"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS",
        "caption": "Table 4: Comparison of different pre-trained speaker encoders used as supervisors for TLA-SA.",
        "body": "Pre-trained Encoder\n#Params.\nSim-WavLM ↑\\uparrow\nSim-ERes2Net ↑\\uparrow\n\n\n-\n-\n0.510\n0.571\n\n\n\n\n\nWavLM [taslp2022-chensanyuan-wavlm_sv]\n\n313M\n0.538\n0.596\n\n\n\nERes2Net [icassp2025-chenyafeng-3dspeaker]\n\n6.6M\n0.544\n0.606\n\n\n\nCAM++ [wang2023cam++]\n\n7.2M\n0.544\n0.609",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Pre-trained Encoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">#Params.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.510</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.571</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WavLM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">taslp2022-chensanyuan-wavlm_sv</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">313M</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.538</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.596</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ERes2Net&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2025-chenyafeng-3dspeaker</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6M</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.544</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.606</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CAM++&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023cam++</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.2M</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.544</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.609</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pretrained",
            "used",
            "encoder",
            "cam",
            "wavlm",
            "wang2023cam",
            "supervisors",
            "speaker",
            "simwavlm",
            "simeres2net",
            "params",
            "taslp2022chensanyuanwavlmsv",
            "icassp2025chenyafeng3dspeaker",
            "66m",
            "tlasa",
            "↑uparrow",
            "eres2net",
            "different",
            "72m",
            "313m",
            "encoders",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we search for the optimal speaker encoder of TLA-SA using LibriTTS. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Ablation on SA encoder &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CAM++ and ERes2Net outperform the WavLM-based model while using </span>\n  <math alttext=\"42.5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">42.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">42.5\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"46.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">46.4</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">46.4\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> fewer parameters, indicating that the WavLM-based model contains less speaker information even with fine-tuning on speaker verification.\nMoreover, TLA-SA with CAM++ achieves comparable or superior performance to WavLM and ERes2Net (0.544 vs. 0.538, 0.609 vs. 0.606), highlighting that TLA-SA enhances general speaker modeling in FM-based TTS rather than merely transferring information from a specific speaker encoder.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose <span class=\"ltx_text ltx_font_italic\">Time-Layer Adaptive Speaker Alignment</span> (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most state-of-the-art FM-based TTS models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are based on Flow-Matching (FM) due to its strong modeling power and generalization capability.\nThe use of FM modules in mainstream TTS systems falls into two paradigms:\n(1) conditioned on upstream intermediate representations (e.g., LM-generated tokens) to learn token-to-spectrogram mappings;\n(2) conditioned only on text (with optional prompts) to directly generate spectrograms.\nIn the first type, the FM module takes input from a speech-text LM, such as&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> integrates a text-speech LM with an FM decoder, achieving high naturalness and quality with low inference latency. IndexTTS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhousiyi-indextts2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhances speech duration control and disentangles speaker identity from emotion, enabling precise control over duration, timbre, and affect. Minimax&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-zhangbowen-minimax</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a learnable speaker encoder and an auxiliary FM module, yielding strong zero-shot generalization and generation quality.\nIn the second type, the FM TTS model generates Mel spectrogram conditioned on text and speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without a LM. VoiceFlow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2024-yiweiguo-voiceflow</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces a rectified FM algorithm that enhances generation efficiency. E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enables end-to-end FM-based TTS by padding blank tokens during generation. F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves E2-TTS by incorporating an additional text encoder and a tailored sampling strategy, achieving strong performance with high real-time efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite the success of FM-based frameworks, their training objective is limited: the standard loss implicitly models the high-dimensional speech distribution without explicitly enforcing perceptual attributes such as speaker identity. This often results in insufficient refinement of fine-grained features in zero-shot scenarios, e.g., speaker similarity. Auxiliary supervision is a promising direction to explicitly guide such attributes.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iclr2025-yusihyun-repa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-tri-taro</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that external representation alignment applied to intermediate FM features can accelerate convergence and improve performance. In the context of TTS, previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">is2025-jeongsoo-f5tts_ctc_speaker_areg</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aligns text and speech modalities to increase efficiency.\nHowever, two questions remain underexplored: (1) how speaker information distributes across denoising steps and hierarchical layers in FM-based TTS, and (2) how to best design and integrate speaker supervision to leverage this structure.\nWe first analyze speaker information flow in FM-based models and find that it varies significantly across layers and timesteps, motivating adaptive alignment. We thus propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which dynamically aligns latent FM representations with speaker embeddings from a pre-trained encoder, conditioned on both timestep and depth.\nOur contributions are threefold:</span>\n</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "speaker",
                    "encoder",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TLA-SA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> loss to enhance speaker consistency for zero-shot TTS. The proposed TLA-SA aligns intermediate representation of FM with a pre-trained encoder and integrates temporal and hierarchical information of FM, and thus provides a dynamic speaker supervision to TM-based TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "speaker",
                    "encoder",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Experimental results confirm that the proposed TLA-SA loss consistently improves speaker modeling across large-scale and small-scale dataset sizes and mainstream model architectures. We will open-source our large-scale dataset</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The dataset is currently being organized and will be open-sourced soon.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To analyze the distribution of speaker information in the FM-based TTS model, we adopt the training-free Centered Kernel Nearest-Neighbor Alignment (CKNNA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv-huh-platonic</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> criterion, which quantifies the similarity between two sets of embeddings.\nWe first examine whether CKNNA faithfully reflects the speaker modeling quality of TTS models.\nFor each baseline checkpoint at different training steps, we first assess speaker similarity using a pre-trained speaker encoder for </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> synthesized utterances from one of our test sets.\nSimultaneously, we extract intermediate speaker representations from each layer of the checkpoint. Specifically, given the intermediate representation </span>\n  <math alttext=\"E_{i}\\in\\mathbb{R}^{B\\times T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E_{i}\\in\\mathbb{R}^{B\\times T\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer of the FM model with </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layers, temporal mean pooling is applied to derive the speaker representation:</span>\n</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "speaker",
                    "encoder",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We extract speaker embeddings </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from corresponding </span>\n  <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">B</mi>\n      <annotation encoding=\"application/x-tex\">B</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> prompt speech in a test set with 37 distinct speakers and 500 utterances with the same encoder.\nWe utilize CKNNA score of </span>\n  <math alttext=\"E_{i}^{\\text{Speaker}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{i}^{\\text{Speaker}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E^{\\text{SA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to reflect the distribution of speaker information at </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th layer.\nAs shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CKNNA exhibits a strong correlation with speaker similarity performance, validating its efficacy for analyzing speaker information.</span>\n</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Time-Layer Adaptive Speaker Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (TLA-SA), which facilitates both temporal and hierarchical information to boost speaker modeling in FM-based TTS.\nThe key idea is to align intermediate representations of the FM module with embeddings extracted from a pre-trained speaker encoder.\nThe speaker alignment loss at layer </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, denoted by </span>\n  <math alttext=\"\\mathcal{L}^{\\text{SA}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">SA</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\text{SA}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, minimizes the discrepancy between </span>\n  <math alttext=\"E^{\\text{Speaker}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n        <mtext mathsize=\"0.900em\">Speaker</mtext>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E^{\\text{Speaker}}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as mentioned in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the speaker alignment embedding </span>\n  <math alttext=\"E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mtext mathsize=\"0.900em\">SA</mtext>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">E^{\\text{SA}}\\in\\mathbb{R}^{B\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a pre-trained speaker encoder:</span>\n</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "speaker",
                    "encoder",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Paradigm.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To evaluate the effectiveness and generalization of TLA-SA, we experiment with two FM-based TTS architectures. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-based FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Following CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model comprises a speech-text language model and a Transformer U-Net CFM decoder with 2 down-sampling layers, 12 middle layers, and 2 up-sampling layers. TLA-SA supervision is applied to the middle layers.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LM-free FM TTS.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We utilize an FM-based TTS model with a transformer-based text encoder and 9 MMDiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icml2024-esser-mmdit</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> decoder layers, which is similar to&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configuration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> All models are trained from scratch on utterances with 24kHz sampling rate. A cosine learning rate scheduler with a 0.25-epoch warm-up is used, and optimization is performed with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2017-loshchilov-adamw</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For TLASA-100k, models are trained for 156k steps with batch size 16 and gradient accumulation of 4. The learning rate decays from 8e-4 to 1e-6. For LibriTTS, models are trained for 125k steps with batch size 16, no gradient accumulation, and a learning rate decaying from 2e-4 to 1e-4. Unless specified, we use a WavLM-based&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">taslp2022-chensanyuan-wavlm_sv</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model fine-tuned on speaker verification task for the TLA-SA loss.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker",
                    "used",
                    "taslp2022chensanyuanwavlmsv"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER/CER.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate Word Error Rate (WER) for English test sets and Character Error Rate (CER) for Mandarin test sets, using the speech recognition models provided by Seed-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker Similarity (SS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We measure the similarity between generated speech and the prompt speech using the cosine similarity of their embeddings. Two pre-trained models are used: a speaker-related fine-tuned WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-anastassiou-seedtts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-WavLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the ERes2Net&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">icassp2025-chenyafeng-3dspeaker</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sim-ERes2Net</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "simwavlm",
                    "simeres2net",
                    "eres2net",
                    "icassp2025chenyafeng3dspeaker",
                    "wavlm",
                    "pretrained",
                    "speaker",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F4\" style=\"font-size:90%;\" title=\"In 4.1 TLA-SA on large-scale dataset &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA improves speaker similarity by 2.2% on Sim-WavLM, enhancing speaker modeling in FM-based TTS.\nFurthermore, it accelerates convergence by </span>\n  <math alttext=\"2.9\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.9</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.9\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the large-scale TLASA-100k dataset, demon strating its efficacy in guiding speaker representation learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker",
                    "simwavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S3.T1\" style=\"font-size:90%;\" title=\"In 3.3 Evaluation metrics &#8227; 3 Experimental Setups &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that TLA-SA improves speaker similarity by 3.0% (WavLM) and 2.1% (ERes2Net) on average compared to our model without TLA-SA, with negligible impact on CER/WER. These results confirm that TLA-SA provides an effective yet simple approach to improving speaker modeling during pre-training, consistent with our findings in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that the model with TLA-SA shows stronger speaker modeling capability.\nFree of speaker SFT stage, our CosyVoice 2 significantly outperforms FireRedTTS, which not only incorporates 50% additional data but also a SFT stage, demonstrating that TLA-SA offers a simple yet effective method to improve speaker modeling ability.\nMoreover, the TLA-SA boosted model achieves comparable performance to the official CosyVoice 2 with 50.0%, with a gap of only 2.1% on average against the model trained with </span>\n  <math alttext=\"2\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> data and SFT.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker",
                    "eres2net",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on the high-quality LibriTTS dataset, which consists of only 0.6% of TLASA-100k. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying TLA-SA consistently improves speaker similarity by 2.8% and 2.5% (0.538 vs. 0.510, 0.596 vs. 0.571) on WavLM and ERes2Net, using supervision from a single pre-trained speaker encoder. These results indicate that TLA-SA enhances general speaker modeling rather than overfitting to specific evaluation models.\nMoreover, gains from layer and time adaptation underscore the importance of jointly leveraging temporal and hierarchical information for effective speaker alignment in FM-based TTS models.</span>\n</p>\n\n",
                "matched_terms": [
                    "eres2net",
                    "wavlm",
                    "pretrained",
                    "speaker",
                    "encoder",
                    "tlasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, we visualize the adaptive weight </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> across time steps and model layers. As shown in &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.F5\" style=\"font-size:90%;\" title=\"In 4.2 TLA-SA on high-quality LibriTTS &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TLA-SA loss applies selective supervision strength along both time and layer axes, effectively incorporating temporal and hierarchical information, which is consistent with observations in&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S2.F3\" style=\"font-size:90%;\" title=\"In 2.2 Speaker distribution in the FM-based TTS &#8227; 2 Methodology &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSpecifically, higher weights are assigned to shallow layers and later time steps, suggesting their greater importance in speaker modeling and the need for stronger speaker-specific supervision.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the generalization capability of our approach across FM-based TTS architectures, we evaluate it on models employing a decoder-only LM (e.g., CosyVoice series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2024-duzhihao-cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">arxiv2025-duzhihao-cosyvoice3</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and those without a LM (e.g., E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">slt2024-eskimez-e2tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acl2025-chenyushen-f5tts</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09995v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Performance on different training paradigms &#8227; 4 Results and Analysis &#8227; Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed TLA-SA consistently enhances speaker modeling performance in both settings, indicating a strong cross-architecture generalization. Furthermore, the proposed TLA-SA gains an absolute improvement on the LM-free architecture (6% vs. 2.8% on WavLM and 7.1% vs. 2.5% on ERes2Net), demonstrating that TLA-SA effectively assists the speaker modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker",
                    "eres2net",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TLA-SA, a loss that leverages temporal and hierarchical speaker alignment to enhance speaker consistency in zero-shot FM-based TTS systems. By introducing joint alignment across time steps and model layers, TLA-SA effectively strengthens speaker modeling. TLA-SA improves speaker similarity on large- and small-scale datasets, enabling models to achieve comparable or superior performance without speaker-specific fine-tuning, regardless of the training data size. Furthermore, TLA-SA yields consistent improvement on both FM TTS models with and without a cascaded LM respectively, demonstrating strong generalization across architectures. Future work will extend TLA-SA to broader FM variants and explore additional alignment signals to further enhance TTS performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "tlasa",
                    "speaker"
                ]
            }
        ]
    }
}