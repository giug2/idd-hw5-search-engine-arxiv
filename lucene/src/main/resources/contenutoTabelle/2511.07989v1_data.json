{
    "p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_top\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Taja Kuzman Punger&#353;ek<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup>, Peter Rupnik<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup>, Ivan Porupski<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup>,</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Vuk Dini&#263;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup>, Nikola Ljube&#353;i&#263;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8224;</span></sup><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8225;</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup>Jo&#382;ef Stefan Institute;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>Faculty of Computer and Information Science, University of Ljubljana;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>Institute of Contemporary History;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Ljubljana, Slovenia</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">{taja.kuzman, peter.rupnik, ivan.porupski, vuk.dinic, nikola.ljubesic}@ijs.si</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tajakuzman",
            "rupnik∗",
            "peter",
            "pungeršek∗",
            "history",
            "institute",
            "∗jožef",
            "kuzman",
            "vuk",
            "information",
            "peterrupnik",
            "nikolaljubesicijssi",
            "university",
            "taja",
            "science",
            "ivanporupski",
            "vukdinic",
            "†faculty",
            "dinić∗",
            "stefan",
            "slovenia",
            "ljubljana",
            "ivan",
            "nikola",
            "porupski∗",
            "computer",
            "contemporary",
            "‡institute",
            "ljubešić∗†‡"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We would like to thank the developers of the llm.ijs.si service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite> for establishing the LLM inference platform deployed at the Jo&#382;ef Stefan Institute, which provided convenient access to the open-source large language models used in this study. We also thank the annotators of the test datasets for their diligence and the time devoted to manual annotation, which resulted in the high-quality evaluation datasets used in this work. Lastly, we would like to thank the <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/k-centre/\" title=\"\">CLASSLA knowledge centre for South Slavic languages</a> and the Slovenian <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/about/\" title=\"\">CLARIN.SI infrastructure</a> for their valuable support.</p>\n\n",
                "matched_terms": [
                    "stefan",
                    "institute"
                ]
            }
        ]
    },
    "S1.tab1": {
        "caption": "Table 1: Information on test datasets in English (EN), Croatian (HR), Serbian (SR), Bosnian (BS), Slovenian (SL), and Macedonian (MK).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Dataset</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">Lang</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\"># Instances</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\"># Labels</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">% Most and Least Frequent Label</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>\n<span class=\"ltx_p\">gray!20</span>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202; <span class=\"ltx_text ltx_font_italic\">Sentiment classification in parliamentary speeches</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaSent-EN-test</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">EN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">2600</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">40.8% (Neutral), 26.8% (Positive)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaSent-HR-test</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">1336</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">41.9% (Negative), 17.2% (Positive)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaSent-SR-test</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">SR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">1074</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">46.2% (Negative), 17.6% (Positive)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaSent-BS-test</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">BS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">190</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">47.9% (Negative), 14.7% (Positive)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>\n<span class=\"ltx_p\">gray!20</span>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202; <span class=\"ltx_text ltx_font_italic\">Genre classification in web texts</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">EN-GINCO</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">EN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">272</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">23.5% (Information/Explanation), 0.4% (Legal)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">X-GINCO-SL</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">SL</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">15% (Prose/Lyrical), 8.8% (Opinion/Argumentation)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">X-GINCO-HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">16.3% (Promotion), 7.5% (Instruction)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">X-GINCO-MK</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">MK</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">15% (News), 1% (Opinion/Argumentation)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>\n<span class=\"ltx_p\">gray!20</span>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202; <span class=\"ltx_text ltx_font_italic\">Topic classification in news articles</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">IPTC-test-HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">291</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">17</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">11.0% (Economy), 3.8% (Conflict, War and Peace)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">IPTC-test-SL</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">SL</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">282</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">17</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10.6% (Society), 3.2% (Conflict, War and Peace)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>\n<span class=\"ltx_p\">gray!20</span>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\"/>\n</span>&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202;&#8194;&#8202; <span class=\"ltx_text ltx_font_italic\">Topic classification in parliamentary speeches</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaCAP-test-EN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">EN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">876</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">6.4% (Law and Crime), 2.1% (Culture)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaCAP-test-HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">HR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">869</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">8.5% (Government Operations), 1.7% (Immigration)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaCAP-test-SR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">SR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">874</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">7.1% (Government Operations), 1.7% (Immigration)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ParlaCAP-test-BS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:26.0pt;\">BS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:47.7pt;\">824</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.7pt;\">22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10.4% (Other), 0.5% (Culture)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speeches",
            "topic",
            "parlacaptestbs",
            "positive",
            "parlacaptestsr",
            "gray20",
            "label",
            "articles",
            "news",
            "legal",
            "informationexplanation",
            "peace",
            "enginco",
            "parlacaptesthr",
            "immigration",
            "slovenian",
            "rowcolor",
            "instruction",
            "information",
            "parlasententest",
            "proselyrical",
            "conflict",
            "macedonian",
            "parliamentary",
            "parlasentsrtest",
            "most",
            "frequent",
            "genre",
            "xgincomk",
            "crime",
            "law",
            "operations",
            "lang",
            "least",
            "test",
            "xgincohr",
            "war",
            "sentiment",
            "serbian",
            "xgincosl",
            "croatian",
            "labels",
            "opinionargumentation",
            "english",
            "instances",
            "government",
            "classification",
            "economy",
            "bosnian",
            "parlasenthrtest",
            "datasets",
            "society",
            "negative",
            "parlacaptesten",
            "web",
            "iptctesthr",
            "texts",
            "promotion",
            "neutral",
            "other",
            "iptctestsl",
            "culture",
            "parlasentbstest",
            "dataset"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>LLM evaluation, text classification, large language models, South Slavic languages, sentiment identification, topic classification, genre identification</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "topic",
                    "web",
                    "texts",
                    "parliamentary",
                    "english",
                    "articles",
                    "genre",
                    "news",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">In this paper, we focus on South Slavic languages, where research on text classification tasks included in our study has, until recently, been limited or even non-existent <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023survey</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. We take a first step toward systematically evaluating the current state of the art for text classification in these languages. Our evaluation is based on three text classification tasks in three different domains for which manually-annotated test datasets in South Slavic languages and fine-tuned BERT-like classifiers are freely available: sentiment classification of parliamentary speeches, topic classification in news articles, topic classification in parliamentary speeches, and automatic genre identification in web texts. These tasks span different domains and language styles, allowing for a comprehensive analysis of the performance of transformer-based models on text classification tasks. Specifically, we compare the performance of openly available fine-tuned BERT-like models with the zero-shot capabilities of both open-source and closed-source LLMs used via prompting.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "topic",
                    "web",
                    "texts",
                    "parliamentary",
                    "articles",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">By evaluating various models on a selection of text classification tasks in English and various South Slavic languages, we set out to test the following two hypotheses that are based on previous experiments with fine-tuned BERT-like models and LLMs on automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> and sentiment analysis in parliamentary texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2025parlasent</span>)</cite>:</p>\n\n",
                "matched_terms": [
                    "topic",
                    "texts",
                    "parliamentary",
                    "english",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of LLMs used in a zero-shot setup on text classification tasks on South Slavic test datasets is comparable to the performance on English test datasets.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "datasets",
                    "english",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the introduction of transformer architectures, BERT (bidirectional encoder representations from transformers) models have achieved state-of-the-art results in text classification tasks, outperforming earlier non-neural approaches, such as support vector machines (SVMs). They have also demonstrated strong cross-lingual zero-shot capabilities in various classification tasks, including automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023survey</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">petukhova2023mn</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">de2020news</span>)</cite>, and sentiment classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>. However, these models still require fine-tuning on a training dataset, developed during manual annotation campaigns that are time-consuming and costly.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "genre",
                    "news",
                    "classification",
                    "dataset",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instruction-tuned decoder-only transformer models, commonly referred to as large language models (LLMs), have recently shown strong performance in a range of classification tasks, even in zero-shot prompting setups that require no training data <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024dialect</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>. They have achieved promising results on various natural language processing tasks, including stance detection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022would</span>)</cite>, implicit hate speech categorization <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, causal commonsense reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024jsi</span>)</cite>, and machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hendy2023good</span>)</cite>. Due to their promising performance, researchers have even started using them as data annotators, either by generating text and labels <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meng2022generating</span>)</cite> or by annotating pre-existing texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>.\nDespite the growing interest in this topic, the majority of evaluations of LLMs used in text classification tasks are limited only to English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sun2023text</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025pushing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kostina2025large</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2024advancing</span>)</cite>. Systematic multilingual evaluations, especially which would include less-resourced languages such as those in the South Slavic group remain limited. Our work addresses this gap by providing a comparative evaluation of open-source and closed-source LLMs with openly-available fine-tuned BERT-like models across four benchmark families comprising three diverse classification tasks and three different domains in South Slavic languages and English.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "labels",
                    "texts",
                    "english",
                    "genre",
                    "news",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The benchmarks (evaluation datasets) used in this study cover three text classification tasks, namely, sentiment identification, topic classification, and automatic genre identification, and three domains: parliamentary speeches, news articles and web texts. An overview of the datasets is provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S1\" title=\"1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The four benchmark families differ significantly in terms of language coverage, number of test instances, and label granularity.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "speeches",
                    "web",
                    "texts",
                    "parliamentary",
                    "instances",
                    "articles",
                    "label",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The topic classification task is evaluated on two domains: 1) news articles, namely, the Croatian and Slovenian IPTC test datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which comprise around 300 text instances per language, and 2) parliamentary speeches, namely, the Bosnian, Croatian, English and Serbian ParlaCAP test datasets that consist of approximately 820 to 880 instances per language. In the ParlaCAP benchmarks, an instance is a transcription of an utterance given by a parliamentary member in a parliamentary session.</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "topic",
                    "speeches",
                    "croatian",
                    "parliamentary",
                    "instances",
                    "english",
                    "articles",
                    "news",
                    "bosnian",
                    "classification",
                    "test",
                    "slovenian",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The topic classification task involves the highest number of labels, that is, 17 news topic labels from the top level of the IPTC NewsCodes Media Topic hierarchical schema<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://show.newscodes.org/index.html?newscodes=medtop&amp;lang=en-GB&amp;startTo=Show\" title=\"\">https://show.newscodes.org/index.html?newscodes=medtop&amp;lang=en-GB&amp;startTo=Show</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>, and 22 agenda setting topic labels (21 major topics and a label <span class=\"ltx_text ltx_font_italic\">Other</span>) from the Comparative Agendas Project (CAP) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite> Master Codebook&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bevan2019gone</span>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.comparativeagendas.net/pages/master-codebook\" title=\"\">https://www.comparativeagendas.net/pages/master-codebook</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "labels",
                    "label",
                    "news",
                    "classification",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Bosnian, Croatian, English, and Serbian ParlaSent sentiment identification datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) have a significantly lower granularity of labels, with only 3 categories. They are represented by the largest number of instances, ranging from 190 (Bosnian part) to 2600 (English part) sentence-level instances.</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "croatian",
                    "labels",
                    "english",
                    "instances",
                    "bosnian",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With 8 labels, the Croatian, English, Macedonian, and Slovenian GINCO genre datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> represent a midpoint in label granularity among the four benchmark families. However, the genre identification task might be the most difficult one, as genre identification depends on the interpretation of full texts with the focus on author&#8217;s purpose, the common function of the text, and the text&#8217;s conventional form <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">orlikowski1994genre</span>)</cite>. This complexity has also contributed to smaller test datasets in terms of the number of text instances, as manual annotation is more time-consuming. It is also important to note that, unlike the parliamentary datasets, the English portion of the genre datasets is not fully comparable to the South Slavic portions, which are label-balanced and contain fewer ambiguous instances. Nevertheless, the genre datasets remain valuable for evaluating model performance within each language.</p>\n\n",
                "matched_terms": [
                    "croatian",
                    "labels",
                    "texts",
                    "macedonian",
                    "parliamentary",
                    "english",
                    "instances",
                    "label",
                    "most",
                    "genre",
                    "test",
                    "slovenian",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All test datasets were manually annotated by annotators that are deemed reliable based on their satisfactory inter-annotator agreement, namely, Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite> values close to or above the 0.667 threshold for reliable annotation. To prevent large language models from incorporating the test datasets during their training phase, the test datasets are not publicly available, except for the ParlaSent benchmark family. Access to other datasets is granted on request from the corresponding authors. Further details on the test datasets are provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS1\" title=\"A.1. Benchmarking Datasets &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "other",
                    "datasets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluate the main machine learning approaches that have recently been used for our selection of text classification tasks, with the focus on the comparison between the freely available fine-tuned BERT models and the open-source and closed-source LLMs.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The code for the model evaluation and analysis of results is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a>.</span></span></span>\nThe models are evaluated on four families of test datasets that comprise South Slavic languages. The performance of the models is evaluated based on the micro-F1 and macro-F1 metrics, which enable assessment of the model performance at both the instance and label levels, respectively.</p>\n\n",
                "matched_terms": [
                    "label",
                    "classification",
                    "datasets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">dummy classifier</span>: a dummy classifier that predicts the most frequent class in the training data. To allow comparison, the dummy classifiers were trained on the same datasets that were used for fine-tuning the BERT-like models, mentioned below.</p>\n\n",
                "matched_terms": [
                    "most",
                    "frequent",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">fine-tuned BERT-like classifiers</span>: in our study, we evaluate previously developed openly accessible multilingual fine-tuned BERT-like models that have been fine-tuned for the respective task, namely, the XLM-R-ParlaSent (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) model for sentiment identification in parliamentary texts, the X-GENRE classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-classifier-huggingface</span></cite>) for automatic genre identification, the IPTC News Topic classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptc_model_huggingface</span></cite>) for news topic classification, and the ParlaCAP classifier <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> for topic classification in parliamentary speeches. The XLM-R-ParlaSent and the ParlaCAP models are based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on parliamentary proceedings in 30 European languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>. The XLM-R-ParlaSent model was fine-tuned on 13 thousand instances from the ParlaSent sentiment training dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span>)</cite> in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>, while the ParlaCAP model was fine-tuned on around 30 thousand speeches from parliamentary debates annotated with CAP topic labels, originating from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) from 29 European parliaments. The X-GENRE classifier is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite> in English and Slovenian; while the IPTC News Topic classifier is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> that was fine-tuned on the EMMediaTopic dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite> in Catalan, Croatian, Greek, and Slovenian. All fine-tuned models use the same classes as the test datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "speeches",
                    "topic",
                    "croatian",
                    "labels",
                    "parliamentary",
                    "sentiment",
                    "english",
                    "instances",
                    "texts",
                    "genre",
                    "news",
                    "bosnian",
                    "classification",
                    "test",
                    "dataset",
                    "slovenian",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">open-source and closed-source large language models</span>: we use closed-source OpenAI models, namely the GPT-3.5-Turbo (<span class=\"ltx_text ltx_font_typewriter\">gpt-3.5-turbo-0125</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai</span>)</cite>, GPT-4o (<span class=\"ltx_text ltx_font_typewriter\">gpt-4o-2024-08-06</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> and the GPT-5 (<span class=\"ltx_text ltx_font_typewriter\">gpt-5-2025-08-07</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite>; a closed-source Gemini 2.5 Flash model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> by Google DeepMind; a closed-source Mistral Medium 3.1 model (<span class=\"ltx_text ltx_font_typewriter\">mistral-medium-2508</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mistral</span>)</cite> by Mistral AI; and four open-source models, namely, the Meta LLaMA 3.3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>, the Gemma 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, the Qwen 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>, and the DeepSeek-R1-Distill model (<span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite>. It is important to note that while the LLaMA model was pretrained on a web text collection in various languages, it is said to support only 8 languages, namely English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>. The DeepSeek-R1-Distill model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that provides support for more than 29 languages &#8211; not including South Slavic languages though. In contrast, the Gemma 3 model is reported to support over 140 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, and the Qwen 3 model was pretrained on 119 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>. While closed-source models are said to be massively multilingual, with Gemini 2.5 models being pretrained on over 400 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite>, details on their language coverage are very limited.</p>\n\n",
                "matched_terms": [
                    "web",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Open-source models were installed locally and executed via the Ollama API service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite>. OpenAI models were used through the chat completion endpoint via the OpenAI API, whereas other closed-source models were accessed through the OpenRouter platform<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openrouter.ai/\" title=\"\">https://openrouter.ai/</a></span></span></span> that provides a unified API access to various closed-source models. To prevent any bias, all models were used with their default parameters. The only parameter that we defined is the temperature which we set to 0 to ensure a more deterministic behaviour of the models. More details on the models and their implementation, including information on the availability of openly available models and fine-tuning datasets, are provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS2\" title=\"A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "other",
                    "datasets",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of fine-tuned BERT-like models and the instruction-tuned LLMs on a selection of text classification tasks that include test datasets in South Slavic languages. First, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS1\" title=\"5.1. State of the Art in Text Classification Tasks &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we provide results on the four benchmark families with the focus on hypothesis H1, which expects that zero-shot prompting with LLMs can provide performance that is comparable to that of fine-tuned BERT-like models. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS2\" title=\"5.2. Comparison of Large Language Models &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, we compare in more detail the performance of the closed-source and open-source LLMs on the three text classification tasks, which is followed by a discussion on the advantages and limitations of LLMs for data annotation based on text classification tasks (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS3\" title=\"5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Lastly, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS4\" title=\"5.4. Performance on English versus on South Slavic languages &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>, we compare the performance of LLMs on English test datasets with their performance on South Slavic datasets, addressing hypothesis H2, which presumes that the available multilingual LLMs perform similarly on South Slavic languages as on English.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "datasets",
                    "english",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1.sf1\" title=\"In Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> shows that both open-source and closed-source LLMs, used in a zero-shot prompting setup on the sentiment identification task, achieve performance that is comparable or even significantly higher to that of a fine-tuned BERT-like model trained on a large manually-annotated sentiment dataset. The only models that consistently perform worse than the fine-tuned BERT-like model are GPT-3.5-Turbo and DeepSeek-R1-Distill. Sentiment classification appears broad enough that more potent LLMs can interpret label definitions effectively without task-specific fine-tuning, reducing the benefit of additional training.</p>\n\n",
                "matched_terms": [
                    "label",
                    "classification",
                    "sentiment",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, fine-tuned BERT-like models outperform most LLMs on automatic genre identification and topic classification tasks. These tasks depend on predefined label sets based on specific guidelines, and the strong performance of fine-tuned BERT-like models indicates that domain-specific fine-tuning on labelled data still offers an advantage over the general knowledge leveraged by LLMs in zero-shot setups. This advantage is particularly clear in genre identification for South Slavic texts, where the fine-tuned BERT-like model significantly outperforms LLMs. The likely reason for the fine-tuned model&#8217;s very strong performance on South Slavic genre datasets is the curated nature of the test data &#8211; more challenging examples were removed before and during manual annotation, unlike in the English genre test dataset where the instances were randomly sampled from an English web corpus. Nevertheless, despite this limitation, the South Slavic test dataset remains valuable for comparing the performance of LLMs.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "web",
                    "texts",
                    "english",
                    "instances",
                    "label",
                    "most",
                    "genre",
                    "classification",
                    "test",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2\" title=\"Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the performance of open-source and closed-source LLMs, used via prompting, on the tasks of sentiment identification, automatic genre identification, news topic classification, and parliamentary topic classification. The DeepSeek-R1-Distill model is not included in the comparison, as it performs significantly worse than other models, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1\" title=\"Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "parliamentary",
                    "genre",
                    "news",
                    "classification",
                    "other",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While different models perform best across different languages and test datasets, a clear trend emerges: the top-performing models across all four benchmark families are the closed-source GPT-4o and GPT-5 from OpenAI, along with Gemini 2.5 Flash. Although GPT-5 is newer and reportedly more powerful, it does not outperform GPT-4o on all benchmarks. Among open-source models, Gemma 3 generally achieves the best results in sentiment identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>) and news topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>). For automatic genre identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>) and parliamentary topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>), rankings of open-source models vary by language. Overall, the weakest performance is observed with the older closed-source GPT-3.5-Turbo model, highlighting the rapid progress in both open-source and closed-source model development.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "parliamentary",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What is more, the inference speed of all LLMs is significantly slower than that of a fine-tuned BERT-like model. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F3\" title=\"Figure 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the fine-tuned BERT-like model achieves one of the highest macro-F1 scores on the topic classification task for parliamentary speeches, while maintaining a very low inference time of just 0.02 seconds per instance. In contrast, most LLMs have inference times between 0.6 and 1.4 seconds per instance, making them three to seven times slower for annotating the same dataset. The slowest model, GPT-5, takes 5.5 seconds per instance, which renders it impractical for large-scale automatic annotation of text collections. In this regard, fine-tuned BERT-like models offer a key advantage due to their lower computational cost and higher inference speed. Moreover, they can be trained on training data that is annotated by LLMs using the recently introduced LLM teacher-student paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which considerably reduces the effort needed to develop task-specific models.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "topic",
                    "parliamentary",
                    "most",
                    "classification",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another limitation of LLMs, as revealed by the experiments, is their occasional deviation from the defined label set. This issue was especially noticeable in topic classification and, to a lesser extent, in genre identification. The highest rate of label hallucination was found in the DeepSeek-R1-Distill model, which produced non-existing labels for 8% of instances in the news topic test datasets and 4% in the genre test dataset. Similar issues were also observed, though much less frequently (less than 1%), with the LLaMA 3.3, Gemma 3, Qwen 3 and Mistral Medium 3.1 models. In contrast, fine-tuned BERT-like models do not suffer from this issue, as they output probabilities for the predefined classes.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "labels",
                    "instances",
                    "label",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentiment identification ParlaSent and the topic classification ParlaCAP benchmark families comprise test datasets in South Slavic languages and English that were constructed with the same methodology. Thus, they also allow for a comparison of the performance of the LLMs on English, a highly resourced language, with South Slavic languages, which are significantly less represented in the pretraining and instruction-tuning datasets used to develop large language models.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "english",
                    "classification",
                    "test",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>, LLMs generally perform worse on Bosnian compared to other languages. However, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T3\" title=\"Table 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the differences in macro-F1 scores between English and the average of macro-F1 scores for South Slavic languages are relatively small for sentiment identification, ranging from 2 to 7 points. For topic classification, the performance gap is slightly larger, ranging from 3 to 10 points. This is likely due to the increased difficulty of the task, which involves greater label granularity: 22 labels compared to just 3 in sentiment classification. These findings partially confirm hypothesis H2, which stated that LLMs, when used in a zero-shot setup, perform comparably on text classification tasks in South Slavic languages as they do on English.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "labels",
                    "english",
                    "label",
                    "bosnian",
                    "classification",
                    "other",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluated how well current machine learning technologies handle text classification tasks in South Slavic languages. We compared fine-tuned BERT-like models with decoder-only generative large language models (LLMs) that are used in a zero-shot prompting setup across three tasks and three text domains: sentiment classification in parliamentary texts, news topic classification, topic classification in parliamentary texts, and automatic genre identification on web texts.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "web",
                    "texts",
                    "parliamentary",
                    "genre",
                    "news",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show that LLMs used with prompting, where only a brief task description and labels were provided, achieved strong results across all tasks and languages, particularly the closed-source GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite>, GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite> and Gemini 2.5 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> models. The performance of LLMs is comparable or higher to that of fine-tuned BERT-like models specialized for the tasks. On the sentiment identification task, most open-source and closed-source LLMs outperformed the fine-tuned model, demonstrating strong general knowledge of the notion of sentiment. For genre and topic classification, however, fine-tuning BERT-like models remain beneficial, as these tasks rely on predefined label sets and fine-tuning aligns the models more closely with the task requirements.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "labels",
                    "label",
                    "most",
                    "genre",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, LLMs perform similarly in English and South Slavic languages, with rather minor drops in micro- and macro-F1 scores, namely a drop of 2 to 7 points in terms of macro-F1 scores on sentiment classification, and a slightly higher drop from 3 to 10 points on topic classification in parliamentary texts. This suggests that the gap in multilingual performance is smaller than expected, even for open-source models not explicitly dedicated to these languages.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "texts",
                    "parliamentary",
                    "english",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although large language models offer impressive zero-shot performance and reduce the need for annotated data, they come with higher computational costs and are more prone to producing invalid labels. Moreover, their inference speed is at least three times slower than that of the fine-tuned BERT-like models. Thus, their use in use cases with extensive data to be processed, such as automatic enrichment of large corpora with text categories, remains impractical due to their high computational demands. In contrast, fine-tuned BERT-like models are more computationally efficient and can be better tailored to the specific characteristics of a task and its domain. They remain a practical and reliable choice for text classification tasks, especially when computational resources are limited, high inference speed is desired or output reliability is critical. Moreover, it is possible to combine the strengths of both approaches, as proposed by the LLM teacher-student pipeline paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>: LLMs can be used to automatically annotate training data, reducing the need for costly and time-consuming manual annotation, while fine-tuned BERT-like models can then be trained on these datasets.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "labels",
                    "datasets",
                    "least"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study represents only an initial step to systematically benchmark text classification performance in South Slavic languages. Although our evaluation includes four diverse benchmark families, some of the test datasets remain relatively small. Future work will aim to increase dataset sizes, include more South Slavic languages and dialects, and introduce additional classification tasks. As new large language models continue to emerge rapidly, it will also be important to establish ongoing evaluations to track whether their performance continues to improve, particularly on South Slavic languages. Importantly, this study only evaluated the performance of LLMs in a zero-shot prompting setup. In future work, we plan to extend the evaluation to include few-shot prompting and fine-tuning on training data. To support further research and facilitate reproducibility, we have made all code, evaluation scripts, and results publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "dataset",
                    "classification",
                    "datasets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations that should be acknowledged. First, while we aimed to include a broad set of South Slavic languages, some &#8211; most notably Bulgarian &#8211; were not covered in our experiments. We assume that the performance on Bulgarian would be similar to that observed for Macedonian, given their close linguistic proximity, or the results for Bulgarian could be slightly better, as Macedonian is comparatively more low-resourced. Moreover, due to the high computational cost of evaluating the LLMs on all the test datasets and the financial cost associated with the use of closed-source models, each model was evaluated on each dataset only once. This setup prevents us from fully estimating the variance of the results, however, based on our preliminary experiments, we expect this variance to be relatively small. Finally, the scope of our evaluation remains limited in terms of test datasets, language coverage and tasks. Expanding the range of benchmarks would allow for a more comprehensive validation of our findings, particularly regarding the hypothesis that LLMs can perform on par with fine-tuned BERT-like models across diverse natural language understanding tasks, languages and language varieties.</p>\n\n",
                "matched_terms": [
                    "macedonian",
                    "most",
                    "test",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to thank the developers of the llm.ijs.si service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite> for establishing the LLM inference platform deployed at the Jo&#382;ef Stefan Institute, which provided convenient access to the open-source large language models used in this study. We also thank the annotators of the test datasets for their diligence and the time devoted to manual annotation, which resulted in the high-quality evaluation datasets used in this work. Lastly, we would like to thank the <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/k-centre/\" title=\"\">CLASSLA knowledge centre for South Slavic languages</a> and the Slovenian <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/about/\" title=\"\">CLARIN.SI infrastructure</a> for their valuable support.</p>\n\n",
                "matched_terms": [
                    "slovenian",
                    "datasets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide additional information on the datasets used for benchmarking the models on sentiment identification, topic classification, and genre identification tasks in this study.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "genre",
                    "classification",
                    "sentiment",
                    "datasets",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">include Croatian, Serbian, Bosnian, and English data from the multilingual sentiment dataset of parliamentary debates ParlaSent 1.0 (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>, <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1585\" title=\"\">http://hdl.handle.net/11356/1585</a> and in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/classla/ParlaSent\" title=\"\">https://huggingface.co/datasets/classla/ParlaSent</a>.</span></span></span> The dataset comprises sentences that were randomly sampled from Croatian, Serbian, Bosnian and British parliamentary corpora and manually annotated with reported inter-annotator agreement ranging from 0.53 to 0.66 in Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. The annotation involved a more granular six-level sentiment polarity scale that has been mapped to a three-level sentiment polarity scale which we use in our experiments: negative (0), neutral (1), and positive (2).</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "croatian",
                    "positive",
                    "parliamentary",
                    "english",
                    "bosnian",
                    "neutral",
                    "dataset",
                    "sentiment",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise the English EN-GINCO dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> and a multilingual X-GINCO dataset from the AGILE benchmark for Automatic Genre Identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark\" title=\"\">https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark</a></span></span></span> The test instances were sampled from the enTenTen20 English web corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jakubivcek2013tenten</span>)</cite> and the MaCoCu multilingual web corpus collection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banon2022macocu</span>)</cite>. They were manually annotated by experts with a background in linguistics and computational linguistics who had experience with previous genre annotation campaigns <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-rupnik-ljubei:2022:LREC</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> where they reached an acceptable inter-annotator agreement of 0.71 in nominal Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. While the X-GINCO dataset comprises numerous European languages, for the purposes of this study, we focus on three South Slavic languages: Croatian, Macedonian, and Slovenian. The test datasets use the X-GENRE annotation schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> that includes the following genre labels: <span class=\"ltx_text ltx_font_italic\">Information/Explanation</span>, <span class=\"ltx_text ltx_font_italic\">News</span>, <span class=\"ltx_text ltx_font_italic\">Instruction</span>, <span class=\"ltx_text ltx_font_italic\">Opinion/Argumentation</span>, <span class=\"ltx_text ltx_font_italic\">Forum</span>, <span class=\"ltx_text ltx_font_italic\">Prose/Lyrical</span>, <span class=\"ltx_text ltx_font_italic\">Legal</span> and <span class=\"ltx_text ltx_font_italic\">Promotion</span>. While EN-GINCO and X-GINCO datasets have been annotated by the same annotator with the same schema, one should note that there are important differences between them in terms of their construction &#8211; the English test dataset was sampled randomly from the web corpus, resulting in an unbalanced label distribution, while the X-GINCO datasets were curated with more deliberate interventions to ensure a balanced label distribution and a more controlled sampling process. Consequently, the X-GINCO datasets comprise fewer ambiguous instances and could be regarded as an easier test dataset.</p>\n\n",
                "matched_terms": [
                    "label",
                    "news",
                    "legal",
                    "enginco",
                    "slovenian",
                    "instruction",
                    "proselyrical",
                    "macedonian",
                    "genre",
                    "test",
                    "opinionargumentation",
                    "croatian",
                    "labels",
                    "english",
                    "instances",
                    "dataset",
                    "datasets",
                    "web",
                    "promotion",
                    "informationexplanation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> comprise Croatian and Slovenian news articles extracted from the MaCoCu-Genre web corpus collection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">macocu-genre</span>)</cite> and manually annotated by one annotator. The reliability of the annotator was confirmed on a sample of data that was annotated by an additional annotator. The two annotators reached an acceptable inter-annotator agreement of 0.73 in nominal Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. Text instances are annotated with 17 topic labels from the top level of the IPTC NewsCodes Media Topic hierarchical schema, developed by the International Press Telecommunications Council (IPTC) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>. The datasets are more or less balanced by labels.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "croatian",
                    "labels",
                    "web",
                    "instances",
                    "articles",
                    "news",
                    "slovenian",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise parliamentary speeches in Bosnian, Croatian, English, and Serbian, sourced from the ParlaMint 4.1 dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>). These speeches were annotated by a single expert annotator using the 21 CAP categories from the official CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>, along with an additional <span class=\"ltx_text ltx_font_italic\">Other</span> label. The datasets are approximately balanced across labels. To assess the annotation quality, the Croatian dataset was independently annotated by two additional annotators. Inter-annotator agreement between the expert annotator and the others ranged from 0.62 to 0.68 in Krippendorff&#8217;s alpha, which is around the threshold of 0.67 typically considered acceptable for annotation reliability <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "speeches",
                    "croatian",
                    "labels",
                    "parliamentary",
                    "english",
                    "label",
                    "other",
                    "dataset",
                    "bosnian",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BERT (bidirectional encoder representations from transformers) deep neural models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bert</span>)</cite> have revolutionized the field of natural language processing (NLP), outperforming the non-neural methods across various NLP tasks. They have a more complex and computationally expensive architecture featuring transformers &#8211; neural networks with self-attention mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> &#8211; that significantly improves the efficiency of training models on massive text data. Similarly to decoder-only transformer models, BERT models are pretrained on massive amounts of texts, possibly in multiple languages, which establishes their ability to encode the words and texts in high-dimensional vector spaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minaee2020deep</span>)</cite> and enables their application even across languages in a zero-shot classification scenario. To develop BERT-based classifiers, the pretrained models are trained, that is, fine-tuned, on a training dataset comprising text instances annotated with labels. In our study, we evaluate openly-accessible multilingual fine-tuned BERT-like models that have been already developed in recent related research. Namely, we evaluate the following models:</p>\n\n",
                "matched_terms": [
                    "labels",
                    "texts",
                    "instances",
                    "classification",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IPTC News Topic classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><span class=\"ltx_text ltx_font_medium\">The IPTC News Topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier\" title=\"\">https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> is a multilingual fine-tuned BERT-like model for news topic classification according to the top-level IPTC NewsCodes schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>. The model is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on 15,000 training text instances from the EMMediaTopic<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>The EMMediaTopic training dataset is available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1991\" title=\"\">http://hdl.handle.net/11356/1991</a>.</span></span></span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite>. The training dataset contains news article instances in four languages: Catalan, Croatian, Greek, and Slovenian. The training dataset was annotated using an LLM that was shown to achieve annotation reliability comparable to that of human annotators <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. This approach is based on the novel methodology that uses the LLM teacher-student pipeline to develop BERT-like classifiers in the absence of manually-annotated training data.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "croatian",
                    "instances",
                    "news",
                    "classification",
                    "dataset",
                    "slovenian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-R-ParlaSent</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) is a domain-specific multilingual transformer model for sentiment identification in parliamentary texts. It is based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on 1.72 billion words from parliamentary proceedings in 30 European languages.\nTo develop the XLM-R-ParlaSent model,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The XLM-R-ParlaSent model is accessible in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/classla/xlm-r-parlasent\" title=\"\">https://huggingface.co/classla/xlm-r-parlasent</a>.</span></span></span> the pretrained XLM-R-Parla model was fine-tuned on the ParlaSent sentiment training dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian). The training dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>The ParlaSent training and test datasets are freely available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1868\" title=\"\">http://hdl.handle.net/11356/1868</a>.</span></span></span> comprises 13,000 instances sampled from parliamentary proceedings and manually annotated with sentiment labels.</p>\n\n",
                "matched_terms": [
                    "serbian",
                    "croatian",
                    "labels",
                    "texts",
                    "parliamentary",
                    "english",
                    "instances",
                    "bosnian",
                    "slovenian",
                    "test",
                    "dataset",
                    "sentiment",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ParlaCAP classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">12</span></span><span class=\"ltx_text ltx_font_medium\">The ParlaCAP topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/ParlaCAP-Topic-Classifier\" title=\"\">https://huggingface.co/classla/ParlaCAP-Topic-Classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> is a domain-specific multilingual transformer model for topic classification in parliamentary texts based on the CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>. As the XLM-R-ParlaSent model, this model is based on the XLM-R-parla pretrained model (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>). The XLM-R-parla model was then fine-tuned on around 30 thousand speeches from parliamentary debates from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) in 29 European languages. The training dataset was annotated with the CAP categories by a GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> model used in a zero-shot prompting fashion, following the LLM teacher-student framework <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. Based on the inter-annotator agreement, calculated on a sample that was annotated by three human annotators and the LLM annotator, the agreement between the LLM and the human annotators was comparable to the agreement between the human annotators themselves. This indicates that the LLM performs as reliably as human annotators on this task, supporting its use for annotating the training data.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "topic",
                    "texts",
                    "parliamentary",
                    "classification",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">X-GENRE classifier</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span></cite>) is a multilingual fine-tuned BERT-like model for automatic genre identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span>The X-GENRE classifier is freely available in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.57967/hf/0927\" title=\"\">https://doi.org/10.57967/hf/0927</a> and the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1961\" title=\"\">http://hdl.handle.net/11356/1961</a>.</span></span></span> The model is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite>, which contains 1,772 text instances in Slovenian and English, manually-annotated with genre labels from the X-GENRE schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "instances",
                    "english",
                    "genre",
                    "dataset",
                    "slovenian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLaMA 3.3 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">14</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/llama3.3\" title=\"\">https://ollama.com/library/llama3.3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite> is an open-source instruction-tuned multilingual LLM, developed by Meta, with 70 billion parameters. The model was pretrained on a web text collection in various languages, however, it is reported to support only 8 languages, namely, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</p>\n\n",
                "matched_terms": [
                    "web",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DeepSeek-R1-Distill<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">16</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/deepseek-r1:14b\" title=\"\">https://ollama.com/library/deepseek-r1:14b</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite> is an open-source reasoning LLM, developed by DeepSeek AI. We use the distilled model in 14 billion parameter size, namely the <span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span> model. The model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that was fine-tuned using a dataset curated with the DeepSeek-R1 reasoning model. The Qwen 2.5 model provides multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, and Arabic.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent any bias, all models were used with their default parameters. The only parameter that we defined is the temperature which we set to 0 to ensure a more deterministic behaviour of the models. The same prompts were used for all open-source and closed-source models. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4\" title=\"Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we provide prompts that were provided to the LLMs for zero-shot text classification, namely for sentiment classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf1\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>), automatic genre identification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf2\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a>), news topic classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf3\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(c)</span></a>) and topic classification in parliamentary speeches (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf4\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(d)</span></a>). For more details on the setups used to apply fine-tuned BERT-like models and instruction-tuned LLMs to the test datasets, refer to the code published on GitHub.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "speeches",
                    "parliamentary",
                    "genre",
                    "news",
                    "classification",
                    "test",
                    "sentiment",
                    "datasets"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Comparison of models based on their average rank (1 = best-performing, 10 = worst-performing) across all test datasets (first column), and averaged across English (second column) or South Slavic (third column) test datasets.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Model</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Rank</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Rank (EN)</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Rank (South Slavic)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.29</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.55</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-4o</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.36</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.45</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Fine-Tuned BERT-Like Model</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3.21</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">4.67</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2.82</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Gemini 2.5 Flash</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3.50</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3.55</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Mistral Medium 3.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.36</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.45</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Gemma 3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.71</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.67</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.73</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">LLaMA 3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">6.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">6.67</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">5.82</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Qwen 3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">7.43</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">7.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">7.55</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-3.5-Turbo</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">8.79</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">9.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">8.73</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"width:173.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">DeepSeek-R1-Distill</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t\" style=\"width:43.4pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10.00</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "slavic",
            "column",
            "second",
            "finetuned",
            "their",
            "comparison",
            "based",
            "average",
            "deepseekr1distill",
            "qwen",
            "worstperforming",
            "third",
            "test",
            "first",
            "south",
            "gemma",
            "across",
            "medium",
            "english",
            "llama",
            "averaged",
            "gpt5",
            "datasets",
            "bestperforming",
            "gpt4o",
            "gemini",
            "gpt35turbo",
            "rank",
            "model",
            "all",
            "bertlike",
            "flash",
            "mistral"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1\" title=\"Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides results of model evaluation on our selection of text classification tasks. A consistent pattern emerges across all four benchmark families: LLMs, when used in a zero-shot prompting setup, achieve some of the highest scores. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T2\" title=\"Table 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which compares model rankings across tasks, LLMs achieve first place more often on average than the fine-tuned BERT-like model.</p>\n\n",
            "<p class=\"ltx_p\">To conclude, since some LLMs used in a zero-shot prompting setup achieve higher or comparable results to fine-tuned BERT-like models across all classification tasks and languages, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T2\" title=\"Table 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we can confirm hypothesis H1, which proposed that zero-shot prompting with LLMs can perform comparably to fine-tuned BERT-like models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>LLM evaluation, text classification, large language models, South Slavic languages, sentiment identification, topic classification, genre identification</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "across",
                    "slavic",
                    "english",
                    "finetuned",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">State of the Art in Text Classification for South Slavic Languages:\n<br class=\"ltx_break\"/>Fine-Tuning or Prompting?</span>\n</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">Until recently, the dominant approach for text classification tasks relied on fine-tuning BERT-like transformer models on thousands of manually-annotated training examples. Recently, however, the field has shifted with the development of instruction-tuned decoder-only transformer models. These models, also commonly referred to as large language models (LLMs), which were originally developed primarily for text generation tasks, have demonstrated remarkable capabilities across a broad range of natural language processing (NLP) tasks, including text classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">In this paper, we focus on South Slavic languages, where research on text classification tasks included in our study has, until recently, been limited or even non-existent <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023survey</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. We take a first step toward systematically evaluating the current state of the art for text classification in these languages. Our evaluation is based on three text classification tasks in three different domains for which manually-annotated test datasets in South Slavic languages and fine-tuned BERT-like classifiers are freely available: sentiment classification of parliamentary speeches, topic classification in news articles, topic classification in parliamentary speeches, and automatic genre identification in web texts. These tasks span different domains and language styles, allowing for a comprehensive analysis of the performance of transformer-based models on text classification tasks. Specifically, we compare the performance of openly available fine-tuned BERT-like models with the zero-shot capabilities of both open-source and closed-source LLMs used via prompting.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "datasets",
                    "based",
                    "finetuned",
                    "bertlike",
                    "test",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">An important aspect of our study is to examine whether the performance of multilingual models on South Slavic languages is on par with their performance on English. This question is particularly relevant given that the evaluated large language models have been predominantly pretrained and instruction-tuned on English data.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "english",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">By evaluating various models on a selection of text classification tasks in English and various South Slavic languages, we set out to test the following two hypotheses that are based on previous experiments with fine-tuned BERT-like models and LLMs on automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> and sentiment analysis in parliamentary texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2025parlasent</span>)</cite>:</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "based",
                    "english",
                    "finetuned",
                    "bertlike",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot prompting with instruction-tuned large language models (LLMs) can achieve results comparable to the use of BERT-like models fine-tuned on training data that are similar to the test data.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "models",
                    "bertlike",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of LLMs used in a zero-shot setup on text classification tasks on South Slavic test datasets is comparable to the performance on English test datasets.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "english",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instruction-tuned decoder-only transformer models, commonly referred to as large language models (LLMs), have recently shown strong performance in a range of classification tasks, even in zero-shot prompting setups that require no training data <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024dialect</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>. They have achieved promising results on various natural language processing tasks, including stance detection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022would</span>)</cite>, implicit hate speech categorization <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, causal commonsense reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024jsi</span>)</cite>, and machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hendy2023good</span>)</cite>. Due to their promising performance, researchers have even started using them as data annotators, either by generating text and labels <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meng2022generating</span>)</cite> or by annotating pre-existing texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>.\nDespite the growing interest in this topic, the majority of evaluations of LLMs used in text classification tasks are limited only to English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sun2023text</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025pushing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kostina2025large</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2024advancing</span>)</cite>. Systematic multilingual evaluations, especially which would include less-resourced languages such as those in the South Slavic group remain limited. Our work addresses this gap by providing a comparative evaluation of open-source and closed-source LLMs with openly-available fine-tuned BERT-like models across four benchmark families comprising three diverse classification tasks and three different domains in South Slavic languages and English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "across",
                    "slavic",
                    "english",
                    "finetuned",
                    "their",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The benchmarks (evaluation datasets) used in this study cover three text classification tasks, namely, sentiment identification, topic classification, and automatic genre identification, and three domains: parliamentary speeches, news articles and web texts. An overview of the datasets is provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S1\" title=\"1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The four benchmark families differ significantly in terms of language coverage, number of test instances, and label granularity.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The topic classification task is evaluated on two domains: 1) news articles, namely, the Croatian and Slovenian IPTC test datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which comprise around 300 text instances per language, and 2) parliamentary speeches, namely, the Bosnian, Croatian, English and Serbian ParlaCAP test datasets that consist of approximately 820 to 880 instances per language. In the ParlaCAP benchmarks, an instance is a transcription of an utterance given by a parliamentary member in a parliamentary session.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "english",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Bosnian, Croatian, English, and Serbian ParlaSent sentiment identification datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) have a significantly lower granularity of labels, with only 3 categories. They are represented by the largest number of instances, ranging from 190 (Bosnian part) to 2600 (English part) sentence-level instances.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With 8 labels, the Croatian, English, Macedonian, and Slovenian GINCO genre datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> represent a midpoint in label granularity among the four benchmark families. However, the genre identification task might be the most difficult one, as genre identification depends on the interpretation of full texts with the focus on author&#8217;s purpose, the common function of the text, and the text&#8217;s conventional form <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">orlikowski1994genre</span>)</cite>. This complexity has also contributed to smaller test datasets in terms of the number of text instances, as manual annotation is more time-consuming. It is also important to note that, unlike the parliamentary datasets, the English portion of the genre datasets is not fully comparable to the South Slavic portions, which are label-balanced and contain fewer ambiguous instances. Nevertheless, the genre datasets remain valuable for evaluating model performance within each language.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "english",
                    "model",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All test datasets were manually annotated by annotators that are deemed reliable based on their satisfactory inter-annotator agreement, namely, Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite> values close to or above the 0.667 threshold for reliable annotation. To prevent large language models from incorporating the test datasets during their training phase, the test datasets are not publicly available, except for the ParlaSent benchmark family. Access to other datasets is granted on request from the corresponding authors. Further details on the test datasets are provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS1\" title=\"A.1. Benchmarking Datasets &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "based",
                    "their",
                    "all",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluate the main machine learning approaches that have recently been used for our selection of text classification tasks, with the focus on the comparison between the freely available fine-tuned BERT models and the open-source and closed-source LLMs.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The code for the model evaluation and analysis of results is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a>.</span></span></span>\nThe models are evaluated on four families of test datasets that comprise South Slavic languages. The performance of the models is evaluated based on the micro-F1 and macro-F1 metrics, which enable assessment of the model performance at both the instance and label levels, respectively.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "datasets",
                    "based",
                    "finetuned",
                    "model",
                    "test",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">dummy classifier</span>: a dummy classifier that predicts the most frequent class in the training data. To allow comparison, the dummy classifiers were trained on the same datasets that were used for fine-tuning the BERT-like models, mentioned below.</p>\n\n",
                "matched_terms": [
                    "bertlike",
                    "models",
                    "datasets",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">fine-tuned BERT-like classifiers</span>: in our study, we evaluate previously developed openly accessible multilingual fine-tuned BERT-like models that have been fine-tuned for the respective task, namely, the XLM-R-ParlaSent (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) model for sentiment identification in parliamentary texts, the X-GENRE classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-classifier-huggingface</span></cite>) for automatic genre identification, the IPTC News Topic classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptc_model_huggingface</span></cite>) for news topic classification, and the ParlaCAP classifier <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> for topic classification in parliamentary speeches. The XLM-R-ParlaSent and the ParlaCAP models are based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on parliamentary proceedings in 30 European languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>. The XLM-R-ParlaSent model was fine-tuned on 13 thousand instances from the ParlaSent sentiment training dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span>)</cite> in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>, while the ParlaCAP model was fine-tuned on around 30 thousand speeches from parliamentary debates annotated with CAP topic labels, originating from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) from 29 European parliaments. The X-GENRE classifier is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite> in English and Slovenian; while the IPTC News Topic classifier is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> that was fine-tuned on the EMMediaTopic dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite> in Catalan, Croatian, Greek, and Slovenian. All fine-tuned models use the same classes as the test datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "models",
                    "based",
                    "english",
                    "finetuned",
                    "model",
                    "all",
                    "bertlike",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">open-source and closed-source large language models</span>: we use closed-source OpenAI models, namely the GPT-3.5-Turbo (<span class=\"ltx_text ltx_font_typewriter\">gpt-3.5-turbo-0125</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai</span>)</cite>, GPT-4o (<span class=\"ltx_text ltx_font_typewriter\">gpt-4o-2024-08-06</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> and the GPT-5 (<span class=\"ltx_text ltx_font_typewriter\">gpt-5-2025-08-07</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite>; a closed-source Gemini 2.5 Flash model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> by Google DeepMind; a closed-source Mistral Medium 3.1 model (<span class=\"ltx_text ltx_font_typewriter\">mistral-medium-2508</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mistral</span>)</cite> by Mistral AI; and four open-source models, namely, the Meta LLaMA 3.3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>, the Gemma 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, the Qwen 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>, and the DeepSeek-R1-Distill model (<span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite>. It is important to note that while the LLaMA model was pretrained on a web text collection in various languages, it is said to support only 8 languages, namely English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>. The DeepSeek-R1-Distill model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that provides support for more than 29 languages &#8211; not including South Slavic languages though. In contrast, the Gemma 3 model is reported to support over 140 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, and the Qwen 3 model was pretrained on 119 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>. While closed-source models are said to be massively multilingual, with Gemini 2.5 models being pretrained on over 400 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite>, details on their language coverage are very limited.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "gemma",
                    "slavic",
                    "medium",
                    "based",
                    "english",
                    "gpt4o",
                    "gemini",
                    "llama",
                    "their",
                    "deepseekr1distill",
                    "gpt35turbo",
                    "model",
                    "qwen",
                    "flash",
                    "gpt5",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Open-source models were installed locally and executed via the Ollama API service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite>. OpenAI models were used through the chat completion endpoint via the OpenAI API, whereas other closed-source models were accessed through the OpenRouter platform<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openrouter.ai/\" title=\"\">https://openrouter.ai/</a></span></span></span> that provides a unified API access to various closed-source models. To prevent any bias, all models were used with their default parameters. The only parameter that we defined is the temperature which we set to 0 to ensure a more deterministic behaviour of the models. More details on the models and their implementation, including information on the availability of openly available models and fine-tuning datasets, are provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS2\" title=\"A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "their",
                    "all",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All instruction-tuned LLMs are used in a zero-shot prompting setup, meaning that they receive only a task description and label definitions.\nThe models are instructed to output a label, represented by a digit. The same prompt per benchmark family is used for all LLMs. Prompts are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4\" title=\"Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS2\" title=\"A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of fine-tuned BERT-like models and the instruction-tuned LLMs on a selection of text classification tasks that include test datasets in South Slavic languages. First, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS1\" title=\"5.1. State of the Art in Text Classification Tasks &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we provide results on the four benchmark families with the focus on hypothesis H1, which expects that zero-shot prompting with LLMs can provide performance that is comparable to that of fine-tuned BERT-like models. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS2\" title=\"5.2. Comparison of Large Language Models &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, we compare in more detail the performance of the closed-source and open-source LLMs on the three text classification tasks, which is followed by a discussion on the advantages and limitations of LLMs for data annotation based on text classification tasks (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS3\" title=\"5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Lastly, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS4\" title=\"5.4. Performance on English versus on South Slavic languages &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>, we compare the performance of LLMs on English test datasets with their performance on South Slavic datasets, addressing hypothesis H2, which presumes that the available multilingual LLMs perform similarly on South Slavic languages as on English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "datasets",
                    "based",
                    "english",
                    "finetuned",
                    "their",
                    "bertlike",
                    "test",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1.sf1\" title=\"In Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> shows that both open-source and closed-source LLMs, used in a zero-shot prompting setup on the sentiment identification task, achieve performance that is comparable or even significantly higher to that of a fine-tuned BERT-like model trained on a large manually-annotated sentiment dataset. The only models that consistently perform worse than the fine-tuned BERT-like model are GPT-3.5-Turbo and DeepSeek-R1-Distill. Sentiment classification appears broad enough that more potent LLMs can interpret label definitions effectively without task-specific fine-tuning, reducing the benefit of additional training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpt35turbo",
                    "finetuned",
                    "deepseekr1distill",
                    "model",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, fine-tuned BERT-like models outperform most LLMs on automatic genre identification and topic classification tasks. These tasks depend on predefined label sets based on specific guidelines, and the strong performance of fine-tuned BERT-like models indicates that domain-specific fine-tuning on labelled data still offers an advantage over the general knowledge leveraged by LLMs in zero-shot setups. This advantage is particularly clear in genre identification for South Slavic texts, where the fine-tuned BERT-like model significantly outperforms LLMs. The likely reason for the fine-tuned model&#8217;s very strong performance on South Slavic genre datasets is the curated nature of the test data &#8211; more challenging examples were removed before and during manual annotation, unlike in the English genre test dataset where the instances were randomly sampled from an English web corpus. Nevertheless, despite this limitation, the South Slavic test dataset remains valuable for comparing the performance of LLMs.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "based",
                    "english",
                    "finetuned",
                    "model",
                    "bertlike",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2\" title=\"Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the performance of open-source and closed-source LLMs, used via prompting, on the tasks of sentiment identification, automatic genre identification, news topic classification, and parliamentary topic classification. The DeepSeek-R1-Distill model is not included in the comparison, as it performs significantly worse than other models, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1\" title=\"Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "deepseekr1distill",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While different models perform best across different languages and test datasets, a clear trend emerges: the top-performing models across all four benchmark families are the closed-source GPT-4o and GPT-5 from OpenAI, along with Gemini 2.5 Flash. Although GPT-5 is newer and reportedly more powerful, it does not outperform GPT-4o on all benchmarks. Among open-source models, Gemma 3 generally achieves the best results in sentiment identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>) and news topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>). For automatic genre identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>) and parliamentary topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>), rankings of open-source models vary by language. Overall, the weakest performance is observed with the older closed-source GPT-3.5-Turbo model, highlighting the rapid progress in both open-source and closed-source model development.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "models",
                    "across",
                    "gpt4o",
                    "gemini",
                    "gpt35turbo",
                    "model",
                    "all",
                    "flash",
                    "test",
                    "gpt5",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A clear advantage of LLMs is that they do not require manually-annotated training data for specific tasks, yet still achieve strong performance when provided only with task instructions and brief label descriptions. However, these models are significantly more computationally expensive than fine-tuned BERT-like models. While closed-source models deliver the best performance, as shown in previous sections, they come with several limitations: they are costly to use, their architectures and pre-training data are not publicly disclosed, and access through APIs hinders reproducibility, in contrast to open-source LLMs and fine-tuned BERT-like models.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "models",
                    "their",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What is more, the inference speed of all LLMs is significantly slower than that of a fine-tuned BERT-like model. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F3\" title=\"Figure 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the fine-tuned BERT-like model achieves one of the highest macro-F1 scores on the topic classification task for parliamentary speeches, while maintaining a very low inference time of just 0.02 seconds per instance. In contrast, most LLMs have inference times between 0.6 and 1.4 seconds per instance, making them three to seven times slower for annotating the same dataset. The slowest model, GPT-5, takes 5.5 seconds per instance, which renders it impractical for large-scale automatic annotation of text collections. In this regard, fine-tuned BERT-like models offer a key advantage due to their lower computational cost and higher inference speed. Moreover, they can be trained on training data that is annotated by LLMs using the recently introduced LLM teacher-student paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which considerably reduces the effort needed to develop task-specific models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "their",
                    "model",
                    "all",
                    "bertlike",
                    "gpt5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another limitation of LLMs, as revealed by the experiments, is their occasional deviation from the defined label set. This issue was especially noticeable in topic classification and, to a lesser extent, in genre identification. The highest rate of label hallucination was found in the DeepSeek-R1-Distill model, which produced non-existing labels for 8% of instances in the news topic test datasets and 4% in the genre test dataset. Similar issues were also observed, though much less frequently (less than 1%), with the LLaMA 3.3, Gemma 3, Qwen 3 and Mistral Medium 3.1 models. In contrast, fine-tuned BERT-like models do not suffer from this issue, as they output probabilities for the predefined classes.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "models",
                    "medium",
                    "finetuned",
                    "llama",
                    "their",
                    "deepseekr1distill",
                    "model",
                    "qwen",
                    "bertlike",
                    "test",
                    "mistral",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentiment identification ParlaSent and the topic classification ParlaCAP benchmark families comprise test datasets in South Slavic languages and English that were constructed with the same methodology. Thus, they also allow for a comparison of the performance of the LLMs on English, a highly resourced language, with South Slavic languages, which are significantly less represented in the pretraining and instruction-tuning datasets used to develop large language models.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "datasets",
                    "english",
                    "test",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>, LLMs generally perform worse on Bosnian compared to other languages. However, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T3\" title=\"Table 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the differences in macro-F1 scores between English and the average of macro-F1 scores for South Slavic languages are relatively small for sentiment identification, ranging from 2 to 7 points. For topic classification, the performance gap is slightly larger, ranging from 3 to 10 points. This is likely due to the increased difficulty of the task, which involves greater label granularity: 22 labels compared to just 3 in sentiment classification. These findings partially confirm hypothesis H2, which stated that LLMs, when used in a zero-shot setup, perform comparably on text classification tasks in South Slavic languages as they do on English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "average",
                    "slavic",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, even the open-source LLaMA 3.3 model &#8211; reported to support only eight languages, excluding the South Slavic group &#8211; does not show a substantial performance drop when applied to South Slavic languages compared to English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "english",
                    "llama",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluated how well current machine learning technologies handle text classification tasks in South Slavic languages. We compared fine-tuned BERT-like models with decoder-only generative large language models (LLMs) that are used in a zero-shot prompting setup across three tasks and three text domains: sentiment classification in parliamentary texts, news topic classification, topic classification in parliamentary texts, and automatic genre identification on web texts.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "across",
                    "slavic",
                    "finetuned",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show that LLMs used with prompting, where only a brief task description and labels were provided, achieved strong results across all tasks and languages, particularly the closed-source GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite>, GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite> and Gemini 2.5 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> models. The performance of LLMs is comparable or higher to that of fine-tuned BERT-like models specialized for the tasks. On the sentiment identification task, most open-source and closed-source LLMs outperformed the fine-tuned model, demonstrating strong general knowledge of the notion of sentiment. For genre and topic classification, however, fine-tuning BERT-like models remain beneficial, as these tasks rely on predefined label sets and fine-tuning aligns the models more closely with the task requirements.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "gpt4o",
                    "gemini",
                    "finetuned",
                    "model",
                    "all",
                    "bertlike",
                    "flash",
                    "gpt5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, LLMs perform similarly in English and South Slavic languages, with rather minor drops in micro- and macro-F1 scores, namely a drop of 2 to 7 points in terms of macro-F1 scores on sentiment classification, and a slightly higher drop from 3 to 10 points on topic classification in parliamentary texts. This suggests that the gap in multilingual performance is smaller than expected, even for open-source models not explicitly dedicated to these languages.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although large language models offer impressive zero-shot performance and reduce the need for annotated data, they come with higher computational costs and are more prone to producing invalid labels. Moreover, their inference speed is at least three times slower than that of the fine-tuned BERT-like models. Thus, their use in use cases with extensive data to be processed, such as automatic enrichment of large corpora with text categories, remains impractical due to their high computational demands. In contrast, fine-tuned BERT-like models are more computationally efficient and can be better tailored to the specific characteristics of a task and its domain. They remain a practical and reliable choice for text classification tasks, especially when computational resources are limited, high inference speed is desired or output reliability is critical. Moreover, it is possible to combine the strengths of both approaches, as proposed by the LLM teacher-student pipeline paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>: LLMs can be used to automatically annotate training data, reducing the need for costly and time-consuming manual annotation, while fine-tuned BERT-like models can then be trained on these datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "their",
                    "bertlike",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study represents only an initial step to systematically benchmark text classification performance in South Slavic languages. Although our evaluation includes four diverse benchmark families, some of the test datasets remain relatively small. Future work will aim to increase dataset sizes, include more South Slavic languages and dialects, and introduce additional classification tasks. As new large language models continue to emerge rapidly, it will also be important to establish ongoing evaluations to track whether their performance continues to improve, particularly on South Slavic languages. Importantly, this study only evaluated the performance of LLMs in a zero-shot prompting setup. In future work, we plan to extend the evaluation to include few-shot prompting and fine-tuning on training data. To support further research and facilitate reproducibility, we have made all code, evaluation scripts, and results publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "their",
                    "all",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations that should be acknowledged. First, while we aimed to include a broad set of South Slavic languages, some &#8211; most notably Bulgarian &#8211; were not covered in our experiments. We assume that the performance on Bulgarian would be similar to that observed for Macedonian, given their close linguistic proximity, or the results for Bulgarian could be slightly better, as Macedonian is comparatively more low-resourced. Moreover, due to the high computational cost of evaluating the LLMs on all the test datasets and the financial cost associated with the use of closed-source models, each model was evaluated on each dataset only once. This setup prevents us from fully estimating the variance of the results, however, based on our preliminary experiments, we expect this variance to be relatively small. Finally, the scope of our evaluation remains limited in terms of test datasets, language coverage and tasks. Expanding the range of benchmarks would allow for a more comprehensive validation of our findings, particularly regarding the hypothesis that LLMs can perform on par with fine-tuned BERT-like models across diverse natural language understanding tasks, languages and language varieties.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "across",
                    "slavic",
                    "datasets",
                    "based",
                    "finetuned",
                    "their",
                    "model",
                    "all",
                    "bertlike",
                    "test",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to thank the developers of the llm.ijs.si service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite> for establishing the LLM inference platform deployed at the Jo&#382;ef Stefan Institute, which provided convenient access to the open-source large language models used in this study. We also thank the annotators of the test datasets for their diligence and the time devoted to manual annotation, which resulted in the high-quality evaluation datasets used in this work. Lastly, we would like to thank the <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/k-centre/\" title=\"\">CLASSLA knowledge centre for South Slavic languages</a> and the Slovenian <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/about/\" title=\"\">CLARIN.SI infrastructure</a> for their valuable support.</p>\n\n",
                "matched_terms": [
                    "south",
                    "models",
                    "slavic",
                    "their",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work was supported in part by the projects &#8220;Spoken Language Resources and Speech Technologies for the Slovenian Language&#8221; (Grant J7-4642), &#8220;Large Language Models for Digital Humanities&#8221; (Grant GC-0002), the research programme &#8220;Language Resources and Technologies for Slovene&#8221; (Grant P6-0411), all funded by the ARIS Slovenian Research and Innovation Agency, and the research project &#8220;Embeddings-based techniques for Media Monitoring Applications&#8221; (L2-50070), co-funded by the Kliping d.o.o. agency. The authors acknowledge the OSCARS project &#8211; and its ParlaCAP cascading grant project &#8211;, which has received funding from the European Commission&#8217;s Horizon Europe Research and Innovation programme under grant agreement No. 101129751.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide additional information on the datasets used for benchmarking the models on sentiment identification, topic classification, and genre identification tasks in this study.</p>\n\n",
                "matched_terms": [
                    "models",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise the English EN-GINCO dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> and a multilingual X-GINCO dataset from the AGILE benchmark for Automatic Genre Identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark\" title=\"\">https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark</a></span></span></span> The test instances were sampled from the enTenTen20 English web corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jakubivcek2013tenten</span>)</cite> and the MaCoCu multilingual web corpus collection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banon2022macocu</span>)</cite>. They were manually annotated by experts with a background in linguistics and computational linguistics who had experience with previous genre annotation campaigns <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-rupnik-ljubei:2022:LREC</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> where they reached an acceptable inter-annotator agreement of 0.71 in nominal Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. While the X-GINCO dataset comprises numerous European languages, for the purposes of this study, we focus on three South Slavic languages: Croatian, Macedonian, and Slovenian. The test datasets use the X-GENRE annotation schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> that includes the following genre labels: <span class=\"ltx_text ltx_font_italic\">Information/Explanation</span>, <span class=\"ltx_text ltx_font_italic\">News</span>, <span class=\"ltx_text ltx_font_italic\">Instruction</span>, <span class=\"ltx_text ltx_font_italic\">Opinion/Argumentation</span>, <span class=\"ltx_text ltx_font_italic\">Forum</span>, <span class=\"ltx_text ltx_font_italic\">Prose/Lyrical</span>, <span class=\"ltx_text ltx_font_italic\">Legal</span> and <span class=\"ltx_text ltx_font_italic\">Promotion</span>. While EN-GINCO and X-GINCO datasets have been annotated by the same annotator with the same schema, one should note that there are important differences between them in terms of their construction &#8211; the English test dataset was sampled randomly from the web corpus, resulting in an unbalanced label distribution, while the X-GINCO datasets were curated with more deliberate interventions to ensure a balanced label distribution and a more controlled sampling process. Consequently, the X-GINCO datasets comprise fewer ambiguous instances and could be regarded as an easier test dataset.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "english",
                    "their",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise parliamentary speeches in Bosnian, Croatian, English, and Serbian, sourced from the ParlaMint 4.1 dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>). These speeches were annotated by a single expert annotator using the 21 CAP categories from the official CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>, along with an additional <span class=\"ltx_text ltx_font_italic\">Other</span> label. The datasets are approximately balanced across labels. To assess the annotation quality, the Croatian dataset was independently annotated by two additional annotators. Inter-annotator agreement between the expert annotator and the others ranged from 0.62 to 0.68 in Krippendorff&#8217;s alpha, which is around the threshold of 0.67 typically considered acceptable for annotation reliability <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "datasets",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following subsections, we outline the models included in the evaluation &#8211; the fine-tuned BERT-like classifiers (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS2.SSS1\" title=\"A.2.1. Fine-Tuned BERT-like Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>) and the open-source and closed-source LLMs (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.SS2.SSS2\" title=\"A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "models",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BERT (bidirectional encoder representations from transformers) deep neural models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bert</span>)</cite> have revolutionized the field of natural language processing (NLP), outperforming the non-neural methods across various NLP tasks. They have a more complex and computationally expensive architecture featuring transformers &#8211; neural networks with self-attention mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> &#8211; that significantly improves the efficiency of training models on massive text data. Similarly to decoder-only transformer models, BERT models are pretrained on massive amounts of texts, possibly in multiple languages, which establishes their ability to encode the words and texts in high-dimensional vector spaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minaee2020deep</span>)</cite> and enables their application even across languages in a zero-shot classification scenario. To develop BERT-based classifiers, the pretrained models are trained, that is, fine-tuned, on a training dataset comprising text instances annotated with labels. In our study, we evaluate openly-accessible multilingual fine-tuned BERT-like models that have been already developed in recent related research. Namely, we evaluate the following models:</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "finetuned",
                    "their",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IPTC News Topic classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><span class=\"ltx_text ltx_font_medium\">The IPTC News Topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier\" title=\"\">https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> is a multilingual fine-tuned BERT-like model for news topic classification according to the top-level IPTC NewsCodes schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>. The model is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on 15,000 training text instances from the EMMediaTopic<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>The EMMediaTopic training dataset is available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1991\" title=\"\">http://hdl.handle.net/11356/1991</a>.</span></span></span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite>. The training dataset contains news article instances in four languages: Catalan, Croatian, Greek, and Slovenian. The training dataset was annotated using an LLM that was shown to achieve annotation reliability comparable to that of human annotators <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. This approach is based on the novel methodology that uses the LLM teacher-student pipeline to develop BERT-like classifiers in the absence of manually-annotated training data.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "bertlike",
                    "model",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-R-ParlaSent</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) is a domain-specific multilingual transformer model for sentiment identification in parliamentary texts. It is based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on 1.72 billion words from parliamentary proceedings in 30 European languages.\nTo develop the XLM-R-ParlaSent model,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The XLM-R-ParlaSent model is accessible in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/classla/xlm-r-parlasent\" title=\"\">https://huggingface.co/classla/xlm-r-parlasent</a>.</span></span></span> the pretrained XLM-R-Parla model was fine-tuned on the ParlaSent sentiment training dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian). The training dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>The ParlaSent training and test datasets are freely available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1868\" title=\"\">http://hdl.handle.net/11356/1868</a>.</span></span></span> comprises 13,000 instances sampled from parliamentary proceedings and manually annotated with sentiment labels.</p>\n\n",
                "matched_terms": [
                    "based",
                    "english",
                    "finetuned",
                    "model",
                    "test",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ParlaCAP classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">12</span></span><span class=\"ltx_text ltx_font_medium\">The ParlaCAP topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/ParlaCAP-Topic-Classifier\" title=\"\">https://huggingface.co/classla/ParlaCAP-Topic-Classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> is a domain-specific multilingual transformer model for topic classification in parliamentary texts based on the CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>. As the XLM-R-ParlaSent model, this model is based on the XLM-R-parla pretrained model (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>). The XLM-R-parla model was then fine-tuned on around 30 thousand speeches from parliamentary debates from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) in 29 European languages. The training dataset was annotated with the CAP categories by a GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> model used in a zero-shot prompting fashion, following the LLM teacher-student framework <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. Based on the inter-annotator agreement, calculated on a sample that was annotated by three human annotators and the LLM annotator, the agreement between the LLM and the human annotators was comparable to the agreement between the human annotators themselves. This indicates that the LLM performs as reliably as human annotators on this task, supporting its use for annotating the training data.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "based",
                    "finetuned",
                    "model",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">X-GENRE classifier</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span></cite>) is a multilingual fine-tuned BERT-like model for automatic genre identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span>The X-GENRE classifier is freely available in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.57967/hf/0927\" title=\"\">https://doi.org/10.57967/hf/0927</a> and the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1961\" title=\"\">http://hdl.handle.net/11356/1961</a>.</span></span></span> The model is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite>, which contains 1,772 text instances in Slovenian and English, manually-annotated with genre labels from the X-GENRE schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "based",
                    "english",
                    "finetuned",
                    "model",
                    "bertlike"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the BERT models, decoder-only large language models are based on a transformer deep neural architecture and are pretrained on massive text collections. However, while the development of fine-tuned BERT-like classifiers necessitates large amounts of annotated training data, recent advances in the field have shown that the instruction-tuned LLMs are capable of text classification in a zero-shot or few-shot prompting setups which does not require any training data. We assess the performance of the following large language models:</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "models",
                    "bertlike",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpenAI models</span>, namely the GPT-3.5-Turbo (<span class=\"ltx_text ltx_font_typewriter\">gpt-3.5-turbo-0125</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai</span>)</cite>, GPT-4o (<span class=\"ltx_text ltx_font_typewriter\">gpt-4o-2024-08-06</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> and the GPT-5 (<span class=\"ltx_text ltx_font_typewriter\">gpt-5-2025-08-07</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite>. These closed-source instruction-tuned LLMs were developed by OpenAI. OpenAI states that the models are trained on large multilingual web corpora, however, specific details about the training data, procedures, and architecture are not publicly known.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpt5",
                    "gpt4o",
                    "gpt35turbo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemini 2.5 Flash model</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> is a closed-source multilingual and multimodal instruction-tuned LLM by Google DeepMind. The model is reported to be pretrained on over 400 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite>, however, details on the language coverage are not available.</p>\n\n",
                "matched_terms": [
                    "flash",
                    "model",
                    "gemini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mistral Medium 3.1 model</span> (<span class=\"ltx_text ltx_font_typewriter\">mistral-medium-2508</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mistral</span>)</cite> is a closed-source multimodal instruction-tuned model by Mistral AI. Available details on the model architecture and language coverage are very limited.</p>\n\n",
                "matched_terms": [
                    "medium",
                    "mistral",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLaMA 3.3 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">14</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/llama3.3\" title=\"\">https://ollama.com/library/llama3.3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite> is an open-source instruction-tuned multilingual LLM, developed by Meta, with 70 billion parameters. The model was pretrained on a web text collection in various languages, however, it is reported to support only 8 languages, namely, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemma 3 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">15</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/gemma3\" title=\"\">https://ollama.com/library/gemma3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite> is an open-source multilingual instruction-tuned LLM, developed by Google DeepMind. The model was pretrained on multimodal data with large quantities of multilingual texts and is reported to support over 140 languages. We use the model in 27 billion parameter size.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DeepSeek-R1-Distill<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">16</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/deepseek-r1:14b\" title=\"\">https://ollama.com/library/deepseek-r1:14b</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite> is an open-source reasoning LLM, developed by DeepSeek AI. We use the distilled model in 14 billion parameter size, namely the <span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span> model. The model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that was fine-tuned using a dataset curated with the DeepSeek-R1 reasoning model. The Qwen 2.5 model provides multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, and Arabic.</p>\n\n",
                "matched_terms": [
                    "based",
                    "english",
                    "finetuned",
                    "model",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen 3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">17</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/qwen3\" title=\"\">https://ollama.com/library/qwen3</a></span></span></span></span> (<span class=\"ltx_text ltx_font_typewriter\">Qwen3-2504</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite> is an open-source LLM, developed by Alibaba Cloud. We use the model with the 32 billion parameter size, namely, the <span class=\"ltx_text ltx_font_typewriter\">qwen3:32b</span> model. The model is said to support over 100 languages and dialects <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent any bias, all models were used with their default parameters. The only parameter that we defined is the temperature which we set to 0 to ensure a more deterministic behaviour of the models. The same prompts were used for all open-source and closed-source models. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4\" title=\"Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we provide prompts that were provided to the LLMs for zero-shot text classification, namely for sentiment classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf1\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>), automatic genre identification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf2\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a>), news topic classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf3\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(c)</span></a>) and topic classification in parliamentary speeches (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf4\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(d)</span></a>). For more details on the setups used to apply fine-tuned BERT-like models and instruction-tuned LLMs to the test datasets, refer to the code published on GitHub.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "their",
                    "all",
                    "bertlike",
                    "test",
                    "datasets"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Difference between model performance in macro-F1 scores obtained on sentiment and topic classification in parliamentary texts on English versus the average macro-F1 scores on South Slavic languages.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Model</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Difference (sentiment)</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Difference (topic)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.05</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-4o</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.08</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Gemini 2.5 Flash</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.08</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Gemma 3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.07</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">LLaMA 3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.07</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Mistral Medium 3.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.07</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.09</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Qwen 3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.07</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.10</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">GPT-3.5-Turbo</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.07</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.03</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "topic",
            "slavic",
            "parliamentary",
            "difference",
            "average",
            "qwen",
            "between",
            "sentiment",
            "scores",
            "south",
            "performance",
            "gemma",
            "medium",
            "english",
            "versus",
            "llama",
            "classification",
            "gpt5",
            "languages",
            "texts",
            "gpt4o",
            "gemini",
            "obtained",
            "gpt35turbo",
            "model",
            "flash",
            "macrof1",
            "mistral"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>, LLMs generally perform worse on Bosnian compared to other languages. However, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T3\" title=\"Table 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the differences in macro-F1 scores between English and the average of macro-F1 scores for South Slavic languages are relatively small for sentiment identification, ranging from 2 to 7 points. For topic classification, the performance gap is slightly larger, ranging from 3 to 10 points. This is likely due to the increased difficulty of the task, which involves greater label granularity: 22 labels compared to just 3 in sentiment classification. These findings partially confirm hypothesis H2, which stated that LLMs, when used in a zero-shot setup, perform comparably on text classification tasks in South Slavic languages as they do on English.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>LLM evaluation, text classification, large language models, South Slavic languages, sentiment identification, topic classification, genre identification</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "english",
                    "classification",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">State of the Art in Text Classification for South Slavic Languages:\n<br class=\"ltx_break\"/>Fine-Tuning or Prompting?</span>\n</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "classification",
                    "slavic"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">In this paper, we focus on South Slavic languages, where research on text classification tasks included in our study has, until recently, been limited or even non-existent <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023survey</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. We take a first step toward systematically evaluating the current state of the art for text classification in these languages. Our evaluation is based on three text classification tasks in three different domains for which manually-annotated test datasets in South Slavic languages and fine-tuned BERT-like classifiers are freely available: sentiment classification of parliamentary speeches, topic classification in news articles, topic classification in parliamentary speeches, and automatic genre identification in web texts. These tasks span different domains and language styles, allowing for a comprehensive analysis of the performance of transformer-based models on text classification tasks. Specifically, we compare the performance of openly available fine-tuned BERT-like models with the zero-shot capabilities of both open-source and closed-source LLMs used via prompting.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "classification",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">An important aspect of our study is to examine whether the performance of multilingual models on South Slavic languages is on par with their performance on English. This question is particularly relevant given that the evaluated large language models have been predominantly pretrained and instruction-tuned on English data.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">By evaluating various models on a selection of text classification tasks in English and various South Slavic languages, we set out to test the following two hypotheses that are based on previous experiments with fine-tuned BERT-like models and LLMs on automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> and sentiment analysis in parliamentary texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2025parlasent</span>)</cite>:</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "english",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of LLMs used in a zero-shot setup on text classification tasks on South Slavic test datasets is comparable to the performance on English test datasets.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "english",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the introduction of transformer architectures, BERT (bidirectional encoder representations from transformers) models have achieved state-of-the-art results in text classification tasks, outperforming earlier non-neural approaches, such as support vector machines (SVMs). They have also demonstrated strong cross-lingual zero-shot capabilities in various classification tasks, including automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023survey</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">petukhova2023mn</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">de2020news</span>)</cite>, and sentiment classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>. However, these models still require fine-tuning on a training dataset, developed during manual annotation campaigns that are time-consuming and costly.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instruction-tuned decoder-only transformer models, commonly referred to as large language models (LLMs), have recently shown strong performance in a range of classification tasks, even in zero-shot prompting setups that require no training data <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024dialect</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>. They have achieved promising results on various natural language processing tasks, including stance detection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022would</span>)</cite>, implicit hate speech categorization <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2023chatgpt</span>)</cite>, news topic classification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, automatic genre identification <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>, causal commonsense reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ljubevsic2024jsi</span>)</cite>, and machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hendy2023good</span>)</cite>. Due to their promising performance, researchers have even started using them as data annotators, either by generating text and labels <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meng2022generating</span>)</cite> or by annotating pre-existing texts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>.\nDespite the growing interest in this topic, the majority of evaluations of LLMs used in text classification tasks are limited only to English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sun2023text</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025pushing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kostina2025large</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2024advancing</span>)</cite>. Systematic multilingual evaluations, especially which would include less-resourced languages such as those in the South Slavic group remain limited. Our work addresses this gap by providing a comparative evaluation of open-source and closed-source LLMs with openly-available fine-tuned BERT-like models across four benchmark families comprising three diverse classification tasks and three different domains in South Slavic languages and English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "english",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The benchmarks (evaluation datasets) used in this study cover three text classification tasks, namely, sentiment identification, topic classification, and automatic genre identification, and three domains: parliamentary speeches, news articles and web texts. An overview of the datasets is provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S1\" title=\"1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The four benchmark families differ significantly in terms of language coverage, number of test instances, and label granularity.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "texts",
                    "parliamentary",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The topic classification task is evaluated on two domains: 1) news articles, namely, the Croatian and Slovenian IPTC test datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which comprise around 300 text instances per language, and 2) parliamentary speeches, namely, the Bosnian, Croatian, English and Serbian ParlaCAP test datasets that consist of approximately 820 to 880 instances per language. In the ParlaCAP benchmarks, an instance is a transcription of an utterance given by a parliamentary member in a parliamentary session.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification",
                    "parliamentary",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The topic classification task involves the highest number of labels, that is, 17 news topic labels from the top level of the IPTC NewsCodes Media Topic hierarchical schema<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://show.newscodes.org/index.html?newscodes=medtop&amp;lang=en-GB&amp;startTo=Show\" title=\"\">https://show.newscodes.org/index.html?newscodes=medtop&amp;lang=en-GB&amp;startTo=Show</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>, and 22 agenda setting topic labels (21 major topics and a label <span class=\"ltx_text ltx_font_italic\">Other</span>) from the Comparative Agendas Project (CAP) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite> Master Codebook&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bevan2019gone</span>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.comparativeagendas.net/pages/master-codebook\" title=\"\">https://www.comparativeagendas.net/pages/master-codebook</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Bosnian, Croatian, English, and Serbian ParlaSent sentiment identification datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) have a significantly lower granularity of labels, with only 3 categories. They are represented by the largest number of instances, ranging from 190 (Bosnian part) to 2600 (English part) sentence-level instances.</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With 8 labels, the Croatian, English, Macedonian, and Slovenian GINCO genre datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> represent a midpoint in label granularity among the four benchmark families. However, the genre identification task might be the most difficult one, as genre identification depends on the interpretation of full texts with the focus on author&#8217;s purpose, the common function of the text, and the text&#8217;s conventional form <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">orlikowski1994genre</span>)</cite>. This complexity has also contributed to smaller test datasets in terms of the number of text instances, as manual annotation is more time-consuming. It is also important to note that, unlike the parliamentary datasets, the English portion of the genre datasets is not fully comparable to the South Slavic portions, which are label-balanced and contain fewer ambiguous instances. Nevertheless, the genre datasets remain valuable for evaluating model performance within each language.</p>\n\n",
                "matched_terms": [
                    "south",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "english",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluate the main machine learning approaches that have recently been used for our selection of text classification tasks, with the focus on the comparison between the freely available fine-tuned BERT models and the open-source and closed-source LLMs.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The code for the model evaluation and analysis of results is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a>.</span></span></span>\nThe models are evaluated on four families of test datasets that comprise South Slavic languages. The performance of the models is evaluated based on the micro-F1 and macro-F1 metrics, which enable assessment of the model performance at both the instance and label levels, respectively.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "model",
                    "classification",
                    "macrof1",
                    "between",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">fine-tuned BERT-like classifiers</span>: in our study, we evaluate previously developed openly accessible multilingual fine-tuned BERT-like models that have been fine-tuned for the respective task, namely, the XLM-R-ParlaSent (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) model for sentiment identification in parliamentary texts, the X-GENRE classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-classifier-huggingface</span></cite>) for automatic genre identification, the IPTC News Topic classifier (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptc_model_huggingface</span></cite>) for news topic classification, and the ParlaCAP classifier <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> for topic classification in parliamentary speeches. The XLM-R-ParlaSent and the ParlaCAP models are based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on parliamentary proceedings in 30 European languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>. The XLM-R-ParlaSent model was fine-tuned on 13 thousand instances from the ParlaSent sentiment training dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span>)</cite> in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span>)</cite>, while the ParlaCAP model was fine-tuned on around 30 thousand speeches from parliamentary debates annotated with CAP topic labels, originating from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) from 29 European parliaments. The X-GENRE classifier is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite> in English and Slovenian; while the IPTC News Topic classifier is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> that was fine-tuned on the EMMediaTopic dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite> in Catalan, Croatian, Greek, and Slovenian. All fine-tuned models use the same classes as the test datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "languages",
                    "texts",
                    "parliamentary",
                    "english",
                    "model",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">open-source and closed-source large language models</span>: we use closed-source OpenAI models, namely the GPT-3.5-Turbo (<span class=\"ltx_text ltx_font_typewriter\">gpt-3.5-turbo-0125</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai</span>)</cite>, GPT-4o (<span class=\"ltx_text ltx_font_typewriter\">gpt-4o-2024-08-06</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> and the GPT-5 (<span class=\"ltx_text ltx_font_typewriter\">gpt-5-2025-08-07</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite>; a closed-source Gemini 2.5 Flash model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> by Google DeepMind; a closed-source Mistral Medium 3.1 model (<span class=\"ltx_text ltx_font_typewriter\">mistral-medium-2508</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mistral</span>)</cite> by Mistral AI; and four open-source models, namely, the Meta LLaMA 3.3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>, the Gemma 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, the Qwen 3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>, and the DeepSeek-R1-Distill model (<span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite>. It is important to note that while the LLaMA model was pretrained on a web text collection in various languages, it is said to support only 8 languages, namely English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite>. The DeepSeek-R1-Distill model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that provides support for more than 29 languages &#8211; not including South Slavic languages though. In contrast, the Gemma 3 model is reported to support over 140 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite>, and the Qwen 3 model was pretrained on 119 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>. While closed-source models are said to be massively multilingual, with Gemini 2.5 models being pretrained on over 400 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite>, details on their language coverage are very limited.</p>\n\n",
                "matched_terms": [
                    "south",
                    "gemma",
                    "medium",
                    "slavic",
                    "languages",
                    "gpt4o",
                    "english",
                    "gemini",
                    "gpt35turbo",
                    "llama",
                    "model",
                    "qwen",
                    "flash",
                    "gpt5",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of fine-tuned BERT-like models and the instruction-tuned LLMs on a selection of text classification tasks that include test datasets in South Slavic languages. First, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS1\" title=\"5.1. State of the Art in Text Classification Tasks &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, we provide results on the four benchmark families with the focus on hypothesis H1, which expects that zero-shot prompting with LLMs can provide performance that is comparable to that of fine-tuned BERT-like models. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS2\" title=\"5.2. Comparison of Large Language Models &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, we compare in more detail the performance of the closed-source and open-source LLMs on the three text classification tasks, which is followed by a discussion on the advantages and limitations of LLMs for data annotation based on text classification tasks (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS3\" title=\"5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>). Lastly, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.SS4\" title=\"5.4. Performance on English versus on South Slavic languages &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>, we compare the performance of LLMs on English test datasets with their performance on South Slavic datasets, addressing hypothesis H2, which presumes that the available multilingual LLMs perform similarly on South Slavic languages as on English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "english",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1\" title=\"Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides results of model evaluation on our selection of text classification tasks. A consistent pattern emerges across all four benchmark families: LLMs, when used in a zero-shot prompting setup, achieve some of the highest scores. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T2\" title=\"Table 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which compares model rankings across tasks, LLMs achieve first place more often on average than the fine-tuned BERT-like model.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "average",
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1.sf1\" title=\"In Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> shows that both open-source and closed-source LLMs, used in a zero-shot prompting setup on the sentiment identification task, achieve performance that is comparable or even significantly higher to that of a fine-tuned BERT-like model trained on a large manually-annotated sentiment dataset. The only models that consistently perform worse than the fine-tuned BERT-like model are GPT-3.5-Turbo and DeepSeek-R1-Distill. Sentiment classification appears broad enough that more potent LLMs can interpret label definitions effectively without task-specific fine-tuning, reducing the benefit of additional training.</p>\n\n",
                "matched_terms": [
                    "gpt35turbo",
                    "model",
                    "classification",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, fine-tuned BERT-like models outperform most LLMs on automatic genre identification and topic classification tasks. These tasks depend on predefined label sets based on specific guidelines, and the strong performance of fine-tuned BERT-like models indicates that domain-specific fine-tuning on labelled data still offers an advantage over the general knowledge leveraged by LLMs in zero-shot setups. This advantage is particularly clear in genre identification for South Slavic texts, where the fine-tuned BERT-like model significantly outperforms LLMs. The likely reason for the fine-tuned model&#8217;s very strong performance on South Slavic genre datasets is the curated nature of the test data &#8211; more challenging examples were removed before and during manual annotation, unlike in the English genre test dataset where the instances were randomly sampled from an English web corpus. Nevertheless, despite this limitation, the South Slavic test dataset remains valuable for comparing the performance of LLMs.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "slavic",
                    "texts",
                    "english",
                    "model",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To conclude, since some LLMs used in a zero-shot prompting setup achieve higher or comparable results to fine-tuned BERT-like models across all classification tasks and languages, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.T2\" title=\"Table 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we can confirm hypothesis H1, which proposed that zero-shot prompting with LLMs can perform comparably to fine-tuned BERT-like models.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2\" title=\"Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the performance of open-source and closed-source LLMs, used via prompting, on the tasks of sentiment identification, automatic genre identification, news topic classification, and parliamentary topic classification. The DeepSeek-R1-Distill model is not included in the comparison, as it performs significantly worse than other models, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F1\" title=\"Figure 1 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "parliamentary",
                    "model",
                    "classification",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While different models perform best across different languages and test datasets, a clear trend emerges: the top-performing models across all four benchmark families are the closed-source GPT-4o and GPT-5 from OpenAI, along with Gemini 2.5 Flash. Although GPT-5 is newer and reportedly more powerful, it does not outperform GPT-4o on all benchmarks. Among open-source models, Gemma 3 generally achieves the best results in sentiment identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>) and news topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>). For automatic genre identification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>) and parliamentary topic classification (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F2.sf4\" title=\"In Figure 2 &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">2(d)</span></a>), rankings of open-source models vary by language. Overall, the weakest performance is observed with the older closed-source GPT-3.5-Turbo model, highlighting the rapid progress in both open-source and closed-source model development.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "topic",
                    "languages",
                    "parliamentary",
                    "gpt4o",
                    "gemini",
                    "gpt35turbo",
                    "model",
                    "classification",
                    "flash",
                    "gpt5",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What is more, the inference speed of all LLMs is significantly slower than that of a fine-tuned BERT-like model. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#S5.F3\" title=\"Figure 3 &#8227; 5.3. Advantages and Disadvantages of LLMs &#8227; 5. Results &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the fine-tuned BERT-like model achieves one of the highest macro-F1 scores on the topic classification task for parliamentary speeches, while maintaining a very low inference time of just 0.02 seconds per instance. In contrast, most LLMs have inference times between 0.6 and 1.4 seconds per instance, making them three to seven times slower for annotating the same dataset. The slowest model, GPT-5, takes 5.5 seconds per instance, which renders it impractical for large-scale automatic annotation of text collections. In this regard, fine-tuned BERT-like models offer a key advantage due to their lower computational cost and higher inference speed. Moreover, they can be trained on training data that is annotated by LLMs using the recently introduced LLM teacher-student paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>, which considerably reduces the effort needed to develop task-specific models.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "parliamentary",
                    "model",
                    "classification",
                    "macrof1",
                    "between",
                    "gpt5",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another limitation of LLMs, as revealed by the experiments, is their occasional deviation from the defined label set. This issue was especially noticeable in topic classification and, to a lesser extent, in genre identification. The highest rate of label hallucination was found in the DeepSeek-R1-Distill model, which produced non-existing labels for 8% of instances in the news topic test datasets and 4% in the genre test dataset. Similar issues were also observed, though much less frequently (less than 1%), with the LLaMA 3.3, Gemma 3, Qwen 3 and Mistral Medium 3.1 models. In contrast, fine-tuned BERT-like models do not suffer from this issue, as they output probabilities for the predefined classes.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "topic",
                    "medium",
                    "llama",
                    "model",
                    "qwen",
                    "classification",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentiment identification ParlaSent and the topic classification ParlaCAP benchmark families comprise test datasets in South Slavic languages and English that were constructed with the same methodology. Thus, they also allow for a comparison of the performance of the LLMs on English, a highly resourced language, with South Slavic languages, which are significantly less represented in the pretraining and instruction-tuning datasets used to develop large language models.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "english",
                    "classification",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, even the open-source LLaMA 3.3 model &#8211; reported to support only eight languages, excluding the South Slavic group &#8211; does not show a substantial performance drop when applied to South Slavic languages compared to English.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "english",
                    "llama",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we evaluated how well current machine learning technologies handle text classification tasks in South Slavic languages. We compared fine-tuned BERT-like models with decoder-only generative large language models (LLMs) that are used in a zero-shot prompting setup across three tasks and three text domains: sentiment classification in parliamentary texts, news topic classification, topic classification in parliamentary texts, and automatic genre identification on web texts.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show that LLMs used with prompting, where only a brief task description and labels were provided, achieved strong results across all tasks and languages, particularly the closed-source GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite>, GPT-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite> and Gemini 2.5 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> models. The performance of LLMs is comparable or higher to that of fine-tuned BERT-like models specialized for the tasks. On the sentiment identification task, most open-source and closed-source LLMs outperformed the fine-tuned model, demonstrating strong general knowledge of the notion of sentiment. For genre and topic classification, however, fine-tuning BERT-like models remain beneficial, as these tasks rely on predefined label sets and fine-tuning aligns the models more closely with the task requirements.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "languages",
                    "gpt4o",
                    "gemini",
                    "model",
                    "classification",
                    "flash",
                    "gpt5",
                    "sentiment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, LLMs perform similarly in English and South Slavic languages, with rather minor drops in micro- and macro-F1 scores, namely a drop of 2 to 7 points in terms of macro-F1 scores on sentiment classification, and a slightly higher drop from 3 to 10 points on topic classification in parliamentary texts. This suggests that the gap in multilingual performance is smaller than expected, even for open-source models not explicitly dedicated to these languages.</p>\n\n",
                "matched_terms": [
                    "south",
                    "topic",
                    "languages",
                    "slavic",
                    "texts",
                    "parliamentary",
                    "english",
                    "classification",
                    "macrof1",
                    "performance",
                    "sentiment",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although large language models offer impressive zero-shot performance and reduce the need for annotated data, they come with higher computational costs and are more prone to producing invalid labels. Moreover, their inference speed is at least three times slower than that of the fine-tuned BERT-like models. Thus, their use in use cases with extensive data to be processed, such as automatic enrichment of large corpora with text categories, remains impractical due to their high computational demands. In contrast, fine-tuned BERT-like models are more computationally efficient and can be better tailored to the specific characteristics of a task and its domain. They remain a practical and reliable choice for text classification tasks, especially when computational resources are limited, high inference speed is desired or output reliability is critical. Moreover, it is possible to combine the strengths of both approaches, as proposed by the LLM teacher-student pipeline paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>: LLMs can be used to automatically annotate training data, reducing the need for costly and time-consuming manual annotation, while fine-tuned BERT-like models can then be trained on these datasets.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study represents only an initial step to systematically benchmark text classification performance in South Slavic languages. Although our evaluation includes four diverse benchmark families, some of the test datasets remain relatively small. Future work will aim to increase dataset sizes, include more South Slavic languages and dialects, and introduce additional classification tasks. As new large language models continue to emerge rapidly, it will also be important to establish ongoing evaluations to track whether their performance continues to improve, particularly on South Slavic languages. Importantly, this study only evaluated the performance of LLMs in a zero-shot prompting setup. In future work, we plan to extend the evaluation to include few-shot prompting and fine-tuning on training data. To support further research and facilitate reproducibility, we have made all code, evaluation scripts, and results publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study has several limitations that should be acknowledged. First, while we aimed to include a broad set of South Slavic languages, some &#8211; most notably Bulgarian &#8211; were not covered in our experiments. We assume that the performance on Bulgarian would be similar to that observed for Macedonian, given their close linguistic proximity, or the results for Bulgarian could be slightly better, as Macedonian is comparatively more low-resourced. Moreover, due to the high computational cost of evaluating the LLMs on all the test datasets and the financial cost associated with the use of closed-source models, each model was evaluated on each dataset only once. This setup prevents us from fully estimating the variance of the results, however, based on our preliminary experiments, we expect this variance to be relatively small. Finally, the scope of our evaluation remains limited in terms of test datasets, language coverage and tasks. Expanding the range of benchmarks would allow for a more comprehensive validation of our findings, particularly regarding the hypothesis that LLMs can perform on par with fine-tuned BERT-like models across diverse natural language understanding tasks, languages and language varieties.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to thank the developers of the llm.ijs.si service <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maric_local_llms</span>)</cite> for establishing the LLM inference platform deployed at the Jo&#382;ef Stefan Institute, which provided convenient access to the open-source large language models used in this study. We also thank the annotators of the test datasets for their diligence and the time devoted to manual annotation, which resulted in the high-quality evaluation datasets used in this work. Lastly, we would like to thank the <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/k-centre/\" title=\"\">CLASSLA knowledge centre for South Slavic languages</a> and the Slovenian <a class=\"ltx_ref ltx_href\" href=\"https://www.clarin.si/info/about/\" title=\"\">CLARIN.SI infrastructure</a> for their valuable support.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide additional information on the datasets used for benchmarking the models on sentiment identification, topic classification, and genre identification tasks in this study.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">include Croatian, Serbian, Bosnian, and English data from the multilingual sentiment dataset of parliamentary debates ParlaSent 1.0 (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>, <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1585\" title=\"\">http://hdl.handle.net/11356/1585</a> and in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/classla/ParlaSent\" title=\"\">https://huggingface.co/datasets/classla/ParlaSent</a>.</span></span></span> The dataset comprises sentences that were randomly sampled from Croatian, Serbian, Bosnian and British parliamentary corpora and manually annotated with reported inter-annotator agreement ranging from 0.53 to 0.66 in Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. The annotation involved a more granular six-level sentiment polarity scale that has been mapped to a three-level sentiment polarity scale which we use in our experiments: negative (0), neutral (1), and positive (2).</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "parliamentary",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise the English EN-GINCO dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> and a multilingual X-GINCO dataset from the AGILE benchmark for Automatic Genre Identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark\" title=\"\">https://github.com/TajaKuzman/AGILE-Automatic-Genre-Identification-Benchmark</a></span></span></span> The test instances were sampled from the enTenTen20 English web corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jakubivcek2013tenten</span>)</cite> and the MaCoCu multilingual web corpus collection <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banon2022macocu</span>)</cite>. They were manually annotated by experts with a background in linguistics and computational linguistics who had experience with previous genre annotation campaigns <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-rupnik-ljubei:2022:LREC</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> where they reached an acceptable inter-annotator agreement of 0.71 in nominal Krippendorff&#8217;s alpha <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>. While the X-GINCO dataset comprises numerous European languages, for the purposes of this study, we focus on three South Slavic languages: Croatian, Macedonian, and Slovenian. The test datasets use the X-GENRE annotation schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite> that includes the following genre labels: <span class=\"ltx_text ltx_font_italic\">Information/Explanation</span>, <span class=\"ltx_text ltx_font_italic\">News</span>, <span class=\"ltx_text ltx_font_italic\">Instruction</span>, <span class=\"ltx_text ltx_font_italic\">Opinion/Argumentation</span>, <span class=\"ltx_text ltx_font_italic\">Forum</span>, <span class=\"ltx_text ltx_font_italic\">Prose/Lyrical</span>, <span class=\"ltx_text ltx_font_italic\">Legal</span> and <span class=\"ltx_text ltx_font_italic\">Promotion</span>. While EN-GINCO and X-GINCO datasets have been annotated by the same annotator with the same schema, one should note that there are important differences between them in terms of their construction &#8211; the English test dataset was sampled randomly from the web corpus, resulting in an unbalanced label distribution, while the X-GINCO datasets were curated with more deliberate interventions to ensure a balanced label distribution and a more controlled sampling process. Consequently, the X-GINCO datasets comprise fewer ambiguous instances and could be regarded as an easier test dataset.</p>\n\n",
                "matched_terms": [
                    "south",
                    "languages",
                    "slavic",
                    "english",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">comprise parliamentary speeches in Bosnian, Croatian, English, and Serbian, sourced from the ParlaMint 4.1 dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>). These speeches were annotated by a single expert annotator using the 21 CAP categories from the official CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>, along with an additional <span class=\"ltx_text ltx_font_italic\">Other</span> label. The datasets are approximately balanced across labels. To assess the annotation quality, the Croatian dataset was independently annotated by two additional annotators. Inter-annotator agreement between the expert annotator and the others ranged from 0.62 to 0.68 in Krippendorff&#8217;s alpha, which is around the threshold of 0.67 typically considered acceptable for annotation reliability <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">krippendorff2018content</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "english",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BERT (bidirectional encoder representations from transformers) deep neural models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bert</span>)</cite> have revolutionized the field of natural language processing (NLP), outperforming the non-neural methods across various NLP tasks. They have a more complex and computationally expensive architecture featuring transformers &#8211; neural networks with self-attention mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite> &#8211; that significantly improves the efficiency of training models on massive text data. Similarly to decoder-only transformer models, BERT models are pretrained on massive amounts of texts, possibly in multiple languages, which establishes their ability to encode the words and texts in high-dimensional vector spaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minaee2020deep</span>)</cite> and enables their application even across languages in a zero-shot classification scenario. To develop BERT-based classifiers, the pretrained models are trained, that is, fine-tuned, on a training dataset comprising text instances annotated with labels. In our study, we evaluate openly-accessible multilingual fine-tuned BERT-like models that have been already developed in recent related research. Namely, we evaluate the following models:</p>\n\n",
                "matched_terms": [
                    "languages",
                    "classification",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IPTC News Topic classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><span class=\"ltx_text ltx_font_medium\">The IPTC News Topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier\" title=\"\">https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite> is a multilingual fine-tuned BERT-like model for news topic classification according to the top-level IPTC NewsCodes schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iptcGroupsNewsCodes</span>)</cite>. The model is based on the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on 15,000 training text instances from the EMMediaTopic<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>The EMMediaTopic training dataset is available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1991\" title=\"\">http://hdl.handle.net/11356/1991</a>.</span></span></span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emmediatopic</span>)</cite>. The training dataset contains news article instances in four languages: Catalan, Croatian, Greek, and Slovenian. The training dataset was annotated using an LLM that was shown to achieve annotation reliability comparable to that of human annotators <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. This approach is based on the novel methodology that uses the LLM teacher-student pipeline to develop BERT-like classifiers in the absence of manually-annotated training data.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-R-ParlaSent</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-model</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>) is a domain-specific multilingual transformer model for sentiment identification in parliamentary texts. It is based on the XLM-R-parla pretrained model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span>)</cite> that was developed by additionally pretraining the large-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> on 1.72 billion words from parliamentary proceedings in 30 European languages.\nTo develop the XLM-R-ParlaSent model,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The XLM-R-ParlaSent model is accessible in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/classla/xlm-r-parlasent\" title=\"\">https://huggingface.co/classla/xlm-r-parlasent</a>.</span></span></span> the pretrained XLM-R-Parla model was fine-tuned on the ParlaSent sentiment training dataset (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlasent-repository</span></cite>) in seven European languages (Bosnian, Croatian, Czech, English, Serbian, Slovak, and Slovenian). The training dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>The ParlaSent training and test datasets are freely available in the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1868\" title=\"\">http://hdl.handle.net/11356/1868</a>.</span></span></span> comprises 13,000 instances sampled from parliamentary proceedings and manually annotated with sentiment labels.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "texts",
                    "parliamentary",
                    "english",
                    "model",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ParlaCAP classifier<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">12</span></span><span class=\"ltx_text ltx_font_medium\">The ParlaCAP topic classifier is available in the Hugging Face repository at </span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://huggingface.co/classla/ParlaCAP-Topic-Classifier\" title=\"\">https://huggingface.co/classla/ParlaCAP-Topic-Classifier</a><span class=\"ltx_text ltx_font_medium\">.</span></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlacap_model</span>)</cite> is a domain-specific multilingual transformer model for topic classification in parliamentary texts based on the CAP schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baumgartner2019comparative</span>)</cite>. As the XLM-R-ParlaSent model, this model is based on the XLM-R-parla pretrained model (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xlm-r-parla</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mochtak2024parlasent</span></cite>). The XLM-R-parla model was then fine-tuned on around 30 thousand speeches from parliamentary debates from the ParlaMint 4.1 parliamentary datasets (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">parlamint_41</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erjavec2025parlamint</span></cite>) in 29 European languages. The training dataset was annotated with the CAP categories by a GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> model used in a zero-shot prompting fashion, following the LLM teacher-student framework <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman-iptc-classification</span>)</cite>. Based on the inter-annotator agreement, calculated on a sample that was annotated by three human annotators and the LLM annotator, the agreement between the LLM and the human annotators was comparable to the agreement between the human annotators themselves. This indicates that the LLM performs as reliably as human annotators on this task, supporting its use for annotating the training data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "topic",
                    "texts",
                    "parliamentary",
                    "gpt4o",
                    "model",
                    "classification",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">X-GENRE classifier</span> (<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span></cite>; <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-repository</span></cite>) is a multilingual fine-tuned BERT-like model for automatic genre identification.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span>The X-GENRE classifier is freely available in the Hugging Face repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.57967/hf/0927\" title=\"\">https://doi.org/10.57967/hf/0927</a> and the CLARIN.SI repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://hdl.handle.net/11356/1961\" title=\"\">http://hdl.handle.net/11356/1961</a>.</span></span></span> The model is based on the base-sized XLM-RoBERTa model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2020unsupervised</span>)</cite> and was fine-tuned on the training split of the X-GENRE dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">x-genre-dataset</span>)</cite>, which contains 1,772 text instances in Slovenian and English, manually-annotated with genre labels from the X-GENRE schema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuzman2023automatic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the BERT models, decoder-only large language models are based on a transformer deep neural architecture and are pretrained on massive text collections. However, while the development of fine-tuned BERT-like classifiers necessitates large amounts of annotated training data, recent advances in the field have shown that the instruction-tuned LLMs are capable of text classification in a zero-shot or few-shot prompting setups which does not require any training data. We assess the performance of the following large language models:</p>\n\n",
                "matched_terms": [
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpenAI models</span>, namely the GPT-3.5-Turbo (<span class=\"ltx_text ltx_font_typewriter\">gpt-3.5-turbo-0125</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai</span>)</cite>, GPT-4o (<span class=\"ltx_text ltx_font_typewriter\">gpt-4o-2024-08-06</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai-gpt4o</span>)</cite> and the GPT-5 (<span class=\"ltx_text ltx_font_typewriter\">gpt-5-2025-08-07</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt-5</span>)</cite>. These closed-source instruction-tuned LLMs were developed by OpenAI. OpenAI states that the models are trained on large multilingual web corpora, however, specific details about the training data, procedures, and architecture are not publicly known.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "gpt4o",
                    "gpt35turbo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemini 2.5 Flash model</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite> is a closed-source multilingual and multimodal instruction-tuned LLM by Google DeepMind. The model is reported to be pretrained on over 400 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>)</cite>, however, details on the language coverage are not available.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "flash",
                    "model",
                    "gemini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mistral Medium 3.1 model</span> (<span class=\"ltx_text ltx_font_typewriter\">mistral-medium-2508</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mistral</span>)</cite> is a closed-source multimodal instruction-tuned model by Mistral AI. Available details on the model architecture and language coverage are very limited.</p>\n\n",
                "matched_terms": [
                    "medium",
                    "mistral",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLaMA 3.3 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">14</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/llama3.3\" title=\"\">https://ollama.com/library/llama3.3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama33modelcard</span>)</cite> is an open-source instruction-tuned multilingual LLM, developed by Meta, with 70 billion parameters. The model was pretrained on a web text collection in various languages, however, it is reported to support only 8 languages, namely, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model",
                    "languages",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemma 3 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">15</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/gemma3\" title=\"\">https://ollama.com/library/gemma3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemma3</span>)</cite> is an open-source multilingual instruction-tuned LLM, developed by Google DeepMind. The model was pretrained on multimodal data with large quantities of multilingual texts and is reported to support over 140 languages. We use the model in 27 billion parameter size.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "languages",
                    "texts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DeepSeek-R1-Distill<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">16</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/deepseek-r1:14b\" title=\"\">https://ollama.com/library/deepseek-r1:14b</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite> is an open-source reasoning LLM, developed by DeepSeek AI. We use the distilled model in 14 billion parameter size, namely the <span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1-Distill-Qwen-14B</span> model. The model is based on the Qwen 2.5 model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>)</cite> that was fine-tuned using a dataset curated with the DeepSeek-R1 reasoning model. The Qwen 2.5 model provides multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, and Arabic.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "english",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen 3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">17</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://ollama.com/library/qwen3\" title=\"\">https://ollama.com/library/qwen3</a></span></span></span></span> (<span class=\"ltx_text ltx_font_typewriter\">Qwen3-2504</span>) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite> is an open-source LLM, developed by Alibaba Cloud. We use the model with the 32 billion parameter size, namely, the <span class=\"ltx_text ltx_font_typewriter\">qwen3:32b</span> model. The model is said to support over 100 languages and dialects <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent any bias, all models were used with their default parameters. The only parameter that we defined is the temperature which we set to 0 to ensure a more deterministic behaviour of the models. The same prompts were used for all open-source and closed-source models. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4\" title=\"Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we provide prompts that were provided to the LLMs for zero-shot text classification, namely for sentiment classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf1\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>), automatic genre identification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf2\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a>), news topic classification (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf3\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(c)</span></a>) and topic classification in parliamentary speeches (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07989v1#A1.F4.sf4\" title=\"In Figure 4 &#8227; A.2.2. Instruction-Tuned Large Language Models &#8227; A.2. Models &#8227; Appendix A Appendix &#8227; 1. Introduction &#8227; State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?\"><span class=\"ltx_text ltx_ref_tag\">4(d)</span></a>). For more details on the setups used to apply fine-tuned BERT-like models and instruction-tuned LLMs to the test datasets, refer to the code published on GitHub.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote20\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">20</sup><span class=\"ltx_tag ltx_tag_note\">20</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic\" title=\"\">https://github.com/TajaKuzman/Benchmarking-Text-Classification-on-South-Slavic</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "classification",
                    "sentiment",
                    "parliamentary"
                ]
            }
        ]
    }
}