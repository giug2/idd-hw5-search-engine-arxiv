{
    "S5.T1": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 1: Performance evaluation of UniFlow-Audio and baselines across all tasks.",
        "body": "Task\nModel\nObjective Evaluation\nSubjective Evaluation\n\n\nMetrics\nResults\nMetrics\nResults\n\n\nTTS\nNaturalSpeech 2222We use the open-source version https://huggingface.co/amphion/naturalspeech2_libritts. (Shen et al., 2024)\n\nWER↓\\downarrow ∣\\mid SIM↑\\uparrow\n\n9.94 ∣\\mid 34.8\nMOS ↑\\uparrow ∣\\mid SMOS↑\\uparrow\n\n2.72 ∣\\mid 3.43\n\n\n\nUniFlow-Audio\n\n3.09 ∣\\mid 55.8\n\n\n3.79 ∣\\mid 3.21\n\n\nSVS\nDiffSinger (Liu et al., 2022c)\n\nF0↓\\downarrow ∣\\mid SA↑\\uparrow\n\n\n0.144 ∣\\mid 58.0\nMOS↑\\uparrow ∣\\mid SMOS↑\\uparrow\n\n\n4.26 ∣\\mid 4.43\n\n\n\nUniFlow-Audio\n0.147 ∣\\mid 59.9\n\n4.05 ∣\\mid 4.31\n\n\nT2A\nAudioLDM 2 (Liu et al., 2024b)\n\nFD↓\\downarrow ∣\\mid CLAP↑\\uparrow\n\n21.8 ∣\\mid 0.476\n\nOVL↑\\uparrow ∣\\mid REL↑\\uparrow\n\n\n3.57 ∣\\mid 3.48\n\n\nUniFlow-Audio\n\n17.2 ∣\\mid 0.476\n\n3.41 ∣\\mid 3.54\n\n\n\nT2M\nMusicGen (Copet et al., 2023)\n\nFD↓\\downarrow ∣\\mid CLAP↑\\uparrow\n\n29.5 ∣\\mid 0.245\n\nOVL↑\\uparrow ∣\\mid REL↑\\uparrow\n\n\n3.45 ∣\\mid 3.08\n\n\nUniFlow-Audio\n\n27.1 ∣\\mid 0.241\n3.37 ∣\\mid 3.09\n\n\n\nSE\nDOSE (Tai et al., 2023)\n\nPESQ↑\\uparrow ∣\\mid STOI↑\\uparrow\n\n2.50 ∣\\mid 0.931\nMOS↑\\uparrow\n\n3.43\n\n\nUniFlow-Audio\n\n2.91 ∣\\mid 0.944\n\n4.76\n\n\nSR\nAudioSR (Liu et al., 2024a)\n\nLSD↓\\downarrow\n\n1.75\nMOS↑\\uparrow\n\n3.58\n\n\nUniFlow-Audio\n1.49\n4.19\n\n\nV2A\nDiffFoley (Luo et al., 2023)\n\nIB↑\\uparrow ∣\\mid SYNC↓\\downarrow\n\n22.7 ∣\\mid 922\n\nOVL↑\\uparrow ∣\\mid SYNC↑\\uparrow\n2.80 ∣\\mid 2.94\n\n\nUniFlow-Audio\n\n28.6 ∣\\mid 1145\n\n3.61 ∣\\mid 3.55",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Results</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Results</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">TTS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NaturalSpeech 2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We use the open-source version <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/amphion/naturalspeech2_libritts\" title=\"\">https://huggingface.co/amphion/naturalspeech2_libritts</a>.</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.94 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 34.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.72 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">3.43</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.09</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">55.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.79</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SVS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib33\" title=\"\">2022c</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">F0<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SA<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">0.144</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 58.0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">4.26</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m18\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">4.43</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.147 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m19\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">59.9</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.05 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m20\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 4.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">T2A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m21\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m22\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> CLAP<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m23\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.8 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m24\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.476</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">OVL<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m25\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m26\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> REL<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m27\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.57</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m28\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 3.48</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">17.2</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m29\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.476</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.41 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m30\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">3.54</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">T2M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m31\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m32\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> CLAP<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m33\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.5 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m34\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.245</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">OVL<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m35\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m36\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> REL<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m37\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.45</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m38\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 3.08</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">27.1</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m39\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 0.241</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.37 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m40\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">3.09</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DOSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib49\" title=\"\">2023</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m41\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m42\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> STOI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m43\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.50 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m44\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 0.931</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m45\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.43</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">2.91</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m46\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.944</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AudioSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">LSD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m47\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.75</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m48\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.58</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">V2A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DiffFoley&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib35\" title=\"\">2023</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">IB<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m49\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m50\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SYNC<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m51\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.7 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m52\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">922</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">OVL<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m53\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m54\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> SYNC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m55\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.80 <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m56\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 2.94</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniFlow-Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">28.6</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m57\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> 1145</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.61</span> <math alttext=\"\\mid\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m58\" intent=\":literal\"><semantics><mo>&#8739;</mo><annotation encoding=\"application/x-tex\">\\mid</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">3.55</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "2024a",
            "task",
            "2024b",
            "tasks",
            "subjective",
            "ovl↑uparrow",
            "fd↓downarrow",
            "copet",
            "difffoley",
            "2022c",
            "f0↓downarrow",
            "tai",
            "shen",
            "mos↑uparrow",
            "objective",
            "across",
            "all",
            "metrics",
            "tts",
            "sync↑uparrow",
            "smos↑uparrow",
            "rel↑uparrow",
            "audiosr",
            "svs",
            "diffsinger",
            "results",
            "t2a",
            "dose",
            "ib↑uparrow",
            "model",
            "naturalspeech",
            "mos",
            "evaluation",
            "2222we",
            "opensource",
            "version",
            "sim↑uparrow",
            "stoi↑uparrow",
            "performance",
            "↑uparrow",
            "wer↓downarrow",
            "liu",
            "luo",
            "pesq↑uparrow",
            "uniflowaudio",
            "musicgen",
            "t2m",
            "lsd↓downarrow",
            "∣mid",
            "clap↑uparrow",
            "sync↓downarrow",
            "v2a",
            "use",
            "audioldm",
            "httpshuggingfacecoamphionnaturalspeech2libritts",
            "baselines",
            "sa↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "across",
                    "tasks",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "t2a",
                    "liu",
                    "tts",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "tts",
                    "v2a",
                    "objective",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent works have explored unified audio generation with autoregressive (AR) architectures, unified non-autoregressive (NAR) approaches remain relatively underexplored.\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> adopts an AR paradigm, achieving strong zero-shot performance on both AR and NAR tasks.\nHowever, AR models rely on sequential decoding and discrete tokenizers, whereas NAR models generate continuous audio representations in parallel, which may offer advantages in latency and quality.\nThus, NAR-based unified audio generation remains worth exploring.\nAudioX represents an NAR attempt, but it focuses exclusively on NTA tasks and cannot handle TA tasks such as TTS, which require variable-length generation.\nMeanwhile, task-specific NAR models like VoiceFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib17\" title=\"\">2024</a>)</cite> perform well on TA tasks by temporally aligning content embeddings with audio latents, yet this modeling paradigm does not generalize to NTA tasks.\nThis leaves a gap for a single NAR framework capable of unifying both TA and NTA tasks within one modeling paradigm.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "tts",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "across",
                    "tasks",
                    "all",
                    "performance",
                    "baselines",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose UniFlow-Audio, the first flow-matching-based universal audio generation framework that unifies TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design model architectures and data sampling strategies to balance TA and NTA tasks while ensuring the generation quality, including a dual-fusion mechanism, block-wise fusion, and task-balanced sampling.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio achieves strong results with limited open-source data and parameters on a variety of tasks, demonstrating the advantages of a unified audio generation model.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "tasks",
                    "opensource",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We open-source the code and model to provide a potential unified NAR audio generation foundation model, enabling further theoretical exploration and practical applications.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "performance",
                    "model",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "tts",
                    "shen",
                    "audioldm",
                    "liu",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.F2\" title=\"In 3.2 Content Encoding with Task Instruction &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, UniFlow-Audio is a unified flow-matching-based audio generation framework that consists of four parts: a variational autoencoder (VAE) that compresses the raw long audio signal into a short sequence, a content encoding part for extracting features from the input content and task instruction, a duration adapter that generates TA content embeddings, and a Transformer-based flow matching backbone.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text</span>: For T2A and text-to-music generation (T2M), the input is a coarse text description without the alignment information.\nWe use Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib5\" title=\"\">2024</a>)</cite> as the encoder following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Majumder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib36\" title=\"\">2024</a>; Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video</span>: For video input in video-to-audio generation (V2A), we use CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib43\" title=\"\">2021</a>)</cite> combined as the encoder.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "t2m",
                    "tasks",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding each task, we design 10 diverse textual instructions that describe the objective (details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3\" title=\"Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nDuring training, one instruction is randomly selected from each task as the input, whereas during inference, a fixed instruction is used.</p>\n\n",
                "matched_terms": [
                    "task",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With task-involved content embeddings <math alttext=\"\\mathbf{C}^{\\mathbf{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C}^{\\mathbf{I}}</annotation></semantics></math>, a clip duration <math alttext=\"d_{c}\\in\\mathbb{R}^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mo>+</mo></msup></mrow><annotation encoding=\"application/x-tex\">d_{c}\\in\\mathbb{R}^{+}</annotation></semantics></math> and a sequence duration <math alttext=\"d_{s}\\in(\\mathbb{R}^{+})^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m3\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#8477;</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">d_{s}\\in(\\mathbb{R}^{+})^{L}</annotation></semantics></math> are predicted.\nSince UniFlow-Audio is an NAR model, both TA and NTA tasks rely on <math alttext=\"d_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">d_{c}</annotation></semantics></math> to determine the output length.\n<math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is only required by TA tasks for duration adaptation, which will be introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS3\" title=\"3.3 Duration Adapter &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.\nFor the duration predictor, we adopt the architecture in FastSpeech2.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this insight, we introduce a unified <span class=\"ltx_text ltx_font_italic\">duration adapter</span> to explicitly align content embeddings with audio latents across all TA tasks.\nWe posit that this explicit alignment offers superior efficacy for TA tasks than the implicit mechanisms of cross-attention.\nSpecifically, <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> is expanded to a time-aligned content <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>. That is,</p>\n\n",
                "matched_terms": [
                    "all",
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "v2a",
                    "tts",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "t2m",
                    "tasks",
                    "all",
                    "tts",
                    "evaluation",
                    "svs",
                    "v2a",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "task",
                    "tasks",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained on eight A100 GPUs with a batch size on each GPU of 24.\nWe train three versions with different sizes: small, medium, and large.\nConfiguration and training details are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS2\" title=\"D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS3\" title=\"D.3 Training &amp; Inference Setup &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\nThe small version takes about 7 days to train, while the large version takes about 12 days.</p>\n\n",
                "matched_terms": [
                    "version",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all tasks, both objective and subjective evaluation are conducted.\nSince UniFlow-Audio is evaluated on a variety of tasks and datasets, we adopt task-specific commonly-adopted metrics, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A2\" title=\"Appendix B Evaluation Metrics &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "tasks",
                    "subjective",
                    "all",
                    "metrics",
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first compare the performance of UniFlow-Audio with baselines on all tasks to evaluate the overall generation quality.\nThen, we explore the effect of CFG scale on different tasks.\nFinally, we conduct ablation studies on our training and architecture design.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "tasks",
                    "all",
                    "performance",
                    "baselines"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "t2m",
                    "across",
                    "tasks",
                    "tts",
                    "svs",
                    "performance",
                    "results",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of two key hyper-parameters in flow matching on generation performance: the guidance scale and the number of inference steps.\nInterestingly, we observe two distinct patterns across all tasks: SE and SR fall into one pattern, while the remaining tasks follow another.\nWe take SE and T2A as representative tasks of the two patterns and report their CLAP and PESQ scores, with higher values indicating better performance for both metrics.\nResults are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks",
                    "all",
                    "metrics",
                    "performance",
                    "results",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "task",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "2024b",
                    "all",
                    "performance",
                    "liu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
                "matched_terms": [
                    "task",
                    "2024b",
                    "tasks",
                    "t2m",
                    "performance",
                    "liu",
                    "results",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the architectural design, we examine the effect of fusing time-aligned content embeddings only at the input layer, referred to as <em class=\"ltx_emph ltx_font_italic\">input fusion</em>.\nThis follows the design of F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nAs shown in the middle row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, input fusion leads to a substantial performance drop on time-aligned tasks.\nSince content embeddings are integrated via cross-attention in each DiT block, injecting time-aligned inputs solely at the input layer makes their influence much weaker than that of non-time-aligned inputs.\nConsequently, non-time-aligned tasks are only marginally affected, while the performance on time-aligned tasks degrades significantly.\nIn contrast, UniFlow-Audio employs <em class=\"ltx_emph ltx_font_italic\">block-wise fusion</em>, where time-aligned content embeddings are injected into each DiT block.\nThis progressive fusion allows richer interactions between time-aligned content and audio latents, and proves essential for achieving robust performance across different task types.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "task",
                    "across",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we investigate the impact of the proposed task-balanced data sampling strategy.\nAs shown in the last row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing balanced sampling (<em class=\"ltx_emph ltx_font_italic\">w/o balanced sampling</em>) results in degraded performance on non-time-aligned tasks (T2A and T2M), while performance on time-aligned tasks remain relatively stable.\nThis aligns with the number of datasets from different task types: under the original round-robin sampling strategy, time-aligned tasks are overrepresented.\nWithout explicit balancing, the model is more exposed to time-aligned tasks, which amplifies the influence of time-aligned content input.\nIn contrast, the task-balanced sampling strategy ensures that each task type is adequately represented, mitigating the effects of task imbalance and leading to more consistent and reliable performance across both time-aligned and non-time-aligned tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "across",
                    "tasks",
                    "t2m",
                    "performance",
                    "results",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "t2m",
                    "tasks",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model",
                    "across",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "all",
                    "tts",
                    "evaluation",
                    "svs",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The official training subset of AudioCaps is used for T2A training.\nEach sample contains 5 captions in the test subset.\nFollowing TANGO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib14\" title=\"\">2023</a>)</cite>, we randomly select one caption per sample for evaluation, and we use the same selected captions as in their setup.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "use",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For T2M, we use songs from MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite> combined with LP-MusicCaps-MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Doh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib7\" title=\"\">2023</a>)</cite> captions as the training data.\nThe original song in MSD can be as long as 14 minutes.\nDuring training, we randomly crop 10 seconds for training.\nThe widely-used benchmark MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> is used for evaluation.</p>\n\n",
                "matched_terms": [
                    "musicgen",
                    "t2m",
                    "copet",
                    "evaluation",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SR, we mainly follow the setup of AudioSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>)</cite>, while prioritizing the available sources for ease of collection.\nThe training datasets include MUSDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafii et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib44\" title=\"\">2019</a>)</cite>, MoisesDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pereira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib40\" title=\"\">2023</a>)</cite>, HQ-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib29\" title=\"\">2022b</a>)</cite> and FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib38\" title=\"\">2024</a>)</cite>, while the evaluation uses ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Piczak, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib41\" title=\"\">2015</a>)</cite>, VCTK-test <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, and MUSDB.\nAll high-quality recordings are first resampled to 24&#160;kHz.\nSince our VAE is designed to process 24&#160;kHz audio, we choose a cutoff range of [2,6]&#160;KHz for the downsampled audio. Based on the method introduced in NVSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we then apply the low-pass filter within this range to simulate low-high resolution audio pairs.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "all",
                    "evaluation",
                    "audiosr",
                    "liu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "v2a",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib55\" title=\"\">2024</a>)</cite>, we use Word Error Rate (WER)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> as an objective metric to evaluate the accuracy of generated speech with respect to the given transcription, and Speaker Similarity (SIM)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/speakerverification_en_titanet_large\" title=\"\">https://huggingface.co/nvidia/speakerverification_en_titanet_large</a></span></span></span> to assess the consistency of speaker characteristics between the generated and reference speech. For subjective evaluation, we employ the Mean Opinion Score (MOS) to measure overall speech naturalness and the Similarity MOS (SMOS) to assess perceived speaker similarity.</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "mos",
                    "evaluation",
                    "use",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib58\" title=\"\">2024</a>)</cite>, we use root mean square error of fundamental frequency (F0) and semitone accuracy (SA)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171\" title=\"\">https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171</a></span></span></span> for objective evaluation.\nSame as TTS, MOS and SMOS are used as subjective metrics for accessing singing quality and singer similarity.</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "mos",
                    "metrics",
                    "tts",
                    "evaluation",
                    "use",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "2024b",
                    "evaluation",
                    "liu",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib49\" title=\"\">2023</a>)</cite>, we choose Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) for SE evaluation.\nPESQ measures perceptual speech quality, and STOI estimates speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "objective",
                    "tai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we adopt Log-Spectral Distance (LSD) for objective evaluation.\nLSD measures the discrepancy between the original high-frequency audio and the generated audio.\nNote that the baseline model AudioSR generates 48&#160;kHz audio, while ours operates at 24&#160;kHz.\nFor fair comparison, AudioSR outputs are downsampled to 24&#160;kHz before evaluation.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "model",
                    "evaluation",
                    "audiosr",
                    "liu",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Viertola et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite>, we evaluate V2A performance using ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> (IB) and Synchformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib21\" title=\"\">2024</a>)</cite> (SYNC).\nIB measures semantic modality consistency by computing the cosine similarity between audio and video embeddings.\nSYNC assesses synchronization based on temporal offsets between audio and visual modality estimated by Synchformer.\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T8\" title=\"In D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the architectural configurations of different UniFlow-Audio versions.\nNotably, the small variant contains only approximately 200M trainable parameters, yet it achieves competitive performance as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "performance",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs were used as assistive tools in this work.\nSpecifically, they were employed to help with limited code writing and debugging, as well as for polishing the language of the paper.\nThe LLMs involved include mainstream models such as GPT, Claude, and Gemini.\nThese model were used for grammar correction, sentence restructuring, and enhancing overall readability.\nAll technical content, experimental design, results, and conclusions were authored and verified solely by the human authors.\nLLMs did not contribute to the generation of ideas, methods, or data analysis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all tasks, we conduct MOS-based subjective tests with explicit instructions for raters.\nEach sample is rated on a 1&#8211;5 Likert scale.\nWe recruit ten raters with college-level education and normal hearing ability for subjective evaluation.\nExamples of the rating interface and detailed instructions are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A6.F5\" title=\"In Appendix F Subjective Evaluation Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nBelow we describe the setup for each task.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "subjective",
                    "all",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "mos",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">T2A</span> and <span class=\"ltx_text ltx_font_bold\">T2M</span>, we follow AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib26\" title=\"\">2022</a>)</cite> and MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> to evaluate overall quality (OVL) and relevance (REL) to the input caption.</p>\n\n",
                "matched_terms": [
                    "copet",
                    "musicgen",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">V2A</span>, we evaluate overall acceptability (OVL) and synchronization (SYNC) with the reference video.\nIn SYNC evaluation, the raters judge whether audio events are temporally aligned with visual cues such as lip movements, object impacts, or musical actions.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "evaluation"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 2: Generation performance across different model sizes.",
        "body": "Model\n\n\n# Trainable\nParams\n\n\nTTS\nSVS\nT2A\nT2M\nSE\nSR\nV2A\n\n\nWER↓\\downarrow\n\nSA↑\\uparrow\n\nFD↓\\downarrow\n\nFD↓\\downarrow\n\nPESQ↑\\uparrow\n\nLSD↓\\downarrow\n\nIB↑\\uparrow\n\n\n\nPrior Works\n-\n9.94\n58.0\n21.8\n29.5\n2.50\n1.75\n22.7\n\n\n\n\nUniFlow-Audio small\n208M\n3.23\n56.6\n19.7\n26.2\n2.60\n1.58\n25.5\n\n\nUniFlow-Audio medium\n395M\n3.03\n58.4\n17.8\n26.6\n2.72\n1.53\n26.5\n\n\nUniFlow-Audio large\n847M\n3.09\n59.9\n17.2\n27.1\n2.91\n1.49\n28.6",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\"># Trainable</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Params</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TTS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SVS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">T2A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">T2M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">V2A</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SA<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">LSD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">IB<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Prior Works</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">9.94</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">58.0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">21.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">29.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">2.50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.75</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">22.7</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">UniFlow-Audio small</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">208M</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">26.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">UniFlow-Audio medium</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">395M</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.03</span></td>\n<td class=\"ltx_td ltx_align_center\">58.4</td>\n<td class=\"ltx_td ltx_align_center\">17.8</td>\n<td class=\"ltx_td ltx_align_center\">26.6</td>\n<td class=\"ltx_td ltx_align_center\">2.72</td>\n<td class=\"ltx_td ltx_align_center\">1.53</td>\n<td class=\"ltx_td ltx_align_center\">26.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">UniFlow-Audio large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">847M</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">59.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">17.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">27.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">28.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "medium",
            "fd↓downarrow",
            "prior",
            "trainable",
            "large",
            "208m",
            "sizes",
            "across",
            "tts",
            "svs",
            "t2a",
            "generation",
            "ib↑uparrow",
            "model",
            "params",
            "performance",
            "wer↓downarrow",
            "pesq↑uparrow",
            "uniflowaudio",
            "395m",
            "t2m",
            "lsd↓downarrow",
            "847m",
            "different",
            "small",
            "v2a",
            "works",
            "sa↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T8\" title=\"In D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the architectural configurations of different UniFlow-Audio versions.\nNotably, the small variant contains only approximately 200M trainable parameters, yet it achieves competitive performance as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model",
                    "across",
                    "different",
                    "small",
                    "works",
                    "performance",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "tts",
                    "works",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "v2a",
                    "tts",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent works have explored unified audio generation with autoregressive (AR) architectures, unified non-autoregressive (NAR) approaches remain relatively underexplored.\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> adopts an AR paradigm, achieving strong zero-shot performance on both AR and NAR tasks.\nHowever, AR models rely on sequential decoding and discrete tokenizers, whereas NAR models generate continuous audio representations in parallel, which may offer advantages in latency and quality.\nThus, NAR-based unified audio generation remains worth exploring.\nAudioX represents an NAR attempt, but it focuses exclusively on NTA tasks and cannot handle TA tasks such as TTS, which require variable-length generation.\nMeanwhile, task-specific NAR models like VoiceFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib17\" title=\"\">2024</a>)</cite> perform well on TA tasks by temporally aligning content embeddings with audio latents, yet this modeling paradigm does not generalize to NTA tasks.\nThis leaves a gap for a single NAR framework capable of unifying both TA and NTA tasks within one modeling paradigm.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "performance",
                    "tts",
                    "works"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "sizes",
                    "uniflowaudio",
                    "model",
                    "across",
                    "different",
                    "small",
                    "works",
                    "prior",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose UniFlow-Audio, the first flow-matching-based universal audio generation framework that unifies TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design model architectures and data sampling strategies to balance TA and NTA tasks while ensuring the generation quality, including a dual-fusion mechanism, block-wise fusion, and task-balanced sampling.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio achieves strong results with limited open-source data and parameters on a variety of tasks, demonstrating the advantages of a unified audio generation model.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We open-source the code and model to provide a potential unified NAR audio generation foundation model, enabling further theoretical exploration and practical applications.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model",
                    "works",
                    "performance",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "tts",
                    "works",
                    "prior",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.F2\" title=\"In 3.2 Content Encoding with Task Instruction &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, UniFlow-Audio is a unified flow-matching-based audio generation framework that consists of four parts: a variational autoencoder (VAE) that compresses the raw long audio signal into a short sequence, a content encoding part for extracting features from the input content and task instruction, a duration adapter that generates TA content embeddings, and a Transformer-based flow matching backbone.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, we employ a VAE that operates on raw waveforms for direct waveform generation and reducing latency.\nThe VAE encoder compresses the waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{L}</annotation></semantics></math> into a latent representation <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mi>L</mi><mo>/</mo><msup><mn>2</mn><mi>R</mi></msup></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> denote the waveform length, compression ratio and latent dimension, respectively.\nThe VAE architecture also follows <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, with details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS1\" title=\"D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.\nWe train the VAE on a mixture of high-quality speech, music, singing voice and general audio datasets to improve the generation performance on various domains.\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text</span>: For T2A and text-to-music generation (T2M), the input is a coarse text description without the alignment information.\nWe use Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib5\" title=\"\">2024</a>)</cite> as the encoder following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Majumder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib36\" title=\"\">2024</a>; Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video</span>: For video input in video-to-audio generation (V2A), we use CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib43\" title=\"\">2021</a>)</cite> combined as the encoder.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "model",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With task-involved content embeddings <math alttext=\"\\mathbf{C}^{\\mathbf{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C}^{\\mathbf{I}}</annotation></semantics></math>, a clip duration <math alttext=\"d_{c}\\in\\mathbb{R}^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mo>+</mo></msup></mrow><annotation encoding=\"application/x-tex\">d_{c}\\in\\mathbb{R}^{+}</annotation></semantics></math> and a sequence duration <math alttext=\"d_{s}\\in(\\mathbb{R}^{+})^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m3\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#8477;</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">d_{s}\\in(\\mathbb{R}^{+})^{L}</annotation></semantics></math> are predicted.\nSince UniFlow-Audio is an NAR model, both TA and NTA tasks rely on <math alttext=\"d_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">d_{c}</annotation></semantics></math> to determine the output length.\n<math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is only required by TA tasks for duration adaptation, which will be introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS3\" title=\"3.3 Duration Adapter &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.\nFor the duration predictor, we adopt the architecture in FastSpeech2.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "t2m",
                    "tts",
                    "svs",
                    "v2a",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
                "matched_terms": [
                    "sizes",
                    "t2m",
                    "different",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained on eight A100 GPUs with a batch size on each GPU of 24.\nWe train three versions with different sizes: small, medium, and large.\nConfiguration and training details are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS2\" title=\"D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS3\" title=\"D.3 Training &amp; Inference Setup &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\nThe small version takes about 7 days to train, while the large version takes about 12 days.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "sizes",
                    "medium",
                    "different",
                    "small",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first compare the performance of UniFlow-Audio with baselines on all tasks to evaluate the overall generation quality.\nThen, we explore the effect of CFG scale on different tasks.\nFinally, we conduct ablation studies on our training and architecture design.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "performance",
                    "different",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model",
                    "tts",
                    "svs",
                    "works",
                    "prior",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of two key hyper-parameters in flow matching on generation performance: the guidance scale and the number of inference steps.\nInterestingly, we observe two distinct patterns across all tasks: SE and SR fall into one pattern, while the remaining tasks follow another.\nWe take SE and T2A as representative tasks of the two patterns and report their CLAP and PESQ scores, with higher values indicating better performance for both metrics.\nResults are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "performance",
                    "across",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "sizes",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "generation",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "t2m",
                    "prior",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the architectural design, we examine the effect of fusing time-aligned content embeddings only at the input layer, referred to as <em class=\"ltx_emph ltx_font_italic\">input fusion</em>.\nThis follows the design of F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nAs shown in the middle row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, input fusion leads to a substantial performance drop on time-aligned tasks.\nSince content embeddings are integrated via cross-attention in each DiT block, injecting time-aligned inputs solely at the input layer makes their influence much weaker than that of non-time-aligned inputs.\nConsequently, non-time-aligned tasks are only marginally affected, while the performance on time-aligned tasks degrades significantly.\nIn contrast, UniFlow-Audio employs <em class=\"ltx_emph ltx_font_italic\">block-wise fusion</em>, where time-aligned content embeddings are injected into each DiT block.\nThis progressive fusion allows richer interactions between time-aligned content and audio latents, and proves essential for achieving robust performance across different task types.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "performance",
                    "different",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we investigate the impact of the proposed task-balanced data sampling strategy.\nAs shown in the last row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing balanced sampling (<em class=\"ltx_emph ltx_font_italic\">w/o balanced sampling</em>) results in degraded performance on non-time-aligned tasks (T2A and T2M), while performance on time-aligned tasks remain relatively stable.\nThis aligns with the number of datasets from different task types: under the original round-robin sampling strategy, time-aligned tasks are overrepresented.\nWithout explicit balancing, the model is more exposed to time-aligned tasks, which amplifies the influence of time-aligned content input.\nIn contrast, the task-balanced sampling strategy ensures that each task type is adequately represented, mitigating the effects of task imbalance and leading to more consistent and reliable performance across both time-aligned and non-time-aligned tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "t2m",
                    "across",
                    "different",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model",
                    "t2m",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "uniflowaudio",
                    "model",
                    "across",
                    "small",
                    "performance",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Viertola et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite>, we evaluate V2A performance using ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> (IB) and Synchformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib21\" title=\"\">2024</a>)</cite> (SYNC).\nIB measures semantic modality consistency by computing the cosine similarity between audio and video embeddings.\nSYNC assesses synchronization based on temporal offsets between audio and visual modality estimated by Synchformer.\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "different",
                    "generation",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "performance",
                    "uniflowaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs were used as assistive tools in this work.\nSpecifically, they were employed to help with limited code writing and debugging, as well as for polishing the language of the paper.\nThe LLMs involved include mainstream models such as GPT, Claude, and Gemini.\nThese model were used for grammar correction, sentence restructuring, and enhancing overall readability.\nAll technical content, experimental design, results, and conclusions were authored and verified solely by the human authors.\nLLMs did not contribute to the generation of ideas, methods, or data analysis.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">T2A</span> and <span class=\"ltx_text ltx_font_bold\">T2M</span>, we follow AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib26\" title=\"\">2022</a>)</cite> and MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> to evaluate overall quality (OVL) and relevance (REL) to the input caption.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "t2a"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 3: Ablation results on the architecture design and data sampling strategies of UniAudio-Flow. The best results are highlighted in bold, while the second-best are underlined.",
        "body": "Setting\nTime Aligned\nNon Time Aligned\n\n\nTTS\nSVS\nSE\nSR\nV2A\nT2A\nT2M\n\n\nWER↓\\downarrow\n\nSA↑\\uparrow\n\nPESQ↑\\uparrow\n\nLSD↓\\downarrow\n\nIB↑\\uparrow\n\nFD↓\\downarrow\n\nFD↓\\downarrow\n\n\n\nUniFlow-Audio-small\n3.23\n56.6\n2.60\n1.58\n25.5\n19.7\n26.2\n\n\n    w. cross attention\n27.6\n55.0\n1.10\n2.42\n24.5\n30.1\n37.2\n\n\n    w. double fusion\n3.42\n56.9\n2.65\n1.58\n25.5\n22.3\n30.5\n\n\n    w. input fusion\n42.0\n41.8\n1.07\n1.59\n13.7\n20.9\n28.7\n\n\n    w/o. balanced sampling\n3.30\n56.5\n2.54\n1.53\n26.0\n22.9\n27.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Setting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\">Time Aligned</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Non Time Aligned</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SVS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">V2A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T2A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T2M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">SA<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">LSD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">IB<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">FD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">UniFlow-Audio-small</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">56.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">25.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">26.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#8194;&#8202;&#8195; w. cross attention</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195; w. double fusion</td>\n<td class=\"ltx_td ltx_align_center\">3.42</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">25.5</span></td>\n<td class=\"ltx_td ltx_align_center\">22.3</td>\n<td class=\"ltx_td ltx_align_center\">30.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#8194;&#8202;&#8195; w. input fusion</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">13.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">&#8194;&#8202;&#8195; w/o. balanced sampling</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">56.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">2.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">26.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">22.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">27.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "strategies",
            "ablation",
            "fd↓downarrow",
            "architecture",
            "input",
            "tts",
            "balanced",
            "highlighted",
            "non",
            "svs",
            "results",
            "t2a",
            "ib↑uparrow",
            "cross",
            "fusion",
            "bold",
            "wer↓downarrow",
            "aligned",
            "uniflowaudiosmall",
            "pesq↑uparrow",
            "underlined",
            "time",
            "t2m",
            "setting",
            "sampling",
            "lsd↓downarrow",
            "uniaudioflow",
            "double",
            "design",
            "attention",
            "best",
            "v2a",
            "while",
            "data",
            "sa↑uparrow",
            "secondbest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
            "<p class=\"ltx_p\">To further validate the architectural design, we examine the effect of fusing time-aligned content embeddings only at the input layer, referred to as <em class=\"ltx_emph ltx_font_italic\">input fusion</em>.\nThis follows the design of F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nAs shown in the middle row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, input fusion leads to a substantial performance drop on time-aligned tasks.\nSince content embeddings are integrated via cross-attention in each DiT block, injecting time-aligned inputs solely at the input layer makes their influence much weaker than that of non-time-aligned inputs.\nConsequently, non-time-aligned tasks are only marginally affected, while the performance on time-aligned tasks degrades significantly.\nIn contrast, UniFlow-Audio employs <em class=\"ltx_emph ltx_font_italic\">block-wise fusion</em>, where time-aligned content embeddings are injected into each DiT block.\nThis progressive fusion allows richer interactions between time-aligned content and audio latents, and proves essential for achieving robust performance across different task types.</p>\n\n",
            "<p class=\"ltx_p\">Finally, we investigate the impact of the proposed task-balanced data sampling strategy.\nAs shown in the last row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing balanced sampling (<em class=\"ltx_emph ltx_font_italic\">w/o balanced sampling</em>) results in degraded performance on non-time-aligned tasks (T2A and T2M), while performance on time-aligned tasks remain relatively stable.\nThis aligns with the number of datasets from different task types: under the original round-robin sampling strategy, time-aligned tasks are overrepresented.\nWithout explicit balancing, the model is more exposed to time-aligned tasks, which amplifies the influence of time-aligned content input.\nIn contrast, the task-balanced sampling strategy ensures that each task type is adequately represented, mitigating the effects of task imbalance and leading to more consistent and reliable performance across both time-aligned and non-time-aligned tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "input",
                    "while",
                    "data",
                    "results",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "tts",
                    "input",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent works have explored unified audio generation with autoregressive (AR) architectures, unified non-autoregressive (NAR) approaches remain relatively underexplored.\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> adopts an AR paradigm, achieving strong zero-shot performance on both AR and NAR tasks.\nHowever, AR models rely on sequential decoding and discrete tokenizers, whereas NAR models generate continuous audio representations in parallel, which may offer advantages in latency and quality.\nThus, NAR-based unified audio generation remains worth exploring.\nAudioX represents an NAR attempt, but it focuses exclusively on NTA tasks and cannot handle TA tasks such as TTS, which require variable-length generation.\nMeanwhile, task-specific NAR models like VoiceFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib17\" title=\"\">2024</a>)</cite> perform well on TA tasks by temporally aligning content embeddings with audio latents, yet this modeling paradigm does not generalize to NTA tasks.\nThis leaves a gap for a single NAR framework capable of unifying both TA and NTA tasks within one modeling paradigm.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "sampling",
                    "input",
                    "while",
                    "fusion",
                    "data",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design model architectures and data sampling strategies to balance TA and NTA tasks while ensuring the generation quality, including a dual-fusion mechanism, block-wise fusion, and task-balanced sampling.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "sampling",
                    "design",
                    "while",
                    "fusion",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio achieves strong results with limited open-source data and parameters on a variety of tasks, demonstrating the advantages of a unified audio generation model.</p>\n\n",
                "matched_terms": [
                    "data",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "while",
                    "input",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "data",
                    "attention",
                    "tts",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text</span>: For T2A and text-to-music generation (T2M), the input is a coarse text description without the alignment information.\nWe use Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib5\" title=\"\">2024</a>)</cite> as the encoder following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Majumder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib36\" title=\"\">2024</a>; Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video</span>: For video input in video-to-audio generation (V2A), we use CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib43\" title=\"\">2021</a>)</cite> combined as the encoder.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "input",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding each task, we design 10 diverse textual instructions that describe the objective (details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3\" title=\"Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nDuring training, one instruction is randomly selected from each task as the input, whereas during inference, a fixed instruction is used.</p>\n\n",
                "matched_terms": [
                    "input",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "while",
                    "tts",
                    "aligned",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "input",
                    "design",
                    "svs",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent interference between the two fusion streams, we replace the ineffective input with learnable dummy embeddings.\nSpecifically, <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for TA tasks and <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> for NTA tasks are set as dummy embeddings.</p>\n\n",
                "matched_terms": [
                    "input",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "tts",
                    "svs",
                    "v2a",
                    "while",
                    "data",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sampling",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first compare the performance of UniFlow-Audio with baselines on all tasks to evaluate the overall generation quality.\nThen, we explore the effect of CFG scale on different tasks.\nFinally, we conduct ablation studies on our training and architecture design.</p>\n\n",
                "matched_terms": [
                    "design",
                    "architecture",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "tts",
                    "architecture",
                    "svs",
                    "while",
                    "data",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "tts",
                    "svs",
                    "while",
                    "results",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of two key hyper-parameters in flow matching on generation performance: the guidance scale and the number of inference steps.\nInterestingly, we observe two distinct patterns across all tasks: SE and SR fall into one pattern, while the remaining tasks follow another.\nWe take SE and T2A as representative tasks of the two patterns and report their CLAP and PESQ scores, with higher values indicating better performance for both metrics.\nResults are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "while",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "while",
                    "input",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct ablation studies to validate several components of UniFlow-Audio: 1) architecture design, including dual-fusion and layerwise fusion mechanisms, and 2) the task-balanced data sampling strategy.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "ablation",
                    "architecture",
                    "design",
                    "fusion",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "double",
                    "fusion",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "tts",
                    "input",
                    "while",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors have read and adhere to the ICLR Code of Ethics.\nThis work does not involve human subjects, identifiable private data, or harmful applications.\nAll datasets used are publicly available and were used in accordance with their original licenses and intended purposes.\nNo external sponsorship or conflict of interest influenced the design or conclusions of this work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For T2M, we use songs from MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite> combined with LP-MusicCaps-MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Doh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib7\" title=\"\">2023</a>)</cite> captions as the training data.\nThe original song in MSD can be as long as 14 minutes.\nDuring training, we randomly crop 10 seconds for training.\nThe widely-used benchmark MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> is used for evaluation.</p>\n\n",
                "matched_terms": [
                    "data",
                    "t2m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "aligned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "while",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "while",
                    "architecture",
                    "data",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "while",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs were used as assistive tools in this work.\nSpecifically, they were employed to help with limited code writing and debugging, as well as for polishing the language of the paper.\nThe LLMs involved include mainstream models such as GPT, Claude, and Gemini.\nThese model were used for grammar correction, sentence restructuring, and enhancing overall readability.\nAll technical content, experimental design, results, and conclusions were authored and verified solely by the human authors.\nLLMs did not contribute to the generation of ideas, methods, or data analysis.</p>\n\n",
                "matched_terms": [
                    "data",
                    "results",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">T2A</span> and <span class=\"ltx_text ltx_font_bold\">T2M</span>, we follow AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib26\" title=\"\">2022</a>)</cite> and MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> to evaluate overall quality (OVL) and relevance (REL) to the input caption.</p>\n\n",
                "matched_terms": [
                    "input",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">V2A</span>, we evaluate overall acceptability (OVL) and synchronization (SYNC) with the reference video.\nIn SYNC evaluation, the raters judge whether audio events are temporally aligned with visual cues such as lip movements, object impacts, or musical actions.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "aligned"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 4: Training and evaluation data details of UniFlow-Audio.",
        "body": "Task\nTraining\nEvaluation\nTraining Duration / h\n\n\nTTS\nLibriTTS (Zen et al., 2019)\n\n555\n\n\nSVS\nM4Singer (Zhang et al., 2022)\n\n30\n\n\nT2A\nAudioCaps (Kim et al., 2019)\n\n253\n\n\nSE\nLibriTTS+Wham!\n\n\nVoiceBank+Demand\n(Botinhao et al., 2016)\n\n\n460\n\n\nVCTK+Wham!\n44\n\n\nLJSpeech+Musan\n24\n\n\nVoiceBank+Demand\n10\n\n\nSR\nHQ-TTS\nVCTK\n85\n\n\nMUSDB\nMUSDB\n47\n\n\nMoisesDB\n26\n\n\nFreeSound\nESC\n158\n\n\nT2M\nMSD (McFee et al., 2012)\n\nMusicCaps (Agostinelli et al., 2023)\n\n5789\n\n\nV2A\nVisualSound (Viertola et al., 2025)\n\n236\n\n\nTotal\n-\n7717",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Training</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Evaluation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Training Duration / h</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">LibriTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib62\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">555</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SVS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib63\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">T2A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib23\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">253</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\">SE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LibriTTS+Wham!</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">VoiceBank+Demand</span>\n<span class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Botinhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib2\" title=\"\">2016</a>)</cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">460</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VCTK+Wham!</td>\n<td class=\"ltx_td ltx_align_center\">44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LJSpeech+Musan</td>\n<td class=\"ltx_td ltx_align_center\">24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoiceBank+Demand</td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\">SR</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">HQ-TTS</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">VCTK</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MUSDB</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" rowspan=\"2\">MUSDB</td>\n<td class=\"ltx_td ltx_align_center\">47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MoisesDB</td>\n<td class=\"ltx_td ltx_align_center\">26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FreeSound</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ESC</td>\n<td class=\"ltx_td ltx_align_center\">158</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">T2M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MusicCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib1\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5789</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">V2A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">236</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" colspan=\"2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">7717</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "m4singer",
            "training",
            "task",
            "librittswham",
            "zhang",
            "mcfee",
            "audiocaps",
            "esc",
            "vctk",
            "details",
            "voicebankdemand",
            "vctkwham",
            "tts",
            "botinhao",
            "svs",
            "agostinelli",
            "msd",
            "visualsound",
            "viertola",
            "t2a",
            "hqtts",
            "duration",
            "musiccaps",
            "evaluation",
            "ljspeechmusan",
            "musdb",
            "freesound",
            "zen",
            "uniflowaudio",
            "t2m",
            "kim",
            "libritts",
            "total",
            "v2a",
            "data",
            "moisesdb"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
            "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
            "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "tts",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "details",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio achieves strong results with limited open-source data and parameters on a variety of tasks, demonstrating the advantages of a unified audio generation model.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "details",
                    "uniflowaudio",
                    "tts",
                    "data",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.F2\" title=\"In 3.2 Content Encoding with Task Instruction &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, UniFlow-Audio is a unified flow-matching-based audio generation framework that consists of four parts: a variational autoencoder (VAE) that compresses the raw long audio signal into a short sequence, a content encoding part for extracting features from the input content and task instruction, a duration adapter that generates TA content embeddings, and a Transformer-based flow matching backbone.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "duration",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "duration",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text</span>: For T2A and text-to-music generation (T2M), the input is a coarse text description without the alignment information.\nWe use Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib5\" title=\"\">2024</a>)</cite> as the encoder following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Majumder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib36\" title=\"\">2024</a>; Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "training",
                    "task",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding each task, we design 10 diverse textual instructions that describe the objective (details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3\" title=\"Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nDuring training, one instruction is randomly selected from each task as the input, whereas during inference, a fixed instruction is used.</p>\n\n",
                "matched_terms": [
                    "details",
                    "training",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With task-involved content embeddings <math alttext=\"\\mathbf{C}^{\\mathbf{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C}^{\\mathbf{I}}</annotation></semantics></math>, a clip duration <math alttext=\"d_{c}\\in\\mathbb{R}^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mo>+</mo></msup></mrow><annotation encoding=\"application/x-tex\">d_{c}\\in\\mathbb{R}^{+}</annotation></semantics></math> and a sequence duration <math alttext=\"d_{s}\\in(\\mathbb{R}^{+})^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m3\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#8477;</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">d_{s}\\in(\\mathbb{R}^{+})^{L}</annotation></semantics></math> are predicted.\nSince UniFlow-Audio is an NAR model, both TA and NTA tasks rely on <math alttext=\"d_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">d_{c}</annotation></semantics></math> to determine the output length.\n<math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is only required by TA tasks for duration adaptation, which will be introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS3\" title=\"3.3 Duration Adapter &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.\nFor the duration predictor, we adopt the architecture in FastSpeech2.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training",
                    "tts",
                    "svs",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> denotes model parameters, <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is the flow step, and <math alttext=\"\\mathcal{L}_{\\mathrm{FM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>FM</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{FM}}</annotation></semantics></math> is the flow-matching training loss.\nThe two duration predictors are trained together with the backbone using the following losses:</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\hat{d}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{g}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m9\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are ground-truth clip duration and sequence duration.\nFor NTA tasks, <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math> is omitted.\nIn practice, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m11\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m12\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are converted to frame numbers in the logarithmic domain to calculate <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>, following FastSpeech2.\nThe final training loss is <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m14\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>FM</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-clip</mtext></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>.\nDuring inference, classifier-free guidance (CFG) is employed to balance the trade-off between generated sample diversity and their fidelity to the input content:\n<math alttext=\"v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m15\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>v</mi><mi>&#952;</mi><mtext>CFG</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>w</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">(</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),</annotation></semantics></math>\nwhere <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m16\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is the guidance scale.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained on eight A100 GPUs with a batch size on each GPU of 24.\nWe train three versions with different sizes: small, medium, and large.\nConfiguration and training details are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS2\" title=\"D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS3\" title=\"D.3 Training &amp; Inference Setup &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\nThe small version takes about 7 days to train, while the large version takes about 12 days.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "details",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all tasks, both objective and subjective evaluation are conducted.\nSince UniFlow-Audio is evaluated on a variety of tasks and datasets, we adopt task-specific commonly-adopted metrics, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A2\" title=\"Appendix B Evaluation Metrics &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first compare the performance of UniFlow-Audio with baselines on all tasks to evaluate the overall generation quality.\nThen, we explore the effect of CFG scale on different tasks.\nFinally, we conduct ablation studies on our training and architecture design.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "training",
                    "task",
                    "tts",
                    "svs",
                    "data",
                    "visualsound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "training",
                    "t2m",
                    "tts",
                    "svs",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct ablation studies to validate several components of UniFlow-Audio: 1) architecture design, including dual-fusion and layerwise fusion mechanisms, and 2) the task-balanced data sampling strategy.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "duration",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "t2m",
                    "task",
                    "data",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the architectural design, we examine the effect of fusing time-aligned content embeddings only at the input layer, referred to as <em class=\"ltx_emph ltx_font_italic\">input fusion</em>.\nThis follows the design of F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nAs shown in the middle row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, input fusion leads to a substantial performance drop on time-aligned tasks.\nSince content embeddings are integrated via cross-attention in each DiT block, injecting time-aligned inputs solely at the input layer makes their influence much weaker than that of non-time-aligned inputs.\nConsequently, non-time-aligned tasks are only marginally affected, while the performance on time-aligned tasks degrades significantly.\nIn contrast, UniFlow-Audio employs <em class=\"ltx_emph ltx_font_italic\">block-wise fusion</em>, where time-aligned content embeddings are injected into each DiT block.\nThis progressive fusion allows richer interactions between time-aligned content and audio latents, and proves essential for achieving robust performance across different task types.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we investigate the impact of the proposed task-balanced data sampling strategy.\nAs shown in the last row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing balanced sampling (<em class=\"ltx_emph ltx_font_italic\">w/o balanced sampling</em>) results in degraded performance on non-time-aligned tasks (T2A and T2M), while performance on time-aligned tasks remain relatively stable.\nThis aligns with the number of datasets from different task types: under the original round-robin sampling strategy, time-aligned tasks are overrepresented.\nWithout explicit balancing, the model is more exposed to time-aligned tasks, which amplifies the influence of time-aligned content input.\nIn contrast, the task-balanced sampling strategy ensures that each task type is adequately represented, mitigating the effects of task imbalance and leading to more consistent and reliable performance across both time-aligned and non-time-aligned tasks.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "task",
                    "data",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "t2m",
                    "training",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "uniflowaudio",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The official training subset of AudioCaps is used for T2A training.\nEach sample contains 5 captions in the test subset.\nFollowing TANGO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib14\" title=\"\">2023</a>)</cite>, we randomly select one caption per sample for evaluation, and we use the same selected captions as in their setup.</p>\n\n",
                "matched_terms": [
                    "t2a",
                    "evaluation",
                    "training",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For T2M, we use songs from MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite> combined with LP-MusicCaps-MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Doh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib7\" title=\"\">2023</a>)</cite> captions as the training data.\nThe original song in MSD can be as long as 14 minutes.\nDuring training, we randomly crop 10 seconds for training.\nThe widely-used benchmark MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> is used for evaluation.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "training",
                    "evaluation",
                    "mcfee",
                    "msd",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SE, we utilize the method in URGENT challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib64\" title=\"\">2024</a>)</cite>\nto simulate noisy speech.\nThe clean speech datasets include LibriTTS, VCTK Corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yamagishi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib59\" title=\"\">2019</a>)</cite> and LJSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib22\" title=\"\">2017</a>)</cite>, while the noise datasets contain WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib57\" title=\"\">2019</a>)</cite> and noise subset of Musan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib48\" title=\"\">2015</a>)</cite>.\nRoom Impulse Rresponses (RIRs) dataset for simulation is the RIRs dataset in <cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib24\" title=\"\">2017</a>)</cite>.\nWe choose VoiceBank+Demand&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Botinhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib2\" title=\"\">2016</a>)</cite> for both train and evaluation, which is widely used as a benchmark in SE.</p>\n\n",
                "matched_terms": [
                    "voicebankdemand",
                    "evaluation",
                    "zhang",
                    "libritts",
                    "botinhao",
                    "vctk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SR, we mainly follow the setup of AudioSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>)</cite>, while prioritizing the available sources for ease of collection.\nThe training datasets include MUSDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafii et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib44\" title=\"\">2019</a>)</cite>, MoisesDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pereira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib40\" title=\"\">2023</a>)</cite>, HQ-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib29\" title=\"\">2022b</a>)</cite> and FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib38\" title=\"\">2024</a>)</cite>, while the evaluation uses ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Piczak, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib41\" title=\"\">2015</a>)</cite>, VCTK-test <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, and MUSDB.\nAll high-quality recordings are first resampled to 24&#160;kHz.\nSince our VAE is designed to process 24&#160;kHz audio, we choose a cutoff range of [2,6]&#160;KHz for the downsampled audio. Based on the method introduced in NVSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we then apply the low-pass filter within this range to simulate low-high resolution audio pairs.</p>\n\n",
                "matched_terms": [
                    "hqtts",
                    "training",
                    "evaluation",
                    "musdb",
                    "freesound",
                    "moisesdb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "training",
                    "evaluation",
                    "v2a",
                    "visualsound",
                    "viertola"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib58\" title=\"\">2024</a>)</cite>, we use root mean square error of fundamental frequency (F0) and semitone accuracy (SA)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171\" title=\"\">https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171</a></span></span></span> for objective evaluation.\nSame as TTS, MOS and SMOS are used as subjective metrics for accessing singing quality and singer similarity.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Viertola et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite>, we evaluate V2A performance using ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> (IB) and Synchformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib21\" title=\"\">2024</a>)</cite> (SYNC).\nIB measures semantic modality consistency by computing the cosine similarity between audio and video embeddings.\nSYNC assesses synchronization based on temporal offsets between audio and visual modality estimated by Synchformer.\n</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "viertola"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "details",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "total",
                    "uniflowaudio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all tasks, we conduct MOS-based subjective tests with explicit instructions for raters.\nEach sample is rated on a 1&#8211;5 Likert scale.\nWe recruit ten raters with college-level education and normal hearing ability for subjective evaluation.\nExamples of the rating interface and detailed instructions are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A6.F5\" title=\"In Appendix F Subjective Evaluation Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nBelow we describe the setup for each task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">T2A</span> and <span class=\"ltx_text ltx_font_bold\">T2M</span>, we follow AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib26\" title=\"\">2022</a>)</cite> and MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> to evaluate overall quality (OVL) and relevance (REL) to the input caption.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">V2A</span>, we evaluate overall acceptability (OVL) and synchronization (SYNC) with the reference video.\nIn SYNC evaluation, the raters judge whether audio events are temporally aligned with visual cues such as lip movements, object impacts, or musical actions.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "evaluation"
                ]
            }
        ]
    },
    "A3.T5": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 5: Examples of detailed task instructions.",
        "body": "TTS\nProduce human-like speech from phoneme inputs and speaker representations.\n\n\nGenerate natural speech from speaker embeddings and phoneme sequences while maintaining\n\n\naccurate pronunciation.\n\n\nConvert phoneme sequences into natural speech using speaker embeddings, with precise articulation\n\n\nof words and adaptation to the textual emotional content.\n\n\nT2A\nGenerate an audio clip based on the given text description.\n\n\nSynthesize an audio signal from the given text, ensuring the fidelity of sound event representation\n\n\nand the naturalness of the audio output.\n\n\nConvert the given text into a natural-sounding audio clip, maintaining high fidelity in sound event\n\n\nreproduction (volume, positioning, timing, repetition) and ensuring realistic scene acoustics and\n\n\nevent relationships.\n\n\nSVS\nRender a singing performance from musical notation, including phonemes, notes, durations, and slurs.\n\n\nProduce a singing voice rendering derived from the notated score that maintains parametric fidelity to\n\n\nthe given phonemes, notes, durations, and slurs.\n\n\nSynthesize a singing voice that matches the input musica score’s specifications (phonemes, notes,\n\n\ndurations, slurs) while adapting phoneme durations for natural flow and preserving textual\n\n\nemotional tone.\n\n\nSE\nEnhance noisy speech signals by reducing background noise and reverberation.\n\n\nImprove degraded speech quality by suppressing noise and reverberation while preserving natural\n\n\nvoice characteristics.\n\n\nEnhance speech signals by dynamically suppressing diverse noise types (environmental/mechanical)\n\n\nand reverberation, preserving tonal qualities and timbre across varying SNR conditions.\n\n\nSR\nEnhance audio quality by increasing its sampling rate or resolution.\n\n\nConvert low-sampling-rate audio to high-resolution output, recovering lost high-frequency components\n\n\nand subtle sonic characteristics.\n\n\nUpsample low-resolution audio signals to higher sampling rates while preserving original signal details\n\n\nand recovering high-frequency components without introducing audible artifacts.\n\n\nV2A\nGenerate high-fidelity audio synchronized to video.\n\n\nProduce high-quality audio that matches the video’s scene, with accurate timing, spatial positioning,\n\n\nand realistic sound properties.\n\n\nGenerate high-fidelity audio for the video, ensuring strict temporal alignment, correct spatial direction,\n\n\nloudness, and frequency of sounds, while maintaining realism and coherence with visual content.\n\n\nT2M\nDevelop a music clip that precisely matches the textual description in all aspects.\n\n\nProduce a musical piece that faithfully represents the given description, incorporating all\n\n\nspecified instruments, intended emotions, genre characteristics, and vocal properties.\n\n\nGenerate a musical output that perfectly matches the provided text, incorporating the exact instruments\n\n\nmentioned, upholding authentic stylistic qualities, and delivering the desired emotional impact.\n\n\nIf vocals are required, precisely implement the described gender, age, vocal properties, and singing manner.",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_t\" rowspan=\"5\"><span class=\"ltx_text ltx_font_bold\">TTS</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t\">Produce human-like speech from phoneme inputs and speaker representations.</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Generate natural speech from speaker embeddings and phoneme sequences while maintaining</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">accurate pronunciation.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Convert phoneme sequences into natural speech using speaker embeddings, with precise articulation</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">of words and adaptation to the textual emotional content.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"6\"><span class=\"ltx_text ltx_font_bold\">T2A</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Generate an audio clip based on the given text description.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Synthesize an audio signal from the given text, ensuring the fidelity of sound event representation</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">and the naturalness of the audio output.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Convert the given text into a natural-sounding audio clip, maintaining high fidelity in sound event</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">reproduction (volume, positioning, timing, repetition) and ensuring realistic scene acoustics and</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">event relationships.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"6\"><span class=\"ltx_text ltx_font_bold\">SVS</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Render a singing performance from musical notation, including phonemes, notes, durations, and slurs.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Produce a singing voice rendering derived from the notated score that maintains parametric fidelity to</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">the given phonemes, notes, durations, and slurs.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Synthesize a singing voice that matches the input musica score&#8217;s specifications (phonemes, notes,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">durations, slurs) while adapting phoneme durations for natural flow and preserving textual</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">emotional tone.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"5\"><span class=\"ltx_text ltx_font_bold\">SE</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Enhance noisy speech signals by reducing background noise and reverberation.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Improve degraded speech quality by suppressing noise and reverberation while preserving natural</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">voice characteristics.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Enhance speech signals by dynamically suppressing diverse noise types (environmental/mechanical)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">and reverberation, preserving tonal qualities and timbre across varying SNR conditions.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"5\"><span class=\"ltx_text ltx_font_bold\">SR</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Enhance audio quality by increasing its sampling rate or resolution.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Convert low-sampling-rate audio to high-resolution output, recovering lost high-frequency components</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">and subtle sonic characteristics.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Upsample low-resolution audio signals to higher sampling rates while preserving original signal details</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">and recovering high-frequency components without introducing audible artifacts.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"5\"><span class=\"ltx_text ltx_font_bold\">V2A</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Generate high-fidelity audio synchronized to video.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Produce high-quality audio that matches the video&#8217;s scene, with accurate timing, spatial positioning,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">and realistic sound properties.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Generate high-fidelity audio for the video, ensuring strict temporal alignment, correct spatial direction,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">loudness, and frequency of sounds, while maintaining realism and coherence with visual content.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\" rowspan=\"6\"><span class=\"ltx_text ltx_font_bold\">T2M</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Develop a music clip that precisely matches the textual description in all aspects.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Produce a musical piece that faithfully represents the given description, incorporating all</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">specified instruments, intended emotions, genre characteristics, and vocal properties.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Generate a musical output that perfectly matches the provided text, incorporating the exact instruments</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mentioned, upholding authentic stylistic qualities, and delivering the desired emotional impact.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">If vocals are required, precisely implement the described gender, age, vocal properties, and singing manner.</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sequences",
            "naturalness",
            "coherence",
            "sonic",
            "audible",
            "dynamically",
            "diverse",
            "video",
            "types",
            "realism",
            "representations",
            "convert",
            "details",
            "high",
            "intended",
            "described",
            "environmentalmechanical",
            "maintaining",
            "suppressing",
            "represents",
            "tone",
            "event",
            "piece",
            "noise",
            "direction",
            "temporal",
            "derived",
            "maintains",
            "improve",
            "lowresolution",
            "sounds",
            "rendering",
            "rates",
            "synchronized",
            "scene",
            "manner",
            "detailed",
            "notation",
            "vocal",
            "music",
            "characteristics",
            "its",
            "highresolution",
            "based",
            "durations",
            "authentic",
            "articulation",
            "score’s",
            "produce",
            "reproduction",
            "recovering",
            "loudness",
            "voice",
            "reducing",
            "across",
            "all",
            "given",
            "original",
            "highfidelity",
            "textual",
            "exact",
            "realistic",
            "required",
            "spatial",
            "ensuring",
            "examples",
            "musical",
            "specified",
            "performance",
            "embeddings",
            "timbre",
            "fidelity",
            "instructions",
            "inputs",
            "specifications",
            "sound",
            "naturalsounding",
            "acoustics",
            "adapting",
            "provided",
            "emotional",
            "text",
            "output",
            "volume",
            "input",
            "rate",
            "including",
            "score",
            "flow",
            "speech",
            "incorporating",
            "repetition",
            "tts",
            "conditions",
            "instruments",
            "upsample",
            "generate",
            "genre",
            "from",
            "implement",
            "frequency",
            "noisy",
            "relationships",
            "tonal",
            "emotions",
            "synthesize",
            "slurs",
            "qualities",
            "increasing",
            "components",
            "positioning",
            "age",
            "timing",
            "accurate",
            "highfrequency",
            "impact",
            "alignment",
            "into",
            "delivering",
            "render",
            "highquality",
            "upholding",
            "signals",
            "sampling",
            "enhance",
            "notated",
            "v2a",
            "parametric",
            "representation",
            "snr",
            "task",
            "quality",
            "visual",
            "artifacts",
            "musica",
            "develop",
            "mentioned",
            "video’s",
            "aspects",
            "signal",
            "content",
            "words",
            "vocals",
            "pronunciation",
            "stylistic",
            "clip",
            "adaptation",
            "reverberation",
            "perfectly",
            "svs",
            "notes",
            "desired",
            "precisely",
            "subtle",
            "speaker",
            "gender",
            "properties",
            "t2a",
            "lost",
            "introducing",
            "matches",
            "humanlike",
            "correct",
            "description",
            "lowsamplingrate",
            "without",
            "singing",
            "faithfully",
            "preserving",
            "phoneme",
            "resolution",
            "t2m",
            "phonemes",
            "varying",
            "strict",
            "degraded",
            "natural",
            "while",
            "precise",
            "higher",
            "audio",
            "background"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For each task, we prompt the LLM to generate <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> instructions ranging from simple to complex.\nThese instructions span from basic definitions of the task to detailed specifications of task requirements.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> examples of simple, medium, and complex instructions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "output",
                    "input",
                    "video",
                    "types",
                    "including",
                    "flow",
                    "speech",
                    "across",
                    "alignment",
                    "into",
                    "performance",
                    "sampling",
                    "phonemes",
                    "sound",
                    "music",
                    "natural",
                    "while",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "text",
                    "tts",
                    "music",
                    "content",
                    "audio",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "text",
                    "description",
                    "output",
                    "tts",
                    "input",
                    "temporal",
                    "sound",
                    "strict",
                    "music",
                    "diverse",
                    "video",
                    "v2a",
                    "alignment",
                    "audio",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent works have explored unified audio generation with autoregressive (AR) architectures, unified non-autoregressive (NAR) approaches remain relatively underexplored.\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> adopts an AR paradigm, achieving strong zero-shot performance on both AR and NAR tasks.\nHowever, AR models rely on sequential decoding and discrete tokenizers, whereas NAR models generate continuous audio representations in parallel, which may offer advantages in latency and quality.\nThus, NAR-based unified audio generation remains worth exploring.\nAudioX represents an NAR attempt, but it focuses exclusively on NTA tasks and cannot handle TA tasks such as TTS, which require variable-length generation.\nMeanwhile, task-specific NAR models like VoiceFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib17\" title=\"\">2024</a>)</cite> perform well on TA tasks by temporally aligning content embeddings with audio latents, yet this modeling paradigm does not generalize to NTA tasks.\nThis leaves a gap for a single NAR framework capable of unifying both TA and NTA tasks within one modeling paradigm.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "tts",
                    "generate",
                    "content",
                    "while",
                    "represents",
                    "representations",
                    "performance",
                    "embeddings",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "text",
                    "visual",
                    "input",
                    "types",
                    "including",
                    "flow",
                    "details",
                    "across",
                    "all",
                    "from",
                    "ensuring",
                    "performance",
                    "embeddings",
                    "highquality",
                    "sampling",
                    "signals",
                    "while",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide a novel perspective that formulates diverse audio generation tasks through temporal alignment.</p>\n\n",
                "matched_terms": [
                    "diverse",
                    "temporal",
                    "audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design model architectures and data sampling strategies to balance TA and NTA tasks while ensuring the generation quality, including a dual-fusion mechanism, block-wise fusion, and task-balanced sampling.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "sampling",
                    "ensuring",
                    "while",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "sequences",
                    "text",
                    "input",
                    "enhance",
                    "from",
                    "video",
                    "improve",
                    "while",
                    "into",
                    "performance",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "highfidelity",
                    "noise",
                    "details",
                    "high",
                    "speech",
                    "t2a",
                    "tts",
                    "given",
                    "into",
                    "audio",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.F2\" title=\"In 3.2 Content Encoding with Task Instruction &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, UniFlow-Audio is a unified flow-matching-based audio generation framework that consists of four parts: a variational autoencoder (VAE) that compresses the raw long audio signal into a short sequence, a content encoding part for extracting features from the input content and task instruction, a duration adapter that generates TA content embeddings, and a Transformer-based flow matching backbone.</p>\n\n",
                "matched_terms": [
                    "task",
                    "input",
                    "from",
                    "signal",
                    "content",
                    "into",
                    "embeddings",
                    "audio",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, we employ a VAE that operates on raw waveforms for direct waveform generation and reducing latency.\nThe VAE encoder compresses the waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{L}</annotation></semantics></math> into a latent representation <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mi>L</mi><mo>/</mo><msup><mn>2</mn><mi>R</mi></msup></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> denote the waveform length, compression ratio and latent dimension, respectively.\nThe VAE architecture also follows <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, with details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS1\" title=\"D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.\nWe train the VAE on a mixture of high-quality speech, music, singing voice and general audio datasets to improve the generation performance on various domains.\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "details",
                    "speech",
                    "highquality",
                    "voice",
                    "reducing",
                    "performance",
                    "singing",
                    "music",
                    "into",
                    "improve",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All inputs are transformed into continuous embeddings <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> instead of discrete tokens to avoid information loss by modality-specific content encoders:</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "all",
                    "content",
                    "into",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "voice",
                    "phonemes",
                    "tts",
                    "input",
                    "singing",
                    "svs",
                    "from",
                    "content",
                    "speaker",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text</span>: For T2A and text-to-music generation (T2M), the input is a coarse text description without the alignment information.\nWe use Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib5\" title=\"\">2024</a>)</cite> as the encoder following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Majumder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib36\" title=\"\">2024</a>; Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "text",
                    "description",
                    "input",
                    "without",
                    "alignment",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio</span>: For audio input, we reuse the VAE as the encoder to compress the sequence length.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video</span>: For video input in video-to-audio generation (V2A), we use CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib43\" title=\"\">2021</a>)</cite> combined as the encoder.</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "clip",
                    "video",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "textual",
                    "clip",
                    "task",
                    "t2m",
                    "input",
                    "from",
                    "content",
                    "into",
                    "embeddings",
                    "instructions",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding each task, we design 10 diverse textual instructions that describe the objective (details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3\" title=\"Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nDuring training, one instruction is randomly selected from each task as the input, whereas during inference, a fixed instruction is used.</p>\n\n",
                "matched_terms": [
                    "details",
                    "textual",
                    "instructions",
                    "task",
                    "input",
                    "from",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With task-involved content embeddings <math alttext=\"\\mathbf{C}^{\\mathbf{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C}^{\\mathbf{I}}</annotation></semantics></math>, a clip duration <math alttext=\"d_{c}\\in\\mathbb{R}^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m2\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mo>+</mo></msup></mrow><annotation encoding=\"application/x-tex\">d_{c}\\in\\mathbb{R}^{+}</annotation></semantics></math> and a sequence duration <math alttext=\"d_{s}\\in(\\mathbb{R}^{+})^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m3\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#8477;</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">d_{s}\\in(\\mathbb{R}^{+})^{L}</annotation></semantics></math> are predicted.\nSince UniFlow-Audio is an NAR model, both TA and NTA tasks rely on <math alttext=\"d_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">d_{c}</annotation></semantics></math> to determine the output length.\n<math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p7.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is only required by TA tasks for duration adaptation, which will be introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS3\" title=\"3.3 Duration Adapter &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.\nFor the duration predictor, we adopt the architecture in FastSpeech2.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "required",
                    "adaptation",
                    "output",
                    "content",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "temporal",
                    "input",
                    "content",
                    "while",
                    "noisy",
                    "alignment",
                    "embeddings",
                    "audio",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this insight, we introduce a unified <span class=\"ltx_text ltx_font_italic\">duration adapter</span> to explicitly align content embeddings with audio latents across all TA tasks.\nWe posit that this explicit alignment offers superior efficacy for TA tasks than the implicit mechanisms of cross-attention.\nSpecifically, <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> is expanded to a time-aligned content <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>. That is,</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "content",
                    "alignment",
                    "embeddings",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "matches",
                    "tts",
                    "input",
                    "svs",
                    "video",
                    "v2a",
                    "audio",
                    "based",
                    "durations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generation backbone is a flow-matching Transformer composed of multiple Transformer blocks.\nWe employ a dual-fusion mechanism to integrate both <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> and <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> into generation.\nWithin each block, the audio latent <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> is first processed by self-attention.\nThe flow matching timestep <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is incorporated by adaptive layer norm (AdaLN) as</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent interference between the two fusion streams, we replace the ineffective input with learnable dummy embeddings.\nSpecifically, <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for TA tasks and <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> for NTA tasks are set as dummy embeddings.</p>\n\n",
                "matched_terms": [
                    "embeddings",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\hat{d}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{g}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m9\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are ground-truth clip duration and sequence duration.\nFor NTA tasks, <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math> is omitted.\nIn practice, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m11\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m12\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are converted to frame numbers in the logarithmic domain to calculate <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>, following FastSpeech2.\nThe final training loss is <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m14\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>FM</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-clip</mtext></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>.\nDuring inference, classifier-free guidance (CFG) is employed to balance the trade-off between generated sample diversity and their fidelity to the input content:\n<math alttext=\"v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m15\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>v</mi><mi>&#952;</mi><mtext>CFG</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>w</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">(</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),</annotation></semantics></math>\nwhere <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m16\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is the guidance scale.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "content",
                    "input",
                    "fidelity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
                "matched_terms": [
                    "details",
                    "resolution",
                    "t2m",
                    "all",
                    "tts",
                    "svs",
                    "v2a",
                    "while",
                    "audio",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "t2m",
                    "sampling",
                    "upsample",
                    "from",
                    "types",
                    "performance",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained on eight A100 GPUs with a batch size on each GPU of 24.\nWe train three versions with different sizes: small, medium, and large.\nConfiguration and training details are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS2\" title=\"D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS3\" title=\"D.3 Training &amp; Inference Setup &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\nThe small version takes about 7 days to train, while the large version takes about 12 days.</p>\n\n",
                "matched_terms": [
                    "details",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first compare the performance of UniFlow-Audio with baselines on all tasks to evaluate the overall generation quality.\nThen, we explore the effect of CFG scale on different tasks.\nFinally, we conduct ablation studies on our training and architecture design.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "high",
                    "task",
                    "quality",
                    "all",
                    "tts",
                    "music",
                    "speaker",
                    "svs",
                    "from",
                    "singing",
                    "while",
                    "specified",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "phoneme",
                    "required",
                    "t2m",
                    "across",
                    "performance",
                    "inputs",
                    "tts",
                    "vocal",
                    "sound",
                    "delivering",
                    "without",
                    "svs",
                    "its",
                    "from",
                    "while",
                    "improve",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of two key hyper-parameters in flow matching on generation performance: the guidance scale and the number of inference steps.\nInterestingly, we observe two distinct patterns across all tasks: SE and SR fall into one pattern, while the remaining tasks follow another.\nWe take SE and T2A as representative tasks of the two patterns and report their CLAP and PESQ scores, with higher values indicating better performance for both metrics.\nResults are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "flow",
                    "impact",
                    "while",
                    "into",
                    "performance",
                    "higher",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "quality",
                    "input",
                    "signal",
                    "speech",
                    "reducing",
                    "all",
                    "from",
                    "t2a",
                    "noise",
                    "textual",
                    "increasing",
                    "description",
                    "without",
                    "performance",
                    "fidelity",
                    "inputs",
                    "characteristics",
                    "while",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we conduct ablation studies to validate several components of UniFlow-Audio: 1) architecture design, including dual-fusion and layerwise fusion mechanisms, and 2) the task-balanced data sampling strategy.</p>\n\n",
                "matched_terms": [
                    "including",
                    "components",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "adaptation",
                    "task",
                    "all",
                    "types",
                    "content",
                    "into",
                    "performance",
                    "embeddings",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "task",
                    "t2m",
                    "described",
                    "its",
                    "types",
                    "content",
                    "while",
                    "into",
                    "performance",
                    "embeddings",
                    "based",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the architectural design, we examine the effect of fusing time-aligned content embeddings only at the input layer, referred to as <em class=\"ltx_emph ltx_font_italic\">input fusion</em>.\nThis follows the design of F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nAs shown in the middle row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, input fusion leads to a substantial performance drop on time-aligned tasks.\nSince content embeddings are integrated via cross-attention in each DiT block, injecting time-aligned inputs solely at the input layer makes their influence much weaker than that of non-time-aligned inputs.\nConsequently, non-time-aligned tasks are only marginally affected, while the performance on time-aligned tasks degrades significantly.\nIn contrast, UniFlow-Audio employs <em class=\"ltx_emph ltx_font_italic\">block-wise fusion</em>, where time-aligned content embeddings are injected into each DiT block.\nThis progressive fusion allows richer interactions between time-aligned content and audio latents, and proves essential for achieving robust performance across different task types.</p>\n\n",
                "matched_terms": [
                    "task",
                    "across",
                    "inputs",
                    "input",
                    "types",
                    "content",
                    "while",
                    "into",
                    "performance",
                    "embeddings",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we investigate the impact of the proposed task-balanced data sampling strategy.\nAs shown in the last row of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing balanced sampling (<em class=\"ltx_emph ltx_font_italic\">w/o balanced sampling</em>) results in degraded performance on non-time-aligned tasks (T2A and T2M), while performance on time-aligned tasks remain relatively stable.\nThis aligns with the number of datasets from different task types: under the original round-robin sampling strategy, time-aligned tasks are overrepresented.\nWithout explicit balancing, the model is more exposed to time-aligned tasks, which amplifies the influence of time-aligned content input.\nIn contrast, the task-balanced sampling strategy ensures that each task type is adequately represented, mitigating the effects of task imbalance and leading to more consistent and reliable performance across both time-aligned and non-time-aligned tasks.</p>\n\n",
                "matched_terms": [
                    "task",
                    "across",
                    "t2m",
                    "sampling",
                    "performance",
                    "input",
                    "without",
                    "degraded",
                    "from",
                    "content",
                    "types",
                    "impact",
                    "while",
                    "original",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "t2m",
                    "voice",
                    "inputs",
                    "tts",
                    "input",
                    "music",
                    "while",
                    "speaker",
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "introducing",
                    "audio",
                    "across",
                    "text",
                    "without",
                    "enhance",
                    "its",
                    "including",
                    "performance",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors have read and adhere to the ICLR Code of Ethics.\nThis work does not involve human subjects, identifiable private data, or harmful applications.\nAll datasets used are publicly available and were used in accordance with their original licenses and intended purposes.\nNo external sponsorship or conflict of interest influenced the design or conclusions of this work.</p>\n\n",
                "matched_terms": [
                    "all",
                    "intended",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All code and source files are provided in the supplementary material and will be publicly released.</p>\n\n",
                "matched_terms": [
                    "all",
                    "provided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n",
                "matched_terms": [
                    "details",
                    "all",
                    "tts",
                    "described",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For T2M, we use songs from MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite> combined with LP-MusicCaps-MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Doh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib7\" title=\"\">2023</a>)</cite> captions as the training data.\nThe original song in MSD can be as long as 14 minutes.\nDuring training, we randomly crop 10 seconds for training.\nThe widely-used benchmark MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> is used for evaluation.</p>\n\n",
                "matched_terms": [
                    "original",
                    "from",
                    "t2m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SE, we utilize the method in URGENT challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib64\" title=\"\">2024</a>)</cite>\nto simulate noisy speech.\nThe clean speech datasets include LibriTTS, VCTK Corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yamagishi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib59\" title=\"\">2019</a>)</cite> and LJSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib22\" title=\"\">2017</a>)</cite>, while the noise datasets contain WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib57\" title=\"\">2019</a>)</cite> and noise subset of Musan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib48\" title=\"\">2015</a>)</cite>.\nRoom Impulse Rresponses (RIRs) dataset for simulation is the RIRs dataset in <cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib24\" title=\"\">2017</a>)</cite>.\nWe choose VoiceBank+Demand&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Botinhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib2\" title=\"\">2016</a>)</cite> for both train and evaluation, which is widely used as a benchmark in SE.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "noise",
                    "while",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SR, we mainly follow the setup of AudioSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>)</cite>, while prioritizing the available sources for ease of collection.\nThe training datasets include MUSDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafii et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib44\" title=\"\">2019</a>)</cite>, MoisesDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pereira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib40\" title=\"\">2023</a>)</cite>, HQ-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib29\" title=\"\">2022b</a>)</cite> and FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib38\" title=\"\">2024</a>)</cite>, while the evaluation uses ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Piczak, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib41\" title=\"\">2015</a>)</cite>, VCTK-test <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, and MUSDB.\nAll high-quality recordings are first resampled to 24&#160;kHz.\nSince our VAE is designed to process 24&#160;kHz audio, we choose a cutoff range of [2,6]&#160;KHz for the downsampled audio. Based on the method introduced in NVSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we then apply the low-pass filter within this range to simulate low-high resolution audio pairs.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "resolution",
                    "all",
                    "while",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "high",
                    "ensuring",
                    "without",
                    "from",
                    "video",
                    "v2a",
                    "performance",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib55\" title=\"\">2024</a>)</cite>, we use Word Error Rate (WER)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> as an objective metric to evaluate the accuracy of generated speech with respect to the given transcription, and Speaker Similarity (SIM)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/speakerverification_en_titanet_large\" title=\"\">https://huggingface.co/nvidia/speakerverification_en_titanet_large</a></span></span></span> to assess the consistency of speaker characteristics between the generated and reference speech. For subjective evaluation, we employ the Mean Opinion Score (MOS) to measure overall speech naturalness and the Similarity MOS (SMOS) to assess perceived speaker similarity.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "given",
                    "rate",
                    "characteristics",
                    "score",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib58\" title=\"\">2024</a>)</cite>, we use root mean square error of fundamental frequency (F0) and semitone accuracy (SA)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171\" title=\"\">https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171</a></span></span></span> for objective evaluation.\nSame as TTS, MOS and SMOS are used as subjective metrics for accessing singing quality and singer similarity.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "frequency",
                    "quality",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "t2m",
                    "music",
                    "while",
                    "alignment",
                    "score",
                    "audio",
                    "based",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib49\" title=\"\">2023</a>)</cite>, we choose Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) for SE evaluation.\nPESQ measures perceptual speech quality, and STOI estimates speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we adopt Log-Spectral Distance (LSD) for objective evaluation.\nLSD measures the discrepancy between the original high-frequency audio and the generated audio.\nNote that the baseline model AudioSR generates 48&#160;kHz audio, while ours operates at 24&#160;kHz.\nFor fair comparison, AudioSR outputs are downsampled to 24&#160;kHz before evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "original",
                    "highfrequency",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Viertola et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite>, we evaluate V2A performance using ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> (IB) and Synchformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib21\" title=\"\">2024</a>)</cite> (SYNC).\nIB measures semantic modality consistency by computing the cosine similarity between audio and video embeddings.\nSYNC assesses synchronization based on temporal offsets between audio and visual modality estimated by Synchformer.\n</p>\n\n",
                "matched_terms": [
                    "temporal",
                    "visual",
                    "video",
                    "v2a",
                    "performance",
                    "embeddings",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "details",
                    "clip",
                    "highfidelity",
                    "audio",
                    "across",
                    "provided",
                    "singing",
                    "music",
                    "from",
                    "types",
                    "while",
                    "including",
                    "diverse",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To measure the reconstruction quality of VAE, we evaluate the mean squared error (MSE) and signal-to-noise ratio (SNR) on held-out test sets.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T7\" title=\"Table 7 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our VAE achieves consistently lower MSE and higher SNR than the one in EzAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib18\" title=\"\">Hai et&#160;al., </a>)</cite>, which was only trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib13\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "snr",
                    "higher",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the potential negative influence from <math alttext=\"\\mathcal{L}_{\\text{dur-clip}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-clip</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-clip}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>, we apply gradient scaling to the duration predictors.\nSpecifically, we scale the gradients from the duration losses by a factor <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> before backpropagation, thereby reducing their influence on the model.</p>\n\n",
                "matched_terms": [
                    "from",
                    "reducing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "performance",
                    "rate",
                    "content",
                    "impact",
                    "while",
                    "improve",
                    "embeddings",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs were used as assistive tools in this work.\nSpecifically, they were employed to help with limited code writing and debugging, as well as for polishing the language of the paper.\nThe LLMs involved include mainstream models such as GPT, Claude, and Gemini.\nThese model were used for grammar correction, sentence restructuring, and enhancing overall readability.\nAll technical content, experimental design, results, and conclusions were authored and verified solely by the human authors.\nLLMs did not contribute to the generation of ideas, methods, or data analysis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all tasks, we conduct MOS-based subjective tests with explicit instructions for raters.\nEach sample is rated on a 1&#8211;5 Likert scale.\nWe recruit ten raters with college-level education and normal hearing ability for subjective evaluation.\nExamples of the rating interface and detailed instructions are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A6.F5\" title=\"In Appendix F Subjective Evaluation Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nBelow we describe the setup for each task.</p>\n\n",
                "matched_terms": [
                    "task",
                    "detailed",
                    "all",
                    "examples",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "matches",
                    "quality",
                    "voice",
                    "tts",
                    "singing",
                    "svs",
                    "characteristics",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">T2A</span> and <span class=\"ltx_text ltx_font_bold\">T2M</span>, we follow AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib26\" title=\"\">2022</a>)</cite> and MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> to evaluate overall quality (OVL) and relevance (REL) to the input caption.</p>\n\n",
                "matched_terms": [
                    "input",
                    "t2m",
                    "quality",
                    "t2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">SE</span>, raters assess the intelligibility and naturalness\nof enhanced speech.\nEach output is presented together with its clean reference target, and the MOS scores reflect residual noise, processing artifacts, and overall listening quality.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "noise",
                    "quality",
                    "output",
                    "artifacts",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">V2A</span>, we evaluate overall acceptability (OVL) and synchronization (SYNC) with the reference video.\nIn SYNC evaluation, the raters judge whether audio events are temporally aligned with visual cues such as lip movements, object impacts, or musical actions.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "video",
                    "v2a",
                    "musical",
                    "audio"
                ]
            }
        ]
    },
    "A4.T6": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 6: Datasets used for training the waveform-based VAE.",
        "body": "Domain\nDatasets\n\n\n\n\nSpeech\nAISHELL-3 (Shi et al., 2021), TTS-HQ, LJSpeech, LibriTTS, VCTK\n\n\nSinging\nOpenSinger (Huang et al., 2021), M4Singer, OpenCpop (Wang et al., 2022), PopCS (Liu et al., 2022c)\n\n\n\nMusic\nMUSDB, MoisesDB, MusicCaps\n\n\nGeneral Audio\nAudioSet (Gemmeke et al., 2017)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Domain</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Datasets</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib47\" title=\"\">2021</a>)</cite>, TTS-HQ, LJSpeech, LibriTTS, VCTK</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Singing</th>\n<td class=\"ltx_td ltx_align_left\">OpenSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib20\" title=\"\">2021</a>)</cite>, M4Singer, OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib56\" title=\"\">2022</a>)</cite>, PopCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib33\" title=\"\">2022c</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Music</th>\n<td class=\"ltx_td ltx_align_left\">MUSDB, MoisesDB, MusicCaps</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">General Audio</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib13\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "m4singer",
            "training",
            "gemmeke",
            "2022c",
            "datasets",
            "used",
            "vctk",
            "vae",
            "speech",
            "opensinger",
            "wang",
            "audioset",
            "aishell3",
            "musiccaps",
            "ljspeech",
            "singing",
            "huang",
            "musdb",
            "opencpop",
            "ttshq",
            "popcs",
            "liu",
            "domain",
            "general",
            "waveformbased",
            "libritts",
            "music",
            "shi",
            "audio",
            "moisesdb"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "general",
                    "training",
                    "music",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "wang",
                    "music",
                    "liu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent NAR generative models, diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>)</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, have attracted significant attention in audio generation due to their strong generative capabilities and the fast inference speed through parallel generation.\nNaturalSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib46\" title=\"\">2024</a>)</cite>, E3-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib12\" title=\"\">2023</a>)</cite>, and AudioLDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> demonstrate the capabilities of latent diffusion models on speech and audio generation.\nTo achieve high-fidelity generation with extremely few steps, flow matching is adopted for T2A and TTS with low latency&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib9\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib16\" title=\"\">2024</a>)</cite>.\nIt alleviates the high inference latency inherent to the iterative denoising process in diffusion models by directly learning a continuous velocity field that transports noise into data in a few integration steps, rather than requiring a substantial number of discrete denoising iterations.\nFlow matching is also employed in hybrid TTS systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib8\" title=\"\">2024</a>)</cite> to refine acoustic details given discrete tokens predicted by the AR component.\nMotivated by the success of flow matching in prior speech and audio generation works, UniFlow-Audio adopts flow matching as the backbone.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "liu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.F2\" title=\"In 3.2 Content Encoding with Task Instruction &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, UniFlow-Audio is a unified flow-matching-based audio generation framework that consists of four parts: a variational autoencoder (VAE) that compresses the raw long audio signal into a short sequence, a content encoding part for extracting features from the input content and task instruction, a duration adapter that generates TA content embeddings, and a Transformer-based flow matching backbone.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, we employ a VAE that operates on raw waveforms for direct waveform generation and reducing latency.\nThe VAE encoder compresses the waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{L}</annotation></semantics></math> into a latent representation <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mi>L</mi><mo>/</mo><msup><mn>2</mn><mi>R</mi></msup></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> denote the waveform length, compression ratio and latent dimension, respectively.\nThe VAE architecture also follows <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, with details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS1\" title=\"D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.\nWe train the VAE on a mixture of high-quality speech, music, singing voice and general audio datasets to improve the generation performance on various domains.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "general",
                    "singing",
                    "music",
                    "datasets",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme &amp; MIDI</span>:\nFor TTS, phonemes from grapheme-to-phoneme conversion (g2p)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and x-vectors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib54\" title=\"\">2023b</a>)</cite> for speaker information are used as input.\nWe use the Transformer-based encoder from FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib45\" title=\"\">2020</a>)</cite> as the content encoder.\nSinging voice synthesis (SVS) is similar to TTS, except that the input is MIDI rather than phonemes.\nIn addition to phoneme embeddings, the MIDI encoder incorporates pitch, pitch duration, and slur information, which are fused with the phoneme embeddings through addition.</p>\n\n",
                "matched_terms": [
                    "used",
                    "wang",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio</span>: For audio input, we reuse the VAE as the encoder to compress the sequence length.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE, Flan-T5, and CLIP are frozen during training.\nAfter obtaining <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> from the content encoder, we further integrate task instructions to inject explicit task-specific information, enabling the model to distinguish between tasks that share the same input modality (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, T2A and T2M).\nThis integration is achieved through an instruction encoder and a content adapter: the former maps the textual instruction into embeddings <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math>, and the latter fuses <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> with <math alttext=\"\\mathbf{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mi>&#119816;</mi><annotation encoding=\"application/x-tex\">\\mathbf{I}</annotation></semantics></math> via cross-attention (Attn) by</p>\n\n",
                "matched_terms": [
                    "training",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding each task, we design 10 diverse textual instructions that describe the objective (details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A3\" title=\"Appendix C Task Instructions &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nDuring training, one instruction is randomly selected from each task as the input, whereas during inference, a fixed instruction is used.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1\" title=\"1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, audio generation tasks can be divided into TA and NTA categories by their temporal alignment constraint.\nIn NTA tasks where input and target audio lack temporal correspondence, cross-attention mechanism is typically used to integrate <math alttext=\"\\mathbf{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119810;</mi><annotation encoding=\"application/x-tex\">\\mathbf{C}</annotation></semantics></math> into the generation process.\nIn TA tasks, alignment information is often explicitly leveraged for generation.\nFor instance, TTS relies on phoneme-to-frame alignment to expand linguistic units, while speech enhancement (SE) inherently operates on frame-aligned noisy and clean audio pairs.\nIn such cases, content embeddings are aligned and concatenated with audio features, a process that may require a duration adapter.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the sequence duration <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math>, the duration adapter repeats each embedding <math alttext=\"c^{I}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mi>i</mi><mi>I</mi></msubsup><annotation encoding=\"application/x-tex\">c^{I}_{i}</annotation></semantics></math> in <math alttext=\"\\mathbf{C^{I}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119810;</mi><mi>&#119816;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}}</annotation></semantics></math> for <math alttext=\"(d_{s})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(d_{s})_{i}</annotation></semantics></math> steps, producing <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math> that matches the length of the audio latents.\nFor TTS and SVS, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> specifies the number of audio latents per phoneme.\nFor SE and V2A, each value in <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is fixed, since each input audio latent or video frame corresponds to a fixed number of target audio latents.\nFor NTA tasks, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> is set to a constant dummy value to achieve a unified design.\nDuring training, ground-truth durations are used to obtain <math alttext=\"\\mathbf{C^{I}_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119810;</mi><mi>&#119827;</mi><mi>&#119816;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{C^{I}_{T}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\hat{d}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{g}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m9\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are ground-truth clip duration and sequence duration.\nFor NTA tasks, <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math> is omitted.\nIn practice, <math alttext=\"d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m11\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">d_{s}</annotation></semantics></math> and <math alttext=\"\\hat{d}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m12\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{s}</annotation></semantics></math> are converted to frame numbers in the logarithmic domain to calculate <math alttext=\"\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>, following FastSpeech2.\nThe final training loss is <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m14\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>FM</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-clip</mtext></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur-seq</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\mathrm{FM}}+\\mathcal{L}_{\\text{dur-clip}}+\\mathcal{L}_{\\text{dur-seq}}</annotation></semantics></math>.\nDuring inference, classifier-free guidance (CFG) is employed to balance the trade-off between generated sample diversity and their fidelity to the input content:\n<math alttext=\"v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m15\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>v</mi><mi>&#952;</mi><mtext>CFG</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>w</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">(</mo><mrow><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><msup><mi>&#119810;</mi><mi>I</mi></msup><mo>,</mo><msubsup><mi>&#119810;</mi><mi>T</mi><mi>I</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#964;</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"1.600em\" minsize=\"1.600em\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">v_{\\theta}^{\\text{CFG}}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})=v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)+w\\cdot\\Big(v_{\\theta}(\\mathbf{z}_{\\tau},\\mathbf{C}^{I},\\mathbf{C}^{I}_{T})-v_{\\theta}(\\mathbf{z}_{\\tau},\\varnothing,\\varnothing)\\Big),</annotation></semantics></math>\nwhere <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m16\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is the guidance scale.</p>\n\n",
                "matched_terms": [
                    "domain",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nSeven tasks are involved: TTS, SVS, T2A, T2M, SE, audio Super Resolution (SR) and V2A.\nAmong them, T2A and T2M are NTA tasks, while the rest are TA tasks.\nDetails of all training and evaluation data are demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nA total of 7.7K hours of data are used for training, which is substantially less than that employed in UniAudio and AudioX.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, different tasks&#8217; dataset sizes vary substantially due to discrepancies in collection difficulty and availability.\nTo prevent overexposure to small-scale datasets caused by random sampling, a straightforward approach is to adopt a task-based round-robin sampling strategy: sample data from each task in turn.\nHowever, since the number of different task types is imbalanced (five TA tasks and two NTA tasks), task-based round-robin sampling disproportionately favors TA tasks during training, which may in turn affect the model&#8217;s overall performance.\nTo this end, we upsample data from NTA tasks: T2M by 3 times and T2A by 2 times.\nWe refer to this sampling strategy as <em class=\"ltx_emph ltx_font_italic\">task-balanced sampling</em>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "training",
                    "singing",
                    "music",
                    "datasets",
                    "used",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed dual-fusion mechanism, we replace it with alternative fusion strategies and compare their generation performance.\nAs <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F4\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates, we investigate two alternative fusion mechanisms: <em class=\"ltx_emph ltx_font_italic\">cross-attention fusion</em> and <em class=\"ltx_emph ltx_font_italic\">double fusion</em>.\nCross-attention fusion is the most straightforward approach, where all contents are fused with the audio latent via cross-attention, similar to AudioLDM2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>.\nDouble fusion resembles our proposed dual fusion mechanism but differs in one aspect: content embeddings both before and after duration adaptation are fed into the backbone, regardless of the task type.\nIn contrast, in dual fusion, ineffective content embeddings based on task types are set to dummy embeddings.\nThis design may introduce interference between the learning of different task types.\nIn contrast, the dual fusion mechanism employs dummy embeddings, which provide better guidance for the model to attend to different sources depending on the task type, thereby mitigating such interference.\n</p>\n\n",
                "matched_terms": [
                    "liu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T3\" title=\"In 5.3 Ablation Studies &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the results of alternative content fusion mechanisms, which are consistent with our assumptions.\nAlthough cross-attention has shown strong performance in prior T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, applying it directly to a mixture of task types results in poor performance.\nEven on non-time-aligned T2A and T2M tasks, its performance is significantly worse than that of dual fusion, suggesting that the presence of rich time-aligned data adversely affects models based on cross-attention.\nCompared with double fusion, dual fusion achieves similar performance on time-aligned tasks, while substantially outperforming it on non-time-aligned tasks.\nThis demonstrates the effectiveness of the dummy embedding design.\nAs described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S3.SS4\" title=\"3.4 Dual-Fusion Flow Matching Transformer &#8227; 3 UniFlow-Audio &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, for non-time-aligned tasks, the duration used for content expansion is a dummy value.\nConsequently, the incorporation of expanded content embeddings into the generation process acts as noise.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "training",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors have read and adhere to the ICLR Code of Ethics.\nThis work does not involve human subjects, identifiable private data, or harmful applications.\nAll datasets used are publicly available and were used in accordance with their original licenses and intended purposes.\nNo external sponsorship or conflict of interest influenced the design or conclusions of this work.</p>\n\n",
                "matched_terms": [
                    "used",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained and evaluated on a series of public datasets.\nDetails of all training and evaluation data are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A1.T4\" title=\"In Appendix A Data Details &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nFor TTS and SVS, we use the official training / validation / test splits of LibriTTS and M4Singer.\nDetails of other datasets are described in the following:</p>\n\n",
                "matched_terms": [
                    "m4singer",
                    "training",
                    "datasets",
                    "libritts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The official training subset of AudioCaps is used for T2A training.\nEach sample contains 5 captions in the test subset.\nFollowing TANGO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib14\" title=\"\">2023</a>)</cite>, we randomly select one caption per sample for evaluation, and we use the same selected captions as in their setup.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For T2M, we use songs from MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib37\" title=\"\">2012</a>)</cite> combined with LP-MusicCaps-MSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Doh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib7\" title=\"\">2023</a>)</cite> captions as the training data.\nThe original song in MSD can be as long as 14 minutes.\nDuring training, we randomly crop 10 seconds for training.\nThe widely-used benchmark MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> is used for evaluation.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SE, we utilize the method in URGENT challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib64\" title=\"\">2024</a>)</cite>\nto simulate noisy speech.\nThe clean speech datasets include LibriTTS, VCTK Corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yamagishi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib59\" title=\"\">2019</a>)</cite> and LJSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib22\" title=\"\">2017</a>)</cite>, while the noise datasets contain WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib57\" title=\"\">2019</a>)</cite> and noise subset of Musan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib48\" title=\"\">2015</a>)</cite>.\nRoom Impulse Rresponses (RIRs) dataset for simulation is the RIRs dataset in <cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib24\" title=\"\">2017</a>)</cite>.\nWe choose VoiceBank+Demand&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Botinhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib2\" title=\"\">2016</a>)</cite> for both train and evaluation, which is widely used as a benchmark in SE.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ljspeech",
                    "libritts",
                    "datasets",
                    "used",
                    "vctk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SR, we mainly follow the setup of AudioSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>)</cite>, while prioritizing the available sources for ease of collection.\nThe training datasets include MUSDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafii et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib44\" title=\"\">2019</a>)</cite>, MoisesDB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pereira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib40\" title=\"\">2023</a>)</cite>, HQ-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib29\" title=\"\">2022b</a>)</cite> and FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib38\" title=\"\">2024</a>)</cite>, while the evaluation uses ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Piczak, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib41\" title=\"\">2015</a>)</cite>, VCTK-test <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, and MUSDB.\nAll high-quality recordings are first resampled to 24&#160;kHz.\nSince our VAE is designed to process 24&#160;kHz audio, we choose a cutoff range of [2,6]&#160;KHz for the downsampled audio. Based on the method introduced in NVSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we then apply the low-pass filter within this range to simulate low-high resolution audio pairs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training",
                    "musdb",
                    "datasets",
                    "liu",
                    "vae",
                    "moisesdb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2A, since the widely used VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite> dataset is constructed from in-the-wild videos without ensuring high audio-video correspondence, it includes a considerable amount of modality-mismatched samples where the video and audio are not semantically related.\nThis limitation is detrimental to training stability and the inherent irrelevance is harmful to the performance.\nTherefore, we adopt the smaller but better audio-visual aligned VisualSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib52\" title=\"\">2025</a>)</cite> for both training and evaluation, which is curated based on ImageBind scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib15\" title=\"\">2023</a>)</cite> to identify videos with poor audio-visual correspondence.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib55\" title=\"\">2024</a>)</cite>, we use Word Error Rate (WER)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> as an objective metric to evaluate the accuracy of generated speech with respect to the given transcription, and Speaker Similarity (SIM)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/speakerverification_en_titanet_large\" title=\"\">https://huggingface.co/nvidia/speakerverification_en_titanet_large</a></span></span></span> to assess the consistency of speaker characteristics between the generated and reference speech. For subjective evaluation, we employ the Mean Opinion Score (MOS) to measure overall speech naturalness and the Similarity MOS (SMOS) to assess perceived speaker similarity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib58\" title=\"\">2024</a>)</cite>, we use root mean square error of fundamental frequency (F0) and semitone accuracy (SA)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171\" title=\"\">https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/svs1/svs.sh#L1171</a></span></span></span> for objective evaluation.\nSame as TTS, MOS and SMOS are used as subjective metrics for accessing singing quality and singer similarity.</p>\n\n",
                "matched_terms": [
                    "used",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous T2A and T2M studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib32\" title=\"\">2024b</a>)</cite>, we adopt Frechet Distance (FD) and CLAP score for audio and music generation evaluation.\nFD measures the similarity of the distribution between generated and reference audio based on PANNs CNN14&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib25\" title=\"\">2020</a>)</cite> features, while CLAP score serves as a reference-free metric that captures the semantic alignment between textual descriptions and generated audio.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib31\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib28\" title=\"\">2022a</a>)</cite>, we adopt Log-Spectral Distance (LSD) for objective evaluation.\nLSD measures the discrepancy between the original high-frequency audio and the generated audio.\nNote that the baseline model AudioSR generates 48&#160;kHz audio, while ours operates at 24&#160;kHz.\nFor fair comparison, AudioSR outputs are downsampled to 24&#160;kHz before evaluation.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To measure the reconstruction quality of VAE, we evaluate the mean squared error (MSE) and signal-to-noise ratio (SNR) on held-out test sets.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T7\" title=\"Table 7 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our VAE achieves consistently lower MSE and higher SNR than the one in EzAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib18\" title=\"\">Hai et&#160;al., </a>)</cite>, which was only trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib13\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audioset",
                    "gemmeke",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained using AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib34\" title=\"\">2017</a>)</cite> with a constant learning rate of 5e-5 with a warmup step of 10K steps and a total training step of 400K steps.\nTo mitigate the negative impact of excessively long audio content sequence on training efficiency, we take a maximum of 5 second audio segments randomly during training for SE and SR.\nDuring inference, we take an inference step of 25 by default.\nSway sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib4\" title=\"\">2025</a>)</cite> is adopted to improve the generation performance.\nDuring training, both TA and NTA content embeddings are randomly masked with a ratio of 0.2 to train conditional and unconditional generation simultaneously.\nDuring inference, a CFG scale of 5.0 is adopted for tasks except SE and SR while CFG is not applied for these two tasks, due to the influence of CFG on them (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.F3\" title=\"In 5.2 Effect of CFG and Inference Steps &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">TTS</span> and <span class=\"ltx_text ltx_font_bold\">SVS</span>, we evaluate speech quality MOS (MOS) and speaker similarity MOS (SMOS).\nFor MOS, raters judge the overall naturalness and listening quality of the synthesized speech or singing voice.\nFor SMOS, raters judge whether the generated audio matches the target/reference speaker in terms of timbre-related characteristics, disregarding prosodic variations.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "singing"
                ]
            }
        ]
    },
    "A4.T7": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 7: Reconstruction performance of VAE.",
        "body": "Domain\nSpeech\nMusic\n\n\nMSE ↓\\downarrow\nSNR (dB) ↑\\uparrow\nMSE ↓\\downarrow\nSNR (dB) ↑\\uparrow\n\n\n\n\nEzAudio VAE\n4.43×10−54.43\\times 10^{-5}\n17.06\n1.13×10−41.13\\times 10^{-4}\n18.09\n\n\nOurs\n3.84×10−53.84\\times 10^{-5}\n17.63\n8.42×10−58.42\\times 10^{-5}\n19.27",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Domain</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Music</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">MSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">SNR (dB) <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">MSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">SNR (dB) <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">EzAudio VAE</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.43\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m5\" intent=\":literal\"><semantics><mrow><mn>4.43</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.43\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">17.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.13\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m6\" intent=\":literal\"><semantics><mrow><mn>1.13</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.13\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"3.84\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m7\" intent=\":literal\"><semantics><mrow><mn>3.84</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.84\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">17.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"8.42\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T7.m8\" intent=\":literal\"><semantics><mrow><mn>8.42</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">8.42\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">19.27</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "domain",
            "speech",
            "snr",
            "842×10−5842times",
            "↑uparrow",
            "443×10−5443times",
            "384×10−5384times",
            "↓downarrow",
            "ours",
            "music",
            "mse",
            "performance",
            "113×10−4113times",
            "vae",
            "ezaudio",
            "reconstruction"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To measure the reconstruction quality of VAE, we evaluate the mean squared error (MSE) and signal-to-noise ratio (SNR) on held-out test sets.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T7\" title=\"Table 7 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our VAE achieves consistently lower MSE and higher SNR than the one in EzAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib18\" title=\"\">Hai et&#160;al., </a>)</cite>, which was only trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib13\" title=\"\">2017</a>)</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid evolution of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib51\" title=\"\">2017</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib19\" title=\"\">2020</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib27\" title=\"\">2023</a>)</cite>, recent works have achieved remarkable improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib10\" title=\"\">2024</a>; Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib42\" title=\"\">2024</a>)</cite>, promoting popularity of artificial intelligence generated content (AIGC).\nAs an important modality, audio has also made remarkable progress in various generation tasks, with text-to-speech synthesis (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib53\" title=\"\">2023a</a>)</cite> and text-to-audio (T2A) generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib30\" title=\"\">2023</a>)</cite> serving as representative tasks.\nTraditional audio generation models are designed for specific tasks, such as converting text to speech or music.\nThis paradigm is suboptimal, as it overlooks the interconnected nature of real-world auditory information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this limitation, we aim at a unified framework for audio generation that accommodates diverse input (text, audio, video) and output modalities (speech, music, sound effect).\nWe observe that despite their differences, these tasks can be fundamentally categorized by the temporal relationship between input and output: either <span class=\"ltx_text ltx_font_bold ltx_font_italic\">time-aligned (TA)</span> or <span class=\"ltx_text ltx_font_bold ltx_font_italic\">non-time-aligned (NTA)</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S1.F1\" title=\"In 1 Introduction &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor TA tasks, there is strict temporal alignment between input and output, such as the monotonic alignment in text-to-speech (TTS), the one-to-one frame alignment in speech enhancement (SE), and one-to-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> frame alignment in video-to-audio&#160;(V2A).\nIn contrast, NTA tasks, such as T2A, do not require such a temporal alignment constraint: the input sequence (textual description) corresponds holistically to the entire output soundscape, with semantic consistency being the primary objective rather than temporal correspondence.\nThis fundamental difference in alignment requirements has historically necessitated specialized modeling approaches for TA and NTA tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, we employ a VAE that operates on raw waveforms for direct waveform generation and reducing latency.\nThe VAE encoder compresses the waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{L}</annotation></semantics></math> into a latent representation <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mi>L</mi><mo>/</mo><msup><mn>2</mn><mi>R</mi></msup></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L/2^{R}\\times D}</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> denote the waveform length, compression ratio and latent dimension, respectively.\nThe VAE architecture also follows <cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>, with details shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS1\" title=\"D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.\nWe train the VAE on a mixture of high-quality speech, music, singing voice and general audio datasets to improve the generation performance on various domains.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "vae",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music",
                    "performance",
                    "vae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the T2A task, the effects of the guidance scale and inference steps are consistent with typical findings in diffusion-based models: larger guidance scales and more inference steps yield steady performance improvements.\nThis is expected, as stronger guidance provides more effective conditioning from the textual description, while more inference steps allow smaller step sizes in the denoising trajectory, which improves fidelity by reducing error accumulation.\nHowever, SE exhibits a sharp performance decline as the guidance scale increases, with PESQ dropping from 2.9 to 1.75.\nWe attribute this to the characteristics of SE: the input inherently contains both signal and noise.\nStronger guidance thus amplifies not only the signal but also the noise, leading to reduced perceptual quality in the generated speech.\nIn contrast, the input of T2A is a textual description without &#8220;noise&#8221;, so all information should ideally be reflected in the generated audio.\nRegarding inference steps, increasing the number of steps is also detrimental to the performance, although the effect is considerably smaller than that of the guidance scale (<math alttext=\"2.98\\rightarrow 2.89\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.98</mn><mo stretchy=\"false\">&#8594;</mo><mn>2.89</mn></mrow><annotation encoding=\"application/x-tex\">2.98\\rightarrow 2.89</annotation></semantics></math>).\nThis degradation may also stem from the fact that SE inputs contain both signal and noise.\nWith more inference steps, residual noise can accumulate through the iterative denoising process, slightly reducing the perceptual quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE adopts a fully-convolutional architecture with residual 1D blocks and Snake activations, following the design from <cite class=\"ltx_cite ltx_citemacro_cite\">Evans et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib11\" title=\"\">2025</a>)</cite>.\nThe encoder maps raw waveforms into a compact latent sequence at a downsampling ratio of 480 with 128 channels, while the decoder mirrors the encoder by progressively upsampling the latent sequence with transposed convolution to reconstruct the waveform.\nTo achieve high-fidelity audio generation across different audio types, we train the VAE using a diverse set of datasets from multiple categories, including speech, singing, music, and general audio, with details provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T6\" title=\"Table 6 &#8227; D.1 Waveform-based VAE &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nThe model is trained for 1M steps on this extensive collection of approximately 6000 hours data, where each audio clip is randomly cropped to 1.5s segments during training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vae",
                    "music"
                ]
            }
        ]
    },
    "A4.T8": {
        "source_file": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities",
        "caption": "Table 8: Model configurations.",
        "body": "Model Size\nDepth\nEmbed Size\nNum Heads\n# Total / Trainable Params\n\n\n\n\nSmall\n12\n512\n8\n593M / 208M\n\n\nMedium\n16\n768\n12\n780M / 395M\n\n\nLarge\n24\n1024\n16\n1.2B / 847M",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Model Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Depth</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Embed Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Num Heads</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"># Total / Trainable Params</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Small</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">593M / 208M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Medium</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">780M / 395M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.2B / 847M</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "medium",
            "size",
            "trainable",
            "large",
            "208m",
            "12b",
            "780m",
            "model",
            "params",
            "depth",
            "embed",
            "593m",
            "395m",
            "num",
            "847m",
            "configurations",
            "total",
            "small",
            "heads"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.T8\" title=\"In D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the architectural configurations of different UniFlow-Audio versions.\nNotably, the small variant contains only approximately 200M trainable parameters, yet it achieves competitive performance as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio generation, including speech, music and sound effects, has advanced rapidly in recent years.\nThese tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available.\nSince modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories.\nHowever, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation.\nPrevious unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored.\nIn this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching.\nWe propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block.\nTask-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks.\nUniFlow-Audio supports omni-modalities, including text, audio, and video.\nBy leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters.\nEven the small variant with only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m19\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation.\nCode and models will be available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsntxxn.github.io/uniflow_audio\" title=\"\">https://wsntxxn.github.io/uniflow_audio</a>.</p>\n\n",
                "matched_terms": [
                    "small",
                    "trainable",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching that unifies both TA and NTA tasks within a single non-auto-regressive (NAR) model.\nFrom the modeling perspective, we propose a dual-fusion mechanism to temporally align audio latents with input features for TA tasks, while utilizing cross-attention to integrate input features for NTA tasks, ensuring high-quality generation across both categories.\nTo avoid interference between the two fusion strategies, task-irrelevant features (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, NTA features for TA tasks and TA features for NTA tasks) are replaced with learnable dummy embeddings, keeping TA and NTA feature integration disentangled.\nBoth TA and NTA tasks are integrated in each block of the backbone (block-wise fusion), enabling the input to more effectively guide the generation.\nTo balance the amount of different data types, we adopt a task-balanced sampling strategy to balance the ratio between TA and NTA data during training.\nMoreover, UniFlow-Audio supports a broader range of input modalities than prior works, including text, audio, and visual signals.\nWith all these modalities and tasks involved, UniFlow-Audio learns the shared knowledge across different tasks, which in turn yields competitive or superior performance compared to task-specific baselines.\nNotably, compared with other unified audio generation models (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> for details on data and model sizes), our small variant (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200M parameters), trained on fewer than 8K hours of public data, achieves strong results, underscoring the data efficiency and parameter effectiveness achieved by UniFlow-Audio.</p>\n\n",
                "matched_terms": [
                    "small",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, the research paradigm in audio generation has shifted from task-specific models to unified frameworks capable of handling multiple tasks within a single model.\nSuch frameworks facilitate cross-domain knowledge sharing and improve data efficiency.\nRepresentative works include UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>.\nUniAudio is a large language model (LLM) based AR model that discretizes audio and various input modalities into token sequences and leverages a multi-scale Transformer to model inter- and intra-frame correlations.\nUniAudio is trained on 165K hours of data.\nDespite 11 tasks being included, the video input modality is not supported in UniAudio.\nIn contrast, AudioX adopts an NAR Diffusion Transformer (DiT) with a multi-modal input masking strategy to enhance robustness and generation performance.\nWhile trained on 29K hours of large-scale curated data, it focuses exclusively on NTA tasks.\nCompared with these pioneering works, UniFlow-Audio proposed a flow-matching-based unified NAR framework that achieves good performance on both TA and NTA tasks, with omni input modalities involved (text, audio, video) whilst trained on smaller datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniFlow-Audio is trained on eight A100 GPUs with a batch size on each GPU of 24.\nWe train three versions with different sizes: small, medium, and large.\nConfiguration and training details are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS2\" title=\"D.2 Flow Matching Backbone &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#A4.SS3\" title=\"D.3 Training &amp; Inference Setup &#8227; Appendix D Architecture &amp; Hyper-Parameters &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\nThe small version takes about 7 days to train, while the large version takes about 12 days.</p>\n\n",
                "matched_terms": [
                    "small",
                    "large",
                    "medium",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comparison between UniFlow-Audio and prior works is demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T1\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nFor each task, we select a model from prior single-task works whose architecture and training data are closely aligned with our setting, while also demonstrating competitive performance.\nAlthough unified audio generation has been explored (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S2.SS0.SSS0.Px1\" title=\"Unified Audio Generation &#8227; 2 Related Work &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>), UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> is trained on much larger data ,while AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite> can only handle audio and music generation tasks.\nTherefore, we do not incorporate them for comparison.\nExcept for LM-based MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite>, all other baseline models adopt the diffusion architecture.\nUniFlow-Audio achieves at least comparable performance to baselines and significantly outperforms baselines on TTS, SE and SR.\nFor TTS, both objective and subjective scores show the better synthesis quality of UniFlow-Audio.\nCompared with NaturalSpeech 2, we observe greater diversity in the prosody of the synthesized speech, which leads to lower subjective scores for speaker similarity.\nFor SVS, a specified vocoder with high reconstruction quality is used in DiffSinger, while UniFlow-Audio uses a universal VAE, resulting in slightly lower singing synthesis quality on soprano samples.\nFor other tasks, UniFlow-Audio performs quite competitively with training only on limited public datasets.\nIn comparison, MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib6\" title=\"\">2023</a>)</cite> was trained on private datasets, and DiffFoley is trained on VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib3\" title=\"\">2020</a>)</cite>, which is ten times the size of VisualSound.</p>\n\n",
                "matched_terms": [
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also explore the effect of model size on the generation performance.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#S5.T2\" title=\"In 5.1 Unified Audio Generation &#8227; 5 Results &#8227; UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that UniFlow-Audio achieves competitive performance even with relatively few parameters.\nUniFlow-Audio small, with only 208M trainable parameters, already outperforms baseline models across most tasks.\nThis demonstrates that UniFlow-Audio is parameter-efficient, delivering strong results without relying on excessively large model sizes.\nWe assume it can be attributed to the benefit of multi-task training since there is intrinsic commonality in the knowledge required by different tasks.\nFor example, TTS and SVS both require generating vocal from phoneme inputs, while T2A and T2M both require generating sound from coarse textual descriptions.\nIn contrast, other universal generation models, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib60\" title=\"\">2024</a>)</cite> and AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24391v1#bib.bib50\" title=\"\">2025</a>)</cite>, both contain more than 1B parameters.\nAlthough medium and large model versions further improve performance on certain tasks, the performance gap between the small model and its larger counterparts remains moderate.</p>\n\n",
                "matched_terms": [
                    "208m",
                    "model",
                    "medium",
                    "size",
                    "small",
                    "trainable",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite unifying TA and NTA audio generation within a flow-matching-based NAR framework, UniFlow-Audio has several limitations.\nFirst, tasks involving multiple TA/NTA inputs, such as voice conversion (source speech + target speaker utterance), are not explored.\nSecond, the model&#8217;s generalization to unseen tasks or input modalities, similar to the zero-shot generalization capabilities of LLMs, has not been investigated.\nThird, the data and model size have not been scaled.\nExcept for T2M, most tasks have under 1,000 hours of training data.\nFinally, UniFlow-Audio currently focuses on single-stream audio generation, while multi-stream or multi-source generation (e.g., TTS with background music) remains largely underexplored.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present UniFlow-Audio, a flow-matching-based universal audio generation framework that unifies both TA and NTA tasks within a single NAR model.\nBy introducing a dual-fusion mechanism with block-wise integration, UniFlow-Audio effectively combines TA and NTA features without cross-task interference.\nThe model leverages shared knowledge across multiple modalities, including text, audio, and vision, to enhance generation performance through unified audio modeling.\nExtensive experiments demonstrate that, even with limited training data and moderate model size (as small as 200M trainable parameters), UniFlow-Audio achieves competitive performance across diverse tasks, highlighting its potential as a foundation model for unified NAR audio generation.</p>\n\n",
                "matched_terms": [
                    "small",
                    "trainable",
                    "model",
                    "size"
                ]
            }
        ]
    }
}