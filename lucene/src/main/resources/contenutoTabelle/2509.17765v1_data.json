{
    "S2.T1": {
        "caption": "Table 1: The architectural design of Qwen3-Omni-30B-A3B and the end-to-end first-packet latency for Audio/Video (ms).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Module</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Architecture</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Streaming</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Audio Encoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AuT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">650M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Vision Encoder</td>\n<td class=\"ltx_td ltx_align_center\">SigLIP2-So400M</td>\n<td class=\"ltx_td ltx_align_center\">540M</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Thinker</td>\n<td class=\"ltx_td ltx_align_center\">MoE Transformer</td>\n<td class=\"ltx_td ltx_align_center\">30B-A3B</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Talker</td>\n<td class=\"ltx_td ltx_align_center\">MoE Transformer</td>\n<td class=\"ltx_td ltx_align_center\">3B-A0.3B</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MTP</td>\n<td class=\"ltx_td ltx_align_center\">Dense Transformer</td>\n<td class=\"ltx_td ltx_align_center\">80M</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Code2wav</td>\n<td class=\"ltx_td ltx_align_center\">ConvNet</td>\n<td class=\"ltx_td ltx_align_center\">200M</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"4\">End-to-End First-Packet Latency: <span class=\"ltx_text ltx_font_bold\">234/547ms</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "234547ms",
            "qwen3omni30ba3b",
            "aut",
            "transformer",
            "mtp",
            "dense",
            "audio",
            "architectural",
            "params",
            "architecture",
            "streaming",
            "talker",
            "80m",
            "encoder",
            "moe",
            "code2wav",
            "3ba03b",
            "30ba3b",
            "convnet",
            "thinker",
            "540m",
            "vision",
            "audiovideo",
            "650m",
            "siglip2so400m",
            "design",
            "200m",
            "firstpacket",
            "âœ“checkmark",
            "latency",
            "endtoend",
            "module"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "architecture",
                    "talker",
                    "streaming",
                    "convnet",
                    "latency",
                    "moe",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer",
                    "aut",
                    "mtp",
                    "architecture",
                    "talker",
                    "convnet",
                    "thinker",
                    "encoder",
                    "moe",
                    "code2wav"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "streaming",
                    "latency",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the Thinker and Talker adopt Mixture-of-Experts (MoE) architectures to support high concurrency and fast inference.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "streaming",
                    "talker",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since textual representations are decoupled, the Thinker and Talker can use distinct system prompts, independently controlling the Thinker&#8217;s response style and the Talker&#8217;s audio style.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "thinker",
                    "talker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Talker adopts a multi-codebook autoregressive scheme: Talker generates one codec frame per step, while the MTP module produces the remaining residual codebooks.</p>\n\n",
                "matched_terms": [
                    "mtp",
                    "talker",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Code2Wav is implemented as a lightweight causal ConvNet, simplifying the final stage of audio synthesis.</p>\n\n",
                "matched_terms": [
                    "convnet",
                    "audio",
                    "code2wav"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training and inference, the Talker directly ingests high-dimensional multimodal features from the Thinker and shares access to the full conversational history. As a result, the system operates as a cohesive single model, enabling end-to-end training and unified inference.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "aut",
                    "streaming",
                    "thinker",
                    "encoder",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer",
                    "aut",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "siglip2so400m",
                    "audio",
                    "aut",
                    "thinker",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the context of multimodal audiovisual streams, the audio component is encoded with a temporal ID for every 80 ms. The video is treated as a sequence of frames with monotonically increasing temporal IDs that are dynamically adjusted based on their actual timestamps to ensure a consistent temporal resolution of 80 ms per ID. The height and width IDs for video frames are assigned in the same manner as for still images. To prevent positional conflicts when processing multiple modalities, the position numbering is made contiguous, with each subsequent modality commencing from one plus the maximum position ID of the preceding modality. This refined approach to positional encoding enables the model to effectively integrate and jointly model information from diverse modalities. In a departure from Qwen2.5-Omni, which segments audiovisual representations into fixed 2-second chunks, Qwen3-Omni directly aligns these representations using their temporal IDs, which are explicitly anchored to absolute time. This design choice affords the model the flexibility to support streaming inputs of arbitrary duration.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "streaming",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech synthesis in multi-turn dialogues, our Talker module is conditioned on a rich context inherited from a &#8221;Thinker&#8221; component, comprising historical textual tokens, multimodal representations, and the current turn&#8217;s streamed text. This reliance on long-context information is critical, as high-fidelity speech synthesis must adapt acoustic attributes like prosody, loudness, and emotion to the ongoing discourse, a principle well-established in context-aware generative models.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Architecturally, our approach departs from <cite class=\"ltx_cite ltx_citemacro_citet\">Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> by operating directly on RVQ tokens. The Talker employs a hierarchical prediction scheme: the backbone ingests the aggregated codebook features of the current frame and uses a linear head to predict the zeroth codebook, after which a multi-token prediction (MTP) module generates all residual codebooks. This strategy enables the model to learn a complete representation of acoustic details, enhancing vocal expressivity. Consequently, waveform reconstruction is simplified to a lightweight causal ConvNet (Code2Wav), which significantly reduces inference latency and computational cost (FLOPs) while achieving superior audio fidelity compared to more complex DiT-based vocoders.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mtp",
                    "talker",
                    "convnet",
                    "latency",
                    "code2wav",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In streaming audiovisual interaction scenarios, the first-packet latency is a critical factor affecting user experience, and the model&#8217;s concurrency capability is key to reducing service costs and improving response speed. This section discusses how Qwen3-Omni enhances concurrency and reduces first-packet latency through algorithmic and architectural optimizations.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "architectural",
                    "latency",
                    "firstpacket"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Qwen3-Omni, we retain the chunked-prefilling mechanism as implemented in Qwen2.5-Omni, whose audio and vision encoders are capable of outputting chunks along the temporal dimension. During real-time interaction, Thinker and Talker modules perform asynchronous prefilling: when Thinker completes prefilling the current chunk, its output high-level representations are immediately used to prefill the Talker&#8217;s current chunk asynchronously, while Thinker prefills its next chunk. This approach significantly reduces the Time-To-First-Token (TTFT) for both the Thinker and the Talker. Architecturally, both Thinker and the Talker in Qwen3-Omni adopt the MoE design, which is highly effective for improving service throughput. Compared to dense models, the MoE architecture significantly decreases IO consumption arising from KV cache during processing of long sequences, thereby increasing tokens per second (TPS) during generation and enhancing concurrency.</p>\n\n",
                "matched_terms": [
                    "design",
                    "audio",
                    "architecture",
                    "talker",
                    "thinker",
                    "moe",
                    "dense",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize the user&#8217;s waiting time for receiving the first generated packet, we propose a <span class=\"ltx_text ltx_font_italic\">left context only multi-codebook generation</span> mechanism. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, once Talker generates the first token, the MTP module predicts the remaining tokens for the current frame. These tokens are then decoded into waveform by a streaming multi-codebook codec decoder that only attends to the left context. Unlike Qwen2.5-Omni that requires waiting for sufficient block-context from the Talker before synthesis, Qwen3-Omni can output the waveform immediately after the Talker generates each token, significantly reducing first-packet latency.</p>\n\n",
                "matched_terms": [
                    "firstpacket",
                    "mtp",
                    "streaming",
                    "talker",
                    "latency",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the MTP module and codec decoder are lightweight modules, which have low computational FLOPs and support batched inference, making them well-suited for high-concurrency scenarios. The MTP Module is an ultra-lightweight fixed-step autoregressive dense transformer, with low memory bandwidth requirements on inference hardware, thereby naturally enabling efficient batch processing of high throughput requests. Its fixed-step autoregressive inference mechanism allows it to effectively leverage a fixed KV cache memory space for acceleration, achieving low inference latency. Meanwhile, the ConvNet-based codec decoder also achieves high throughput with low latency because its convolutional architecture enjoys extensive hardware acceleration support across diverse inference platforms, and it enables efficient batched inference.</p>\n\n",
                "matched_terms": [
                    "transformer",
                    "mtp",
                    "architecture",
                    "latency",
                    "dense",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.T2\" title=\"Table 2 &#8227; Lightweight MTP module and ConvNet. &#8227; 2.5 Designs for Streaming and Concurrency &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the theoretical first-packet latency for Qwen3-Omni under typical computational resources across varying concurrency scenarios. Experiments are conducted on the vLLM framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib32\" title=\"\">2023</a>)</cite> to process concurrent audiovisual streams, with optimizations applied via <span class=\"ltx_text ltx_font_italic\">torch.compile</span> and CUDA Graph acceleration to the MTP Module and codec decoder. Several factors influence the total first-packet latency. First, the model sizes of Thinker and Talker impact their tail packet preprocessing latency (multi-modal data preprocessing and inference for Audio and Vision Encoder) and Time-To-First-Token (TTPT). Second, the architectures and sizes of the MTP Module and Codec Decoder affect their inference latency. Due to the sequential dependency between these components, the total first-packet latency represents the sum of these individual latencies. As shown in the results, the MoE architecture of Thinker and Talker ensures that their prefill latency and TTPT remain largely unaffected under high concurrency. Meanwhile, the lightweight design of the MTP Module and Codec Decoder minimizes their computational overhead, resulting in a lower impact on first-packet latency. Furthermore, after the initial packet is output and the model starts streaming audio synthesis, the 12.5Hz token rate Talker requires only one token to synthesize 80ms audio. Consequently, the Generation Real Time Factor (RTF) is calculated by dividing the sum of: (1) the time taken by Thinker and Talker to generate one token; and (2) the processing time per token for the MTP Module and Codec Decoder by 80ms. As demonstrated, the RTF consistently remains below 1 across varying concurrency levels, ensuring that users receive continuously streaming audio responses.</p>\n\n",
                "matched_terms": [
                    "design",
                    "audio",
                    "firstpacket",
                    "mtp",
                    "architecture",
                    "streaming",
                    "talker",
                    "thinker",
                    "encoder",
                    "moe",
                    "latency",
                    "vision",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training of Qwen3-Omni is structured into three distinct stages. In the first stage, we lock the LLM parameters and focus on training the vision and audio encoders, utilizing a vast corpus of audio-text and image-text pairs to enhance semantic understanding within the LLM. In the second stage, we unfreeze all parameters and train with a wider range of multimodal data for more comprehensive learning. In the final stage, we use data with a sequence length of 32,768 to enhance the model&#8217;s ability to understand complex long-sequence data:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Encoder Alignment Stage (S1)</span>: During the initial pretraining phase, the LLM component of Qwen3-Omni is initialized with parameters from Qwen3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, while the vision encoder is adopted from Qwen3-VL, and the audio encoder is initialized with AuT. The two encoders are trained separately on the fixed LLM, with both initially focusing on training their respective adapters before training the encoders. We abandon the stage used in <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>); Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> where the encoder and adapter are trained jointly while keeping the LLM frozen, because this approach may cause the encoder to compensate for the limitations of the frozen LLM, which can lead to degraded perception capabilities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "aut",
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training phase comprises a three-stage training process for Thinker, enabling Qwen3-Omni to possess instruction-following capabilities. The dataset, designed in the ChatML <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib39\" title=\"\">2022</a>)</cite> format, includes pure text-based dialogue data, visual modality conversation data, audio modality conversation data, and mixed-modality conversation data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a four-stage training process for Talker, enabling Qwen3-Omni to generate speech response simultaneously with text. All training data is structured in the ChatML format to ensure consistency with Thinker.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Captioning is a foundational task in multimodal understanding, integral to the training and evaluation of large multimodal models. However, the vast majority of existing research has concentrated on visual captioning, largely neglecting the audio modality. This omission is significant, as auditory perception is a crucial component of human sensory experience and interaction with the world. To address this gap and facilitate more comprehensive research in multimodal perception, we introduce the Qwen3-Omni-30B-A3B-Captioner. This model was developed by fine-tuning the Qwen3-Omni-30B-A3B on a large-scale dataset of detailed audio descriptions. The resulting system generates detailed, low-hallucination captions for arbitrary audio inputs. The <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS2\" title=\"9.2 Qualitative Results from Qwen3-Omni-30B-A3B-Captioner &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> provides qualitative results that demonstrate our model&#8217;s captioning capabilities across diverse acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate Qwen3-Omni&#8217;s ability to comprehend various multimodal inputs (text, audio, vision, and audiovisual video) and generate textual responses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "design",
                    "30ba3b",
                    "audio",
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "architecture",
                    "streaming",
                    "latency",
                    "endtoend",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "latency",
                    "endtoend",
                    "vision"
                ]
            }
        ]
    },
    "S2.T2": {
        "caption": "Table 2: Theoretical First-Packet Latency of Qwen3-Omni wit Different Concurrency.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-30B-A3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">1 Concurrency</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">4 Concurrency</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">6 Concurrency</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Thinker-Talker Tail Packet Preprocessing Latency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">72/160ms</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">94/180ms</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">100/200ms</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Thinker Time-to-First-Token (TTPT)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88/160ms</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">468/866ms</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">673/1330ms</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Talker Time-to-First-Token (TTPT)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">57/210ms</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">145/450ms</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">376/734ms</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">MTP Module Time Cost Per Token</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">14ms</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">16ms</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">18ms</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Codec Decoder Time Cost Per Code</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">3ms</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">5ms</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">5ms</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">Overral Latency (Audio/Video)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">234/547ms</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">728/1517ms</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">1172/2284ms</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Thinker Token Generation Rate (TPS)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75 tokens/s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">63 tokens/s</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">53 tokens/s</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">Talker Token Generation Rate (TPS)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">140 tokens/s</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">125 tokens/s</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">110 tokens/s</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">Generation RTF(Real Time Factor)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">0.56</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">0.66</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "234547ms",
            "468866ms",
            "decoder",
            "qwen3omni30ba3b",
            "mtp",
            "ttpt",
            "tokenss",
            "6731330ms",
            "57210ms",
            "thinkertalker",
            "18ms",
            "time",
            "5ms",
            "11722284ms",
            "14ms",
            "timetofirsttoken",
            "tail",
            "token",
            "rate",
            "145450ms",
            "rtfreal",
            "talker",
            "generation",
            "100200ms",
            "code",
            "cost",
            "packet",
            "qwen3omni",
            "94180ms",
            "thinker",
            "tps",
            "concurrency",
            "wit",
            "factor",
            "audiovideo",
            "72160ms",
            "16ms",
            "7281517ms",
            "firstpacket",
            "theoretical",
            "376734ms",
            "preprocessing",
            "codec",
            "overral",
            "latency",
            "88160ms",
            "3ms",
            "different",
            "module"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.T2\" title=\"Table 2 &#8227; Lightweight MTP module and ConvNet. &#8227; 2.5 Designs for Streaming and Concurrency &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the theoretical first-packet latency for Qwen3-Omni under typical computational resources across varying concurrency scenarios. Experiments are conducted on the vLLM framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib32\" title=\"\">2023</a>)</cite> to process concurrent audiovisual streams, with optimizations applied via <span class=\"ltx_text ltx_font_italic\">torch.compile</span> and CUDA Graph acceleration to the MTP Module and codec decoder. Several factors influence the total first-packet latency. First, the model sizes of Thinker and Talker impact their tail packet preprocessing latency (multi-modal data preprocessing and inference for Audio and Vision Encoder) and Time-To-First-Token (TTPT). Second, the architectures and sizes of the MTP Module and Codec Decoder affect their inference latency. Due to the sequential dependency between these components, the total first-packet latency represents the sum of these individual latencies. As shown in the results, the MoE architecture of Thinker and Talker ensures that their prefill latency and TTPT remain largely unaffected under high concurrency. Meanwhile, the lightweight design of the MTP Module and Codec Decoder minimizes their computational overhead, resulting in a lower impact on first-packet latency. Furthermore, after the initial packet is output and the model starts streaming audio synthesis, the 12.5Hz token rate Talker requires only one token to synthesize 80ms audio. Consequently, the Generation Real Time Factor (RTF) is calculated by dividing the sum of: (1) the time taken by Thinker and Talker to generate one token; and (2) the processing time per token for the MTP Module and Codec Decoder by 80ms. As demonstrated, the RTF consistently remains below 1 across varying concurrency levels, ensuring that users receive continuously streaming audio responses.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "time",
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "theoretical",
                    "codec",
                    "qwen3omni",
                    "talker",
                    "generation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "qwen3omni",
                    "mtp",
                    "talker",
                    "generation",
                    "thinker",
                    "code",
                    "concurrency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "latency",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Qwen3-Omni employs Thinker-Talker architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Compared with Qwen2.5-Omni, Qwen3-Omni introduces the following changes for greater scalability and control:</p>\n\n",
                "matched_terms": [
                    "thinkertalker",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the Thinker and Talker adopt Mixture-of-Experts (MoE) architectures to support high concurrency and fast inference.</p>\n\n",
                "matched_terms": [
                    "concurrency",
                    "talker",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "generation",
                    "preprocessing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since textual representations are decoupled, the Thinker and Talker can use distinct system prompts, independently controlling the Thinker&#8217;s response style and the Talker&#8217;s audio style.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Talker adopts a multi-codebook autoregressive scheme: Talker generates one codec frame per step, while the MTP module produces the remaining residual codebooks.</p>\n\n",
                "matched_terms": [
                    "mtp",
                    "talker",
                    "codec",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training and inference, the Talker directly ingests high-dimensional multimodal features from the Thinker and shares access to the full conversational history. As a result, the system operates as a cohesive single model, enabling end-to-end training and unified inference.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "thinker",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "token",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "thinker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the context of multimodal audiovisual streams, the audio component is encoded with a temporal ID for every 80 ms. The video is treated as a sequence of frames with monotonically increasing temporal IDs that are dynamically adjusted based on their actual timestamps to ensure a consistent temporal resolution of 80 ms per ID. The height and width IDs for video frames are assigned in the same manner as for still images. To prevent positional conflicts when processing multiple modalities, the position numbering is made contiguous, with each subsequent modality commencing from one plus the maximum position ID of the preceding modality. This refined approach to positional encoding enables the model to effectively integrate and jointly model information from diverse modalities. In a departure from Qwen2.5-Omni, which segments audiovisual representations into fixed 2-second chunks, Qwen3-Omni directly aligns these representations using their temporal IDs, which are explicitly anchored to absolute time. This design choice affords the model the flexibility to support streaming inputs of arbitrary duration.</p>\n\n",
                "matched_terms": [
                    "time",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech synthesis in multi-turn dialogues, our Talker module is conditioned on a rich context inherited from a &#8221;Thinker&#8221; component, comprising historical textual tokens, multimodal representations, and the current turn&#8217;s streamed text. This reliance on long-context information is critical, as high-fidelity speech synthesis must adapt acoustic attributes like prosody, loudness, and emotion to the ongoing discourse, a principle well-established in context-aware generative models.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Architecturally, our approach departs from <cite class=\"ltx_cite ltx_citemacro_citet\">Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> by operating directly on RVQ tokens. The Talker employs a hierarchical prediction scheme: the backbone ingests the aggregated codebook features of the current frame and uses a linear head to predict the zeroth codebook, after which a multi-token prediction (MTP) module generates all residual codebooks. This strategy enables the model to learn a complete representation of acoustic details, enhancing vocal expressivity. Consequently, waveform reconstruction is simplified to a lightweight causal ConvNet (Code2Wav), which significantly reduces inference latency and computational cost (FLOPs) while achieving superior audio fidelity compared to more complex DiT-based vocoders.</p>\n\n",
                "matched_terms": [
                    "mtp",
                    "talker",
                    "latency",
                    "cost",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In streaming audiovisual interaction scenarios, the first-packet latency is a critical factor affecting user experience, and the model&#8217;s concurrency capability is key to reducing service costs and improving response speed. This section discusses how Qwen3-Omni enhances concurrency and reduces first-packet latency through algorithmic and architectural optimizations.</p>\n\n",
                "matched_terms": [
                    "firstpacket",
                    "qwen3omni",
                    "latency",
                    "concurrency",
                    "factor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Qwen3-Omni, we retain the chunked-prefilling mechanism as implemented in Qwen2.5-Omni, whose audio and vision encoders are capable of outputting chunks along the temporal dimension. During real-time interaction, Thinker and Talker modules perform asynchronous prefilling: when Thinker completes prefilling the current chunk, its output high-level representations are immediately used to prefill the Talker&#8217;s current chunk asynchronously, while Thinker prefills its next chunk. This approach significantly reduces the Time-To-First-Token (TTFT) for both the Thinker and the Talker. Architecturally, both Thinker and the Talker in Qwen3-Omni adopt the MoE design, which is highly effective for improving service throughput. Compared to dense models, the MoE architecture significantly decreases IO consumption arising from KV cache during processing of long sequences, thereby increasing tokens per second (TPS) during generation and enhancing concurrency.</p>\n\n",
                "matched_terms": [
                    "timetofirsttoken",
                    "qwen3omni",
                    "talker",
                    "generation",
                    "thinker",
                    "tps",
                    "concurrency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize the user&#8217;s waiting time for receiving the first generated packet, we propose a <span class=\"ltx_text ltx_font_italic\">left context only multi-codebook generation</span> mechanism. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, once Talker generates the first token, the MTP module predicts the remaining tokens for the current frame. These tokens are then decoded into waveform by a streaming multi-codebook codec decoder that only attends to the left context. Unlike Qwen2.5-Omni that requires waiting for sufficient block-context from the Talker before synthesis, Qwen3-Omni can output the waveform immediately after the Talker generates each token, significantly reducing first-packet latency.</p>\n\n",
                "matched_terms": [
                    "time",
                    "packet",
                    "token",
                    "firstpacket",
                    "codec",
                    "qwen3omni",
                    "mtp",
                    "talker",
                    "generation",
                    "latency",
                    "decoder",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the MTP module and codec decoder are lightweight modules, which have low computational FLOPs and support batched inference, making them well-suited for high-concurrency scenarios. The MTP Module is an ultra-lightweight fixed-step autoregressive dense transformer, with low memory bandwidth requirements on inference hardware, thereby naturally enabling efficient batch processing of high throughput requests. Its fixed-step autoregressive inference mechanism allows it to effectively leverage a fixed KV cache memory space for acceleration, achieving low inference latency. Meanwhile, the ConvNet-based codec decoder also achieves high throughput with low latency because its convolutional architecture enjoys extensive hardware acceleration support across diverse inference platforms, and it enables efficient batched inference.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "mtp",
                    "latency",
                    "decoder",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training phase comprises a three-stage training process for Thinker, enabling Qwen3-Omni to possess instruction-following capabilities. The dataset, designed in the ChatML <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib39\" title=\"\">2022</a>)</cite> format, includes pure text-based dialogue data, visual modality conversation data, audio modality conversation data, and mixed-modality conversation data.</p>\n\n",
                "matched_terms": [
                    "thinker",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a four-stage training process for Talker, enabling Qwen3-Omni to generate speech response simultaneously with text. All training data is structured in the ChatML format to ensure consistency with Thinker.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "thinker",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we leverage hundreds of millions of speech data with multimodal context to train Talker, establishing a monotonic mapping from multimodal representation to speech. In the second stage, we perform Continual Pretraining (CPT) with high-quality data, which alleviates hallucinations caused by noisy data in the first stage and significantly improve the quality of generated speech. Concurrently, we perform long-context training that enhances Talker&#8217;s ability to process extended and complex inputs and generate contextually appropriate speech response. In the third stage, to improve the generalization of multilingual speech generation and system stability, we construct preference pairs from diverse multilingual speech samples and optimize the model using Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib44\" title=\"\">2023</a>)</cite>. Finally, we apply speaker fine-tuning on the aforementioned base model, enabling Talker to adopt specific voices while refining the naturalness, expressiveness, and controllability of its speech response.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "qwen3omni30ba3b",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the speech generation capabilities of Qwen3-Omni. Due to the lack of relevant assessments, the evaluation of speech generation focuses primarily speech generation given texts, similarity to text-to-speech (TTS), on following three aspects:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "factor",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "different",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "latency",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "cost",
                    "firstpacket",
                    "qwen3omni30ba3b",
                    "latency"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Languages and dialects support of Qwen3-Omni-30B-A3B.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Modality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"># Langs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Languages</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">119</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">See Qwen3 for the full list.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Speech Input</td>\n<td class=\"ltx_td ltx_align_center\">19</td>\n<td class=\"ltx_td ltx_align_center\">ar, de, en, es, fr, id, it, ja, ko, ms, nl, pt, ru, th, tr, ur, vi, yue, zh</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Speech Output</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">de, en, es, fr, it, ja, ko, pt, ru, zh</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "languages",
            "full",
            "langs",
            "list",
            "yue",
            "see",
            "modality",
            "qwen3omni30ba3b",
            "output",
            "dialects",
            "support",
            "speech",
            "qwen3",
            "text",
            "input"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "modality",
                    "qwen3omni30ba3b",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "output",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "output",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "input",
                    "modality",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the context of multimodal audiovisual streams, the audio component is encoded with a temporal ID for every 80 ms. The video is treated as a sequence of frames with monotonically increasing temporal IDs that are dynamically adjusted based on their actual timestamps to ensure a consistent temporal resolution of 80 ms per ID. The height and width IDs for video frames are assigned in the same manner as for still images. To prevent positional conflicts when processing multiple modalities, the position numbering is made contiguous, with each subsequent modality commencing from one plus the maximum position ID of the preceding modality. This refined approach to positional encoding enables the model to effectively integrate and jointly model information from diverse modalities. In a departure from Qwen2.5-Omni, which segments audiovisual representations into fixed 2-second chunks, Qwen3-Omni directly aligns these representations using their temporal IDs, which are explicitly anchored to absolute time. This design choice affords the model the flexibility to support streaming inputs of arbitrary duration.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech synthesis in multi-turn dialogues, our Talker module is conditioned on a rich context inherited from a &#8221;Thinker&#8221; component, comprising historical textual tokens, multimodal representations, and the current turn&#8217;s streamed text. This reliance on long-context information is critical, as high-fidelity speech synthesis must adapt acoustic attributes like prosody, loudness, and emotion to the ongoing discourse, a principle well-established in context-aware generative models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a four-stage training process for Talker, enabling Qwen3-Omni to generate speech response simultaneously with text. All training data is structured in the ChatML format to ensure consistency with Thinker.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Captioning is a foundational task in multimodal understanding, integral to the training and evaluation of large multimodal models. However, the vast majority of existing research has concentrated on visual captioning, largely neglecting the audio modality. This omission is significant, as auditory perception is a crucial component of human sensory experience and interaction with the world. To address this gap and facilitate more comprehensive research in multimodal perception, we introduce the Qwen3-Omni-30B-A3B-Captioner. This model was developed by fine-tuning the Qwen3-Omni-30B-A3B on a large-scale dataset of detailed audio descriptions. The resulting system generates detailed, low-hallucination captions for arbitrary audio inputs. The <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS2\" title=\"9.2 Qualitative Results from Qwen3-Omni-30B-A3B-Captioner &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> provides qualitative results that demonstrate our model&#8217;s captioning capabilities across diverse acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialects",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "modality",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "languages",
                    "qwen3omni30ba3b",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "qwen3omni30ba3b",
                    "support",
                    "speech",
                    "text"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Text â†’\\to Text performance of Qwen3-Omni-Instruct and other non-reasoning baselines. The highest\nscores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Multilingual</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Tasks</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "nonreasoning",
            "multilingual",
            "bold",
            "baselines",
            "highest",
            "scores",
            "other",
            "qwen3omniinstruct",
            "â†’to",
            "tasks",
            "text",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multilingual",
                    "â†’to",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "multilingual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "qwen3omniinstruct",
                    "other",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "other",
                    "â†’to",
                    "tasks",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "other",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multilingual",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multilingual",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Text â†’\\to Text performance of Qwen3-Omni-Thinking and other reasoning baselines. The highest\nscores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_italic\">Multilingual</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_italic\">Tasks</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "multilingual",
            "bold",
            "baselines",
            "highest",
            "scores",
            "other",
            "qwen3omnithinking",
            "â†’to",
            "tasks",
            "reasoning",
            "text",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "â†’to",
                    "tasks",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s ability to process dynamic multi-modal information, we first assessed its performance on the WorldSense benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib30\" title=\"\">2025</a>)</cite>. This benchmark is designed to measure the integration of visual and auditory signals, a foundational capability for operating in complex, open-world environments. To further examine the model&#8217;s higher-order cognitive functions, we then evaluated its performance on two audiovisual reasoning benchmarks: DailyOmni <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib73\" title=\"\">2025b</a>)</cite> and VideoHolmes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "qwen3omnithinking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "multilingual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "tasks",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "â†’to",
                    "tasks",
                    "reasoning",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "reasoning",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multilingual",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multilingual",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "qwen3omnithinking",
                    "tasks",
                    "performance"
                ]
            }
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Transcription performance for Audioâ†’\\toText tasks (ASR & S2TT), comparing Qwen3-Omni-Instruct with the baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Fleurs-avg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">(19 lang)<sup class=\"ltx_sup\">a</sup>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "comparing",
            "audioâ†’totext",
            "fleursavg",
            "transcription",
            "s2tt",
            "bold",
            "baselines",
            "highest",
            "scores",
            "qwen3omniinstruct",
            "tasks",
            "asr",
            "langa",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "tasks",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            }
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Voice interaction and audio reasoning performance for Audioâ†’\\toText tasks, comparing Qwen3-Omni with the baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-Flash-Thinking</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "comparing",
            "audioâ†’totext",
            "audio",
            "interaction",
            "bold",
            "voice",
            "qwen3omni",
            "baselines",
            "highest",
            "scores",
            "tasks",
            "flashthinking",
            "reasoning",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction",
                    "qwen3omni",
                    "tasks",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction",
                    "tasks",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the algorithms and architecture of Qwen3-Omni. Sections 3 and 4 describe the pretraining and post-training datasets and pipelines, respectively. Section 5 reports the experimental results. Section 6 compares Qwen3-Omni with recent Qwen models of comparable parameter scales, demonstrating multimodal performance without modality-induced degradation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasks",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the context of multimodal audiovisual streams, the audio component is encoded with a temporal ID for every 80 ms. The video is treated as a sequence of frames with monotonically increasing temporal IDs that are dynamically adjusted based on their actual timestamps to ensure a consistent temporal resolution of 80 ms per ID. The height and width IDs for video frames are assigned in the same manner as for still images. To prevent positional conflicts when processing multiple modalities, the position numbering is made contiguous, with each subsequent modality commencing from one plus the maximum position ID of the preceding modality. This refined approach to positional encoding enables the model to effectively integrate and jointly model information from diverse modalities. In a departure from Qwen2.5-Omni, which segments audiovisual representations into fixed 2-second chunks, Qwen3-Omni directly aligns these representations using their temporal IDs, which are explicitly anchored to absolute time. This design choice affords the model the flexibility to support streaming inputs of arbitrary duration.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In streaming audiovisual interaction scenarios, the first-packet latency is a critical factor affecting user experience, and the model&#8217;s concurrency capability is key to reducing service costs and improving response speed. This section discusses how Qwen3-Omni enhances concurrency and reduces first-packet latency through algorithmic and architectural optimizations.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Qwen3-Omni, we retain the chunked-prefilling mechanism as implemented in Qwen2.5-Omni, whose audio and vision encoders are capable of outputting chunks along the temporal dimension. During real-time interaction, Thinker and Talker modules perform asynchronous prefilling: when Thinker completes prefilling the current chunk, its output high-level representations are immediately used to prefill the Talker&#8217;s current chunk asynchronously, while Thinker prefills its next chunk. This approach significantly reduces the Time-To-First-Token (TTFT) for both the Thinker and the Talker. Architecturally, both Thinker and the Talker in Qwen3-Omni adopt the MoE design, which is highly effective for improving service throughput. Compared to dense models, the MoE architecture significantly decreases IO consumption arising from KV cache during processing of long sequences, thereby increasing tokens per second (TPS) during generation and enhancing concurrency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.T2\" title=\"Table 2 &#8227; Lightweight MTP module and ConvNet. &#8227; 2.5 Designs for Streaming and Concurrency &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the theoretical first-packet latency for Qwen3-Omni under typical computational resources across varying concurrency scenarios. Experiments are conducted on the vLLM framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib32\" title=\"\">2023</a>)</cite> to process concurrent audiovisual streams, with optimizations applied via <span class=\"ltx_text ltx_font_italic\">torch.compile</span> and CUDA Graph acceleration to the MTP Module and codec decoder. Several factors influence the total first-packet latency. First, the model sizes of Thinker and Talker impact their tail packet preprocessing latency (multi-modal data preprocessing and inference for Audio and Vision Encoder) and Time-To-First-Token (TTPT). Second, the architectures and sizes of the MTP Module and Codec Decoder affect their inference latency. Due to the sequential dependency between these components, the total first-packet latency represents the sum of these individual latencies. As shown in the results, the MoE architecture of Thinker and Talker ensures that their prefill latency and TTPT remain largely unaffected under high concurrency. Meanwhile, the lightweight design of the MTP Module and Codec Decoder minimizes their computational overhead, resulting in a lower impact on first-packet latency. Furthermore, after the initial packet is output and the model starts streaming audio synthesis, the 12.5Hz token rate Talker requires only one token to synthesize 80ms audio. Consequently, the Generation Real Time Factor (RTF) is calculated by dividing the sum of: (1) the time taken by Thinker and Talker to generate one token; and (2) the processing time per token for the MTP Module and Codec Decoder by 80ms. As demonstrated, the RTF consistently remains below 1 across varying concurrency levels, ensuring that users receive continuously streaming audio responses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training of Qwen3-Omni is structured into three distinct stages. In the first stage, we lock the LLM parameters and focus on training the vision and audio encoders, utilizing a vast corpus of audio-text and image-text pairs to enhance semantic understanding within the LLM. In the second stage, we unfreeze all parameters and train with a wider range of multimodal data for more comprehensive learning. In the final stage, we use data with a sequence length of 32,768 to enhance the model&#8217;s ability to understand complex long-sequence data:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Encoder Alignment Stage (S1)</span>: During the initial pretraining phase, the LLM component of Qwen3-Omni is initialized with parameters from Qwen3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, while the vision encoder is adopted from Qwen3-VL, and the audio encoder is initialized with AuT. The two encoders are trained separately on the fixed LLM, with both initially focusing on training their respective adapters before training the encoders. We abandon the stage used in <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>); Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> where the encoder and adapter are trained jointly while keeping the LLM frozen, because this approach may cause the encoder to compensate for the limitations of the frozen LLM, which can lead to degraded perception capabilities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-training phase comprises a three-stage training process for Thinker, enabling Qwen3-Omni to possess instruction-following capabilities. The dataset, designed in the ChatML <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib39\" title=\"\">2022</a>)</cite> format, includes pure text-based dialogue data, visual modality conversation data, audio modality conversation data, and mixed-modality conversation data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Captioning is a foundational task in multimodal understanding, integral to the training and evaluation of large multimodal models. However, the vast majority of existing research has concentrated on visual captioning, largely neglecting the audio modality. This omission is significant, as auditory perception is a crucial component of human sensory experience and interaction with the world. To address this gap and facilitate more comprehensive research in multimodal perception, we introduce the Qwen3-Omni-30B-A3B-Captioner. This model was developed by fine-tuning the Qwen3-Omni-30B-A3B on a large-scale dataset of detailed audio descriptions. The resulting system generates detailed, low-hallucination captions for arbitrary audio inputs. The <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS2\" title=\"9.2 Qualitative Results from Qwen3-Omni-30B-A3B-Captioner &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> provides qualitative results that demonstrate our model&#8217;s captioning capabilities across diverse acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "tasks",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s ability to process dynamic multi-modal information, we first assessed its performance on the WorldSense benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib30\" title=\"\">2025</a>)</cite>. This benchmark is designed to measure the integration of visual and auditory signals, a foundational capability for operating in complex, open-world environments. To further examine the model&#8217;s higher-order cognitive functions, we then evaluated its performance on two audiovisual reasoning benchmarks: DailyOmni <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib73\" title=\"\">2025b</a>)</cite> and VideoHolmes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasks",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni",
                    "tasks",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "qwen3omni",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance",
                    "qwen3omni",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "qwen3omni",
                    "tasks",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            }
        ]
    },
    "S5.T8": {
        "caption": "Table 8: Music understanding performance for Audioâ†’\\toText tasks, comparing Qwen3-Omni-Instruct with baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">41.6 (MuQ)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib75\" title=\"\">2025</a>)</cite></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "comparing",
            "zhu",
            "audioâ†’totext",
            "bold",
            "understanding",
            "baselines",
            "music",
            "qwen3omniinstruct",
            "tasks",
            "highest",
            "scores",
            "muq",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "zhu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "music",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "qwen3omniinstruct",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "qwen3omniinstruct",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "music",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "music",
                    "performance"
                ]
            }
        ]
    },
    "S5.T9": {
        "caption": "Table 9: Vision â†’\\to Text performance of Qwen3-Omni-Instruct and other non-reasoning baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-Flash</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-Instruct</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "instruct",
            "nonreasoning",
            "bold",
            "baselines",
            "highest",
            "scores",
            "qwen3omniinstruct",
            "â†’to",
            "other",
            "qwen3omniflash",
            "text",
            "vision",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate Qwen3-Omni&#8217;s ability to comprehend various multimodal inputs (text, audio, vision, and audiovisual video) and generate textual responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "â†’to"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "other",
                    "instruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "instruct",
                    "performance"
                ]
            }
        ]
    },
    "S5.T10": {
        "caption": "Table 10: Vision â†’\\to Text performance of Qwen3-Omni-Thinking and other reasoning baselines. The highest\nscores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-Flash</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-Thinking</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "thinking",
            "bold",
            "baselines",
            "highest",
            "scores",
            "other",
            "qwen3omnithinking",
            "â†’to",
            "qwen3omniflash",
            "reasoning",
            "text",
            "vision",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "thinking",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "thinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate Qwen3-Omni&#8217;s ability to comprehend various multimodal inputs (text, audio, vision, and audiovisual video) and generate textual responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "â†’to"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s ability to process dynamic multi-modal information, we first assessed its performance on the WorldSense benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib30\" title=\"\">2025</a>)</cite>. This benchmark is designed to measure the integration of visual and auditory signals, a foundational capability for operating in complex, open-world environments. To further examine the model&#8217;s higher-order cognitive functions, we then evaluated its performance on two audiovisual reasoning benchmarks: DailyOmni <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib73\" title=\"\">2025b</a>)</cite> and VideoHolmes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "other",
                    "thinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "qwen3omnithinking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "â†’to",
                    "reasoning",
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "reasoning",
                    "text",
                    "vision",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "qwen3omnithinking",
                    "performance"
                ]
            }
        ]
    },
    "S5.T11": {
        "caption": "Table 11: AudioVisual â†’\\to Text performance of Qwen3-Omni-Instruct and other non-reasoning baselines. The highest\nscores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-Flash</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-Instruct</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "instruct",
            "nonreasoning",
            "bold",
            "baselines",
            "highest",
            "scores",
            "qwen3omniinstruct",
            "â†’to",
            "other",
            "audiovisual",
            "qwen3omniflash",
            "text",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate Qwen3-Omni&#8217;s ability to comprehend various multimodal inputs (text, audio, vision, and audiovisual video) and generate textual responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "â†’to"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s ability to process dynamic multi-modal information, we first assessed its performance on the WorldSense benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib30\" title=\"\">2025</a>)</cite>. This benchmark is designed to measure the integration of visual and auditory signals, a foundational capability for operating in complex, open-world environments. To further examine the model&#8217;s higher-order cognitive functions, we then evaluated its performance on two audiovisual reasoning benchmarks: DailyOmni <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib73\" title=\"\">2025b</a>)</cite> and VideoHolmes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "other",
                    "instruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omniinstruct",
                    "â†’to",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omniinstruct",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "instruct",
                    "performance"
                ]
            }
        ]
    },
    "S5.T12": {
        "caption": "Table 12: AudioVisual â†’\\to Text performance of Qwen3-Omni-30B-A3B-Thinking and other reasoning baselines. The highest\nscores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-Flash</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-Thinking</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "thinking",
            "bold",
            "baselines",
            "highest",
            "scores",
            "other",
            "â†’to",
            "qwen3omniflash",
            "audiovisual",
            "reasoning",
            "qwen3omni30ba3bthinking",
            "text",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "audiovisual",
                    "reasoning",
                    "qwen3omni30ba3bthinking",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "thinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "qwen3omni30ba3bthinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate Qwen3-Omni&#8217;s ability to comprehend various multimodal inputs (text, audio, vision, and audiovisual video) and generate textual responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "â†’to"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s ability to process dynamic multi-modal information, we first assessed its performance on the WorldSense benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib30\" title=\"\">2025</a>)</cite>. This benchmark is designed to measure the integration of visual and auditory signals, a foundational capability for operating in complex, open-world environments. To further examine the model&#8217;s higher-order cognitive functions, we then evaluated its performance on two audiovisual reasoning benchmarks: DailyOmni <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib73\" title=\"\">2025b</a>)</cite> and VideoHolmes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audiovisual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "other",
                    "qwen3omni30ba3bthinking",
                    "thinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "other",
                    "â†’to",
                    "reasoning",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omni30ba3bthinking",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "audiovisual",
                    "reasoning",
                    "qwen3omni30ba3bthinking",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "performance"
                ]
            }
        ]
    },
    "S5.T13": {
        "caption": "Table 13: Zero-Shot Speech Generation on Seed-TTS Test Set. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span class=\"ltx_text ltx_font_bold\">SEED</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">\n<span class=\"ltx_text ltx_font_italic\">test-zh</span> &#8212; <span class=\"ltx_text ltx_font_italic\">test-en</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "zeroshot",
            "seedtts",
            "bold",
            "seed",
            "highest",
            "generation",
            "testen",
            "test",
            "speech",
            "testzh",
            "scores"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we leverage hundreds of millions of speech data with multimodal context to train Talker, establishing a monotonic mapping from multimodal representation to speech. In the second stage, we perform Continual Pretraining (CPT) with high-quality data, which alleviates hallucinations caused by noisy data in the first stage and significantly improve the quality of generated speech. Concurrently, we perform long-context training that enhances Talker&#8217;s ability to process extended and complex inputs and generate contextually appropriate speech response. In the third stage, to improve the generalization of multilingual speech generation and system stability, we construct preference pairs from diverse multilingual speech samples and optimize the model using Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib44\" title=\"\">2023</a>)</cite>. Finally, we apply speaker fine-tuning on the aforementioned base model, enabling Talker to adopt specific voices while refining the naturalness, expressiveness, and controllability of its speech response.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "set",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the speech generation capabilities of Qwen3-Omni. Due to the lack of relevant assessments, the evaluation of speech generation focuses primarily speech generation given texts, similarity to text-to-speech (TTS), on following three aspects:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Shot Speech Generation</span>: We assess the content consistency (WER) and speaker similarity (SIM) of our model in zero-shot speech generation on SEED <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib3\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "zeroshot",
                    "seed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual Speech Generation</span>: We assess the content consistency and speaker similarity of our model in zero-shot multilingual speech generation on MiniMax multilingual test set <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib69\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zeroshot",
                    "generation",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Lingual Speech Generation</span>: We assess the content consistency of our model in zero-shot cross-lingual speech generation on CV3-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib21\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            }
        ]
    },
    "S5.T14": {
        "caption": "Table 14: Multilingual Speech Generation on MiniMax Multilingual Test Set. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-30B-A3B</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "minimax",
            "30ba3b",
            "multilingual",
            "bold",
            "qwen3omni",
            "highest",
            "generation",
            "test",
            "speech",
            "scores"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Qwen3-Omni, we retain the chunked-prefilling mechanism as implemented in Qwen2.5-Omni, whose audio and vision encoders are capable of outputting chunks along the temporal dimension. During real-time interaction, Thinker and Talker modules perform asynchronous prefilling: when Thinker completes prefilling the current chunk, its output high-level representations are immediately used to prefill the Talker&#8217;s current chunk asynchronously, while Thinker prefills its next chunk. This approach significantly reduces the Time-To-First-Token (TTFT) for both the Thinker and the Talker. Architecturally, both Thinker and the Talker in Qwen3-Omni adopt the MoE design, which is highly effective for improving service throughput. Compared to dense models, the MoE architecture significantly decreases IO consumption arising from KV cache during processing of long sequences, thereby increasing tokens per second (TPS) during generation and enhancing concurrency.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize the user&#8217;s waiting time for receiving the first generated packet, we propose a <span class=\"ltx_text ltx_font_italic\">left context only multi-codebook generation</span> mechanism. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F2\" title=\"Figure 2 &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, once Talker generates the first token, the MTP module predicts the remaining tokens for the current frame. These tokens are then decoded into waveform by a streaming multi-codebook codec decoder that only attends to the left context. Unlike Qwen2.5-Omni that requires waiting for sufficient block-context from the Talker before synthesis, Qwen3-Omni can output the waveform immediately after the Talker generates each token, significantly reducing first-packet latency.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.T2\" title=\"Table 2 &#8227; Lightweight MTP module and ConvNet. &#8227; 2.5 Designs for Streaming and Concurrency &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the theoretical first-packet latency for Qwen3-Omni under typical computational resources across varying concurrency scenarios. Experiments are conducted on the vLLM framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib32\" title=\"\">2023</a>)</cite> to process concurrent audiovisual streams, with optimizations applied via <span class=\"ltx_text ltx_font_italic\">torch.compile</span> and CUDA Graph acceleration to the MTP Module and codec decoder. Several factors influence the total first-packet latency. First, the model sizes of Thinker and Talker impact their tail packet preprocessing latency (multi-modal data preprocessing and inference for Audio and Vision Encoder) and Time-To-First-Token (TTPT). Second, the architectures and sizes of the MTP Module and Codec Decoder affect their inference latency. Due to the sequential dependency between these components, the total first-packet latency represents the sum of these individual latencies. As shown in the results, the MoE architecture of Thinker and Talker ensures that their prefill latency and TTPT remain largely unaffected under high concurrency. Meanwhile, the lightweight design of the MTP Module and Codec Decoder minimizes their computational overhead, resulting in a lower impact on first-packet latency. Furthermore, after the initial packet is output and the model starts streaming audio synthesis, the 12.5Hz token rate Talker requires only one token to synthesize 80ms audio. Consequently, the Generation Real Time Factor (RTF) is calculated by dividing the sum of: (1) the time taken by Thinker and Talker to generate one token; and (2) the processing time per token for the MTP Module and Codec Decoder by 80ms. As demonstrated, the RTF consistently remains below 1 across varying concurrency levels, ensuring that users receive continuously streaming audio responses.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a four-stage training process for Talker, enabling Qwen3-Omni to generate speech response simultaneously with text. All training data is structured in the ChatML format to ensure consistency with Thinker.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we leverage hundreds of millions of speech data with multimodal context to train Talker, establishing a monotonic mapping from multimodal representation to speech. In the second stage, we perform Continual Pretraining (CPT) with high-quality data, which alleviates hallucinations caused by noisy data in the first stage and significantly improve the quality of generated speech. Concurrently, we perform long-context training that enhances Talker&#8217;s ability to process extended and complex inputs and generate contextually appropriate speech response. In the third stage, to improve the generalization of multilingual speech generation and system stability, we construct preference pairs from diverse multilingual speech samples and optimize the model using Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib44\" title=\"\">2023</a>)</cite>. Finally, we apply speaker fine-tuning on the aforementioned base model, enabling Talker to adopt specific voices while refining the naturalness, expressiveness, and controllability of its speech response.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation of Qwen3-Omni on text <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> text primarily focuses on general tasks, reasoning ability, coding ability, alignment tasks, agent, and multilingual tasks.\nSpecifically, we utilize MMLU-Redux <cite class=\"ltx_cite ltx_citemacro_citep\">(Gema et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib25\" title=\"\">2024</a>)</cite> and GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib45\" title=\"\">2023</a>)</cite> for general tasks, AIME25 <cite class=\"ltx_cite ltx_citemacro_citep\">(AIME, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib2\" title=\"\">2025</a>)</cite> and ZebraLogic <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib35\" title=\"\">2025</a>)</cite> for reasoning evaluation,\nMultiPL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Cassano et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib12\" title=\"\">2023</a>)</cite> for coding, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib71\" title=\"\">2023</a>)</cite>, Creative Writing V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Paech, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib42\" title=\"\">2024</a>)</cite> and WritingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib59\" title=\"\">2025b</a>)</cite> for alignment tasks,\nBFCL-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib61\" title=\"\">2024</a>)</cite> for agent evaluation,\nMultiIF <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib29\" title=\"\">2024</a>)</cite> and PolyMath <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib56\" title=\"\">2025c</a>)</cite> for multilingual tasks.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "set",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multilingual",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the speech generation capabilities of Qwen3-Omni. Due to the lack of relevant assessments, the evaluation of speech generation focuses primarily speech generation given texts, similarity to text-to-speech (TTS), on following three aspects:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Shot Speech Generation</span>: We assess the content consistency (WER) and speaker similarity (SIM) of our model in zero-shot speech generation on SEED <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib3\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual Speech Generation</span>: We assess the content consistency and speaker similarity of our model in zero-shot multilingual speech generation on MiniMax multilingual test set <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib69\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "minimax",
                    "multilingual",
                    "generation",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Lingual Speech Generation</span>: We assess the content consistency of our model in zero-shot cross-lingual speech generation on CV3-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib21\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "generation",
                    "speech",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "multilingual",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "30ba3b",
                    "speech",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            }
        ]
    },
    "S5.T15": {
        "caption": "Table 15: Cross-Lingual Speech Generation on CosyVoice3 Cross-Lingual Test Set. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Qwen3-Omni-30B-A3B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CosyVoice3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">en-to-zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.09</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">13.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ja-to-zh</td>\n<td class=\"ltx_td ltx_align_center\">3.32</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.05</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">48.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ko-to-zh</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_center\">1.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">7.70</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">zh-to-en</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.76</span></td>\n<td class=\"ltx_td ltx_align_center\">2.98</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">6.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ja-to-en</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.31</span></td>\n<td class=\"ltx_td ltx_align_center\">4.20</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">17.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ko-to-en</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center\">4.19</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">11.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">zh-to-ja</td>\n<td class=\"ltx_td ltx_align_center\">8.29</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">7.08</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">13.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">en-to-ja</td>\n<td class=\"ltx_td ltx_align_center\">7.53</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.80</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">14.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ko-to-ja</td>\n<td class=\"ltx_td ltx_align_center\">4.24</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.93</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">zh-to-ko</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">5.13</span></td>\n<td class=\"ltx_td ltx_align_center\">14.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">24.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">en-to-ko</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.96</span></td>\n<td class=\"ltx_td ltx_align_center\">5.87</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">21.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">ja-to-ko</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">6.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.92</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">21.5</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "qwen3omni30ba3b",
            "kotoen",
            "jatoko",
            "entoja",
            "crosslingual",
            "zhtoja",
            "jatoen",
            "generation",
            "test",
            "kotoja",
            "scores",
            "zhtoko",
            "language",
            "zhtoen",
            "bold",
            "entoko",
            "cosyvoice3",
            "cosyvoice2",
            "jatozh",
            "speech",
            "set",
            "kotozh",
            "entozh",
            "highest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "language",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talker no longer consumes the Thinker&#8217;s high-level text representations and conditions only on audio and visual multimodal features. This design is motivated by: (i) for textual content, discrete tokens and embeddings are effectively information-equivalent; and (ii) multimodal conditioning is necessary for audio&#8211;video&#8211;coordinated speech generation such as preserving prosody/timbre in speech translation. Moreover, this decoupling allows external modules (e.g., RAG, function calling, safety filters) to intervene on the Thinker&#8217;s textual output and, if desired, supply text to the Talker via controlled preprocessing for streaming synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we leverage hundreds of millions of speech data with multimodal context to train Talker, establishing a monotonic mapping from multimodal representation to speech. In the second stage, we perform Continual Pretraining (CPT) with high-quality data, which alleviates hallucinations caused by noisy data in the first stage and significantly improve the quality of generated speech. Concurrently, we perform long-context training that enhances Talker&#8217;s ability to process extended and complex inputs and generate contextually appropriate speech response. In the third stage, to improve the generalization of multilingual speech generation and system stability, we construct preference pairs from diverse multilingual speech samples and optimize the model using Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib44\" title=\"\">2023</a>)</cite>. Finally, we apply speaker fine-tuning on the aforementioned base model, enabling Talker to adopt specific voices while refining the naturalness, expressiveness, and controllability of its speech response.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "language",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "language",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the speech generation capabilities of Qwen3-Omni. Due to the lack of relevant assessments, the evaluation of speech generation focuses primarily speech generation given texts, similarity to text-to-speech (TTS), on following three aspects:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Shot Speech Generation</span>: We assess the content consistency (WER) and speaker similarity (SIM) of our model in zero-shot speech generation on SEED <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib3\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual Speech Generation</span>: We assess the content consistency and speaker similarity of our model in zero-shot multilingual speech generation on MiniMax multilingual test set <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib69\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "generation",
                    "speech",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Lingual Speech Generation</span>: We assess the content consistency of our model in zero-shot cross-lingual speech generation on CV3-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib21\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For practical usage, Qwen3-Omni-30B-A3B offers strong text and vision capabilities, robust and reliable ASR, interactive speech support in over 20 languages, very low first-packet latency for interactive use, and stable, naturalistic speech synthesis. Crucially, it exhibits advantages over cascaded pipelines, including stronger cross-modal reasoning, lower end-to-end latency, and lower system complexity and cost. In future work, we will further advance the model along multiple axes, including multi-speaker ASR, video OCR, audiovisual proactive learning, and enhanced support for agent-based workflows and function calling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen3omni30ba3b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            }
        ]
    },
    "S6.T16": {
        "caption": "Table 16: We compare the performance of 30A3 models that are contemporaneous and identical in size in Qwen series. To ensure experimental rigor, all models were trained under the same schedule, using identical datasets for their respective modalities and exactly matched training compute (FLOPs).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_italic\">Video Understanding</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_italic\">Tasks</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "compare",
            "their",
            "size",
            "experimental",
            "qwen",
            "understanding",
            "ensure",
            "tasks",
            "training",
            "trained",
            "performance",
            "rigor",
            "matched",
            "under",
            "datasets",
            "30a3",
            "same",
            "compute",
            "flops",
            "video",
            "series",
            "identical",
            "all",
            "contemporaneous",
            "exactly",
            "modalities",
            "schedule",
            "respective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
            "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "models",
                    "video",
                    "series",
                    "qwen",
                    "understanding",
                    "under",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "models",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "models",
                    "their",
                    "video",
                    "understanding",
                    "all",
                    "tasks",
                    "training",
                    "respective",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni builds on the Thinker&#8211;Talker architecture introduced in Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> and introduces <span class=\"ltx_text ltx_font_bold\">five key upgrades</span>: (1) both the Thinker and Talker are upgraded to Mixture-of-Experts (MoE) designs; (2) we replace Whisper audio encoder with our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio, yielding stronger general-purpose audio representations. AuT employs block-wise window attention to enable real-time prefill caching; (3) on the speech generation side, we adopt a multi-codebook representation,\nwhose increased capacity supports faithful modeling of diverse voices, paralinguistic cues, and acoustic phenomena; (4) the Talker shifts from single-track to multi-track codec modeling, autoregressively predicting multiple codebook layers via MTP modules, while the waveform stage (Code2Wav) replaces block-wise DiT with a lightweight convolutional network (ConvNet); and (5) the input and output audio code rates are reduced to 12.5 Hz, with the output codec enabling single-frame, immediate speech synthesis. Taken together, these changes enable low-latency speech interaction under high concurrency in industrial-scale deployments.</p>\n\n",
                "matched_terms": [
                    "under",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro, Seed-ASR, and GPT-4o-Transcribe.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "modalities",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the algorithms and architecture of Qwen3-Omni. Sections 3 and 4 describe the pretraining and post-training datasets and pipelines, respectively. Section 5 reports the experimental results. Section 6 compares Qwen3-Omni with recent Qwen models of comparable parameter scales, demonstrating multimodal performance without modality-induced degradation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "experimental",
                    "qwen",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following sections, we first introduce with our newly proposed AuT encoder, including its training methodology. Then, describe how Thinker processes various inputs. We then detail Talker&#8217;s multi-codebook streaming speech generation. Finally, we highlight a series of improvements on both the understanding and generation modules aimed at achieving ultra&#8211;low-latency, end-to-end streaming audio inference.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "series",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "training",
                    "trained",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thinker converts text, audio, image, and video (without audio) into a series of representations for input. For text inputs, we use Qwen&#8217;s tokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, which applies byte-level byte-pair encoding with a vocabulary of 151,643 regular tokens. For audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. We adopt AuT encoder as our audio encoder, which is trained from scratch on 20 millions hours of audio data, and each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal. Furthermore, we employ the vision encoder from Qwen3-VL, initialized from SigLIP2-So400m <cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib50\" title=\"\">2025</a>)</cite> with approximately 543 million parameters, enabling handling of both image and video inputs. The vision encoder is trained on a mixture of image and video data, ensuring strong image understanding and video comprehension. To preserve video information as completely as possible while aligning with the audio sampling rate, we sample video frames at a dynamic frame rate.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video",
                    "series",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing inspiration from Qwen2.5-Omni, we employs a Time-aligned Multimodal Rotary Position Embedding (TM-RoPE), which extends the Multimodal Rotary Position Embedding (M-RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>)</cite> by incorporating absolute temporal information. TM-RoPE factorizes the conventional rotary position embedding into three distinct dimensions: temporal, height, and width. In the original M-RoPE formulation, temporal dependencies are modeled using the initial 16 rotary angles, which correspond to higher frequencies and exhibit stronger oscillatory patterns. While this design is effective for capturing fine-grained local temporal variations, it can impede the model&#8217;s ability to extrapolate over extended sequences. To address this limitation, we introduce a modified allocation of rotary angles. Specifically, the temporal, height, and width dimensions are interleaved and assigned 24, 20, and 20 rotary angles, respectively. This redistribution fosters a more balanced representation of both local semantics and long-range dependencies, thereby enhancing the model&#8217;s overall performance. The application of TM-RoPE is tailored to the specific modality of the input data. For text inputs, the three components share identical position identifiers, rendering TM-RoPE functionally equivalent to a one-dimensional RoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib47\" title=\"\">2024</a>)</cite>. Similarly, audio inputs utilize shared position IDs but are further augmented with absolute temporal encodings, where each temporal ID corresponds to a duration of 80 ms. For image data, a constant temporal ID is assigned to all visual tokens, while their distinct row and column positions determine the height and width IDs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "their",
                    "identical",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the context of multimodal audiovisual streams, the audio component is encoded with a temporal ID for every 80 ms. The video is treated as a sequence of frames with monotonically increasing temporal IDs that are dynamically adjusted based on their actual timestamps to ensure a consistent temporal resolution of 80 ms per ID. The height and width IDs for video frames are assigned in the same manner as for still images. To prevent positional conflicts when processing multiple modalities, the position numbering is made contiguous, with each subsequent modality commencing from one plus the maximum position ID of the preceding modality. This refined approach to positional encoding enables the model to effectively integrate and jointly model information from diverse modalities. In a departure from Qwen2.5-Omni, which segments audiovisual representations into fixed 2-second chunks, Qwen3-Omni directly aligns these representations using their temporal IDs, which are explicitly anchored to absolute time. This design choice affords the model the flexibility to support streaming inputs of arbitrary duration.</p>\n\n",
                "matched_terms": [
                    "same",
                    "their",
                    "video",
                    "ensure",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Architecturally, our approach departs from <cite class=\"ltx_cite ltx_citemacro_citet\">Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> by operating directly on RVQ tokens. The Talker employs a hierarchical prediction scheme: the backbone ingests the aggregated codebook features of the current frame and uses a linear head to predict the zeroth codebook, after which a multi-token prediction (MTP) module generates all residual codebooks. This strategy enables the model to learn a complete representation of acoustic details, enhancing vocal expressivity. Consequently, waveform reconstruction is simplified to a lightweight causal ConvNet (Code2Wav), which significantly reduces inference latency and computational cost (FLOPs) while achieving superior audio fidelity compared to more complex DiT-based vocoders.</p>\n\n",
                "matched_terms": [
                    "all",
                    "flops"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.T2\" title=\"Table 2 &#8227; Lightweight MTP module and ConvNet. &#8227; 2.5 Designs for Streaming and Concurrency &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the theoretical first-packet latency for Qwen3-Omni under typical computational resources across varying concurrency scenarios. Experiments are conducted on the vLLM framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib32\" title=\"\">2023</a>)</cite> to process concurrent audiovisual streams, with optimizations applied via <span class=\"ltx_text ltx_font_italic\">torch.compile</span> and CUDA Graph acceleration to the MTP Module and codec decoder. Several factors influence the total first-packet latency. First, the model sizes of Thinker and Talker impact their tail packet preprocessing latency (multi-modal data preprocessing and inference for Audio and Vision Encoder) and Time-To-First-Token (TTPT). Second, the architectures and sizes of the MTP Module and Codec Decoder affect their inference latency. Due to the sequential dependency between these components, the total first-packet latency represents the sum of these individual latencies. As shown in the results, the MoE architecture of Thinker and Talker ensures that their prefill latency and TTPT remain largely unaffected under high concurrency. Meanwhile, the lightweight design of the MTP Module and Codec Decoder minimizes their computational overhead, resulting in a lower impact on first-packet latency. Furthermore, after the initial packet is output and the model starts streaming audio synthesis, the 12.5Hz token rate Talker requires only one token to synthesize 80ms audio. Consequently, the Generation Real Time Factor (RTF) is calculated by dividing the sum of: (1) the time taken by Thinker and Talker to generate one token; and (2) the processing time per token for the MTP Module and Codec Decoder by 80ms. As demonstrated, the RTF consistently remains below 1 across varying concurrency levels, ensuring that users receive continuously streaming audio responses.</p>\n\n",
                "matched_terms": [
                    "their",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni is pre-trained on a diverse dataset that encompasses multiple languages and dialects as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S3.T3\" title=\"Table 3 &#8227; 3 Pretraining &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and modalities, including image-text, video-text, audio-text, video-audio, video-audio-text, and pure text corpora. Unlike Qwen2.5-Omni, which uses a single prompt for each task, we employ a wider range of natural language prompts to enhance both the generalization ability and instruction-following capabilities. To achieve robust performance across all modalities, our training strategy incorporates both unimodal and cross-modal data from the early pretraining stage.</p>\n\n",
                "matched_terms": [
                    "all",
                    "training",
                    "modalities",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training of Qwen3-Omni is structured into three distinct stages. In the first stage, we lock the LLM parameters and focus on training the vision and audio encoders, utilizing a vast corpus of audio-text and image-text pairs to enhance semantic understanding within the LLM. In the second stage, we unfreeze all parameters and train with a wider range of multimodal data for more comprehensive learning. In the final stage, we use data with a sequence length of 32,768 to enhance the model&#8217;s ability to understand complex long-sequence data:</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "all",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Encoder Alignment Stage (S1)</span>: During the initial pretraining phase, the LLM component of Qwen3-Omni is initialized with parameters from Qwen3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>)</cite>, while the vision encoder is adopted from Qwen3-VL, and the audio encoder is initialized with AuT. The two encoders are trained separately on the fixed LLM, with both initially focusing on training their respective adapters before training the encoders. We abandon the stage used in <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>); Xu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite> where the encoder and adapter are trained jointly while keeping the LLM frozen, because this approach may cause the encoder to compensate for the limitations of the frozen LLM, which can lead to degraded perception capabilities.</p>\n\n",
                "matched_terms": [
                    "respective",
                    "their",
                    "trained",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video",
                    "tasks",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Long Context Stage (S3)</span>: In the final pre-training phase, we increased the maximum token length from 8,192 to 32,768 and also raised the proportion of long audio and long video in the training data. Experimental results indicate that these adjustments lead to significant improvements in the model&#8217;s ability to understand long sequence data.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Off-policy Distillation</span>: In the initial phase, outputs generated by teacher models are combined to provide response distillation. This helps lightweight student models acquire fundamental reasoning abilities, establishing a strong foundation for subsequent on-policy training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage GSPO <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib70\" title=\"\">2025</a>)</cite> to comprehensively enhance the model&#8217;s capabilities and stability across various modalities, including text, image, video, and audio. To provide feedback for the aforementioned modalities, we employ two different types of rewards:</p>\n\n",
                "matched_terms": [
                    "video",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "ensure",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a four-stage training process for Talker, enabling Qwen3-Omni to generate speech response simultaneously with text. All training data is structured in the ChatML format to ensure consistency with Thinker.</p>\n\n",
                "matched_terms": [
                    "all",
                    "ensure",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Captioning is a foundational task in multimodal understanding, integral to the training and evaluation of large multimodal models. However, the vast majority of existing research has concentrated on visual captioning, largely neglecting the audio modality. This omission is significant, as auditory perception is a crucial component of human sensory experience and interaction with the world. To address this gap and facilitate more comprehensive research in multimodal perception, we introduce the Qwen3-Omni-30B-A3B-Captioner. This model was developed by fine-tuning the Qwen3-Omni-30B-A3B on a large-scale dataset of detailed audio descriptions. The resulting system generates detailed, low-hallucination captions for arbitrary audio inputs. The <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS2\" title=\"9.2 Qualitative Results from Qwen3-Omni-30B-A3B-Captioner &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> provides qualitative results that demonstrate our model&#8217;s captioning capabilities across diverse acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "video",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading large language models (thinking or instruct). According to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T4\" title=\"Table 4 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T5\" title=\"Table 5 &#8227; 5.1.1 Performance of Text&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,\nnotably, despite a smaller parameter count, Qwen3-Omni-30B-A3B-Instruct surpasses the performance of the larger open-source model Qwen3-235B-A22B Non-Thinking and the formidable closed-source model GPT-4o-0327 across a suite of benchmarks, including GPQA, AIME25, ZebraLogic, WritingBench, and PolyMath. Concurrently, Qwen3-Omni-30B-A3B-Thinking demonstrates performance comparable to that of Gemini-2.5-Flash-Thinking and Qwen3-235B-A22B Non-Thinking. Furthermore, Qwen3-Omni-30B-A3B exhibits textual capabilities on par with its text-only counterparts, namely the Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507.</p>\n\n",
                "matched_terms": [
                    "models",
                    "compare",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "models",
                    "compare"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "models",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "compare",
                    "understanding",
                    "tasks",
                    "respective",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "compare",
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "models",
                    "video",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "experimental",
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "compare",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports speech generation across 10 languages. We evaluate its performance against both the MiniMax-Speech and ElevenLabs Multilingual v2 models for multilingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T14\" title=\"Table 14 &#8227; 5.2.2 Evaluation of Multilingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, Qwen3-Omni surpasses these models by a significant margin for languages such as Chinese, English, and French, while delivering competitive results in the remaining languages. These findings indicate that Qwen3-Omni generates cloned speech with consistent stability and human-like voice across all evaluated languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "all",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A standardized data integration methodology is rendered impractical by the heterogeneous nature of different modalities, each requiring distinct pre-training objectives and optimization techniques. To ensure a fair and rigorous evaluation, we therefore designed a controlled comparative study. Our approach involved pre-training three models with matched parameter counts: a text-only baseline, a vision-only baseline, and a multimodal &#8220;Omni&#8221; model. To isolate the effects of multimodality, all confounding variables were meticulously controlled. Specifically, the Omni model was trained on the identical text and vision corpora as the unimodal baselines. Moreover, we aligned critical training parameters across all models, including learning rate schedules, batch sizes, and the effective number of training epochs for each modality, which was normalized by adjusting data sampling ratios. Consequently, the sole differentiating factor in our experiment was the Omni model&#8217;s inclusion of supplementary audio and audio-visual data during its pre-training phase.</p>\n\n",
                "matched_terms": [
                    "models",
                    "matched",
                    "identical",
                    "all",
                    "ensure",
                    "training",
                    "modalities",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "models",
                    "qwen",
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Research fields often cycle between specialization and integration. In this context, we believe Qwen3-Omni represents a milestone: to our knowledge, it provides the first evidence that fully integrated, end-to-end multimodal training can be achieved without degrading core language capability and other modalities. We are eager to share these findings with the community and hope they will stimulate further research.</p>\n\n",
                "matched_terms": [
                    "training",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            }
        ]
    },
    "S9.T17": {
        "caption": "Table 17: Transcription performance for Audioâ†’\\toText tasks (ASR & S2TT), comparing Qwen3-Omni-Thinking with the baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Fleurs-avg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">(19 lang)<sup class=\"ltx_sup\">a</sup>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "comparing",
            "audioâ†’totext",
            "fleursavg",
            "transcription",
            "s2tt",
            "bold",
            "baselines",
            "highest",
            "scores",
            "tasks",
            "qwen3omnithinking",
            "asr",
            "langa",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "qwen3omnithinking",
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T6\" title=\"Table 6 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, Qwen3-Omni-Instruct achieves state-of-the-art En &amp; Zh ASR and lyric ASR performance on Librispeech, Wenetspeech, Fleurs, CommonVoice, Opencpop-test and MIR-1K (vocal). It also delivers better or comparable performance with other specialist or generalist models like Voxtral-Small and Gemini-2.5-Pro on Multilingual ASR and S2TT. These results show a strong performance of Qwen3-Omni in speech recognition and speech translation.</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            }
        ]
    },
    "S9.T18": {
        "caption": "Table 18: Music understanding performance for Audioâ†’\\toText tasks, comparing Qwen3-Omni-Thinking with baselines. The highest scores are shown in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">41.6</span> (MuQ)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib75\" title=\"\">2025</a>)</cite></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "comparing",
            "zhu",
            "audioâ†’totext",
            "bold",
            "understanding",
            "baselines",
            "music",
            "highest",
            "tasks",
            "qwen3omnithinking",
            "scores",
            "muq",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This section reports the performance of the Qwen3-Omni-thinking model on tasks pertaining to ASR/S2TT and Music.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T17\" title=\"Table 17 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.T18\" title=\"Table 18 &#8227; 9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, in the domains of ASR/S2TT and Music understanding, the Qwen3-Omni-Thinking model is outperformed by its Instruct counterpart, which indicates that for these predominantly perception-based tasks, the engagement of sophisticated reasoning processes fails to yield performance gains. In fact, it may even introduce a higher propensity for hallucinations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\nQwen3-Omni adopts a Thinker&#8211;Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts.\nTo reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms.\nTo further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans perceive visual and auditory inputs in parallel, cognitively process these signals, and emit responses through textual expression, vocalization, and tool-mediated or bodily actions, facilitating information exchange with other organisms and demonstrating intelligence. Building on the rapid advances in the understanding and reasoning capabilities of unimodal large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib11\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib40\" title=\"\">2023</a>; Gemini Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib26\" title=\"\">2024</a>; Anthropic, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib4\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib5\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib6\" title=\"\">2024</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib7\" title=\"\">2023a</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib62\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib63\" title=\"\">2025a</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib49\" title=\"\">2023</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib22\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib34\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib36\" title=\"\">2023</a>; Zhu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib74\" title=\"\">2023</a>; Bai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib8\" title=\"\">2023b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib9\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib17\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib18\" title=\"\">2024</a>)</cite>, natively multimodal systems have drawn substantial attention <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib41\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib19\" title=\"\">2025</a>; Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib60\" title=\"\">2025</a>)</cite>. Human learning typically progresses through the coordinated use of multiple modalities, where complementary specialization and cross-modal synergy improve learning efficiency. However, contemporary LLM-centric multimodal models often exhibit modality trade-offs, with gains in one modality accompanied by degradation in others.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "zhu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this report, we take a step toward resolving this limitation by exploring integrated multimodal training within the prevailing LLM-based paradigm. We demonstrate that joint multimodal training can achieve parity across all modalities&#8212;i.e., no modality-specific performance degradation&#8212;while markedly enhancing cross-modal capabilities such as video understanding. A key ingredient is mixing unimodal and cross-modal data during the early stage of text pretraining. As evidenced by Qwen3-Omni-30B-A3B-Base, its text and vision performance is on par with same-sized single-modal text and vision base models across extensive benchmarks, while simultaneously exhibiting strong audio competence, audiovisual understanding, cross-modal &#8220;thinking&#8221;, and real-time audiovisual interaction. The development of non-degrading multimodal systems is an achievable objective. Such systems are characterized by two key properties: first, their ability to match the performance of specialized unimodal models in their respective tasks, and second, their capacity to facilitate novel cross-modal reasoning and interaction. These latter capabilities represent a significant advantage, as they are not present in traditional unimodal approaches.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with Qwen2.5-Omni, Qwen3-Omni introduces <span class=\"ltx_text ltx_font_bold\">four major improvements</span>: (1) support for audio understanding on inputs exceeding 40 minutes; (2) expanded language coverage to 119 written languages, 19 and 10 spoken languages for understanding and generation respectively ; (3) a Thinking model enabling full-modality reasoning, including audio&#8211;video and audio-only scenarios; and (4) improved streaming performance with end-to-end latency as low as 234 ms.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio Transformer (AuT) is an attention-encoder-decoder model, as is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Audio Transformer (AuT) &#8227; 2 Architecture &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, trained from scratch on 20 million hours of supervised audio data. During training, the filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, reducing the token rate to 12.5 Hz. To learn stronger and more general-purpose audio representations, AuT is trained on large-scale audio datasets with both speech recognition and audio understanding tasks. Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data. To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds. In Qwen3-Omni, we employ the AuT encoder as the audio encoder, which contains approximately 0.6B parameters.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Stage (S2)</span>: The second phase of pretraining utilizes a large-scale dataset containing approximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion). During this stage, the introduction of more diverse multimodal data and tasks enhances the model&#8217;s understanding and interaction capabilities in auditory, visual, textual, and audiovisual information.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model-based Reward</span>: To assess performance on multimodal tasks that lack objective, predefined evaluation metrics, we adopt an LLM-as-a-judge protocol. The role of the automated evaluator is filled by Qwen3 for general tasks, while the specialized vision-language model, Qwen2.5-VL, is used for visually-grounded tasks. To ensure a more robust and grounded assessment, the LLM evaluator is furnished with the corresponding ground-truth or reference answer for a given query, where applicable.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation was performed on a suite of models, including Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and two in-house developed variants, designated Qwen3-Omni-Flash-Instruct and Qwen3-Omni-Flash-Thinking. These &#8220;Flash&#8221; models were designed to improve both computational efficiency and performance efficacy, integrating new functionalities, notably the support for various dialects.\nThe evaluation results are divided into two main categories: understanding (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Text) and speech generation (X<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>Speech).</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation can be categorized into basic audio tasks, including Automatic Speech Recognition (ASR), Speech-to-Text (S2TT), and Music Understanding, as well as advanced audio tasks, including Voice Chatting and Audio Reasoning.\nFor music understanding, we use RUL-MuchoMusic <cite class=\"ltx_cite ltx_citemacro_citep\">(Zang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib68\" title=\"\">2025</a>)</cite> for a comprehensive evaluation of the music understanding capabilities of the model.\nWe utilize MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib46\" title=\"\">2024</a>)</cite> and MMSU <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib52\" title=\"\">2025a</a>)</cite> for audio reasoning tasks, VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib14\" title=\"\">2024b</a>)</cite> for voice-chatting tasks. We also employ multiple datasets including GTZAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tzanetakis &amp; Cook, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib51\" title=\"\">2002</a>)</cite>, four subsets of MTG-Jamendo (MTG, <cite class=\"ltx_cite ltx_citemacro_cite\">Bogdanov et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib10\" title=\"\">2019</a>)</cite>), and MagnaTagATune <cite class=\"ltx_cite ltx_citemacro_citep\">(Law et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib33\" title=\"\">2009</a>)</cite> to evaluate the model&#8217;s capabilities across various music information retrieval tasks including genre identification, emotion and theme recognition, instrument recognition and music keyword annotation. We follow the evaluation set composition in MARBLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib65\" title=\"\">2023</a>)</cite> for GTZAN, MTG-Jamendo and MagnaTagATune.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "music",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of the model&#8217;s vision-to-text capabilities encompasses a suite of benchmarks targeting diverse and challenging tasks. To assess performance in general visual question answering, the model is evaluated on MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib13\" title=\"\">2024a</a>)</cite>, HallusionBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib28\" title=\"\">2024</a>)</cite>, and MM-MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Agrawal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib1\" title=\"\">2024</a>)</cite>. For the specialized domain of mathematical and STEM reasoning, we utilize MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib37\" title=\"\">2024</a>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib53\" title=\"\">2024a</a>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib66\" title=\"\">2023</a>)</cite>, and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Yue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib67\" title=\"\">2024</a>)</cite>. The model&#8217;s proficiency in document understanding is measured using the AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(Kembhavi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib31\" title=\"\">2016</a>)</cite> and ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Masry et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib38\" title=\"\">2022</a>)</cite> benchmarks. Furthermore, the model&#8217;s numerical reasoning and counting abilities are specifically tested on CountBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Paiss et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib43\" title=\"\">2023</a>)</cite>. To evaluate performance on dynamic visual data, we report results on three long video understanding benchmarks: Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib24\" title=\"\">2024</a>)</cite>, LVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib54\" title=\"\">2024b</a>)</cite>, and MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#bib.bib72\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare Qwen3-Omni with other leading specialist and generalist models on ASR &amp; S2TT, voice-chatting, audio reasoning, and music understanding benchmarks. For brevity, we defer the results of the Qwen3-Omni-Thinking model on ASR &amp; S2TT and music understanding to the <span class=\"ltx_text ltx_font_bold\">Appendix</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S9.SS1\" title=\"9.1 More Evaluation on Speech and Music Understanding &#8227; 9 Appendix &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "music",
                    "qwen3omnithinking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, on VoiceBench shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T7\" title=\"Table 7 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Qwen3-Omni-Thinking achieves an impressive average score of 89.5, surpassing all other audio language models except Gemini-2.5-Pro (89.6). This showcases our model&#8217;s strong capabilities in speech interaction. Qwen3-Omni also demonstrates impressive performance in audio reasoning, outperforming the powerful closed-source models Gemini-2.5-Pro and Gemini-2.5-Flash on the MMAU benchmark, as well as Gemini-2.5-Flash and GPT-4o-Audio on MMSU. These results demonstrate the powerful capabilities of Qwen3-Omni in general audio understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For music understanding, we compare Qwen3-Omni-Instruct with both generalist audio language models and specialist models in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For multi-label classification tasks on MTG-Jamendo and MagnaTagATune, we use micro F1 to compare with BERT-like music specialists instead of AP/AUROC, as language models output discrete label sets without calibrated per-label probabilities/scores required by ranking-based metrics. It is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T8\" title=\"Table 8 &#8227; 5.1.2 Performance of Audio&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> that Qwen3-Omni-Instruct achieve state-of-the-art performance on RUL-MuchoMusic. On GTZAN, MTG-Jamendo, and MagnaTagATune, the scores of Qwen3-Omni-Instruct also significantly surpass other audio language models, including Gemini-2.5-Pro and GPT-4o-Audio, as well as self-supervised music specialist models probed on the respective datasets. These results demonstrate the superior capabilities of Qwen3-Omni-Instruct across a variety of music understanding tasks.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "scores",
                    "music",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities on Vision <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> Text, we compare Qwen3-Omni-Instruct with the Qwen2.5-VL-72B and other good-performing closed-source vision-language models. As illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T9\" title=\"Table 9 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Qwen3-Omni-Instruct demonstrates comparable performance to Qwen2.5-VL-72B, and attains better results on Math &amp; STEM related tasks like MMMU-Pro<sub class=\"ltx_sub\">overall</sub>, MathVista<sub class=\"ltx_sub\">mini</sub>, and MATH-Vision<sub class=\"ltx_sub\">full</sub>, than other vision language models including GPT4-o and Gemini-2.0-Flash. These results reveal the excellent capability of our model on image understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess its capabilities, we evaluated the performance of Qwen3-Omni-Thinking against several state-of-the-art reasoning models. The comparative results, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T10\" title=\"Table 10 &#8227; 5.1.3 Performance of Vision &#8594; Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, indicate that our proposed model achieves significant advancements. For instance, on Math and STEM benchmarks, it outperforms the Qwen3-Omni-Instruct baseline by 4.4 points. It is also noteworthy that our Qwen3-Omni-30B-A3B-Thinking model attains a performance level on par with substantially larger baselines, which highlights its excellent balance of effectiveness and computational efficiency.\nA limitation of the current model is its suboptimal performance on long video benchmarks. This deficiency stems from two architectural constraints: a limited capacity for positional extrapolation and a restricted context length. Addressing these constraints is a key objective for future work.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "qwen3omnithinking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As is shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T11\" title=\"Table 11 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the experimental results validate the efficacy of Qwen3-Omni across diverse audiovisual tasks. For general understanding, Qwen3-Omni-Instruct achieves state-of-the-art performance on the WorldSense benchmark, surpassing other Omni models by a substantial margin. This outcome demonstrates its effectiveness in foundational multimodal integration. Moreover, the model exhibits enhanced performance on complex reasoning tasks, as illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T12\" title=\"Table 12 &#8227; 5.1.4 Performance of AudioVisual Video&#8594;Text &#8227; 5.1 Evaluation of X&#8594;Text &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, particularly on benchmarks that necessitate reasoning over interconnected audio and visual information. These findings collectively suggest that Qwen3-Omni possesses considerable potential for advanced perception and reasoning in real-world contexts.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the Qwen3-Omni with state-of-the-art zero-shot TTS systems. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T13\" title=\"Table 13 &#8227; 5.2.1 Evaluation of Zero-Shot Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, Qwen3-Omni demonstrates highly competitive performance, highlighting its robust speech understanding and generation capabilities developed through pretraining and continual pretraining. Additionally, with reinforcement learning (RL) optimization, Qwen3-Omni yields significant improvements in generation stability, which achieves the best performance in the test-en set.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Omni supports not only multilingual voice cloning but also cross-lingual voice cloning. We evaluate its performance against CosyVoice2 and CosyVoice3 for cross-lingual speech generation. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S5.T15\" title=\"Table 15 &#8227; 5.2.3 Evaluation of Cross-Lingual Speech Generation &#8227; 5.2 Evaluation of X&#8594;Speech &#8227; 5 Evaluation &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, Qwen3-Omni outperforms CosyVoice3 in any-to-en (any language to English) and any-to-ko (any language to Korean) voice cloning. Notably, in any-to-ja (any language to Japanese) tasks, Qwen3-Omni achieves comparable performance to CosyVoice3 even without text normalization, despite CosyVoice3 converting all Japanese characters into phonetic kana. These results highlight Qwen3-Omni&#8217;s superiority in cross-lingual speech generation, demonstrating its adaptability across diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, we evaluate comprehensive benchmarks covering a variety of modalities, including the text modality (general tasks, math &amp; STEM tasks, coding tasks, multilingual tasks), the visual modality (college-level problems, OCR-related tasks), and the video modality (video understanding tasks). The experimental results not only demonstrate that mixing unimodal and cross-modal data during the early stage of text pretraining can achieve better performance across all modalities, but also indicate that joint multimodal training enables mutual enhancement between different modalities, leading to improved performance in single modalities as well.\nThis fully showcases the versatility and robustness of Qwen3-Omni across diverse evaluation criteria.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the prohibitive experimental cost, we could not conduct a comprehensive sweep across all model scales. Based on Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17765v1#S6.T16\" title=\"Table 16 &#8227; 6 Evaluating Non&#8209;Degradation Across Modalities &#8227; Qwen3-Omni Technical Report\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and our internal experiments, we observe: (1) early multimodal integration during pretraining allows language models to be co-trained with vision or audio without any degradation in language capability; (2) the inclusion of the text modality substantially improves performance in the vision and audio. In constrast, we do not observe measurable gains in language ability from adding visual or audio signals; (3) empirically, adding audio data consistently improves vision performance on the MMMU benchmark and OCR-related tasks</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, Qwen3-Omni-Flash-Instruct, and Qwen3-Omni-Flash-Thinking models. Qwen3-Omni-30B-A3B matches or surpasses the latest same-size unimodal Qwen models on text and vision benchmarks. Notably, on audio processing and dialogue benchmarks, it attains state-of-the-art performance among open-source systems on 32 benchmarks and is comparable to, or better than, the strong proprietary counterpart Gemini-2.5-Pro. The Qwen3-Omni-30B-A3B Thinking variant achieves further gains on complex tasks spanning text, vision, and audio-visual reasoning. Beyond accuracy, the model supports 119 text languages, 19 languages for speech recognition and 10 languages for speech synthesis, and enables audio understanding and interactive sessions up to 40 minutes. Thanks to its streaming architecture and multi-codebook design, Qwen3-Omni at the 30B-A3B scale still delivers an end-to-end first-packet latency of 234 ms.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we illustrate the performance of our finetuned Qwen3-Omni-30B-A3B-Captioner through three representative case studies. The selected scenarios are designed to test the model&#8217;s proficiency in: (1) analyzing expressive speech, (2) interpreting complex auditory scenes and sound effects, and (3) captioning composite audio that includes speech, music, and ambient sounds. For reproducibility, the original audio samples can be accessed from our public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "music",
                    "performance"
                ]
            }
        ]
    }
}