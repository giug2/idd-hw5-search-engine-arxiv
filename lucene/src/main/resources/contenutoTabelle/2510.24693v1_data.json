{
    "S2.T1": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 1: A comparative overview of our benchmark against other representative audio benchmarks.\n(✓: Fully supported, ◐: Partially supported, ✗: Not supported)",
        "body": "Fully\n\n\nExpert\n\n\nVerified",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Fully</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Expert</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Verified</span></td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "overview",
            "representative",
            "partially",
            "expert",
            "fully",
            "verified",
            "benchmarks",
            "against",
            "comparative",
            "other",
            "benchmark",
            "our",
            "audio",
            "supported",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "benchmarks",
                    "our",
                    "audio",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To capture this human-like audio competence, we propose a new paradigm, called <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>.\nThis is defined as the ability to perform deep reasoning over the dynamics of <span class=\"ltx_text ltx_font_bold\">sound sources</span> in <span class=\"ltx_text ltx_font_bold\">time (1D)</span> and <span class=\"ltx_text ltx_font_bold\">three-dimensional space (3D)</span>, grounded in an understanding of the physical world.\nMastering 4D audio intelligence is crucial for various applications.\nIn embodied AI and robotics, for instance, agents must integrate fine-grained auditory cues to interact naturally with their surroundings, such as using sound to infer the trajectory of an object or to monitor the subtle operations of a machine.\nTo systematically evaluate this paradigm and bridge the gap between current audio benchmarks and real-world auditory intelligence, we introduce the <span class=\"ltx_text ltx_font_bold\">S</span>patio-<span class=\"ltx_text ltx_font_bold\">T</span>emporal <span class=\"ltx_text ltx_font_bold\">A</span>udio <span class=\"ltx_text ltx_font_bold\">R</span>easoning (<span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench</span>) benchmark.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "fully",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio",
                    "expert",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">4)</span> Final Validation via Human Performance Evaluation: To ensure all items in the benchmark are fair, unambiguous, and solvable by humans, we implement a final validation stage. In this phase, domain experts act as examinees and solve our tasks. Only items that are independently and correctly solved by at least two-thirds of the experts are retained. Our rigorous protocol ensures that all problems in our benchmark are well-posed and reliably solvable by human experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "fully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "our",
                    "representative",
                    "audio",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, a comprehensive benchmark for evaluating 4D audio intelligence over time and 3D space.\nWe use rigorous human annotation, consensus review, and expert validation to ensure the high quality of data samples.\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> establishes standardized tasks and protocols for studying 4D audio intelligence, offering actionable diagnostics for model developers.\nWe expect STAR-Bench to accelerate progress on advanced audio models and training with spatialized corpora, capabilities that are crucial for embodied agents.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "expert",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Gemini-2.5-Pro to assist in expanding and consolidating the taxonomy of tasks in our benchmark. Both DeepSeek-V3 and Gemini-2.5-Pro were utilized for the automated pre-screening of candidate data.\nThe final task definitions and data samples are verified by humans.\nWe also used GPT-4o to generate some of the illustrative figures presented in the paper, and used GPT-5 to polish the manuscript text.\nOnly human-verified revisions are included in the final version.</p>\n\n",
                "matched_terms": [
                    "our",
                    "verified",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prompt for Gemini 2.5 Pro audio captioning: &#8220;Please provide a detailed description of the audio, including speech, music, environmental sounds, and any other noticeable elements. Be as specific as possible.&#8221;</p>\n\n",
                "matched_terms": [
                    "audio",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.F10\" title=\"In B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.F11\" title=\"In B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> present our carefully designed prompts, which leverage LLMs and MLLMs to filter candidate data that meet the requirements of audio segment reordering.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models that do not support multi-audio input (only Audio Flamingo 3 and its Think variant among the models we evaluated), we concatenate the audios with a 2-second silence and specify this in the prompt. In contrast, for models that support multiple audio inputs, we feed them sequentially with textual indices.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reliability of model outputs on our benchmark is notably low, as evidenced by the stark contrast between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. Even the top-performing model, Gemini 2.5 Pro, exhibits an average drop of 25.01 percentage points from its AA to its ACR. This issue is even more pronounced for the majority of open-source models, which record an ACR near zero. This score indicates a complete failure to maintain consistent predictions under minor input perturbations. For these models, the instability often manifests as a tendency to concentrate predictions on a specific option, suggesting a reliance on superficial biases rather than genuine understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "not",
                    "fully",
                    "other"
                ]
            }
        ]
    },
    "S3.F4.fig2": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "(b) Statistics.",
        "body": "Statistics\nNumber\n\n\nTotal Questions\n2,353\n\n\nFoundation Perception\n\n\n\nQuestions\n951\n\n\nCategory\n2\n\n\nAttributes\n6\n\n\nTemporal Reasoning\n\n\n\nQuestions\n900\n\n\nCategory\n2\n\n\nSpatial Reasoning\n\n\n\nQuestions\n502\n\n\nCategory\n3\n\n\nAvg. Audio Length\n14.03 sec",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Statistics</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Number</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Total Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">2,353</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">Foundation Perception</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">951</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Category</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Attributes</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">Temporal Reasoning</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">900</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Category</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">Spatial Reasoning</span></td>\n<td class=\"ltx_td\" style=\"padding-left:0.5pt;padding-right:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">502</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Category</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">Avg. Audio Length</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.5pt;padding-right:0.5pt;\">14.03 sec</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "reasoning",
            "category",
            "sec",
            "number",
            "spatial",
            "total",
            "temporal",
            "statistics",
            "length",
            "foundation",
            "attributes",
            "audio",
            "avg",
            "questions",
            "perception"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "attributes",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a fundamental modality of human perception, audio serves a pivotal role in communication, aesthetic appreciation, and situational awareness, complementing the limitations of visual perception.\nWith the rise of Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> and especially Large Audio-Language Models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, these models have shown impressive capabilities in understanding audio, representing a crucial step toward diverse applications such as embodied intelligence <cite class=\"ltx_cite ltx_citemacro_citep\">(Paul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib28\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "questions",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To capture this human-like audio competence, we propose a new paradigm, called <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>.\nThis is defined as the ability to perform deep reasoning over the dynamics of <span class=\"ltx_text ltx_font_bold\">sound sources</span> in <span class=\"ltx_text ltx_font_bold\">time (1D)</span> and <span class=\"ltx_text ltx_font_bold\">three-dimensional space (3D)</span>, grounded in an understanding of the physical world.\nMastering 4D audio intelligence is crucial for various applications.\nIn embodied AI and robotics, for instance, agents must integrate fine-grained auditory cues to interact naturally with their surroundings, such as using sound to infer the trajectory of an object or to monitor the subtle operations of a machine.\nTo systematically evaluate this paradigm and bridge the gap between current audio benchmarks and real-world auditory intelligence, we introduce the <span class=\"ltx_text ltx_font_bold\">S</span>patio-<span class=\"ltx_text ltx_font_bold\">T</span>emporal <span class=\"ltx_text ltx_font_bold\">A</span>udio <span class=\"ltx_text ltx_font_bold\">R</span>easoning (<span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench</span>) benchmark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "attributes",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "perception",
                    "reasoning",
                    "sec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "attributes",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "attributes",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "category",
                    "spatial",
                    "attributes",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "attributes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the model&#8217;s fundamental audio perceptual abilities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), we further introduce holistic temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS1\" title=\"3.2.1 Temporal Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS2\" title=\"3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), which are designed to systematically evaluate a model&#8217;s reasoning ability that is required for audio 4D intelligence.</p>\n\n",
                "matched_terms": [
                    "sec",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">discrete event sequences</span> category requires the model to understand the logical and temporal relationships between multiple, distinct acoustic events, which are governed by function, convention, or causality.\nThe <span class=\"ltx_text ltx_font_bold\">tool &amp; appliance operation</span> sub-category follows the standardized operating procedure for tools and appliances (e.g., a microwave, a power drill), where the sequence is correct when it follows the tool&#8217;s designed function.\nThe <span class=\"ltx_text ltx_font_bold\">daily scene scripts</span> sub-category applies commonsense and contextual script knowledge to follow the conventional sequence of actions in a daily activity (e.g., brushing teeth, drinking water).\nThe <span class=\"ltx_text ltx_font_bold\">event-triggered consequences</span> sub-category applies causal reasoning to infer that a trigger event (e.g., a firework explosion) will be followed by an automatic and irreversible outcome, whether physical (glass shattering) or social (a crowd cheering).</p>\n\n",
                "matched_terms": [
                    "category",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans effortlessly perceive complex 3D auditory scenes (e.g., hearing a voice from behind, following an approaching car, or locating multiple speakers). Such an ability is fundamental for egocentric interaction and embodied AI systems, for instance, robots that navigate and interact with their surroundings. However, existing benchmarks focus primarily on the localization of static sound sources, whereas real-world scenarios demand reasoning that integrates both spatial and temporal cues. To address this gap, we organize the spatial reasoning task into three subcategories.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "foundation",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, evaluating existing LALMs on multi-channel spatial tasks is challenging. The common practice of these models is to average multi-channel audio into a mono signal, resulting in the loss of substantial spatial information.\nWe conduct a simple experiment as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe construct 20 pseudo-stereo signals by assigning the original audio to the left channel and its additive inverse to the right.\nWhile human listeners could easily perform sound event classification on these signals, the models consistently failed due to signal cancellation during the mono conversion.\nThe result confirms their lack of explicit support for genuine stereo audio processing.\nTo provide a comprehensive assessment, we design two complementary strategies: <span class=\"ltx_text ltx_font_bold\">native input</span>, where the model directly processes stereo audio, follow their default processing pipeline to probe its intrinsic ability to exploit spatial cues; and the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span>, where each channel is presented separately with explicit textual instructions, as shown in the bottom right of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, and allow the model to approximate human-like use of interaural cues.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "audio",
                    "statistics",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1)</span> Taxonomy Construction and Data Sourcing: We build a hierarchical task taxonomy through a collaborative process involving domain experts and the Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>.\nThis framework guides the sourcing of candidate data from large-scale, real-world audio libraries: Clotho <cite class=\"ltx_cite ltx_citemacro_citep\">(Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib11\" title=\"\">2019</a>)</cite> and FSD50K <cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib12\" title=\"\">2022</a>)</cite> for temporal reasoning, and STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib32\" title=\"\">2023</a>)</cite>, along with audio sourced from the internet for spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2)</span> AI-Assisted Automated Filtering: This process employs an efficient three-stage funnel. First, we discard unsuitable samples based on basic properties like duration and energy. Next, an LLM (e.g., DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib22\" title=\"\">2024a</a>)</cite>) performs an initial screening based on textual metadata, providing justifications for its decisions. Finally, a powerful multimodal model (e.g., Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>) analyzes the audio, metadata, and the LLM&#8217;s outputs.\nThe final step yields a judgment, a quality score, and a preliminary classification, further filtering irrelevant samples.\nThe detailed prompts used to query the LLMs are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.SS3\" title=\"B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "sec",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the underlying causes of the poor performance of existing models, we conduct a detailed error analysis along with a series of ablation studies. Due to space limitation, the ablation study on spatial reasoning is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS2\" title=\"E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "reasoning",
                    "sec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "attributes",
                    "audio",
                    "temporal",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study on Temporal Reasoning.</span>\nTo further pinpoint the specific limitations of temporal reasoning, we augment the baseline audio segment reordering task with two progressively easier settings: (1) <span class=\"ltx_text ltx_font_italic\">+ Global Caption</span>, where a single sentence describing the overall scene is provided as a contextual guide; and (2) <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span>, where the complete, unsegmented audio track is offered as a reference, reducing the task to a straightforward process where the correct order can be determined simply by comparing and grounding each segment within the full audio.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, Gemini 2.5 Pro&#8217;s performance scales effectively with task simplification, culminating in a near-perfect 99% accuracy in the <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span> setting. In contrast, the open-source models show minimal to no improvement across these settings. Their performance remains stagnant even when provided with the complete audio reference, despite the simplified nature of the task. This finding starkly exposes a core weakness in current open-source models: <span class=\"ltx_text ltx_font_bold\">a fundamental inability to effectively compare, ground, and integrate information from multiple audio inputs.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements of large language models (LLMs) and multimodal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib44\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib17\" title=\"\">2024</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib4\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib35\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib24\" title=\"\">2024c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib25\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib50\" title=\"\">2025b</a>; Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib29\" title=\"\">2025</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib42\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib41\" title=\"\">a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib9\" title=\"\">2025</a>; Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib37\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib38\" title=\"\">b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib21\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib49\" title=\"\">2025a</a>)</cite>, recent research has increasingly focused on integrating audio perception with LLMs to enhance audio understanding and reasoning. Existing methods can be broadly grouped into two categories: Large Audio Language Models(LALMs) and Omni Language Models(OLMs).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we implement a robust evaluation strategy to ensure rigorous and reliable results. For perception and spatial tasks, we adopt the CircularEval method from MM-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib23\" title=\"\">2024b</a>)</cite>. Specifically, each question is presented to the model <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> times (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of options), with the option order cyclically rotated in each run to mitigate potential positional biases. For temporal tasks, we conduct three runs per question with different temporal segment orders to evaluate the model&#8217;s robustness to sequence variations.\nNote that due to the significant API costs, GPT-4o Audio was evaluated only once per question.\nThis strategy yields two key metrics: Average Accuracy (AA), the mean accuracy across all evaluation runs, and All-Correct Rate (ACR), the proportion of questions answered correctly in every single run, which serves as a stronger indicator of model reliability.</p>\n\n",
                "matched_terms": [
                    "number",
                    "spatial",
                    "temporal",
                    "audio",
                    "questions",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present detailed results for perception, temporal reasoning, and spatial reasoning on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T4\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T5\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several case studies of error analysis, including temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F12\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Figs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F14\" title=\"Figure 14 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F15\" title=\"Figure 15 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F16\" title=\"Figure 16 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F17\" title=\"Figure 17 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F18\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">18</span></a>).</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 2: Evaluation results of various models on STAR-Bench. The best performance is highlighted in bold, and the second-best ones are underlined.\nMA (Macro Accuracy) denotes the unweighted mean of class-wise accuracies, while OA (Overall Accuracy) denotes the proportion of correctly answered instances.\nAll reported values are AA (Average Accuracy across multiple runs) only; for ACR (All-Correct Rate), see Appendix D.",
        "body": "Models\nSize\nFoundational Perception\nTemporal Reasoning\nSpatial Reasoning\nMA (%)\n\n\nRange\nSensitivity\nMA\nContinuous\nDiscrete\nOA\nLocalization\nRelation\nTrajectory\nOA\n\n\nRandom Guess\n-\n23.75\n26.38\n25.33\n14.29\n14.29\n14.29\n33.33\n33.33\n33.33\n33.33\n24.32\n\n\nHuman\n-\n79.42\n74.55\n75.60\n90.12\n85.51\n88.00\n70.00\n80.00\n77.00\n73.72\n79.11\n\n\nSALMONN [33]\n13B\n27.32\n25.48\n26.22\n14.88\n13.30\n14.15\n26.15\n28.61\n39.94\n29.62\n23.33\n\n\nAudio Flamingo 3 [14]\n8.4B\n31.79\n35.72\n34.15\n9.23\n8.01\n8.67\n37.22\n38.35\n44.03\n38.91\n27.24\n\n\nAudio Flamingo 3 think [14]\n8.4B\n25.54\n34.08\n30.66\n13.22\n14.02\n13.59\n35.45\n37.46\n38.05\n36.45\n26.90\n\n\nQwen2-Audio-Instruct [7]\n8.4B\n29.88\n26.47\n27.84\n13.29\n12.10\n12.74\n21.32\n24.78\n15.09\n20.78\n20.45\n\n\nDeSTA2.5-Audio [26]\n8.8B\n29.87\n19.79\n23.82\n16.53\n17.39\n16.93\n23.67\n34.81\n37.74\n29.15\n23.30\n\n\nBAT [51]\n7B\n22.81\n6.25\n12.87\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.29\n\n\nPhi4-MM [1]\n5.5B\n19.14\n29.85\n25.56\n16.74\n16.99\n16.85\n33.10\n27.14\n34.28\n32.01\n24.81\n\n\nKimi-Audio [18]\n7B\n23.29\n27.50\n25.82\n19.97\n16.83\n18.52\n27.56\n38.94\n44.03\n33.60\n25.98\n\n\nMiDashengLM [10]\n7B\n36.94\n30.78\n33.24\n15.43\n17.31\n16.30\n43.11\n45.43\n46.23\n44.29\n31.28\n\n\nStep-Audio-2-mini [39]\n7B\n29.65\n27.14\n28.14\n15.36\n15.87\n15.59\n33.33\n31.27\n37.74\n33.80\n25.84\n\n\nGemma-3n-E4B-it [34]\n7.5B\n18.55\n25.02\n22.43\n16.87\n16.27\n16.59\n23.32\n41.89\n33.96\n29.75\n22.92\n\n\nMing-Lite-Omni-1.5 [3]\n18.9B\n26.76\n26.76\n26.76\n17.08\n15.54\n16.37\n20.14\n35.10\n38.36\n27.35\n23.49\n\n\nQwen-2.5-Omni [43]\n7B\n28.76\n32.32\n30.90\n16.32\n17.71\n16.96\n39.46\n41.30\n27.04\n37.25\n28.37\n\n\nXiaomi-MiMo-Audio [40]\n7B\n34.95\n31.59\n32.93\n18.18\n19.15\n18.63\n36.16\n41.30\n45.28\n39.24\n30.27\n\n\nXiaomi-MiMo-Audio-think [40]\n7B\n29.90\n24.93\n26.92\n16.80\n19.39\n18.00\n34.28\n44.54\n36.79\n37.12\n27.35\n\n\nMiniCPM-O-v2.6 [48]\n8B\n31.02\n31.87\n31.53\n15.36\n17.39\n16.30\n29.92\n43.36\n38.36\n34.73\n27.52\n\n\nGPT-4o Audio [2]\n-\n27.58\n34.55\n31.76\n15.91\n23.56\n19.44\n41.81\n43.97\n39.94\n41.70\n30.97\n\n\nGemini 2.5 Flash [8]\n-\n33.46\n43.88\n39.72\n27.55\n34.38\n30.70\n24.62\n43.07\n22.64\n28.35\n32.92\n\n\nGemini 2.5 Pro [8]\n-\n39.90\n51.13\n46.64\n54.88\n62.74\n58.52\n40.87\n48.97\n45.28\n43.62\n49.59",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Foundational Perception</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Temporal Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Spatial Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">MA (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Range</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Sensitivity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Continuous</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Discrete</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">OA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Localization</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Relation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Trajectory</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">OA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Random Guess</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.32</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Human</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">79.42</td>\n<td class=\"ltx_td ltx_align_center\">74.55</td>\n<td class=\"ltx_td ltx_align_center\">75.60</td>\n<td class=\"ltx_td ltx_align_center\">90.12</td>\n<td class=\"ltx_td ltx_align_center\">85.51</td>\n<td class=\"ltx_td ltx_align_center\">88.00</td>\n<td class=\"ltx_td ltx_align_center\">70.00</td>\n<td class=\"ltx_td ltx_align_center\">80.00</td>\n<td class=\"ltx_td ltx_align_center\">77.00</td>\n<td class=\"ltx_td ltx_align_center\">73.72</td>\n<td class=\"ltx_td ltx_align_center\">79.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SALMONN [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">33</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3 [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">14</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">31.79</td>\n<td class=\"ltx_td ltx_align_center\">35.72</td>\n<td class=\"ltx_td ltx_align_center\">34.15</td>\n<td class=\"ltx_td ltx_align_center\">9.23</td>\n<td class=\"ltx_td ltx_align_center\">8.01</td>\n<td class=\"ltx_td ltx_align_center\">8.67</td>\n<td class=\"ltx_td ltx_align_center\">37.22</td>\n<td class=\"ltx_td ltx_align_center\">38.35</td>\n<td class=\"ltx_td ltx_align_center\">44.03</td>\n<td class=\"ltx_td ltx_align_center\">38.91</td>\n<td class=\"ltx_td ltx_align_center\">27.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3 think [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">14</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">25.54</td>\n<td class=\"ltx_td ltx_align_center\">34.08</td>\n<td class=\"ltx_td ltx_align_center\">30.66</td>\n<td class=\"ltx_td ltx_align_center\">13.22</td>\n<td class=\"ltx_td ltx_align_center\">14.02</td>\n<td class=\"ltx_td ltx_align_center\">13.59</td>\n<td class=\"ltx_td ltx_align_center\">35.45</td>\n<td class=\"ltx_td ltx_align_center\">37.46</td>\n<td class=\"ltx_td ltx_align_center\">38.05</td>\n<td class=\"ltx_td ltx_align_center\">36.45</td>\n<td class=\"ltx_td ltx_align_center\">26.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2-Audio-Instruct [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">7</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">29.88</td>\n<td class=\"ltx_td ltx_align_center\">26.47</td>\n<td class=\"ltx_td ltx_align_center\">27.84</td>\n<td class=\"ltx_td ltx_align_center\">13.29</td>\n<td class=\"ltx_td ltx_align_center\">12.10</td>\n<td class=\"ltx_td ltx_align_center\">12.74</td>\n<td class=\"ltx_td ltx_align_center\">21.32</td>\n<td class=\"ltx_td ltx_align_center\">24.78</td>\n<td class=\"ltx_td ltx_align_center\">15.09</td>\n<td class=\"ltx_td ltx_align_center\">20.78</td>\n<td class=\"ltx_td ltx_align_center\">20.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DeSTA2.5-Audio [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">26</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">8.8B</td>\n<td class=\"ltx_td ltx_align_center\">29.87</td>\n<td class=\"ltx_td ltx_align_center\">19.79</td>\n<td class=\"ltx_td ltx_align_center\">23.82</td>\n<td class=\"ltx_td ltx_align_center\">16.53</td>\n<td class=\"ltx_td ltx_align_center\">17.39</td>\n<td class=\"ltx_td ltx_align_center\">16.93</td>\n<td class=\"ltx_td ltx_align_center\">23.67</td>\n<td class=\"ltx_td ltx_align_center\">34.81</td>\n<td class=\"ltx_td ltx_align_center\">37.74</td>\n<td class=\"ltx_td ltx_align_center\">29.15</td>\n<td class=\"ltx_td ltx_align_center\">23.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BAT [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">51</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">22.81</td>\n<td class=\"ltx_td ltx_align_center\">6.25</td>\n<td class=\"ltx_td ltx_align_center\">12.87</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phi4-MM [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">1</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">5.5B</td>\n<td class=\"ltx_td ltx_align_center\">19.14</td>\n<td class=\"ltx_td ltx_align_center\">29.85</td>\n<td class=\"ltx_td ltx_align_center\">25.56</td>\n<td class=\"ltx_td ltx_align_center\">16.74</td>\n<td class=\"ltx_td ltx_align_center\">16.99</td>\n<td class=\"ltx_td ltx_align_center\">16.85</td>\n<td class=\"ltx_td ltx_align_center\">33.10</td>\n<td class=\"ltx_td ltx_align_center\">27.14</td>\n<td class=\"ltx_td ltx_align_center\">34.28</td>\n<td class=\"ltx_td ltx_align_center\">32.01</td>\n<td class=\"ltx_td ltx_align_center\">24.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kimi-Audio [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">18</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">23.29</td>\n<td class=\"ltx_td ltx_align_center\">27.50</td>\n<td class=\"ltx_td ltx_align_center\">25.82</td>\n<td class=\"ltx_td ltx_align_center\">19.97</td>\n<td class=\"ltx_td ltx_align_center\">16.83</td>\n<td class=\"ltx_td ltx_align_center\">18.52</td>\n<td class=\"ltx_td ltx_align_center\">27.56</td>\n<td class=\"ltx_td ltx_align_center\">38.94</td>\n<td class=\"ltx_td ltx_align_center\">44.03</td>\n<td class=\"ltx_td ltx_align_center\">33.60</td>\n<td class=\"ltx_td ltx_align_center\">25.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiDashengLM [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">10</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">36.94</span></td>\n<td class=\"ltx_td ltx_align_center\">30.78</td>\n<td class=\"ltx_td ltx_align_center\">33.24</td>\n<td class=\"ltx_td ltx_align_center\">15.43</td>\n<td class=\"ltx_td ltx_align_center\">17.31</td>\n<td class=\"ltx_td ltx_align_center\">16.30</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">43.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">45.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">46.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">44.29</span></td>\n<td class=\"ltx_td ltx_align_center\">31.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Step-Audio-2-mini [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">39</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">29.65</td>\n<td class=\"ltx_td ltx_align_center\">27.14</td>\n<td class=\"ltx_td ltx_align_center\">28.14</td>\n<td class=\"ltx_td ltx_align_center\">15.36</td>\n<td class=\"ltx_td ltx_align_center\">15.87</td>\n<td class=\"ltx_td ltx_align_center\">15.59</td>\n<td class=\"ltx_td ltx_align_center\">33.33</td>\n<td class=\"ltx_td ltx_align_center\">31.27</td>\n<td class=\"ltx_td ltx_align_center\">37.74</td>\n<td class=\"ltx_td ltx_align_center\">33.80</td>\n<td class=\"ltx_td ltx_align_center\">25.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemma-3n-E4B-it [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">34</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7.5B</td>\n<td class=\"ltx_td ltx_align_center\">18.55</td>\n<td class=\"ltx_td ltx_align_center\">25.02</td>\n<td class=\"ltx_td ltx_align_center\">22.43</td>\n<td class=\"ltx_td ltx_align_center\">16.87</td>\n<td class=\"ltx_td ltx_align_center\">16.27</td>\n<td class=\"ltx_td ltx_align_center\">16.59</td>\n<td class=\"ltx_td ltx_align_center\">23.32</td>\n<td class=\"ltx_td ltx_align_center\">41.89</td>\n<td class=\"ltx_td ltx_align_center\">33.96</td>\n<td class=\"ltx_td ltx_align_center\">29.75</td>\n<td class=\"ltx_td ltx_align_center\">22.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni-1.5 [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">3</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">18.9B</td>\n<td class=\"ltx_td ltx_align_center\">26.76</td>\n<td class=\"ltx_td ltx_align_center\">26.76</td>\n<td class=\"ltx_td ltx_align_center\">26.76</td>\n<td class=\"ltx_td ltx_align_center\">17.08</td>\n<td class=\"ltx_td ltx_align_center\">15.54</td>\n<td class=\"ltx_td ltx_align_center\">16.37</td>\n<td class=\"ltx_td ltx_align_center\">20.14</td>\n<td class=\"ltx_td ltx_align_center\">35.10</td>\n<td class=\"ltx_td ltx_align_center\">38.36</td>\n<td class=\"ltx_td ltx_align_center\">27.35</td>\n<td class=\"ltx_td ltx_align_center\">23.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen-2.5-Omni [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">43</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">28.76</td>\n<td class=\"ltx_td ltx_align_center\">32.32</td>\n<td class=\"ltx_td ltx_align_center\">30.90</td>\n<td class=\"ltx_td ltx_align_center\">16.32</td>\n<td class=\"ltx_td ltx_align_center\">17.71</td>\n<td class=\"ltx_td ltx_align_center\">16.96</td>\n<td class=\"ltx_td ltx_align_center\">39.46</td>\n<td class=\"ltx_td ltx_align_center\">41.30</td>\n<td class=\"ltx_td ltx_align_center\">27.04</td>\n<td class=\"ltx_td ltx_align_center\">37.25</td>\n<td class=\"ltx_td ltx_align_center\">28.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">40</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">34.95</td>\n<td class=\"ltx_td ltx_align_center\">31.59</td>\n<td class=\"ltx_td ltx_align_center\">32.93</td>\n<td class=\"ltx_td ltx_align_center\">18.18</td>\n<td class=\"ltx_td ltx_align_center\">19.15</td>\n<td class=\"ltx_td ltx_align_center\">18.63</td>\n<td class=\"ltx_td ltx_align_center\">36.16</td>\n<td class=\"ltx_td ltx_align_center\">41.30</td>\n<td class=\"ltx_td ltx_align_center\">45.28</td>\n<td class=\"ltx_td ltx_align_center\">39.24</td>\n<td class=\"ltx_td ltx_align_center\">30.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio-think [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">40</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">29.90</td>\n<td class=\"ltx_td ltx_align_center\">24.93</td>\n<td class=\"ltx_td ltx_align_center\">26.92</td>\n<td class=\"ltx_td ltx_align_center\">16.80</td>\n<td class=\"ltx_td ltx_align_center\">19.39</td>\n<td class=\"ltx_td ltx_align_center\">18.00</td>\n<td class=\"ltx_td ltx_align_center\">34.28</td>\n<td class=\"ltx_td ltx_align_center\">44.54</td>\n<td class=\"ltx_td ltx_align_center\">36.79</td>\n<td class=\"ltx_td ltx_align_center\">37.12</td>\n<td class=\"ltx_td ltx_align_center\">27.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniCPM-O-v2.6 [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">48</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">8B</td>\n<td class=\"ltx_td ltx_align_center\">31.02</td>\n<td class=\"ltx_td ltx_align_center\">31.87</td>\n<td class=\"ltx_td ltx_align_center\">31.53</td>\n<td class=\"ltx_td ltx_align_center\">15.36</td>\n<td class=\"ltx_td ltx_align_center\">17.39</td>\n<td class=\"ltx_td ltx_align_center\">16.30</td>\n<td class=\"ltx_td ltx_align_center\">29.92</td>\n<td class=\"ltx_td ltx_align_center\">43.36</td>\n<td class=\"ltx_td ltx_align_center\">38.36</td>\n<td class=\"ltx_td ltx_align_center\">34.73</td>\n<td class=\"ltx_td ltx_align_center\">27.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GPT-4o Audio [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">41.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini 2.5 Flash [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">8</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">33.46</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">43.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">39.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">27.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">34.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">30.70</span></td>\n<td class=\"ltx_td ltx_align_center\">24.62</td>\n<td class=\"ltx_td ltx_align_center\">43.07</td>\n<td class=\"ltx_td ltx_align_center\">22.64</td>\n<td class=\"ltx_td ltx_align_center\">28.35</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">32.92</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gemini 2.5 Pro [<cite class=\"ltx_cite ltx_citemacro_citenum\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">8</a></cite>]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">39.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">51.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">46.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">54.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">62.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">58.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">40.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">48.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">45.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">43.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">49.59</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "gemini",
            "secondbest",
            "phi4mm",
            "reasoning",
            "stepaudio2mini",
            "ones",
            "mingliteomni15",
            "bold",
            "while",
            "189b",
            "models",
            "human",
            "temporal",
            "trajectory",
            "range",
            "xiaomimimoaudio",
            "accuracy",
            "minicpmov26",
            "gemma3ne4bit",
            "13b",
            "sensitivity",
            "spatial",
            "average",
            "88b",
            "classwise",
            "discrete",
            "across",
            "qwen2audioinstruct",
            "size",
            "macro",
            "flash",
            "perception",
            "starbench",
            "underlined",
            "qwen25omni",
            "denotes",
            "best",
            "only",
            "allcorrect",
            "results",
            "various",
            "75b",
            "midashenglm",
            "performance",
            "foundational",
            "highlighted",
            "appendix",
            "55b",
            "relation",
            "proportion",
            "accuracies",
            "values",
            "pro",
            "flamingo",
            "mean",
            "see",
            "reported",
            "guess",
            "unweighted",
            "kimiaudio",
            "instances",
            "localization",
            "random",
            "acr",
            "evaluation",
            "rate",
            "continuous",
            "gpt4o",
            "audio",
            "overall",
            "bat",
            "salmonn",
            "think",
            "multiple",
            "desta25audio",
            "answered",
            "84b",
            "correctly",
            "runs",
            "xiaomimimoaudiothink",
            "all"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present a comprehensive evaluation on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.T2\" title=\"In 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Due to the space limit, detailed results on each task are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>. Our key findings are as follows:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "across",
                    "starbench",
                    "performance",
                    "models",
                    "accuracy",
                    "human",
                    "foundational",
                    "spatial",
                    "continuous",
                    "temporal",
                    "localization",
                    "audio",
                    "reasoning",
                    "while",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a fundamental modality of human perception, audio serves a pivotal role in communication, aesthetic appreciation, and situational awareness, complementing the limitations of visual perception.\nWith the rise of Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> and especially Large Audio-Language Models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, these models have shown impressive capabilities in understanding audio, representing a crucial step toward diverse applications such as embodied intelligence <cite class=\"ltx_cite ltx_citemacro_citep\">(Paul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib28\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "models",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "performance",
                    "only",
                    "human",
                    "pro",
                    "trajectory",
                    "reasoning",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To capture this human-like audio competence, we propose a new paradigm, called <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>.\nThis is defined as the ability to perform deep reasoning over the dynamics of <span class=\"ltx_text ltx_font_bold\">sound sources</span> in <span class=\"ltx_text ltx_font_bold\">time (1D)</span> and <span class=\"ltx_text ltx_font_bold\">three-dimensional space (3D)</span>, grounded in an understanding of the physical world.\nMastering 4D audio intelligence is crucial for various applications.\nIn embodied AI and robotics, for instance, agents must integrate fine-grained auditory cues to interact naturally with their surroundings, such as using sound to infer the trajectory of an object or to monitor the subtle operations of a machine.\nTo systematically evaluate this paradigm and bridge the gap between current audio benchmarks and real-world auditory intelligence, we introduce the <span class=\"ltx_text ltx_font_bold\">S</span>patio-<span class=\"ltx_text ltx_font_bold\">T</span>emporal <span class=\"ltx_text ltx_font_bold\">A</span>udio <span class=\"ltx_text ltx_font_bold\">R</span>easoning (<span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench</span>) benchmark.</p>\n\n",
                "matched_terms": [
                    "starbench",
                    "trajectory",
                    "various",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "discrete",
                    "across",
                    "starbench",
                    "performance",
                    "evaluation",
                    "sensitivity",
                    "human",
                    "foundational",
                    "spatial",
                    "continuous",
                    "temporal",
                    "localization",
                    "trajectory",
                    "reasoning",
                    "while",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "evaluation",
                    "multiple",
                    "models",
                    "sensitivity",
                    "spatial",
                    "pro",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "starbench",
                    "evaluation",
                    "foundational",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n",
                "matched_terms": [
                    "across",
                    "appendix",
                    "evaluation",
                    "multiple",
                    "see",
                    "models",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "starbench",
                    "evaluation",
                    "foundational",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "foundational",
                    "spatial",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "foundational",
                    "spatial",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "range",
                    "all",
                    "human",
                    "spatial",
                    "localization",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
                "matched_terms": [
                    "random",
                    "across",
                    "performance",
                    "sensitivity",
                    "human",
                    "spatial",
                    "ones",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the model&#8217;s fundamental audio perceptual abilities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), we further introduce holistic temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS1\" title=\"3.2.1 Temporal Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS2\" title=\"3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), which are designed to systematically evaluate a model&#8217;s reasoning ability that is required for audio 4D intelligence.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "models",
                    "continuous",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">continuous processes</span> assess a model&#8217;s ability to track the subtle, continuous evolution of acoustic features within a single, uninterrupted acoustic event.\nThe <span class=\"ltx_text ltx_font_bold\">object spatial motion</span> subcategory reconstructs the spatio-temporal trajectory of moving sources (e.g., passing cars, airplanes) by interpreting key acoustic cues, such as the Doppler effect (frequency shifts indicating relative velocity) and the inverse-square law (loudness changes indicating distance).\nBesides, the <span class=\"ltx_text ltx_font_bold\">in-situ state evolution</span> subcategory assesses a model&#8217;s ability to track the intrinsic evolution of a stationary object&#8217;s state, a process governed by predictable trend patterns.\nThese trend patterns arise from various underlying principles, including: <span class=\"ltx_text ltx_font_italic\">Fluid &amp; Pneumatic Dynamics</span>, where the sound is governed by principles of turbulence, resonance, and pressure changes (e.g., a toilet flushing, water being poured); <span class=\"ltx_text ltx_font_italic\">Thermodynamic Processes</span>, involving irreversible state changes driven by heat (e.g., water boiling, food frying); <span class=\"ltx_text ltx_font_italic\">Energy Decay</span>, a process governed by resonant decay and frictional damping after a single excitation (e.g., a bell&#8217;s chime, an explosion&#8217;s echo); and complex <span class=\"ltx_text ltx_font_italic\">Biological Rhythms</span> that reflect an evolving physiological or emotional state.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "trajectory",
                    "continuous",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">discrete event sequences</span> category requires the model to understand the logical and temporal relationships between multiple, distinct acoustic events, which are governed by function, convention, or causality.\nThe <span class=\"ltx_text ltx_font_bold\">tool &amp; appliance operation</span> sub-category follows the standardized operating procedure for tools and appliances (e.g., a microwave, a power drill), where the sequence is correct when it follows the tool&#8217;s designed function.\nThe <span class=\"ltx_text ltx_font_bold\">daily scene scripts</span> sub-category applies commonsense and contextual script knowledge to follow the conventional sequence of actions in a daily activity (e.g., brushing teeth, drinking water).\nThe <span class=\"ltx_text ltx_font_bold\">event-triggered consequences</span> sub-category applies causal reasoning to infer that a trigger event (e.g., a firework explosion) will be followed by an automatic and irreversible outcome, whether physical (glass shattering) or social (a crowd cheering).</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "discrete",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans effortlessly perceive complex 3D auditory scenes (e.g., hearing a voice from behind, following an approaching car, or locating multiple speakers). Such an ability is fundamental for egocentric interaction and embodied AI systems, for instance, robots that navigate and interact with their surroundings. However, existing benchmarks focus primarily on the localization of static sound sources, whereas real-world scenarios demand reasoning that integrates both spatial and temporal cues. To address this gap, we organize the spatial reasoning task into three subcategories.</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "spatial",
                    "temporal",
                    "localization",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "relation",
                    "spatial",
                    "temporal",
                    "localization",
                    "trajectory",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, evaluating existing LALMs on multi-channel spatial tasks is challenging. The common practice of these models is to average multi-channel audio into a mono signal, resulting in the loss of substantial spatial information.\nWe conduct a simple experiment as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe construct 20 pseudo-stereo signals by assigning the original audio to the left channel and its additive inverse to the right.\nWhile human listeners could easily perform sound event classification on these signals, the models consistently failed due to signal cancellation during the mono conversion.\nThe result confirms their lack of explicit support for genuine stereo audio processing.\nTo provide a comprehensive assessment, we design two complementary strategies: <span class=\"ltx_text ltx_font_bold\">native input</span>, where the model directly processes stereo audio, follow their default processing pipeline to probe its intrinsic ability to exploit spatial cues; and the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span>, where each channel is presented separately with explicit textual instructions, as shown in the bottom right of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, and allow the model to approximate human-like use of interaural cues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human",
                    "spatial",
                    "average",
                    "audio",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "starbench",
                    "all",
                    "see",
                    "foundational",
                    "human",
                    "audio",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1)</span> Taxonomy Construction and Data Sourcing: We build a hierarchical task taxonomy through a collaborative process involving domain experts and the Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>.\nThis framework guides the sourcing of candidate data from large-scale, real-world audio libraries: Clotho <cite class=\"ltx_cite ltx_citemacro_citep\">(Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib11\" title=\"\">2019</a>)</cite> and FSD50K <cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib12\" title=\"\">2022</a>)</cite> for temporal reasoning, and STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib32\" title=\"\">2023</a>)</cite>, along with audio sourced from the internet for spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "spatial",
                    "temporal",
                    "pro",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2)</span> AI-Assisted Automated Filtering: This process employs an efficient three-stage funnel. First, we discard unsuitable samples based on basic properties like duration and energy. Next, an LLM (e.g., DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib22\" title=\"\">2024a</a>)</cite>) performs an initial screening based on textual metadata, providing justifications for its decisions. Finally, a powerful multimodal model (e.g., Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>) analyzes the audio, metadata, and the LLM&#8217;s outputs.\nThe final step yields a judgment, a quality score, and a preliminary classification, further filtering irrelevant samples.\nThe detailed prompts used to query the LLMs are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.SS3\" title=\"B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3)</span> Human Annotation and Quality Control: We recruit and train 10 undergraduate annotators to label the data using a professional platform.\nDuring this process, AI-generated information is provided as an auxiliary reference. To ensure high-quality labels, we implement a stringent two-round review process: the first round involves inter-annotator cross-validation until a consensus is reached, while the second consists of random spot-checks by three domain experts.</p>\n\n",
                "matched_terms": [
                    "random",
                    "human",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">4)</span> Final Validation via Human Performance Evaluation: To ensure all items in the benchmark are fair, unambiguous, and solvable by humans, we implement a final validation stage. In this phase, domain experts act as examinees and solve our tasks. Only items that are independently and correctly solved by at least two-thirds of the experts are retained. Our rigorous protocol ensures that all problems in our benchmark are well-posed and reliably solvable by human experts.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "evaluation",
                    "only",
                    "human",
                    "correctly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "qwen25omni",
                    "phi4mm",
                    "kimiaudio",
                    "xiaomimimoaudio",
                    "evaluation",
                    "stepaudio2mini",
                    "mingliteomni15",
                    "audio",
                    "bat",
                    "gemma3ne4bit",
                    "salmonn",
                    "midashenglm",
                    "desta25audio",
                    "spatial",
                    "models",
                    "pro",
                    "flash",
                    "flamingo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robust Evaluation.</span>\nAll questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as multiple-choice questions and evaluated using classification accuracy, with correctness determined via string matching of option labels or their full text. To ensure robustness, we evaluate each question multiple times under minor prompt perturbations, a strategy detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A3\" title=\"Appendix C Robust Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>. This approach yields two key metrics: <span class=\"ltx_text ltx_font_bold\">Average Accuracy (AA)</span>, the mean accuracy across all runs, and <span class=\"ltx_text ltx_font_bold\">All-Correct Rate (ACR)</span>, the proportion of questions answered correctly in every run, which serves as a stronger indicator of model reliability. Due to space limitations, we primarily report AA in the main text, while complete experimental results are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "appendix",
                    "across",
                    "starbench",
                    "all",
                    "evaluation",
                    "multiple",
                    "rate",
                    "accuracy",
                    "answered",
                    "allcorrect",
                    "results",
                    "proportion",
                    "average",
                    "correctly",
                    "runs",
                    "mean",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "starbench",
                    "temporal",
                    "reasoning",
                    "random",
                    "acr",
                    "evaluation",
                    "best",
                    "only",
                    "accuracy",
                    "audio",
                    "spatial",
                    "average",
                    "across",
                    "all",
                    "models",
                    "human",
                    "pro",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "xiaomimimoaudio",
                    "performance",
                    "all",
                    "models",
                    "foundational",
                    "spatial",
                    "temporal",
                    "pro",
                    "flash",
                    "bat",
                    "audio",
                    "flamingo",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the underlying causes of the poor performance of existing models, we conduct a detailed error analysis along with a series of ablation studies. Due to space limitation, the ablation study on spatial reasoning is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS2\" title=\"E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "models",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "across",
                    "all",
                    "models",
                    "only",
                    "qwen25omni",
                    "spatial",
                    "temporal",
                    "pro",
                    "trajectory",
                    "reasoning",
                    "while",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "performance",
                    "all",
                    "models",
                    "accuracy",
                    "sensitivity",
                    "human",
                    "results",
                    "temporal",
                    "pro",
                    "audio",
                    "while",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study on Temporal Reasoning.</span>\nTo further pinpoint the specific limitations of temporal reasoning, we augment the baseline audio segment reordering task with two progressively easier settings: (1) <span class=\"ltx_text ltx_font_italic\">+ Global Caption</span>, where a single sentence describing the overall scene is provided as a contextual guide; and (2) <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span>, where the complete, unsegmented audio track is offered as a reference, reducing the task to a straightforward process where the correct order can be determined simply by comparing and grounding each segment within the full audio.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, Gemini 2.5 Pro&#8217;s performance scales effectively with task simplification, culminating in a near-perfect 99% accuracy in the <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span> setting. In contrast, the open-source models show minimal to no improvement across these settings. Their performance remains stagnant even when provided with the complete audio reference, despite the simplified nature of the task. This finding starkly exposes a core weakness in current open-source models: <span class=\"ltx_text ltx_font_bold\">a fundamental inability to effectively compare, ground, and integrate information from multiple audio inputs.</span></p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "performance",
                    "multiple",
                    "models",
                    "accuracy",
                    "temporal",
                    "audio",
                    "overall",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, a comprehensive benchmark for evaluating 4D audio intelligence over time and 3D space.\nWe use rigorous human annotation, consensus review, and expert validation to ensure the high quality of data samples.\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> establishes standardized tasks and protocols for studying 4D audio intelligence, offering actionable diagnostics for model developers.\nWe expect STAR-Bench to accelerate progress on advanced audio models and training with spatialized corpora, capabilities that are crucial for embodied agents.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "models",
                    "starbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Gemini-2.5-Pro to assist in expanding and consolidating the taxonomy of tasks in our benchmark. Both DeepSeek-V3 and Gemini-2.5-Pro were utilized for the automated pre-screening of candidate data.\nThe final task definitions and data samples are verified by humans.\nWe also used GPT-4o to generate some of the illustrative figures presented in the paper, and used GPT-5 to polish the manuscript text.\nOnly human-verified revisions are included in the final version.</p>\n\n",
                "matched_terms": [
                    "only",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements of large language models (LLMs) and multimodal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib44\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib17\" title=\"\">2024</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib4\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib35\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib24\" title=\"\">2024c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib25\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib50\" title=\"\">2025b</a>; Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib29\" title=\"\">2025</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib42\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib41\" title=\"\">a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib9\" title=\"\">2025</a>; Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib37\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib38\" title=\"\">b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib21\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib49\" title=\"\">2025a</a>)</cite>, recent research has increasingly focused on integrating audio perception with LLMs to enhance audio understanding and reasoning. Existing methods can be broadly grouped into two categories: Large Audio Language Models(LALMs) and Omni Language Models(OLMs).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "models",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "midashenglm",
                    "salmonn",
                    "across",
                    "gemini",
                    "performance",
                    "models",
                    "qwen25omni",
                    "perception",
                    "gpt4o",
                    "audio",
                    "flamingo",
                    "reasoning",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "evaluation",
                    "results",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "multiple",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "multiple",
                    "models",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prompt for Gemini 2.5 Pro audio captioning: &#8220;Please provide a detailed description of the audio, including speech, music, environmental sounds, and any other noticeable elements. Be as specific as possible.&#8221;</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> details the ranges and levels used for each acoustic attribute, alongside illustrative examples of our foundational acoustic perception tasks.</p>\n\n",
                "matched_terms": [
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated binaural recordings for foundational perception tasks (azimuth, elevation, distance) in Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> across three rectangular rooms&#8212;small (4.0&#215;3.5&#215;2.8 m), medium (8.0&#215;6.0&#215;3.5 m), and large (20&#215;15&#215;8 m)&#8212;each with a frequency-independent wall absorption coefficient of 0.25. Image-source reflections were modeled up to order 10 at 44.1 kHz (matched to the HRTF sampling rate). For each room, we evaluated two listener positions (distinct Cartesian coordinates) and oriented the head toward the +x axis. Binaural reception used a co-located two-microphone array at the listener position with ear-specific directivity derived from a measured SOFA HRTF<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa\" title=\"\">https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa</a></span></span></span> (MIT KEMAR, &#8220;normal pinna&#8221;; interpolation order 12, 1000 points), loaded via a local SOFA reader and applied to the left/right channels.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "across",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each condition (room &#215; listener), sources were placed on a sphere centered at the listener (radii 1&#8211;10 m; configurable azimuth/elevation), and ear-specific BRIRs were computed. Mono source signals were drawn from three curated audio clips (&#8220;alarm,&#8221; &#8220;applause,&#8221; &#8220;telephones&#8221;), downmixed if necessary. Rendering was performed by convolving each dry signal with the left/right BRIRs after an early/late mix to emphasize distance cues: we preserved the first 80 ms and attenuated the late tail by 0.5. We then applied global peak normalization across the batch to avoid clipping while preserving inter-position level differences.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as clear multiple-choice questions with well-formatted options. We adopt classification accuracy as the evaluation metric. To determine the correctness of a response, we employ string matching to extract either the chosen option label (e.g., <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>A<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>) or the full text content of the option from the model&#8217;s output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "accuracy",
                    "starbench",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we implement a robust evaluation strategy to ensure rigorous and reliable results. For perception and spatial tasks, we adopt the CircularEval method from MM-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib23\" title=\"\">2024b</a>)</cite>. Specifically, each question is presented to the model <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> times (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of options), with the option order cyclically rotated in each run to mitigate potential positional biases. For temporal tasks, we conduct three runs per question with different temporal segment orders to evaluate the model&#8217;s robustness to sequence variations.\nNote that due to the significant API costs, GPT-4o Audio was evaluated only once per question.\nThis strategy yields two key metrics: Average Accuracy (AA), the mean accuracy across all evaluation runs, and All-Correct Rate (ACR), the proportion of questions answered correctly in every single run, which serves as a stronger indicator of model reliability.</p>\n\n",
                "matched_terms": [
                    "temporal",
                    "acr",
                    "evaluation",
                    "only",
                    "accuracy",
                    "rate",
                    "allcorrect",
                    "results",
                    "gpt4o",
                    "audio",
                    "answered",
                    "spatial",
                    "average",
                    "correctly",
                    "runs",
                    "across",
                    "all",
                    "proportion",
                    "mean",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models that do not support multi-audio input (only Audio Flamingo 3 and its Think variant among the models we evaluated), we concatenate the audios with a 2-second silence and specify this in the prompt. In contrast, for models that support multiple audio inputs, we feed them sequentially with textual indices.</p>\n\n",
                "matched_terms": [
                    "think",
                    "multiple",
                    "only",
                    "models",
                    "audio",
                    "flamingo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a human performance baseline, we conduct a human evaluation on a randomly sampled subset of approximately 10% of the data from each task. This evaluation is performed by 10 university students, from whom we explicitly exclude anyone involved in data annotation or with domain-specific expertise, thereby ensuring a general, non-expert perspective.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present detailed results for perception, temporal reasoning, and spatial reasoning on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T4\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T5\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "starbench",
                    "results",
                    "spatial",
                    "temporal",
                    "reasoning",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reliability of model outputs on our benchmark is notably low, as evidenced by the stark contrast between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. Even the top-performing model, Gemini 2.5 Pro, exhibits an average drop of 25.01 percentage points from its AA to its ACR. This issue is even more pronounced for the majority of open-source models, which record an ACR near zero. This score indicates a complete failure to maintain consistent predictions under minor input perturbations. For these models, the instability often manifests as a tendency to concentrate predictions on a specific option, suggesting a reliance on superficial biases rather than genuine understanding.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "gemini",
                    "models",
                    "accuracy",
                    "average",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "human",
                    "results",
                    "spatial",
                    "audio",
                    "overall",
                    "reasoning",
                    "while",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several case studies of error analysis, including temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F12\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Figs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F14\" title=\"Figure 14 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F15\" title=\"Figure 15 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F16\" title=\"Figure 16 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F17\" title=\"Figure 17 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F18\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">18</span></a>).</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning"
                ]
            }
        ]
    },
    "A2.T3": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 3: Task examples of foundational acoustic perception.",
        "body": "125 Hz - 8000 Hz\n\n\n-10dB - 110dB",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">125 Hz - 8000 Hz</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-10dB - 110dB</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "task",
            "10db",
            "foundational",
            "examples",
            "acoustic",
            "110db",
            "perception"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> details the ranges and levels used for each acoustic attribute, alongside illustrative examples of our foundational acoustic perception tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "task",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "task",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "examples",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated binaural recordings for foundational perception tasks (azimuth, elevation, distance) in Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> across three rectangular rooms&#8212;small (4.0&#215;3.5&#215;2.8 m), medium (8.0&#215;6.0&#215;3.5 m), and large (20&#215;15&#215;8 m)&#8212;each with a frequency-independent wall absorption coefficient of 0.25. Image-source reflections were modeled up to order 10 at 44.1 kHz (matched to the HRTF sampling rate). For each room, we evaluated two listener positions (distinct Cartesian coordinates) and oriented the head toward the +x axis. Binaural reception used a co-located two-microphone array at the listener position with ear-specific directivity derived from a measured SOFA HRTF<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa\" title=\"\">https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa</a></span></span></span> (MIT KEMAR, &#8220;normal pinna&#8221;; interpolation order 12, 1000 points), loaded via a local SOFA reader and applied to the left/right channels.</p>\n\n",
                "matched_terms": [
                    "foundational",
                    "perception"
                ]
            }
        ]
    },
    "A5.T4": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 4: Results for the foundational perception task. Each cell reports AA / ACR: Average Accuracy (AA; overall accuracy across all runs) / All-Correct Rate (ACR; proportion of samples that are correct on every run). The best model in each category is shown in bold, and the second best is underlined.",
        "body": "Model\nSize\nAbsolute Perception Range\nRelative Discrimination Sensitivity\nMA (%)\n\n\nPitch&Loudness\nAzimuth\nElevation\nDistance\nPitch\nLoudness\nDuration\nAzimuth\nElevation\nDistance\n\n\nRandom Guess\n—\n25.00 / 0.39\n\n20.00 / 0.03\n\n25.00 / 0.39\n\n25.00 / 0.39\n\n25.00 / 0.39\n\n25.00 / 0.39\n\n25.00 / 0.39\n\n33.33 / 3.7\n\n25.00 / 0.39\n\n25.00 / 0.39\n\n25.33 / 0.68\n\n\n\nHuman\n—\n98.67 / —\n\n73.33 / —\n\n66.67 / —\n\n70.00 / —\n\n83.33 / —\n\n85.56 / —\n\n83.33 / —\n\n83.33 / —\n\n38.09 / —\n\n73.68 / —\n\n75.60 / —\n\n\n\nSALMONN\n13B\n14.34 / 0.00\n\n25.83 / 0.63\n\n35.76 / 0.00\n\n33.33 / 0.00\n\n31.04 / 0.00\n\n25.00 / 0.00\n\n28.54 / 0.00\n\n31.39 / 3.89\n\n24.15 / 0.00\n\n12.77 / 0.00\n\n26.22 / 0.45\n\n\n\nAudio Flamingo 3\n8.4B\n37.59 / 0.00\n\n\n27.92 / 3.13\n\n28.82 / 0.00\n\n32.84 / 0.00\n\n42.50 / 1.67\n\n28.96 / 0.00\n\n34.79 / 0.00\n\n38.61 / 6.67\n\n\n33.90 / 0.00\n\n35.56 / 0.00\n\n34.15 / 1.15\n\n\n\nAudio Flamingo 3 think\n8.4B\n51.75 / 6.99\n\n8.75 / 0.00\n\n33.33 / 1.04\n\n8.33 / 0.00\n\n36.04 / 8.33\n\n\n45.63 / 2.50\n\n59.38 / 38.33\n\n\n41.11 / 4.17\n\n12.29 / 0.00\n\n10.00 / 0.00\n\n30.66 / 6.14\n\n\n\nQwen2-Audio-Instruct\n8.4B\n35.66 / 1.40\n\n22.50 / 0.00\n\n\n48.61 / 10.76\n\n12.75 / 0.98\n\n35.63 / 0.00\n\n16.25 / 0.00\n\n26.46 / 0.00\n\n35.00 / 8.06\n\n21.61 / 1.69\n\n23.88 / 0.00\n\n27.84 / 2.29\n\n\n\nDeSTA2.5-Audio\n8.8B\n16.96 / 0.00\n\n21.25 / 0.42\n\n45.49 / 1.39\n\n35.78 / 1.47\n\n11.67 / 0.00\n\n11.25 / 0.00\n\n22.71 / 0.00\n\n33.06 / 7.78\n\n10.59 / 0.00\n\n29.44 / 0.00\n\n23.82 / 1.11\n\n\n\nBAT\n7B\n0.00 / 0.00\n\n\n26.04 / 26.04\n\n41.67 / 41.67\n\n23.53 / 23.53\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n37.50 / 37.50\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n12.87 / 12.87\n\n\n\nPhi4-MM\n5.5B\n9.44 / 0.00\n\n24.17 / 0.00\n\n15.97 / 0.00\n\n26.96 / 0.00\n\n24.38 / 0.00\n\n30.00 / 0.00\n\n27.92 / 0.00\n\n36.94 / 0.00\n\n32.62 / 0.00\n\n27.22 / 0.00\n\n25.56 / 0.00\n\n\n\nKimi-Audio\n7B\n18.71 / 0.00\n\n18.12 / 0.00\n\n38.19 / 0.00\n\n18.13 / 0.00\n\n24.38 / 0.00\n\n32.29 / 0.00\n\n34.17 / 0.83\n\n39.72 / 3.89\n\n25.00 / 0.85\n\n9.44 / 0.00\n\n25.82 / 0.56\n\n\n\nMiDashengLM\n7B\n48.95 / 33.57\n\n20.63 / 0.00\n\n\n48.26 / 11.81\n\n29.90 / 0.98\n\n40.00 / 34.17\n\n17.08 / 0.83\n\n23.54 / 7.50\n\n34.72 / 8.61\n\n27.12 / 1.69\n\n42.22 / 6.11\n\n33.24 / 10.53\n\n\n\nStep-Audio-2-mini\n7B\n37.59 / 0.00\n\n20.00 / 0.00\n\n31.60 / 0.69\n\n29.41 / 0.00\n\n25.00 / 0.00\n\n29.17 / 0.00\n\n32.29 / 0.00\n\n20.00 / 0.00\n\n31.36 / 0.00\n\n25.00 / 0.00\n\n28.14 / 0.07\n\n\n\nGemma-3n-E4B-it\n7.5B\n7.18 / 0.00\n\n24.38 / 4.17\n\n25.00 / 0.00\n\n17.65 / 0.00\n\n38.75 / 0.00\n\n8.75 / 0.00\n\n15.00 / 5.83\n\n40.56 / 1.94\n\n23.73 / 0.00\n\n23.33 / 0.00\n\n22.43 / 1.19\n\n\n\nMing-Lite-Omni-1.5\n18.9B\n28.67 / 0.00\n\n20.21 / 0.00\n\n27.78 / 0.35\n\n30.39 / 3.92\n\n16.67 / 16.67\n\n16.67 / 16.67\n\n16.67 / 16.67\n\n\n41.67 / 0.28\n\n\n32.81 / 0.00\n\n36.11 / 0.00\n\n26.77 / 5.46\n\n\n\nQwen-2.5-Omni\n7B\n27.45 / 3.50\n\n18.33 / 0.21\n\n27.57 / 1.47\n\n\n41.67 / 1.47\n\n48.13 / 35.00\n\n39.79 / 15.00\n\n38.33 / 26.67\n\n16.11 / 0.28\n\n11.02 / 0.00\n\n40.56 / 2.78\n\n30.90 / 8.64\n\n\n\nXiaomi-MiMo-Audio\n7B\n36.71 / 5.59\n\n18.54 / 19.17\n\n48.26 / 3.82\n\n36.27 / 2.94\n\n46.04 / 24.17\n\n36.46 / 0.83\n\n17.70 / 16.67\n\n40.56 / 2.22\n\n20.98 / 0.00\n\n27.78 / 1.67\n\n32.93 / 7.71\n\n\n\nXiaomi-MiMo-Audio-think\n7B\n43.01 / 14.69\n\n11.67 / 0.00\n\n25.69 / 0.00\n\n39.21 / 4.90\n\n28.13 / 3.33\n\n15.21 / 1.67\n\n22.71 / 1.67\n\n29.44 / 2.50\n\n21.88 / 0.45\n\n32.22 / 1.67\n\n26.92 / 3.09\n\n\n\nMiniCPM-O-v2.6\n8B\n46.33 / 8.39\n\n24.58 / 0.21\n\n23.26 / 0.35\n\n29.90 / 0.00\n\n38.13 / 3.33\n\n38.96 / 4.17\n\n32.08 / 3.33\n\n37.22 / 2.78\n\n22.10 / 0.22\n\n22.78 / 0.00\n\n31.53 / 2.28\n\n\n\nGPT-4o Audio\n—\n45.28 / —\n\n16.67 / —\n\n44.44 / —\n\n3.92 / —\n\n43.33 / —\n\n36.04 / —\n\n46.46 / —\n\n29.58 / —\n\n11.86 / —\n\n40.00 / —\n\n31.76 / —\n\n\n\nGemini 2.5 Flash\n—\n\n62.59 / 18.19\n\n12.50 / 0.00\n\n18.06 / 0.35\n\n40.69 / 1.47\n\n\n48.54 / 21.67\n\n\n40.83 / 6.67\n\n\n63.13 / 27.50\n\n37.08 / 9.17\n\n25.42 / 0.85\n\n\n48.33 / 4.44\n\n\n39.72 / 9.03\n\n\n\nGemini 2.5 Pro\n—\n\n86.71 / 62.94\n\n25.83 / 1.25\n\n5.88 / 0.00\n\n\n41.18 / 5.88\n\n\n63.33 / 52.50\n\n33.75 / 15.83\n\n\n78.96 / 68.33\n\n37.08 / 13.75\n\n29.24 / 6.36\n\n\n64.44 / 12.22\n\n\n46.64 / 23.91",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Absolute Perception Range</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">Relative Discrimination Sensitivity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">MA (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Pitch&amp;Loudness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Azimuth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Elevation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Distance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Pitch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Loudness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Duration</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Azimuth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Elevation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Distance</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Random Guess</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.68</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Human</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">98.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">73.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">66.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">70.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">85.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.09&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">73.68&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">75.60&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SALMONN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.34&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.83&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.63</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.76&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.54&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.39&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.77&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.45</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">37.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">27.92</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.82&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.84&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">42.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.79&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.61&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">33.90</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.15</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3 think</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">51.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.99</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">45.63</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">59.38&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;38.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">41.11</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">10.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">30.66&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.14</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2-Audio-Instruct</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">35.66&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">48.61</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.25&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.61&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.84&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.29</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DeSTA2.5-Audio</td>\n<td class=\"ltx_td ltx_align_center\">8.8B</td>\n<td class=\"ltx_td ltx_align_center\">16.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.25&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">45.49&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.25&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.71&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.06&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">10.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.82&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BAT</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">26.04</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;26.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;41.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.53&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;23.53</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;37.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.87&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;12.87</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phi4-MM</td>\n<td class=\"ltx_td ltx_align_center\">5.5B</td>\n<td class=\"ltx_td ltx_align_center\">9.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.17&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.97&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.38&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">30.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.92&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.94&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.62&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kimi-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">18.71&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.12&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.19&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.13&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.38&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.17&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.72&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">9.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.82&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.56</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiDashengLM</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">48.95&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;33.57</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">48.26</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.81</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.90&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;34.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.54&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.72&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.61</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.12&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">42.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.24&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.53</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Step-Audio-2-mini</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">37.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">31.60&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.41&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.17&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">31.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.14&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.07</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemma-3n-E4B-it</td>\n<td class=\"ltx_td ltx_align_center\">7.5B</td>\n<td class=\"ltx_td ltx_align_center\">7.18&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.38&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.65&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.73&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.43&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.19</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni-1.5</td>\n<td class=\"ltx_td ltx_align_center\">18.9B</td>\n<td class=\"ltx_td ltx_align_center\">28.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.21&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">30.39&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;16.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;16.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;16.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">41.67</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">32.81</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.11&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.77&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.46</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen-2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">27.45&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.57&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">41.67</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">48.13&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;35.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.79&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;26.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.11&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.02&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">30.90&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.64</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">36.71&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.54&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;19.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">48.26&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.82</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.27&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;24.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.70&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;16.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.98&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.93&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.71</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio-think</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">43.01&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;14.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.69&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.21&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.90</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.13&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.21&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.71&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.45</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.92&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.09</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniCPM-O-v2.6</td>\n<td class=\"ltx_td ltx_align_center\">8B</td>\n<td class=\"ltx_td ltx_align_center\">46.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.26&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.90&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.13&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">31.53&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.28</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GPT-4o Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.28&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.92&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.86&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.76&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini 2.5 Flash</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">62.59</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;18.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.06&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.69&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">48.54</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;21.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">40.83</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">63.13</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;27.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.42&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">48.33</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">39.72</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.03</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gemini 2.5 Pro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">86.71</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;62.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.83&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">41.18</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.88</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">63.33</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;52.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">33.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">78.96</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;68.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">37.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;13.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">29.24&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.36</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">64.44</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;12.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">46.64</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;23.91</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "absolute",
            "gemini",
            "cell",
            "every",
            "underlined",
            "qwen25omni",
            "guess",
            "phi4mm",
            "kimiaudio",
            "range",
            "samples",
            "random",
            "acr",
            "pitch",
            "xiaomimimoaudio",
            "best",
            "rate",
            "accuracy",
            "pitchloudness",
            "distance",
            "allcorrect",
            "stepaudio2mini",
            "results",
            "flamingo",
            "mingliteomni15",
            "gpt4o",
            "minicpmov26",
            "each",
            "correct",
            "overall",
            "bold",
            "discrimination",
            "audio",
            "bat",
            "gemma3ne4bit",
            "75b",
            "salmonn",
            "category",
            "elevation",
            "loudness",
            "13b",
            "reports",
            "relative",
            "think",
            "desta25audio",
            "sensitivity",
            "midashenglm",
            "foundational",
            "189b",
            "84b",
            "average",
            "runs",
            "88b",
            "second",
            "xiaomimimoaudiothink",
            "across",
            "qwen2audioinstruct",
            "55b",
            "all",
            "task",
            "size",
            "azimuth",
            "human",
            "proportion",
            "run",
            "duration",
            "pro",
            "flash",
            "model",
            "perception"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we present detailed results for perception, temporal reasoning, and spatial reasoning on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T4\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T5\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "absolute",
                    "across",
                    "relative",
                    "accuracy",
                    "foundational",
                    "human",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a fundamental modality of human perception, audio serves a pivotal role in communication, aesthetic appreciation, and situational awareness, complementing the limitations of visual perception.\nWith the rise of Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> and especially Large Audio-Language Models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, these models have shown impressive capabilities in understanding audio, representing a crucial step toward diverse applications such as embodied intelligence <cite class=\"ltx_cite ltx_citemacro_citep\">(Paul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib28\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gemini",
                    "distance",
                    "human",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "absolute",
                    "pitch",
                    "elevation",
                    "loudness",
                    "every",
                    "relative",
                    "across",
                    "distance",
                    "sensitivity",
                    "foundational",
                    "task",
                    "azimuth",
                    "human",
                    "duration",
                    "audio",
                    "discrimination",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "sensitivity",
                    "pro",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "loudness",
                    "elevation",
                    "distance",
                    "task",
                    "foundational",
                    "azimuth",
                    "duration",
                    "audio",
                    "model",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "samples",
                    "pitch",
                    "loudness",
                    "elevation",
                    "distance",
                    "task",
                    "foundational",
                    "azimuth",
                    "duration",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "absolute",
                    "pitch",
                    "category",
                    "elevation",
                    "loudness",
                    "relative",
                    "all",
                    "distance",
                    "azimuth",
                    "human",
                    "perception",
                    "audio",
                    "model",
                    "second",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
                "matched_terms": [
                    "random",
                    "pitch",
                    "across",
                    "relative",
                    "sensitivity",
                    "task",
                    "human",
                    "each",
                    "audio",
                    "model",
                    "discrimination"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">continuous processes</span> assess a model&#8217;s ability to track the subtle, continuous evolution of acoustic features within a single, uninterrupted acoustic event.\nThe <span class=\"ltx_text ltx_font_bold\">object spatial motion</span> subcategory reconstructs the spatio-temporal trajectory of moving sources (e.g., passing cars, airplanes) by interpreting key acoustic cues, such as the Doppler effect (frequency shifts indicating relative velocity) and the inverse-square law (loudness changes indicating distance).\nBesides, the <span class=\"ltx_text ltx_font_bold\">in-situ state evolution</span> subcategory assesses a model&#8217;s ability to track the intrinsic evolution of a stationary object&#8217;s state, a process governed by predictable trend patterns.\nThese trend patterns arise from various underlying principles, including: <span class=\"ltx_text ltx_font_italic\">Fluid &amp; Pneumatic Dynamics</span>, where the sound is governed by principles of turbulence, resonance, and pressure changes (e.g., a toilet flushing, water being poured); <span class=\"ltx_text ltx_font_italic\">Thermodynamic Processes</span>, involving irreversible state changes driven by heat (e.g., water boiling, food frying); <span class=\"ltx_text ltx_font_italic\">Energy Decay</span>, a process governed by resonant decay and frictional damping after a single excitation (e.g., a bell&#8217;s chime, an explosion&#8217;s echo); and complex <span class=\"ltx_text ltx_font_italic\">Biological Rhythms</span> that reflect an evolving physiological or emotional state.</p>\n\n",
                "matched_terms": [
                    "distance",
                    "loudness",
                    "relative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">discrete event sequences</span> category requires the model to understand the logical and temporal relationships between multiple, distinct acoustic events, which are governed by function, convention, or causality.\nThe <span class=\"ltx_text ltx_font_bold\">tool &amp; appliance operation</span> sub-category follows the standardized operating procedure for tools and appliances (e.g., a microwave, a power drill), where the sequence is correct when it follows the tool&#8217;s designed function.\nThe <span class=\"ltx_text ltx_font_bold\">daily scene scripts</span> sub-category applies commonsense and contextual script knowledge to follow the conventional sequence of actions in a daily activity (e.g., brushing teeth, drinking water).\nThe <span class=\"ltx_text ltx_font_bold\">event-triggered consequences</span> sub-category applies causal reasoning to infer that a trigger event (e.g., a firework explosion) will be followed by an automatic and irreversible outcome, whether physical (glass shattering) or social (a crowd cheering).</p>\n\n",
                "matched_terms": [
                    "correct",
                    "category",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "relative",
                    "task",
                    "each",
                    "model",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, evaluating existing LALMs on multi-channel spatial tasks is challenging. The common practice of these models is to average multi-channel audio into a mono signal, resulting in the loss of substantial spatial information.\nWe conduct a simple experiment as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe construct 20 pseudo-stereo signals by assigning the original audio to the left channel and its additive inverse to the right.\nWhile human listeners could easily perform sound event classification on these signals, the models consistently failed due to signal cancellation during the mono conversion.\nThe result confirms their lack of explicit support for genuine stereo audio processing.\nTo provide a comprehensive assessment, we design two complementary strategies: <span class=\"ltx_text ltx_font_bold\">native input</span>, where the model directly processes stereo audio, follow their default processing pipeline to probe its intrinsic ability to exploit spatial cues; and the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span>, where each channel is presented separately with explicit textual instructions, as shown in the bottom right of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, and allow the model to approximate human-like use of interaural cues.</p>\n\n",
                "matched_terms": [
                    "human",
                    "average",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "all",
                    "task",
                    "foundational",
                    "human",
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1)</span> Taxonomy Construction and Data Sourcing: We build a hierarchical task taxonomy through a collaborative process involving domain experts and the Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>.\nThis framework guides the sourcing of candidate data from large-scale, real-world audio libraries: Clotho <cite class=\"ltx_cite ltx_citemacro_citep\">(Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib11\" title=\"\">2019</a>)</cite> and FSD50K <cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib12\" title=\"\">2022</a>)</cite> for temporal reasoning, and STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib32\" title=\"\">2023</a>)</cite>, along with audio sourced from the internet for spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2)</span> AI-Assisted Automated Filtering: This process employs an efficient three-stage funnel. First, we discard unsuitable samples based on basic properties like duration and energy. Next, an LLM (e.g., DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib22\" title=\"\">2024a</a>)</cite>) performs an initial screening based on textual metadata, providing justifications for its decisions. Finally, a powerful multimodal model (e.g., Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>) analyzes the audio, metadata, and the LLM&#8217;s outputs.\nThe final step yields a judgment, a quality score, and a preliminary classification, further filtering irrelevant samples.\nThe detailed prompts used to query the LLMs are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.SS3\" title=\"B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gemini",
                    "duration",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3)</span> Human Annotation and Quality Control: We recruit and train 10 undergraduate annotators to label the data using a professional platform.\nDuring this process, AI-generated information is provided as an auxiliary reference. To ensure high-quality labels, we implement a stringent two-round review process: the first round involves inter-annotator cross-validation until a consensus is reached, while the second consists of random spot-checks by three domain experts.</p>\n\n",
                "matched_terms": [
                    "random",
                    "human",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">4)</span> Final Validation via Human Performance Evaluation: To ensure all items in the benchmark are fair, unambiguous, and solvable by humans, we implement a final validation stage. In this phase, domain experts act as examinees and solve our tasks. Only items that are independently and correctly solved by at least two-thirds of the experts are retained. Our rigorous protocol ensures that all problems in our benchmark are well-posed and reliably solvable by human experts.</p>\n\n",
                "matched_terms": [
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "gemma3ne4bit",
                    "salmonn",
                    "midashenglm",
                    "gemini",
                    "xiaomimimoaudio",
                    "desta25audio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "phi4mm",
                    "kimiaudio",
                    "flamingo",
                    "mingliteomni15",
                    "pro",
                    "flash",
                    "audio",
                    "model",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robust Evaluation.</span>\nAll questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as multiple-choice questions and evaluated using classification accuracy, with correctness determined via string matching of option labels or their full text. To ensure robustness, we evaluate each question multiple times under minor prompt perturbations, a strategy detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A3\" title=\"Appendix C Robust Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>. This approach yields two key metrics: <span class=\"ltx_text ltx_font_bold\">Average Accuracy (AA)</span>, the mean accuracy across all runs, and <span class=\"ltx_text ltx_font_bold\">All-Correct Rate (ACR)</span>, the proportion of questions answered correctly in every run, which serves as a stronger indicator of model reliability. Due to space limitations, we primarily report AA in the main text, while complete experimental results are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "model",
                    "allcorrect",
                    "results",
                    "proportion",
                    "run",
                    "average",
                    "each",
                    "runs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive evaluation on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.T2\" title=\"In 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Due to the space limit, detailed results on each task are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>. Our key findings are as follows:</p>\n\n",
                "matched_terms": [
                    "results",
                    "task",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "random",
                    "acr",
                    "gemini",
                    "across",
                    "all",
                    "best",
                    "accuracy",
                    "task",
                    "human",
                    "average",
                    "pro",
                    "audio",
                    "model",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "xiaomimimoaudio",
                    "all",
                    "foundational",
                    "perception",
                    "pro",
                    "flash",
                    "audio",
                    "flamingo",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "qwen25omni",
                    "pro",
                    "audio",
                    "model",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "gemini",
                    "loudness",
                    "all",
                    "accuracy",
                    "sensitivity",
                    "task",
                    "human",
                    "results",
                    "duration",
                    "pro",
                    "audio",
                    "model",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study on Temporal Reasoning.</span>\nTo further pinpoint the specific limitations of temporal reasoning, we augment the baseline audio segment reordering task with two progressively easier settings: (1) <span class=\"ltx_text ltx_font_italic\">+ Global Caption</span>, where a single sentence describing the overall scene is provided as a contextual guide; and (2) <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span>, where the complete, unsegmented audio track is offered as a reference, reducing the task to a straightforward process where the correct order can be determined simply by comparing and grounding each segment within the full audio.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, Gemini 2.5 Pro&#8217;s performance scales effectively with task simplification, culminating in a near-perfect 99% accuracy in the <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span> setting. In contrast, the open-source models show minimal to no improvement across these settings. Their performance remains stagnant even when provided with the complete audio reference, despite the simplified nature of the task. This finding starkly exposes a core weakness in current open-source models: <span class=\"ltx_text ltx_font_bold\">a fundamental inability to effectively compare, ground, and integrate information from multiple audio inputs.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "across",
                    "accuracy",
                    "task",
                    "each",
                    "correct",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, a comprehensive benchmark for evaluating 4D audio intelligence over time and 3D space.\nWe use rigorous human annotation, consensus review, and expert validation to ensure the high quality of data samples.\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> establishes standardized tasks and protocols for studying 4D audio intelligence, offering actionable diagnostics for model developers.\nWe expect STAR-Bench to accelerate progress on advanced audio models and training with spatialized corpora, capabilities that are crucial for embodied agents.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Gemini-2.5-Pro to assist in expanding and consolidating the taxonomy of tasks in our benchmark. Both DeepSeek-V3 and Gemini-2.5-Pro were utilized for the automated pre-screening of candidate data.\nThe final task definitions and data samples are verified by humans.\nWe also used GPT-4o to generate some of the illustrative figures presented in the paper, and used GPT-5 to polish the manuscript text.\nOnly human-verified revisions are included in the final version.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gpt4o",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements of large language models (LLMs) and multimodal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib44\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib17\" title=\"\">2024</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib4\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib35\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib24\" title=\"\">2024c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib25\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib50\" title=\"\">2025b</a>; Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib29\" title=\"\">2025</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib42\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib41\" title=\"\">a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib9\" title=\"\">2025</a>; Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib37\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib38\" title=\"\">b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib21\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib49\" title=\"\">2025a</a>)</cite>, recent research has increasingly focused on integrating audio perception with LLMs to enhance audio understanding and reasoning. Existing methods can be broadly grouped into two categories: Large Audio Language Models(LALMs) and Omni Language Models(OLMs).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "salmonn",
                    "midashenglm",
                    "gemini",
                    "across",
                    "qwen25omni",
                    "gpt4o",
                    "perception",
                    "audio",
                    "flamingo",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prompt for Gemini 2.5 Pro audio captioning: &#8220;Please provide a detailed description of the audio, including speech, music, environmental sounds, and any other noticeable elements. Be as specific as possible.&#8221;</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> details the ranges and levels used for each acoustic attribute, alongside illustrative examples of our foundational acoustic perception tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "foundational",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated binaural recordings for foundational perception tasks (azimuth, elevation, distance) in Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> across three rectangular rooms&#8212;small (4.0&#215;3.5&#215;2.8 m), medium (8.0&#215;6.0&#215;3.5 m), and large (20&#215;15&#215;8 m)&#8212;each with a frequency-independent wall absorption coefficient of 0.25. Image-source reflections were modeled up to order 10 at 44.1 kHz (matched to the HRTF sampling rate). For each room, we evaluated two listener positions (distinct Cartesian coordinates) and oriented the head toward the +x axis. Binaural reception used a co-located two-microphone array at the listener position with ear-specific directivity derived from a measured SOFA HRTF<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa\" title=\"\">https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa</a></span></span></span> (MIT KEMAR, &#8220;normal pinna&#8221;; interpolation order 12, 1000 points), loaded via a local SOFA reader and applied to the left/right channels.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "across",
                    "rate",
                    "distance",
                    "azimuth",
                    "foundational",
                    "each",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each condition (room &#215; listener), sources were placed on a sphere centered at the listener (radii 1&#8211;10 m; configurable azimuth/elevation), and ear-specific BRIRs were computed. Mono source signals were drawn from three curated audio clips (&#8220;alarm,&#8221; &#8220;applause,&#8221; &#8220;telephones&#8221;), downmixed if necessary. Rendering was performed by convolving each dry signal with the left/right BRIRs after an early/late mix to emphasize distance cues: we preserved the first 80 ms and attenuated the late tail by 0.5. We then applied global peak normalization across the batch to avoid clipping while preserving inter-position level differences.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "distance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Absolute azimuth:</span>\nEight angles\n<math alttext=\"\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mn>30</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;60</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;120</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;150</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;210</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;240</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;300</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;330</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}</annotation></semantics></math>.\nFor each angle we rendered all combinations of 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, yielding\n<math alttext=\"8\\times(3\\times 2\\times 2)=96\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>96</mn></mrow><annotation encoding=\"application/x-tex\">8\\times(3\\times 2\\times 2)=96</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute elevation:</span>\nSix angles\n<math alttext=\"\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mo>&#8722;</mo><msup><mn>75</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>45</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>15</mn><mo>&#8728;</mo></msup></mrow><mo>,</mo><msup><mn>&#8201;15</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;45</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;75</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}</annotation></semantics></math>.\nPer angle we rendered 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m7\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, for\n<math alttext=\"6\\times(3\\times 2\\times 2)=72\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m8\" intent=\":literal\"><semantics><mrow><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>72</mn></mrow><annotation encoding=\"application/x-tex\">6\\times(3\\times 2\\times 2)=72</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute distance:</span>\nRadii from 1&#8211;10 m with a nonuniform allocation to emphasize near-field cues:\nfor 1&#8211;7 m we generated 6 utterances per meter (42 total),\nand for 8&#8211;10 m we generated 3 per meter (9 total),\ngiving <math alttext=\"42+9=51\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m9\" intent=\":literal\"><semantics><mrow><mrow><mn>42</mn><mo>+</mo><mn>9</mn></mrow><mo>=</mo><mn>51</mn></mrow><annotation encoding=\"application/x-tex\">42+9=51</annotation></semantics></math> utterances per (room <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> listener) set.</p>\n\n",
                "matched_terms": [
                    "absolute",
                    "elevation",
                    "all",
                    "distance",
                    "azimuth",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relative azimuth:</span>\nDifferences were multiples of <math alttext=\"30^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m1\" intent=\":literal\"><semantics><msup><mn>30</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">30^{\\circ}</annotation></semantics></math>:\n<math alttext=\"\\{30^{\\circ},\\,60^{\\circ},\\,90^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,180^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mn>30</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;60</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;90</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;120</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;150</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;180</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{30^{\\circ},\\,60^{\\circ},\\,90^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,180^{\\circ}\\}</annotation></semantics></math> (6 levels),\ntotaling <math alttext=\"6\\times 20=120\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m3\" intent=\":literal\"><semantics><mrow><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>20</mn></mrow><mo>=</mo><mn>120</mn></mrow><annotation encoding=\"application/x-tex\">6\\times 20=120</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Relative elevation:</span>\nFour difference angles <math alttext=\"\\{15^{\\circ},\\,90^{\\circ},\\,120^{\\circ},\\,150^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mn>15</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;90</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;120</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;150</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{15^{\\circ},\\,90^{\\circ},\\,120^{\\circ},\\,150^{\\circ}\\}</annotation></semantics></math>\nwith 18, 17, 17, 12 utterances respectively (64 total).\n<span class=\"ltx_text ltx_font_bold\">Relative distance:</span>\nFour difference levels <math alttext=\"\\{1-2,\\,4-5,\\,6-7,\\,8-9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mn>1</mn><mo>&#8722;</mo><mn>2</mn></mrow><mo>,</mo><mrow><mn>&#8201;4</mn><mo>&#8722;</mo><mn>5</mn></mrow><mo>,</mo><mrow><mn>&#8201;6</mn><mo>&#8722;</mo><mn>7</mn></mrow><mo>,</mo><mrow><mn>&#8201;8</mn><mo>&#8722;</mo><mn>9</mn></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1-2,\\,4-5,\\,6-7,\\,8-9\\}</annotation></semantics></math> m with counts per level\n<math alttext=\"\\{12,\\,12,\\,12,\\,9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>12</mn><mo>,</mo><mn>&#8201;12</mn><mo>,</mo><mn>&#8201;12</mn><mo>,</mo><mn>&#8201;9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{12,\\,12,\\,12,\\,9\\}</annotation></semantics></math>, totaling <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p5.m7\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> utterances.</p>\n\n",
                "matched_terms": [
                    "distance",
                    "azimuth",
                    "elevation",
                    "relative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as clear multiple-choice questions with well-formatted options. We adopt classification accuracy as the evaluation metric. To determine the correctness of a response, we employ string matching to extract either the chosen option label (e.g., <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>A<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>) or the full text content of the option from the model&#8217;s output.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we implement a robust evaluation strategy to ensure rigorous and reliable results. For perception and spatial tasks, we adopt the CircularEval method from MM-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib23\" title=\"\">2024b</a>)</cite>. Specifically, each question is presented to the model <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> times (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of options), with the option order cyclically rotated in each run to mitigate potential positional biases. For temporal tasks, we conduct three runs per question with different temporal segment orders to evaluate the model&#8217;s robustness to sequence variations.\nNote that due to the significant API costs, GPT-4o Audio was evaluated only once per question.\nThis strategy yields two key metrics: Average Accuracy (AA), the mean accuracy across all evaluation runs, and All-Correct Rate (ACR), the proportion of questions answered correctly in every single run, which serves as a stronger indicator of model reliability.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "model",
                    "allcorrect",
                    "results",
                    "proportion",
                    "run",
                    "gpt4o",
                    "average",
                    "each",
                    "audio",
                    "runs",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models that do not support multi-audio input (only Audio Flamingo 3 and its Think variant among the models we evaluated), we concatenate the audios with a 2-second silence and specify this in the prompt. In contrast, for models that support multiple audio inputs, we feed them sequentially with textual indices.</p>\n\n",
                "matched_terms": [
                    "think",
                    "audio",
                    "flamingo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a human performance baseline, we conduct a human evaluation on a randomly sampled subset of approximately 10% of the data from each task. This evaluation is performed by 10 university students, from whom we explicitly exclude anyone involved in data annotation or with domain-specific expertise, thereby ensuring a general, non-expert perspective.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reliability of model outputs on our benchmark is notably low, as evidenced by the stark contrast between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. Even the top-performing model, Gemini 2.5 Pro, exhibits an average drop of 25.01 percentage points from its AA to its ACR. This issue is even more pronounced for the majority of open-source models, which record an ACR near zero. This score indicates a complete failure to maintain consistent predictions under minor input perturbations. For these models, the instability often manifests as a tendency to concentrate predictions on a specific option, suggesting a reliance on superficial biases rather than genuine understanding.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "gemini",
                    "accuracy",
                    "average",
                    "pro",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "results",
                    "each",
                    "audio",
                    "overall",
                    "perception"
                ]
            }
        ]
    },
    "A5.T5": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 5: Results for the temporal reasoning task. Each cell reports AA / ACR: Average Accuracy (AA; overall accuracy across all runs) / All-Correct Rate (ACR; proportion of samples that are correct on every run). The best model in each category is shown in bold, and the second best is underlined.",
        "body": "Model\nSize\nContinuous Processes\nDiscrete Event Sequences\nOA (%)\n\n\nObject Spatial Motion\nIn-Situ State Evolution\nTool & Appliance Operation\nDaily Scene Scripts\nEvent-Triggered Consequences\n\n\nRandom Guess\n—\n14.29 / 0.00\n\n14.29 / 0.00\n\n14.29 / 0.00\n\n14.29 / 0.00\n\n14.29 / 0.00\n\n14.29 / 0.00\n\n\n\nHuman\n—\n91.11 / —\n\n88.89 / —\n\n87.88 / —\n\n83.33 / —\n\n83.33 / —\n\n88.00 / —\n\n\n\nSALMONN\n13B\n13.88 / 0.74\n\n16.12 / 0.00\n\n13.56 / 1.96\n\n13.15 / 1.11\n\n12.50 / 0.00\n\n14.15 / 0.89\n\n\n\nAudio Flamingo 3\n8.4B\n8.55 / 0.00\n\n10.08 / 0.47\n\n8.66 / 0.98\n\n7.22 / 1.11\n\n8.33 / 3.13\n\n8.67 / 0.67\n\n\n\nAudio Flamingo 3 think\n8.4B\n14.37 / 0.00\n\n11.78 / 0.93\n\n15.36 / 1.47\n\n12.96 / 2.22\n\n11.46 / 0.00\n\n13.59 / 1.00\n\n\n\nQwen2-Audio-Instruct\n8.4B\n12.89 / 0.00\n\n13.80 / 0.93\n\n12.09 / 0.00\n\n12.22 / 1.11\n\n11.46 / 0.00\n\n12.74 / 0.44\n\n\n\nDeSTA2.5-Audio\n8.8B\n16.98 / 0.37\n\n15.97 / 1.40\n\n19.93 / 1.47\n\n15.56 / 0.56\n\n11.46 / 0.00\n\n16.93 / 0.89\n\n\n\nBAT\n7B\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n\n\nPhi4-MM\n5.5B\n17.72 / 0.00\n\n15.50 / 0.47\n\n16.34 / 0.98\n\n17.04 / 3.89\n\n20.83 / 3.13\n\n16.85 / 1.22\n\n\n\nKimi-Audio\n7B\n18.71 / 1.49\n\n21.55 / 2.33\n\n18.63 / 0.49\n\n15.19 / 2.22\n\n14.58 / 0.00\n\n18.52 / 1.56\n\n\n\nMiDashengLM\n7B\n17.10 / 0.37\n\n13.33 / 0.00\n\n17.16 / 1.96\n\n16.67 / 2.22\n\n21.88 / 0.00\n\n16.30 / 1.00\n\n\n\nStep-Audio-2-mini\n7B\n16.11 / 0.37\n\n14.42 / 0.00\n\n15.52 / 0.00\n\n16.30 / 0.00\n\n15.63 / 0.00\n\n15.59 / 0.11\n\n\n\nGemma-3n-E4B-it\n7.5B\n17.10 / 0.00\n\n16.59 / 0.00\n\n17.81 / 0.00\n\n13.70 / 0.00\n\n20.83 / 0.00\n\n16.59 / 0.00\n\n\n\nMing-Lite-Omni-1.5\n18.9B\n17.47 / 1.12\n\n16.59 / 0.47\n\n13.89 / 0.00\n\n17.59 / 1.11\n\n14.58 / 0.00\n\n16.37 / 0.67\n\n\n\nQwen-2.5-Omni\n7B\n17.10 / 0.37\n\n15.35 / 0.93\n\n19.77 / 1.47\n\n16.48 / 0.56\n\n11.46 / 0.00\n\n16.96 / 0.78\n\n\n\nXiaomi-MiMo-Audio\n7B\n18.22 / 0.00\n\n18.14 / 0.47\n\n17.16 / 0.98\n\n20.19 / 2.22\n\n26.04 / 3.13\n\n18.63 / 0.89\n\n\n\nXiaomi-MiMo-Audio-think\n7B\n16.36 / 0.37\n\n17.36 / 0.47\n\n19.93 / 1.96\n\n18.70 / 2.22\n\n19.79 / 0.00\n\n18.00 / 1.11\n\n\n\nMiniCPM-O-v2.6\n8B\n16.23 / 0.00\n\n14.26 / 0.93\n\n17.48 / 0.49\n\n17.78 / 0.56\n\n14.58 / 0.00\n\n16.30 / 0.44\n\n\n\nGPT-4o Audio\n—\n15.61 / —\n\n16.28 / —\n\n24.02 / —\n\n22.78 / —\n\n25.00 / —\n\n19.44 / —\n\n\n\nGemini 2.5 Flash\n—\n\n30.86 / 3.35\n\n\n23.41 / 3.72\n\n\n38.07 / 12.75\n\n\n30.19 / 7.22\n\n\n34.38 / 9.38\n\n\n30.70 / 6.56\n\n\n\nGemini 2.5 Pro\n—\n\n63.82 / 38.66\n\n\n43.72 / 17.67\n\n\n69.77 / 46.08\n\n\n57.22 / 38.33\n\n\n48.96 / 28.13\n\n\n58.52 / 34.89",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Continuous Processes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Discrete Event Sequences</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">OA (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Object Spatial Motion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">In-Situ State Evolution</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Tool &amp; Appliance Operation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Daily Scene Scripts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Event-Triggered Consequences</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Random Guess</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.29&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Human</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">91.11&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">88.89&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">87.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">88.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SALMONN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.74</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.12&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.89</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">8.55&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">10.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.66&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.67</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3 think</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">14.37&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">13.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2-Audio-Instruct</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">12.89&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">13.80&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.09&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.74&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.44</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DeSTA2.5-Audio</td>\n<td class=\"ltx_td ltx_align_center\">8.8B</td>\n<td class=\"ltx_td ltx_align_center\">16.98&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.97&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">19.93&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.56</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.93&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.89</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BAT</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phi4-MM</td>\n<td class=\"ltx_td ltx_align_center\">5.5B</td>\n<td class=\"ltx_td ltx_align_center\">17.72&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.34&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.83&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.85&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.22</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kimi-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">18.71&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.55&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.19&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">14.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.52&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.56</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiDashengLM</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">17.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">13.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.16&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.88&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Step-Audio-2-mini</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">16.11&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">14.42&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.52&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemma-3n-E4B-it</td>\n<td class=\"ltx_td ltx_align_center\">7.5B</td>\n<td class=\"ltx_td ltx_align_center\">17.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.81&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">13.70&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.83&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni-1.5</td>\n<td class=\"ltx_td ltx_align_center\">18.9B</td>\n<td class=\"ltx_td ltx_align_center\">17.47&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">13.89&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">14.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.37&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.67</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen-2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">17.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.35&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">19.77&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.48&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.56</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.78</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">18.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.14&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.16&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.19&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">26.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.89</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio-think</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">16.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">19.93&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.70&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">19.79&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniCPM-O-v2.6</td>\n<td class=\"ltx_td ltx_align_center\">8B</td>\n<td class=\"ltx_td ltx_align_center\">16.23&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">14.26&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.48&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.56</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">14.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.44</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GPT-4o Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.61&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.28&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.02&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini 2.5 Flash</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">30.86</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">23.41</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.72</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">38.07</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;12.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">30.19</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">34.38</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.38</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">30.70</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.56</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gemini 2.5 Pro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">63.82</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;38.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">43.72</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">69.77</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;46.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">57.22</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;38.33</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">48.96</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;28.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">58.52</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;34.89</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "gemini",
            "phi4mm",
            "reasoning",
            "eventtriggered",
            "stepaudio2mini",
            "state",
            "mingliteomni15",
            "bold",
            "object",
            "189b",
            "tool",
            "daily",
            "motion",
            "human",
            "model",
            "temporal",
            "operation",
            "samples",
            "xiaomimimoaudio",
            "accuracy",
            "processes",
            "consequences",
            "event",
            "minicpmov26",
            "each",
            "gemma3ne4bit",
            "category",
            "13b",
            "spatial",
            "average",
            "88b",
            "discrete",
            "across",
            "qwen2audioinstruct",
            "task",
            "size",
            "flash",
            "cell",
            "every",
            "underlined",
            "qwen25omni",
            "scene",
            "best",
            "allcorrect",
            "results",
            "correct",
            "75b",
            "midashenglm",
            "scripts",
            "reports",
            "55b",
            "proportion",
            "run",
            "pro",
            "flamingo",
            "guess",
            "kimiaudio",
            "sequences",
            "appliance",
            "random",
            "acr",
            "rate",
            "continuous",
            "gpt4o",
            "audio",
            "overall",
            "bat",
            "salmonn",
            "think",
            "evolution",
            "desta25audio",
            "84b",
            "runs",
            "insitu",
            "second",
            "xiaomimimoaudiothink",
            "all"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we present detailed results for perception, temporal reasoning, and spatial reasoning on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T4\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T5\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "discrete",
                    "across",
                    "accuracy",
                    "human",
                    "processes",
                    "spatial",
                    "continuous",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a fundamental modality of human perception, audio serves a pivotal role in communication, aesthetic appreciation, and situational awareness, complementing the limitations of visual perception.\nWith the rise of Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> and especially Large Audio-Language Models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, these models have shown impressive capabilities in understanding audio, representing a crucial step toward diverse applications such as embodied intelligence <cite class=\"ltx_cite ltx_citemacro_citep\">(Paul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib28\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gemini",
                    "human",
                    "processes",
                    "event",
                    "pro",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To capture this human-like audio competence, we propose a new paradigm, called <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>.\nThis is defined as the ability to perform deep reasoning over the dynamics of <span class=\"ltx_text ltx_font_bold\">sound sources</span> in <span class=\"ltx_text ltx_font_bold\">time (1D)</span> and <span class=\"ltx_text ltx_font_bold\">three-dimensional space (3D)</span>, grounded in an understanding of the physical world.\nMastering 4D audio intelligence is crucial for various applications.\nIn embodied AI and robotics, for instance, agents must integrate fine-grained auditory cues to interact naturally with their surroundings, such as using sound to infer the trajectory of an object or to monitor the subtle operations of a machine.\nTo systematically evaluate this paradigm and bridge the gap between current audio benchmarks and real-world auditory intelligence, we introduce the <span class=\"ltx_text ltx_font_bold\">S</span>patio-<span class=\"ltx_text ltx_font_bold\">T</span>emporal <span class=\"ltx_text ltx_font_bold\">A</span>udio <span class=\"ltx_text ltx_font_bold\">R</span>easoning (<span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench</span>) benchmark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "discrete",
                    "scene",
                    "across",
                    "every",
                    "scripts",
                    "task",
                    "human",
                    "processes",
                    "spatial",
                    "event",
                    "temporal",
                    "continuous",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "spatial",
                    "pro",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "task",
                    "spatial",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "samples",
                    "spatial",
                    "audio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "category",
                    "all",
                    "human",
                    "spatial",
                    "audio",
                    "model",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
                "matched_terms": [
                    "random",
                    "across",
                    "task",
                    "human",
                    "spatial",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the model&#8217;s fundamental audio perceptual abilities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), we further introduce holistic temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS1\" title=\"3.2.1 Temporal Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS2\" title=\"3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), which are designed to systematically evaluate a model&#8217;s reasoning ability that is required for audio 4D intelligence.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "temporal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "model",
                    "processes",
                    "continuous",
                    "event",
                    "temporal",
                    "each",
                    "audio",
                    "sequences",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">continuous processes</span> assess a model&#8217;s ability to track the subtle, continuous evolution of acoustic features within a single, uninterrupted acoustic event.\nThe <span class=\"ltx_text ltx_font_bold\">object spatial motion</span> subcategory reconstructs the spatio-temporal trajectory of moving sources (e.g., passing cars, airplanes) by interpreting key acoustic cues, such as the Doppler effect (frequency shifts indicating relative velocity) and the inverse-square law (loudness changes indicating distance).\nBesides, the <span class=\"ltx_text ltx_font_bold\">in-situ state evolution</span> subcategory assesses a model&#8217;s ability to track the intrinsic evolution of a stationary object&#8217;s state, a process governed by predictable trend patterns.\nThese trend patterns arise from various underlying principles, including: <span class=\"ltx_text ltx_font_italic\">Fluid &amp; Pneumatic Dynamics</span>, where the sound is governed by principles of turbulence, resonance, and pressure changes (e.g., a toilet flushing, water being poured); <span class=\"ltx_text ltx_font_italic\">Thermodynamic Processes</span>, involving irreversible state changes driven by heat (e.g., water boiling, food frying); <span class=\"ltx_text ltx_font_italic\">Energy Decay</span>, a process governed by resonant decay and frictional damping after a single excitation (e.g., a bell&#8217;s chime, an explosion&#8217;s echo); and complex <span class=\"ltx_text ltx_font_italic\">Biological Rhythms</span> that reflect an evolving physiological or emotional state.</p>\n\n",
                "matched_terms": [
                    "object",
                    "evolution",
                    "motion",
                    "state",
                    "processes",
                    "spatial",
                    "event",
                    "continuous",
                    "insitu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">discrete event sequences</span> category requires the model to understand the logical and temporal relationships between multiple, distinct acoustic events, which are governed by function, convention, or causality.\nThe <span class=\"ltx_text ltx_font_bold\">tool &amp; appliance operation</span> sub-category follows the standardized operating procedure for tools and appliances (e.g., a microwave, a power drill), where the sequence is correct when it follows the tool&#8217;s designed function.\nThe <span class=\"ltx_text ltx_font_bold\">daily scene scripts</span> sub-category applies commonsense and contextual script knowledge to follow the conventional sequence of actions in a daily activity (e.g., brushing teeth, drinking water).\nThe <span class=\"ltx_text ltx_font_bold\">event-triggered consequences</span> sub-category applies causal reasoning to infer that a trigger event (e.g., a firework explosion) will be followed by an automatic and irreversible outcome, whether physical (glass shattering) or social (a crowd cheering).</p>\n\n",
                "matched_terms": [
                    "eventtriggered",
                    "discrete",
                    "scene",
                    "category",
                    "daily",
                    "scripts",
                    "model",
                    "consequences",
                    "event",
                    "temporal",
                    "tool",
                    "operation",
                    "correct",
                    "sequences",
                    "reasoning",
                    "appliance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans effortlessly perceive complex 3D auditory scenes (e.g., hearing a voice from behind, following an approaching car, or locating multiple speakers). Such an ability is fundamental for egocentric interaction and embodied AI systems, for instance, robots that navigate and interact with their surroundings. However, existing benchmarks focus primarily on the localization of static sound sources, whereas real-world scenarios demand reasoning that integrates both spatial and temporal cues. To address this gap, we organize the spatial reasoning task into three subcategories.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "task",
                    "spatial",
                    "temporal",
                    "each",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, evaluating existing LALMs on multi-channel spatial tasks is challenging. The common practice of these models is to average multi-channel audio into a mono signal, resulting in the loss of substantial spatial information.\nWe conduct a simple experiment as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe construct 20 pseudo-stereo signals by assigning the original audio to the left channel and its additive inverse to the right.\nWhile human listeners could easily perform sound event classification on these signals, the models consistently failed due to signal cancellation during the mono conversion.\nThe result confirms their lack of explicit support for genuine stereo audio processing.\nTo provide a comprehensive assessment, we design two complementary strategies: <span class=\"ltx_text ltx_font_bold\">native input</span>, where the model directly processes stereo audio, follow their default processing pipeline to probe its intrinsic ability to exploit spatial cues; and the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span>, where each channel is presented separately with explicit textual instructions, as shown in the bottom right of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, and allow the model to approximate human-like use of interaural cues.</p>\n\n",
                "matched_terms": [
                    "human",
                    "processes",
                    "spatial",
                    "event",
                    "average",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "all",
                    "task",
                    "human",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1)</span> Taxonomy Construction and Data Sourcing: We build a hierarchical task taxonomy through a collaborative process involving domain experts and the Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>.\nThis framework guides the sourcing of candidate data from large-scale, real-world audio libraries: Clotho <cite class=\"ltx_cite ltx_citemacro_citep\">(Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib11\" title=\"\">2019</a>)</cite> and FSD50K <cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib12\" title=\"\">2022</a>)</cite> for temporal reasoning, and STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib32\" title=\"\">2023</a>)</cite>, along with audio sourced from the internet for spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "task",
                    "spatial",
                    "temporal",
                    "pro",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2)</span> AI-Assisted Automated Filtering: This process employs an efficient three-stage funnel. First, we discard unsuitable samples based on basic properties like duration and energy. Next, an LLM (e.g., DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib22\" title=\"\">2024a</a>)</cite>) performs an initial screening based on textual metadata, providing justifications for its decisions. Finally, a powerful multimodal model (e.g., Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>) analyzes the audio, metadata, and the LLM&#8217;s outputs.\nThe final step yields a judgment, a quality score, and a preliminary classification, further filtering irrelevant samples.\nThe detailed prompts used to query the LLMs are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.SS3\" title=\"B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gemini",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3)</span> Human Annotation and Quality Control: We recruit and train 10 undergraduate annotators to label the data using a professional platform.\nDuring this process, AI-generated information is provided as an auxiliary reference. To ensure high-quality labels, we implement a stringent two-round review process: the first round involves inter-annotator cross-validation until a consensus is reached, while the second consists of random spot-checks by three domain experts.</p>\n\n",
                "matched_terms": [
                    "random",
                    "human",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">4)</span> Final Validation via Human Performance Evaluation: To ensure all items in the benchmark are fair, unambiguous, and solvable by humans, we implement a final validation stage. In this phase, domain experts act as examinees and solve our tasks. Only items that are independently and correctly solved by at least two-thirds of the experts are retained. Our rigorous protocol ensures that all problems in our benchmark are well-posed and reliably solvable by human experts.</p>\n\n",
                "matched_terms": [
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "gemma3ne4bit",
                    "salmonn",
                    "midashenglm",
                    "gemini",
                    "xiaomimimoaudio",
                    "desta25audio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "phi4mm",
                    "kimiaudio",
                    "spatial",
                    "flamingo",
                    "mingliteomni15",
                    "pro",
                    "flash",
                    "audio",
                    "model",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robust Evaluation.</span>\nAll questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as multiple-choice questions and evaluated using classification accuracy, with correctness determined via string matching of option labels or their full text. To ensure robustness, we evaluate each question multiple times under minor prompt perturbations, a strategy detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A3\" title=\"Appendix C Robust Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>. This approach yields two key metrics: <span class=\"ltx_text ltx_font_bold\">Average Accuracy (AA)</span>, the mean accuracy across all runs, and <span class=\"ltx_text ltx_font_bold\">All-Correct Rate (ACR)</span>, the proportion of questions answered correctly in every run, which serves as a stronger indicator of model reliability. Due to space limitations, we primarily report AA in the main text, while complete experimental results are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "model",
                    "allcorrect",
                    "results",
                    "proportion",
                    "run",
                    "average",
                    "each",
                    "runs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive evaluation on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.T2\" title=\"In 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Due to the space limit, detailed results on each task are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>. Our key findings are as follows:</p>\n\n",
                "matched_terms": [
                    "results",
                    "task",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "random",
                    "acr",
                    "gemini",
                    "across",
                    "all",
                    "best",
                    "accuracy",
                    "task",
                    "human",
                    "spatial",
                    "temporal",
                    "average",
                    "pro",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "xiaomimimoaudio",
                    "all",
                    "spatial",
                    "temporal",
                    "pro",
                    "flash",
                    "audio",
                    "flamingo",
                    "reasoning",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the underlying causes of the poor performance of existing models, we conduct a detailed error analysis along with a series of ablation studies. Due to space limitation, the ablation study on spatial reasoning is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS2\" title=\"E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "qwen25omni",
                    "spatial",
                    "event",
                    "temporal",
                    "pro",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "all",
                    "accuracy",
                    "task",
                    "human",
                    "results",
                    "temporal",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study on Temporal Reasoning.</span>\nTo further pinpoint the specific limitations of temporal reasoning, we augment the baseline audio segment reordering task with two progressively easier settings: (1) <span class=\"ltx_text ltx_font_italic\">+ Global Caption</span>, where a single sentence describing the overall scene is provided as a contextual guide; and (2) <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span>, where the complete, unsegmented audio track is offered as a reference, reducing the task to a straightforward process where the correct order can be determined simply by comparing and grounding each segment within the full audio.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, Gemini 2.5 Pro&#8217;s performance scales effectively with task simplification, culminating in a near-perfect 99% accuracy in the <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span> setting. In contrast, the open-source models show minimal to no improvement across these settings. Their performance remains stagnant even when provided with the complete audio reference, despite the simplified nature of the task. This finding starkly exposes a core weakness in current open-source models: <span class=\"ltx_text ltx_font_bold\">a fundamental inability to effectively compare, ground, and integrate information from multiple audio inputs.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "scene",
                    "gemini",
                    "across",
                    "accuracy",
                    "task",
                    "temporal",
                    "each",
                    "correct",
                    "overall",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, a comprehensive benchmark for evaluating 4D audio intelligence over time and 3D space.\nWe use rigorous human annotation, consensus review, and expert validation to ensure the high quality of data samples.\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> establishes standardized tasks and protocols for studying 4D audio intelligence, offering actionable diagnostics for model developers.\nWe expect STAR-Bench to accelerate progress on advanced audio models and training with spatialized corpora, capabilities that are crucial for embodied agents.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Gemini-2.5-Pro to assist in expanding and consolidating the taxonomy of tasks in our benchmark. Both DeepSeek-V3 and Gemini-2.5-Pro were utilized for the automated pre-screening of candidate data.\nThe final task definitions and data samples are verified by humans.\nWe also used GPT-4o to generate some of the illustrative figures presented in the paper, and used GPT-5 to polish the manuscript text.\nOnly human-verified revisions are included in the final version.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gpt4o",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements of large language models (LLMs) and multimodal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib44\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib17\" title=\"\">2024</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib4\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib35\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib24\" title=\"\">2024c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib25\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib50\" title=\"\">2025b</a>; Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib29\" title=\"\">2025</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib42\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib41\" title=\"\">a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib9\" title=\"\">2025</a>; Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib37\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib38\" title=\"\">b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib21\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib49\" title=\"\">2025a</a>)</cite>, recent research has increasingly focused on integrating audio perception with LLMs to enhance audio understanding and reasoning. Existing methods can be broadly grouped into two categories: Large Audio Language Models(LALMs) and Omni Language Models(OLMs).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "midashenglm",
                    "salmonn",
                    "gemini",
                    "across",
                    "qwen25omni",
                    "gpt4o",
                    "audio",
                    "flamingo",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "results",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "across",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "spatial",
                    "temporal",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prompt for Gemini 2.5 Pro audio captioning: &#8220;Please provide a detailed description of the audio, including speech, music, environmental sounds, and any other noticeable elements. Be as specific as possible.&#8221;</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated binaural recordings for foundational perception tasks (azimuth, elevation, distance) in Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> across three rectangular rooms&#8212;small (4.0&#215;3.5&#215;2.8 m), medium (8.0&#215;6.0&#215;3.5 m), and large (20&#215;15&#215;8 m)&#8212;each with a frequency-independent wall absorption coefficient of 0.25. Image-source reflections were modeled up to order 10 at 44.1 kHz (matched to the HRTF sampling rate). For each room, we evaluated two listener positions (distinct Cartesian coordinates) and oriented the head toward the +x axis. Binaural reception used a co-located two-microphone array at the listener position with ear-specific directivity derived from a measured SOFA HRTF<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa\" title=\"\">https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa</a></span></span></span> (MIT KEMAR, &#8220;normal pinna&#8221;; interpolation order 12, 1000 points), loaded via a local SOFA reader and applied to the left/right channels.</p>\n\n",
                "matched_terms": [
                    "each",
                    "rate",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each condition (room &#215; listener), sources were placed on a sphere centered at the listener (radii 1&#8211;10 m; configurable azimuth/elevation), and ear-specific BRIRs were computed. Mono source signals were drawn from three curated audio clips (&#8220;alarm,&#8221; &#8220;applause,&#8221; &#8220;telephones&#8221;), downmixed if necessary. Rendering was performed by convolving each dry signal with the left/right BRIRs after an early/late mix to emphasize distance cues: we preserved the first 80 ms and attenuated the late tail by 0.5. We then applied global peak normalization across the batch to avoid clipping while preserving inter-position level differences.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Absolute azimuth:</span>\nEight angles\n<math alttext=\"\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mn>30</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;60</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;120</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;150</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;210</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;240</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;300</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;330</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}</annotation></semantics></math>.\nFor each angle we rendered all combinations of 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, yielding\n<math alttext=\"8\\times(3\\times 2\\times 2)=96\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>96</mn></mrow><annotation encoding=\"application/x-tex\">8\\times(3\\times 2\\times 2)=96</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute elevation:</span>\nSix angles\n<math alttext=\"\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mo>&#8722;</mo><msup><mn>75</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>45</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>15</mn><mo>&#8728;</mo></msup></mrow><mo>,</mo><msup><mn>&#8201;15</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;45</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;75</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}</annotation></semantics></math>.\nPer angle we rendered 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m7\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, for\n<math alttext=\"6\\times(3\\times 2\\times 2)=72\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m8\" intent=\":literal\"><semantics><mrow><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>72</mn></mrow><annotation encoding=\"application/x-tex\">6\\times(3\\times 2\\times 2)=72</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute distance:</span>\nRadii from 1&#8211;10 m with a nonuniform allocation to emphasize near-field cues:\nfor 1&#8211;7 m we generated 6 utterances per meter (42 total),\nand for 8&#8211;10 m we generated 3 per meter (9 total),\ngiving <math alttext=\"42+9=51\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m9\" intent=\":literal\"><semantics><mrow><mrow><mn>42</mn><mo>+</mo><mn>9</mn></mrow><mo>=</mo><mn>51</mn></mrow><annotation encoding=\"application/x-tex\">42+9=51</annotation></semantics></math> utterances per (room <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> listener) set.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as clear multiple-choice questions with well-formatted options. We adopt classification accuracy as the evaluation metric. To determine the correctness of a response, we employ string matching to extract either the chosen option label (e.g., <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>A<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>) or the full text content of the option from the model&#8217;s output.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we implement a robust evaluation strategy to ensure rigorous and reliable results. For perception and spatial tasks, we adopt the CircularEval method from MM-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib23\" title=\"\">2024b</a>)</cite>. Specifically, each question is presented to the model <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> times (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of options), with the option order cyclically rotated in each run to mitigate potential positional biases. For temporal tasks, we conduct three runs per question with different temporal segment orders to evaluate the model&#8217;s robustness to sequence variations.\nNote that due to the significant API costs, GPT-4o Audio was evaluated only once per question.\nThis strategy yields two key metrics: Average Accuracy (AA), the mean accuracy across all evaluation runs, and All-Correct Rate (ACR), the proportion of questions answered correctly in every single run, which serves as a stronger indicator of model reliability.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "allcorrect",
                    "results",
                    "spatial",
                    "proportion",
                    "temporal",
                    "average",
                    "run",
                    "gpt4o",
                    "runs",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models that do not support multi-audio input (only Audio Flamingo 3 and its Think variant among the models we evaluated), we concatenate the audios with a 2-second silence and specify this in the prompt. In contrast, for models that support multiple audio inputs, we feed them sequentially with textual indices.</p>\n\n",
                "matched_terms": [
                    "think",
                    "audio",
                    "flamingo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a human performance baseline, we conduct a human evaluation on a randomly sampled subset of approximately 10% of the data from each task. This evaluation is performed by 10 university students, from whom we explicitly exclude anyone involved in data annotation or with domain-specific expertise, thereby ensuring a general, non-expert perspective.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reliability of model outputs on our benchmark is notably low, as evidenced by the stark contrast between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. Even the top-performing model, Gemini 2.5 Pro, exhibits an average drop of 25.01 percentage points from its AA to its ACR. This issue is even more pronounced for the majority of open-source models, which record an ACR near zero. This score indicates a complete failure to maintain consistent predictions under minor input perturbations. For these models, the instability often manifests as a tendency to concentrate predictions on a specific option, suggesting a reliance on superficial biases rather than genuine understanding.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "gemini",
                    "accuracy",
                    "average",
                    "pro",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "results",
                    "spatial",
                    "each",
                    "audio",
                    "overall",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several case studies of error analysis, including temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F12\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Figs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F14\" title=\"Figure 14 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F15\" title=\"Figure 15 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F16\" title=\"Figure 16 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F17\" title=\"Figure 17 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F18\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">18</span></a>).</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "temporal",
                    "reasoning"
                ]
            }
        ]
    },
    "A5.T6": {
        "source_file": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
        "caption": "Table 6: Results for the spatial reasoning task using native and channel-wise audio input. Each cell reports AA / ACR: Average Accuracy (AA; overall accuracy across all runs) / All-Correct Rate (ACR; proportion of samples that are correct on every run). The best model in each category is shown in bold, and the second best is underlined.",
        "body": "Model\nSize\nSingle-Source Static Localization\nMulti-Source Spatial Relation\nDynamic Trajectory Tracking\nOA (%)\n\n\nNative Input\nChannel-wise Input\nNative Input\nChannel-wise Input\nNative Input\nChannel-wise Input\nNative Input\nChannel-wise Input\n\n\nRandom Guess\n—\n33.33 / 3.70\n\n—\n33.33 / 3.70\n\n—\n33.33 / 3.70\n\n—\n33.33 / 3.70\n\n—\n\n\nHuman\n—\n70.00 / —\n\n—\n80.00 / —\n\n—\n77.00 / —\n\n—\n73.72 / —\n\n—\n\n\nSALMONN\n13B\n26.15 / 3.18\n\n26.62 / 3.18\n\n28.61 / 4.42\n\n29.50 / 5.31\n\n39.94 / 0.94\n\n38.36 / 0.94\n\n29.62 / 2.99\n\n29.75 / 3.19\n\n\n\nAudio Flamingo 3\n8.4B\n37.22 / 1.77\n\n\n42.87 / 2.12\n\n38.35 / 4.42\n\n46.31 / 10.62\n\n44.03 / 4.72\n\n\n46.23 / 0.94\n\n38.91 / 2.99\n\n\n44.35 / 3.78\n\n\n\nAudio Flamingo 3 think\n8.4B\n35.45 / 7.42\n\n\n42.87 / 13.78\n\n37.46 / 23.01\n\n46.02 / 23.01\n\n38.05 / 18.87\n\n37.11 / 19.81\n\n36.45 / 13.35\n\n42.36 / 17.13\n\n\n\nQwen2-Audio-Instruct\n8.4B\n21.32 / 8.48\n\n6.36 / 1.77\n\n24.78 / 3.54\n\n12.09 / 4.42\n\n15.09 / 0.94\n\n11.64 / 2.83\n\n20.78 / 5.78\n\n8.76 / 2.59\n\n\n\nDeSTA2.5-Audio\n8.8B\n23.67 / 2.83\n\n20.38 / 4.59\n\n34.81 / 9.73\n\n41.30 / 19.47\n\n37.74 / 10.38\n\n32.08 / 21.70\n\n29.15 / 5.98\n\n27.56 / 11.55\n\n\n\nBAT\n7B\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n0.00 / 0.00\n\n\n\nPhi4-MM\n5.5B\n33.10 / 0.35\n\n32.63 / 0.35\n\n27.14 / 0.88\n\n29.79 / 0.88\n\n34.28 / 0.94\n\n33.02 / 0.00\n\n32.01 / 0.59\n\n32.07 / 0.40\n\n\n\nKimi-Audio\n7B\n27.56 / 3.53\n\n16.49 / 3.53\n\n38.94 / 15.04\n\n22.42 / 8.85\n\n44.03 / 7.55\n\n40.25 / 8.49\n\n33.60 / 6.97\n\n22.84 / 5.77\n\n\n\nMiDashengLM\n7B\n\n43.11 / 15.19\n\n37.22 / 17.67\n\n\n45.43 / 23.89\n\n42.77 / 16.81\n\n\n46.23 / 30.19\n\n45.60 / 21.70\n\n\n44.29 / 20.32\n\n40.24 / 18.33\n\n\n\nStep-Audio-2-mini\n7B\n33.33 / 0.00\n\n33.33 / 0.00\n\n31.27 / 0.00\n\n37.46 / 0.00\n\n37.74 / 6.38\n\n35.22 / 2.83\n\n33.80 / 1.34\n\n34.66 / 0.60\n\n\n\nGemma-3n-E4B-it\n7.5B\n23.32 / 1.41\n\n28.27 / 6.01\n\n41.89 / 15.04\n\n36.58 / 7.96\n\n33.96 / 5.66\n\n40.57 / 8.49\n\n29.75 / 5.37\n\n32.74 / 6.97\n\n\n\nMing-Lite-Omni-1.5\n18.9B\n20.14 / 6.36\n\n34.63 / 6.01\n\n35.10 / 9.73\n\n33.04 / 9.73\n\n38.36 / 18.87\n\n39.94 / 20.75\n\n27.35 / 9.76\n\n35.39 / 9.96\n\n\n\nQwen-2.5-Omni\n7B\n39.46 / 7.07\n\n36.98 / 15.19\n\n41.30 / 18.58\n\n35.10 / 15.93\n\n27.04 / 17.92\n\n34.59 / 8.49\n\n37.25 / 11.95\n\n36.05 / 13.94\n\n\n\nXiaomi-MiMo-Audio\n7B\n36.16 / 0.71\n\n41.58 / 5.65\n\n41.30 / 5.31\n\n38.05 / 4.42\n\n\n45.28 / 9.43\n\n44.34 / 9.43\n\n39.24 / 3.58\n\n41.37 / 6.17\n\n\n\nXiaomi-MiMo-Audio-think\n7B\n34.28 / 7.42\n\n25.44 / 2.83\n\n44.54 / 14.16\n\n37.76 / 7.96\n\n36.79 / 7.55\n\n27.99 / 3.77\n\n37.12 / 8.96\n\n28.75 / 4.18\n\n\n\nMiniCPM-O-v2.6\n8B\n29.92 / 3.18\n\n27.92 / 2.83\n\n43.36 / 11.50\n\n39.53 / 12.39\n\n38.36 / 26.42\n\n35.53 / 17.92\n\n34.73 / 9.96\n\n32.14 / 8.17\n\n\n\nGPT-4o Audio\n—\n\n41.81 / —\n\n\n42.76 / —\n\n43.07 / —\n\n\n54.87 / —\n\n39.94 / —\n\n42.45 / —\n\n41.70 / —\n\n\n45.42 / —\n\n\n\nGemini 2.5 Flash\n—\n24.62 / 4.95\n\n40.75 / 7.42\n\n43.07 / 15.93\n\n43.07 / 17.70\n\n22.64 / 2.83\n\n40.57 / 11.32\n\n28.35 / 6.97\n\n41.23 / 10.56\n\n\n\nGemini 2.5 Pro\n—\n40.87 / 10.95\n\n34.98 / 11.66\n\n\n48.97 / 25.66\n\n\n49.26 / 20.35\n\n\n45.28 / 14.15\n\n\n47.17 / 7.55\n\n\n43.62 / 14.94\n\n40.77 / 12.75",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Single-Source Static Localization</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Multi-Source Spatial Relation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Dynamic Trajectory Tracking</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">OA (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Native Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Channel-wise Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Native Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Channel-wise Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Native Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Channel-wise Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Native Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Channel-wise Input</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Random Guess</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Human</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">70.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">80.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">77.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">73.72&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SALMONN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.62&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.61&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.50&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.94&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.62&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.99</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.19</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">37.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.77</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">42.87</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.35&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.31&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">44.03&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.72</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">46.23</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.91&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.99</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">44.35</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.78</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Audio Flamingo 3 think</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">35.45&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">42.87</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;13.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;23.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.02&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;23.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.05&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;18.87</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.11&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;19.81</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.45&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;13.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">42.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.13</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2-Audio-Instruct</td>\n<td class=\"ltx_td ltx_align_center\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\">21.32&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.48</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">6.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.77</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.09&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.09&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">11.64&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.78&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.76&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.59</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DeSTA2.5-Audio</td>\n<td class=\"ltx_td ltx_align_center\">8.8B</td>\n<td class=\"ltx_td ltx_align_center\">23.67&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">20.38&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.81&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;19.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.74&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.38</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.08&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;21.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.15&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.55</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BAT</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.00&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phi4-MM</td>\n<td class=\"ltx_td ltx_align_center\">5.5B</td>\n<td class=\"ltx_td ltx_align_center\">33.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.14&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.88</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.79&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.88</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.28&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.02&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.01&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.07&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.40</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kimi-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">27.56&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.53</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.49&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.53</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.94&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.42&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">44.03&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.25&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.60&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.84&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.77</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiDashengLM</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">43.11</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">45.43</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;23.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">42.77&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;16.81</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">46.23</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;30.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">45.60&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;21.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">44.29</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;20.32</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.24&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;18.33</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Step-Audio-2-mini</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.33&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">31.27&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.74&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.38</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.22&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.80&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.34</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.66&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.60</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemma-3n-E4B-it</td>\n<td class=\"ltx_td ltx_align_center\">7.5B</td>\n<td class=\"ltx_td ltx_align_center\">23.32&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;1.41</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.27&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.89&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.96&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.57&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">29.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.37</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.74&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.97</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni-1.5</td>\n<td class=\"ltx_td ltx_align_center\">18.9B</td>\n<td class=\"ltx_td ltx_align_center\">20.14&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.36</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.63&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">33.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;18.87</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.94&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;20.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.35&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.39&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.96</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen-2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">39.46&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.98&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;18.58</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.10&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.04&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.59&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.25&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.05&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;13.94</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">36.16&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;0.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.58&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.65</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.30&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;5.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.05&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">45.28</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">44.34&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.24&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.58</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.37&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.17</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xiaomi-MiMo-Audio-think</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">34.28&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">25.44&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">44.54&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;14.16</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.76&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">36.79&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.99&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.77</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">37.12&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.18</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniCPM-O-v2.6</td>\n<td class=\"ltx_td ltx_align_center\">8B</td>\n<td class=\"ltx_td ltx_align_center\">29.92&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;3.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">27.92&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">43.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">39.53&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;12.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.36&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;26.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">35.53&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">34.73&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;9.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.14&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;8.17</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GPT-4o Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">41.81</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">42.76</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.07&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">54.87</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.94&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.45&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.70&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">45.42</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;&#8212;</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini 2.5 Flash</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">24.62&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;4.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.75&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">43.07&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;15.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">43.07&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;17.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">22.64&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;2.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.57&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.32</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.35&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;6.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">41.23&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.56</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gemini 2.5 Pro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">40.87&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;10.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.98&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;11.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">48.97</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;25.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">49.26</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;20.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">45.28</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;14.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">47.17</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;7.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">43.62</span>&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;14.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">40.77&#8201;<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">/&#8201;12.75</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "static",
            "gemini",
            "cell",
            "every",
            "underlined",
            "qwen25omni",
            "guess",
            "channelwise",
            "phi4mm",
            "kimiaudio",
            "localization",
            "trajectory",
            "reasoning",
            "samples",
            "random",
            "acr",
            "xiaomimimoaudio",
            "best",
            "rate",
            "accuracy",
            "stepaudio2mini",
            "native",
            "allcorrect",
            "results",
            "singlesource",
            "flamingo",
            "mingliteomni15",
            "gpt4o",
            "multisource",
            "minicpmov26",
            "each",
            "audio",
            "overall",
            "correct",
            "bold",
            "75b",
            "bat",
            "gemma3ne4bit",
            "salmonn",
            "category",
            "think",
            "midashenglm",
            "13b",
            "reports",
            "desta25audio",
            "189b",
            "84b",
            "tracking",
            "spatial",
            "average",
            "runs",
            "88b",
            "second",
            "input",
            "xiaomimimoaudiothink",
            "across",
            "qwen2audioinstruct",
            "55b",
            "all",
            "task",
            "size",
            "human",
            "relation",
            "proportion",
            "run",
            "dynamic",
            "pro",
            "flash",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we present detailed results for perception, temporal reasoning, and spatial reasoning on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T4\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T5\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.T6\" title=\"In E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, the results reveal a fundamental limitation of LALMs&#8217; spatial understanding in perception. The <span class=\"ltx_text ltx_font_bold\">native input</span> inherently discards part of the multi-channel information during model preprocessing, which leads to a significant loss of spatial cues that are essential for fine-grained reasoning. On the other hand, the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span> explicitly presents each channel with textual instructions, mitigating some of the information loss. However, as most models are not trained on multi-audio inputs, they struggle to align channel representations and to exploit interaural cues reliably.\nOverall, the gap between human and model performance highlights that spatial reasoning in audio remains an unsolved challenge. While channel-wise input can provide partial gains, neither strategy fully captures spatial dependencies, underscoring the need for an audio encoder that natively supports multi-channel audio input.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning.\nWe formalize audio <span class=\"ltx_text ltx_font_bold\">4D intelligence</span> that is defined as reasoning over sound dynamics in time and 3D space, and introduce <span class=\"ltx_text ltx_font_bold\">STAR-Bench</span> to measure it.\nSTAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories.\nOur data curation pipeline uses two methods to ensure high-quality samples.\nFor foundational tasks, we use procedurally synthesized and physics-simulated audio.\nFor holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance.\nUnlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues.\nEvaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning.\nOur STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>\n\n",
                "matched_terms": [
                    "static",
                    "samples",
                    "across",
                    "accuracy",
                    "human",
                    "spatial",
                    "dynamic",
                    "localization",
                    "multisource",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a fundamental modality of human perception, audio serves a pivotal role in communication, aesthetic appreciation, and situational awareness, complementing the limitations of visual perception.\nWith the rise of Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> and especially Large Audio-Language Models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, these models have shown impressive capabilities in understanding audio, representing a crucial step toward diverse applications such as embodied intelligence <cite class=\"ltx_cite ltx_citemacro_citep\">(Paul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib28\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To drive progress, a series of audio benchmarks has been introduced <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite>, covering traditional tasks like Automatic Speech Recognition (ASR) and sound event classification.\nWhile some recent efforts are beginning to emphasize reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite>, we observe that existing benchmarks predominantly focus on coarse-grained semantic content, which is audio information that can be distilled into textual descriptions with minimal loss.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">left</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first use Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> to generate detailed audio captions for samples in recent representative audio benchmarks MMAU (test-mini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite>.\nWe then prompt the model to answer questions based <span class=\"ltx_text ltx_font_italic\">only</span> on these audio captions, and its performance drops by only 5.9% and 9.0%, respectively, compared to when it processes the raw audio.\nThis result suggests that existing benchmarks primarily evaluate audio information that is <span class=\"ltx_text ltx_font_bold\">easily representable by text</span>.\nHowever, human auditory intelligence is not limited to this coarse-grained understanding.\nFor example, humans can intuitively judge the water level in a container from the dynamic changes in the pouring sound, even without being able to precisely articulate the underlying acoustic features.\nSimilarly, we can infer the trajectory and distance of a vehicle approaching from behind to ensure our safety.\nThese abilities are rooted in deep reasoning of audio cues <span class=\"ltx_text ltx_font_bold\">that are difficult to represent linguistically</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "gemini",
                    "human",
                    "dynamic",
                    "pro",
                    "trajectory",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To capture this human-like audio competence, we propose a new paradigm, called <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>.\nThis is defined as the ability to perform deep reasoning over the dynamics of <span class=\"ltx_text ltx_font_bold\">sound sources</span> in <span class=\"ltx_text ltx_font_bold\">time (1D)</span> and <span class=\"ltx_text ltx_font_bold\">three-dimensional space (3D)</span>, grounded in an understanding of the physical world.\nMastering 4D audio intelligence is crucial for various applications.\nIn embodied AI and robotics, for instance, agents must integrate fine-grained auditory cues to interact naturally with their surroundings, such as using sound to infer the trajectory of an object or to monitor the subtle operations of a machine.\nTo systematically evaluate this paradigm and bridge the gap between current audio benchmarks and real-world auditory intelligence, we introduce the <span class=\"ltx_text ltx_font_bold\">S</span>patio-<span class=\"ltx_text ltx_font_bold\">T</span>emporal <span class=\"ltx_text ltx_font_bold\">A</span>udio <span class=\"ltx_text ltx_font_bold\">R</span>easoning (<span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench</span>) benchmark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "trajectory",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> is designed through a hierarchical task structure with two levels.\nAt the <span class=\"ltx_text ltx_font_bold\">Foundational Acoustic Perception</span> level, we conduct a fine-grained, quantitative evaluation of six core audio attributes (pitch, loudness, duration, azimuth, elevation, distance) across both absolute perception ranges and relative discrimination sensitivity.\nWe also introduce a <span class=\"ltx_text ltx_font_bold\">Holistic Spatio-Temporal Reasoning</span> level that evaluates an audio model&#8217;s ability to infer both event order and 3D scene structure.\nTemporal reasoning is tested via segment reordering that spans continuous processes and discrete event scripts, while spatial reasoning covers static localization, multi-source relations, and dynamic trajectory tracking.\nAs shown in the <span class=\"ltx_text ltx_font_bold\">right</span> part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S1.F1\" title=\"In 1 Introduction &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, every question in our holistic tasks is designed to probe a synthesis of three core pillars, such as multi-step reasoning.\nA failure in any one of these pillars will lead to an incorrect response.\nOur <span class=\"ltx_text ltx_font_bold\">data curation pipeline</span> couples procedurally synthesized, fully parameterized audio for foundational perception with large-scale real-world corpora for holistic reasoning.\nFor the latter, we use a four-stage process including <span class=\"ltx_text ltx_font_bold\">human annotation</span> and <span class=\"ltx_text ltx_font_bold\">final selection by human performance</span> to ensure the high quality of our benchmark samples.</p>\n\n",
                "matched_terms": [
                    "static",
                    "samples",
                    "audio",
                    "across",
                    "every",
                    "task",
                    "human",
                    "tracking",
                    "spatial",
                    "dynamic",
                    "localization",
                    "multisource",
                    "trajectory",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive evaluation of 19 models (16 open-source and 3 closed-source) reveals a clear capability hierarchy between the two groups.\nLeading closed-source models like Gemini 2.5 Pro excel in knowledge and reasoning, shifting their primary bottleneck to the more difficult challenge of fine-grained perception. In contrast, open-source models exhibit fundamental weaknesses across all three core capabilities. Through our detailed error analysis and ablation studies, we highlight several key insights for the future development of open-source audio models:\n1) <span class=\"ltx_text ltx_font_bold\">Enhancing dense audio captioning.</span> Open-source models struggle to produce dense, fine-grained captions, which limits their perceptual sensitivity and ability to extract embedded knowledge. Bridging this gap is a crucial first step.\n2) <span class=\"ltx_text ltx_font_bold\">Improving multi-audio reasoning.</span> Open-source models lag significantly in comparing, integrating, and grounding information across multiple audio clips.\n3) <span class=\"ltx_text ltx_font_bold\">Moving beyond channel-averaged audio preprocessing.</span> The common practice of averaging multi-channel audio into a mono signal is a major bottleneck for spatial reasoning. Developing architectures that natively process multi-channel cues is essential for unlocking genuine spatial awareness.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "all",
                    "spatial",
                    "pro",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are summarized as: <span class=\"ltx_text ltx_font_bold\">(1)</span> We formalize <span class=\"ltx_text ltx_font_bold\">audio 4D intelligence</span>, and empirically show that prior benchmarks largely probe text-representable semantics, motivating a shift toward fine-grained, non-linguistic auditory cues.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> We introduce the <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> with foundational acoustic perception and holistic spatio-temporal reasoning tasks, together with a rigorous curation pipeline with expert validation.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> We provide a comprehensive evaluation of 19 LALMs/OLMs. Our analyses and standardized protocols establish strong baselines and testbeds for future research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent progress of Large Audio-Language Models (LALMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib19\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> and Omni-Language Models (OLMs)<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>; AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite> has significantly advanced audio understanding. At the same time, it has spurred the development of numerous benchmarks to comprehensively evaluate their capabilities. Earlier benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> mainly focused on semantic-level understanding tasks (transcription, captioning, and simple question answering), and recent benchmarks<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> have begun to investigate logical audio reasoning tasks. However, existing benchmarks do not address 4D audio intelligence or deep spatio-temporal reasoning across multiple audio inputs, and instead remain limited to single-clip understanding and reasoning. To fill these gaps, we propose a benchmark designed for multi-audio and deep spatio-temporal reasoning, enabling more comprehensive evaluation of audio 4D intelligence. See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S2.T1\" title=\"Tab. 1 &#8227; 2 Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison with existing benchmarks, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A1\" title=\"Appendix A Related Work &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a> for further related works.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understanding dynamic sound sources in both time (1D) and three-dimensional space (3D) is a crucial skill for MLLMs to comprehend the physical world.\nTo address this need, our benchmark, <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, is designed to comprehensively evaluate this 4D intelligence in the audio domain.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our evaluation has two complementary sub-tasks: (1) Foundational Acoustic Perception (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), which uses procedurally synthesized audio to quantitatively profile a model&#8217;s basic perceptual abilities under controlled conditions, and (2) Holistic Spatio-Temporal Reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2\" title=\"3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), which uses real-world audio to evaluate more complex reasoning in dynamic and authentic scenarios.\nWe also elaborate our data curation pipeline in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS3\" title=\"3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning",
                    "dynamic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Foundational Acoustic Perception task is motivated by the need for a robust, quantitative evaluation of the core perceptual abilities that underpin 4D audio intelligence.\nA model&#8217;s capacity for complex reasoning about dynamic audio scenes in the physical world is directly dependent on its ability to accurately perceive fundamental acoustic properties.\nOur foundational acoustic perception task systematically probes a model&#8217;s understanding of three critical auditory attributes: <span class=\"ltx_text ltx_font_bold\">Loudness</span>, <span class=\"ltx_text ltx_font_bold\">Pitch</span>, <span class=\"ltx_text ltx_font_bold\">Duration</span>, and the three spatial dimensions: <span class=\"ltx_text ltx_font_bold\">Azimuth</span>, <span class=\"ltx_text ltx_font_bold\">Elevation</span>, and <span class=\"ltx_text ltx_font_bold\">Distance</span>.\nJust as a solid understanding of grammar is required for writing a complex narrative, a model must be able to accurately perceive these core attributes before it can reason about the dynamic, spatial relationships of sound sources in the physical world.\nWithout a firm grasp of these foundational elements, a model cannot accurately interpret complex, real-world acoustic scenes, which require understanding how sounds change over time and move through space.</p>\n\n",
                "matched_terms": [
                    "task",
                    "spatial",
                    "dynamic",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a targeted synthesis strategy to generate precise evaluation samples in a controlled environment for the foundational perception task.\nFor non-spatial attributes (Loudness, Pitch, Duration), we synthesize pure sine waves by directly specifying their parameters.\nFor spatial attributes (Azimuth, Elevation, Distance), we use the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulation engine to render acoustic scenes.\nThe targeted synthesis strategy allows us to investigate a model&#8217;s audio perceptual abilities under the following two sub-tasks:</p>\n\n",
                "matched_terms": [
                    "samples",
                    "spatial",
                    "audio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Absolute Perception Range,</span> which defines the sensory limits of MLLMs for acoustic attributes.\nFor pitch and loudness, we adapt the design of human audiometry tests to create an &#8220;audiogram&#8221; for the MLLMs. Specifically, we synthesize sine waves with frequencies ranging from <math alttext=\"125\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mn>125</mn><annotation encoding=\"application/x-tex\">125</annotation></semantics></math> Hz to <math alttext=\"8000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mn>8000</mn><annotation encoding=\"application/x-tex\">8000</annotation></semantics></math> Hz and loudness levels from <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">-10</annotation></semantics></math> to <math alttext=\"110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mn>110</mn><annotation encoding=\"application/x-tex\">110</annotation></semantics></math> dB HL and require the model to identify if a clear beep is in the first or second part of an audio clip, or if it&#8217;s not there at all.\nFor spatial attributes, we design interval localization tasks that require the model to identify a sound&#8217;s azimuth within one of four 90&#176; quadrants (from 0&#176; to 360&#176;), its elevation relative to ear-level (above, at, or below, from -90&#176; to 90&#176;), and its distance category (near, medium, or far, within a 0 - 10m range). <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed examples of these absolute perception range tasks. Through these precise tasks, we establish the absolute limits of what the model can hear, which is crucial for developing AI systems that can safely and effectively interact with the physical world.</p>\n\n",
                "matched_terms": [
                    "category",
                    "all",
                    "human",
                    "spatial",
                    "localization",
                    "audio",
                    "model",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Relative Discrimination Sensitivity,</span> which investigates how well a model can detect small changes in acoustic attributes.\nThe ability to detect small changes allows a model to make nuanced judgments, like determining if a sound is getting louder or a pitch is rising.\nAnalogous to measuring the human Just Noticeable Difference (JND), the relative discrimination task presents the model with an audio clip containing two sounds and requires it to compare them based on a specific attribute.\nWe meticulously designed four to six distinct difficulty levels for each of the six attributes, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.T3\" title=\"In B.2 Detail information for foundational acoustic perception &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nLevel 1 serves as a control group to test for random guessing, presenting identical sounds (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>=0) for non-spatial attributes and a sub-threshold difference for spatial ones. Subsequent levels then introduce progressively larger differences, ranging from subtle variations perceptible to humans to more significant, real-world changes.\nBy analyzing the model&#8217;s performance across these different levels of stimulus differences, we can quantitatively assess its discrimination sensitivity for each attribute.</p>\n\n",
                "matched_terms": [
                    "random",
                    "across",
                    "task",
                    "human",
                    "spatial",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the model&#8217;s fundamental audio perceptual abilities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS1\" title=\"3.1 Foundational Acoustic Perception &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), we further introduce holistic temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS1\" title=\"3.2.1 Temporal Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.SS2.SSS2\" title=\"3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), which are designed to systematically evaluate a model&#8217;s reasoning ability that is required for audio 4D intelligence.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of temporal reasoning lies in understanding the intrinsic logic of event sequences, encompassing physical causality, functional procedures, or social conventions.\nTo evaluate this capability, we design a novel <span class=\"ltx_text ltx_font_bold\">Audio Segment Reordering</span> setting.\nSpecifically, we curate a collection of audio events characterized by strong sequential uniqueness, semantic clarity, and logical universality.\nEach event is segmented into three clips, which are then shuffled as inputs to the model.\nThe models are required to restore the original temporal sequence based solely on the audio content.\nOur temporal reasoning tasks are organized into two meta-categories (continuous processes, discrete event sequences) and five subcategories based on their core logical principles.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">continuous processes</span> assess a model&#8217;s ability to track the subtle, continuous evolution of acoustic features within a single, uninterrupted acoustic event.\nThe <span class=\"ltx_text ltx_font_bold\">object spatial motion</span> subcategory reconstructs the spatio-temporal trajectory of moving sources (e.g., passing cars, airplanes) by interpreting key acoustic cues, such as the Doppler effect (frequency shifts indicating relative velocity) and the inverse-square law (loudness changes indicating distance).\nBesides, the <span class=\"ltx_text ltx_font_bold\">in-situ state evolution</span> subcategory assesses a model&#8217;s ability to track the intrinsic evolution of a stationary object&#8217;s state, a process governed by predictable trend patterns.\nThese trend patterns arise from various underlying principles, including: <span class=\"ltx_text ltx_font_italic\">Fluid &amp; Pneumatic Dynamics</span>, where the sound is governed by principles of turbulence, resonance, and pressure changes (e.g., a toilet flushing, water being poured); <span class=\"ltx_text ltx_font_italic\">Thermodynamic Processes</span>, involving irreversible state changes driven by heat (e.g., water boiling, food frying); <span class=\"ltx_text ltx_font_italic\">Energy Decay</span>, a process governed by resonant decay and frictional damping after a single excitation (e.g., a bell&#8217;s chime, an explosion&#8217;s echo); and complex <span class=\"ltx_text ltx_font_italic\">Biological Rhythms</span> that reflect an evolving physiological or emotional state.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "trajectory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">discrete event sequences</span> category requires the model to understand the logical and temporal relationships between multiple, distinct acoustic events, which are governed by function, convention, or causality.\nThe <span class=\"ltx_text ltx_font_bold\">tool &amp; appliance operation</span> sub-category follows the standardized operating procedure for tools and appliances (e.g., a microwave, a power drill), where the sequence is correct when it follows the tool&#8217;s designed function.\nThe <span class=\"ltx_text ltx_font_bold\">daily scene scripts</span> sub-category applies commonsense and contextual script knowledge to follow the conventional sequence of actions in a daily activity (e.g., brushing teeth, drinking water).\nThe <span class=\"ltx_text ltx_font_bold\">event-triggered consequences</span> sub-category applies causal reasoning to infer that a trigger event (e.g., a firework explosion) will be followed by an automatic and irreversible outcome, whether physical (glass shattering) or social (a crowd cheering).</p>\n\n",
                "matched_terms": [
                    "correct",
                    "category",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans effortlessly perceive complex 3D auditory scenes (e.g., hearing a voice from behind, following an approaching car, or locating multiple speakers). Such an ability is fundamental for egocentric interaction and embodied AI systems, for instance, robots that navigate and interact with their surroundings. However, existing benchmarks focus primarily on the localization of static sound sources, whereas real-world scenarios demand reasoning that integrates both spatial and temporal cues. To address this gap, we organize the spatial reasoning task into three subcategories.</p>\n\n",
                "matched_terms": [
                    "static",
                    "task",
                    "spatial",
                    "localization",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">single-source static localization</span> evaluates the model&#8217;s ability to identify the direction of a target sound source among multiple static sources (e.g., judging whether a sound comes from the left or right). It assesses the basic spatial perception capability of the model and provides the foundation for more advanced reasoning.\nThe <span class=\"ltx_text ltx_font_bold\">multi-source spatial relation</span> requires the model to determine the relative spatial relationships among multiple simultaneous sound sources (e.g., comparing the placement of two speakers to decide which one is further to the right). Beyond localizing each source individually, the model must infer their spatial placement and choose the appropriate relational description from multiple candidates.\nThe <span class=\"ltx_text ltx_font_bold\">dynamic trajectory tracking</span> introduces moving sound sources, which require the model to go beyond basic spatial perception to dynamically model spatio-temporal relations for reasoning about complex movement trajectories (e.g., tracking a passing car moving from left to right). This task extends spatial reasoning into the temporal domain and is more faithful to the complexity of real-world acoustic scenarios.</p>\n\n",
                "matched_terms": [
                    "static",
                    "task",
                    "relation",
                    "tracking",
                    "singlesource",
                    "spatial",
                    "dynamic",
                    "localization",
                    "multisource",
                    "each",
                    "trajectory",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, evaluating existing LALMs on multi-channel spatial tasks is challenging. The common practice of these models is to average multi-channel audio into a mono signal, resulting in the loss of substantial spatial information.\nWe conduct a simple experiment as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe construct 20 pseudo-stereo signals by assigning the original audio to the left channel and its additive inverse to the right.\nWhile human listeners could easily perform sound event classification on these signals, the models consistently failed due to signal cancellation during the mono conversion.\nThe result confirms their lack of explicit support for genuine stereo audio processing.\nTo provide a comprehensive assessment, we design two complementary strategies: <span class=\"ltx_text ltx_font_bold\">native input</span>, where the model directly processes stereo audio, follow their default processing pipeline to probe its intrinsic ability to exploit spatial cues; and the <span class=\"ltx_text ltx_font_bold\">channel-wise input</span>, where each channel is presented separately with explicit textual instructions, as shown in the bottom right of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F2\" title=\"In 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, and allow the model to approximate human-like use of interaural cues.</p>\n\n",
                "matched_terms": [
                    "human",
                    "native",
                    "channelwise",
                    "spatial",
                    "average",
                    "each",
                    "audio",
                    "model",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data curation pipeline integrates procedural synthesis with real-world data collection to ensure both comprehensive coverage and ecological validity.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F4\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the distribution and statistics of our <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>.\nAll audio for the <span class=\"ltx_text ltx_font_italic\">foundational perception</span> task is synthesized using precise parameterization or the Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> physics-based simulator, providing complete control over acoustic parameters.\nDomain experts rigorously validate the task difficulty levels, which are then calibrated through human testing.\nFor the <span class=\"ltx_text ltx_font_italic\">holistic spatio-temporal reasoning</span> task, the curation process comprises four key stages (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F5\" title=\"In 3.3 Data Curation Pipeline &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>):</p>\n\n",
                "matched_terms": [
                    "all",
                    "task",
                    "human",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1)</span> Taxonomy Construction and Data Sourcing: We build a hierarchical task taxonomy through a collaborative process involving domain experts and the Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>.\nThis framework guides the sourcing of candidate data from large-scale, real-world audio libraries: Clotho <cite class=\"ltx_cite ltx_citemacro_citep\">(Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib11\" title=\"\">2019</a>)</cite> and FSD50K <cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib12\" title=\"\">2022</a>)</cite> for temporal reasoning, and STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib32\" title=\"\">2023</a>)</cite>, along with audio sourced from the internet for spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "task",
                    "spatial",
                    "pro",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2)</span> AI-Assisted Automated Filtering: This process employs an efficient three-stage funnel. First, we discard unsuitable samples based on basic properties like duration and energy. Next, an LLM (e.g., DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib22\" title=\"\">2024a</a>)</cite>) performs an initial screening based on textual metadata, providing justifications for its decisions. Finally, a powerful multimodal model (e.g., Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>) analyzes the audio, metadata, and the LLM&#8217;s outputs.\nThe final step yields a judgment, a quality score, and a preliminary classification, further filtering irrelevant samples.\nThe detailed prompts used to query the LLMs are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A2.SS3\" title=\"B.3 Prompt Used for AI-Assisted Automated Filtering of Temporal Task Data &#8227; Appendix B Details of Data Annotation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gemini",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3)</span> Human Annotation and Quality Control: We recruit and train 10 undergraduate annotators to label the data using a professional platform.\nDuring this process, AI-generated information is provided as an auxiliary reference. To ensure high-quality labels, we implement a stringent two-round review process: the first round involves inter-annotator cross-validation until a consensus is reached, while the second consists of random spot-checks by three domain experts.</p>\n\n",
                "matched_terms": [
                    "random",
                    "human",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">4)</span> Final Validation via Human Performance Evaluation: To ensure all items in the benchmark are fair, unambiguous, and solvable by humans, we implement a final validation stage. In this phase, domain experts act as examinees and solve our tasks. Only items that are independently and correctly solved by at least two-thirds of the experts are retained. Our rigorous protocol ensures that all problems in our benchmark are well-posed and reliably solvable by human experts.</p>\n\n",
                "matched_terms": [
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Benchmarking Models.</span>\nOur evaluation covers 19 models (16 open-source and 3 closed-source models). The open-source models span three categories: (1) Large Audio Language Models designed for universal audio-text understanding, including SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, Qwen2-Audio Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Audio Flamingo 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant, DeSTA2.5-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib26\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(KimiTeam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib18\" title=\"\">2025</a>)</cite>, Step-Audio-2-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, and Xiaomi-MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite> with its &#8216;think&#8217; variant; (2) a specialized model for spatial audio, BAT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib51\" title=\"\">2024</a>)</cite>; and (3) Omni Language Models with fully multimodal support, including Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Phi4-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, Gemma-3n-E4B-it <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib34\" title=\"\">2025</a>)</cite>, and Ming-Lite-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nWe also include three leading closed-source models: Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite> (updated June 2025), Gemini 2.5 Flash (updated June 2025), and GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite> (version 2025-06-03).</p>\n\n",
                "matched_terms": [
                    "gemma3ne4bit",
                    "salmonn",
                    "midashenglm",
                    "gemini",
                    "xiaomimimoaudio",
                    "desta25audio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "phi4mm",
                    "kimiaudio",
                    "spatial",
                    "flamingo",
                    "mingliteomni15",
                    "pro",
                    "flash",
                    "audio",
                    "model",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robust Evaluation.</span>\nAll questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as multiple-choice questions and evaluated using classification accuracy, with correctness determined via string matching of option labels or their full text. To ensure robustness, we evaluate each question multiple times under minor prompt perturbations, a strategy detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A3\" title=\"Appendix C Robust Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>. This approach yields two key metrics: <span class=\"ltx_text ltx_font_bold\">Average Accuracy (AA)</span>, the mean accuracy across all runs, and <span class=\"ltx_text ltx_font_bold\">All-Correct Rate (ACR)</span>, the proportion of questions answered correctly in every run, which serves as a stronger indicator of model reliability. Due to space limitations, we primarily report AA in the main text, while complete experimental results are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "model",
                    "allcorrect",
                    "results",
                    "proportion",
                    "run",
                    "average",
                    "each",
                    "runs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive evaluation on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.T2\" title=\"In 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Due to the space limit, detailed results on each task are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A4\" title=\"Appendix D Breakdown Results &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>. Our key findings are as follows:</p>\n\n",
                "matched_terms": [
                    "results",
                    "task",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">STAR-Bench<span class=\"ltx_text ltx_font_upright\"> is Challenging</span></span>\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> presents a considerable challenge for existing models. Human evaluators achieve high accuracy across all task categories (e.g., 75.6% on perception, 88.0% on temporal, and 73.7% on spatial tasks), whereas all tested models fall well below this baseline. Most open-source models perform close to random guessing, and even the best closed-source model, Gemini 2.5 Pro, reaches only 49.59% average accuracy.\nIn addition, model predictions on <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> exhibit low reliability, as evidenced by the pronounced gap between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. A detailed discussion of this issue is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS1\" title=\"E.1 High Output Instability and Concentrated Predictions &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.1</span></a>.\nAlthough the underlying audio data for the temporal tasks (e.g., FSD50K, Clotho) is commonly used for model pre-training, our novel task formulation of temporal reasoning deliberately departs from conventional audio QA formats. This design allows for a more thorough evaluation of the integrated capabilities of current models.</p>\n\n",
                "matched_terms": [
                    "random",
                    "acr",
                    "gemini",
                    "across",
                    "all",
                    "best",
                    "accuracy",
                    "task",
                    "human",
                    "spatial",
                    "average",
                    "pro",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Clear Performance Gap between Closed-Source and Open-Source Models</span>\nOn the foundational perception and temporal tasks, Gemini 2.5 Pro establishes a commanding lead among all models. On spatial tasks, however, nearly all models, both closed- and open-source, perform poorly. As indicated by the prior experiment (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S3.F3\" title=\"In 3.2.2 Spatial Reasoning Tasks &#8227; 3.2 Holistic Spatio-Temporal Reasoning &#8227; 3 STAR-Bench &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>), this is likely because most models (except BAT) discard multi-channel information during preprocessing, thereby losing key acoustic cues needed for spatial reasoning.\nAmong closed-source models, Gemini 2.5 Pro surpasses Gemini 2.5 Flash, suggesting that stronger reasoning capabilities deliver substantial gains. In contrast, open-source models show the opposite pattern: the &#8220;think&#8221; modes of Audio Flamingo 3 and Xiaomi-MiMo-Audio perform worse than their no-thinking counterparts, implying that without sufficiently solid perceptual and knowledge foundations, reasoning can be ineffective or even detrimental.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "xiaomimimoaudio",
                    "all",
                    "spatial",
                    "pro",
                    "flash",
                    "audio",
                    "flamingo",
                    "reasoning",
                    "bat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the underlying causes of the poor performance of existing models, we conduct a detailed error analysis along with a series of ablation studies. Due to space limitation, the ablation study on spatial reasoning is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A5.SS2\" title=\"E.2 Ablation Study on Spatial Reasoning. &#8227; Appendix E Further Analysis and Discussion &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Error Analysis.</span>\nWe conduct a manual error analysis on 200 failed predictions sampled equally from temporal and spatial tasks of three representative models (Gemini 2.5 Pro, GPT-4o-audio, and Qwen-2.5-Omni).\nFor temporal tasks, our analysis reveals a clear capability hierarchy across the models. The open-source Qwen-2.5-Omni shows major deficiencies in all three core abilities: its perception is coarse-grained and unable to capture subtle inter-segment distinctions, and a substantial knowledge gap (54%) leads to reasoning that often appears specious due to the absence of physical-world grounding. GPT-4o-audio demonstrates stronger knowledge, but still suffers from perceptual and reasoning limitations, along with low-level issues such as misalignment between reasoning and final answers. In contrast, Gemini 2.5 Pro excels in knowledge and reasoning, shifting its primary bottleneck to the more advanced challenge of fine-grained perception (84%). As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F7\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, Gemini 2.5 Pro is the only model to succeed by providing a remarkably detailed description of acoustic nuances. Our finding suggests that the <span class=\"ltx_text ltx_font_bold\">advanced world knowledge is deeply embedded within detailed audio-text captioning.</span> While open-source models largely remain at a coarse semantic level (e.g., sound event classification), our analysis highlights that enabling them to generate fine-grained acoustic descriptions is critical toward more robust reasoning.\nOn the other hand, most models demonstrate a lack of native spatial awareness in audio tasks, with weaknesses in perception, knowledge, and reasoning. Additionally, a prevalent type of error involves vision-centric hallucinations (e.g., &#8220;&#8230;based on the car&#8217;s trajectory in the video&#8230;&#8221;). This may be attributable to the models&#8217; training on visual spatial tasks, leading them to misapply visual reasoning to auditory inputs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "across",
                    "all",
                    "qwen25omni",
                    "native",
                    "spatial",
                    "pro",
                    "trajectory",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of Human-like Sensitivity in Fine-Grained Perception.</span>\nTo quantify the gap in perceptual sensitivity, we present model audiograms in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (a)(b)(c).\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a> (e)(f)(g) further track the performance of both models and human subjects on the three core acoustic attributes (pitch, loudness, and duration) as task difficulty decreases. The results reveal a stark performance gap between all models and the human baseline, particularly in the perception of fine-grained loudness differences.\nA clear trend is visible even for the top-performing Gemini 2.5 Pro: its accuracy, while competent on easier tasks, plummets as perceptual granularity increases. This directly corroborates our error analysis, identifying fine-grained perception as its primary bottleneck. Notably, its performance on duration perception is an exception, showcasing <span class=\"ltx_text ltx_font_bold\">temporal grounding capabilities superior to those of other models</span> by accurately assessing audio segment lengths.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "all",
                    "accuracy",
                    "task",
                    "human",
                    "results",
                    "pro",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study on Temporal Reasoning.</span>\nTo further pinpoint the specific limitations of temporal reasoning, we augment the baseline audio segment reordering task with two progressively easier settings: (1) <span class=\"ltx_text ltx_font_italic\">+ Global Caption</span>, where a single sentence describing the overall scene is provided as a contextual guide; and (2) <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span>, where the complete, unsegmented audio track is offered as a reference, reducing the task to a straightforward process where the correct order can be determined simply by comparing and grounding each segment within the full audio.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#S4.F9\" title=\"In 4.2 Discussion: Why Do Existing Models Struggle on STAR-Bench? &#8227; 4 Evaluation &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, Gemini 2.5 Pro&#8217;s performance scales effectively with task simplification, culminating in a near-perfect 99% accuracy in the <span class=\"ltx_text ltx_font_italic\">+ Uncut Audio</span> setting. In contrast, the open-source models show minimal to no improvement across these settings. Their performance remains stagnant even when provided with the complete audio reference, despite the simplified nature of the task. This finding starkly exposes a core weakness in current open-source models: <span class=\"ltx_text ltx_font_bold\">a fundamental inability to effectively compare, ground, and integrate information from multiple audio inputs.</span></p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "accuracy",
                    "task",
                    "each",
                    "audio",
                    "overall",
                    "correct",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span>, a comprehensive benchmark for evaluating 4D audio intelligence over time and 3D space.\nWe use rigorous human annotation, consensus review, and expert validation to ensure the high quality of data samples.\n<span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> establishes standardized tasks and protocols for studying 4D audio intelligence, offering actionable diagnostics for model developers.\nWe expect STAR-Bench to accelerate progress on advanced audio models and training with spatialized corpora, capabilities that are crucial for embodied agents.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Gemini-2.5-Pro to assist in expanding and consolidating the taxonomy of tasks in our benchmark. Both DeepSeek-V3 and Gemini-2.5-Pro were utilized for the automated pre-screening of candidate data.\nThe final task definitions and data samples are verified by humans.\nWe also used GPT-4o to generate some of the illustrative figures presented in the paper, and used GPT-5 to polish the manuscript text.\nOnly human-verified revisions are included in the final version.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "gpt4o",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements of large language models (LLMs) and multimodal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib44\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib17\" title=\"\">2024</a>; Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>; Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib4\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib35\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib24\" title=\"\">2024c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib25\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib50\" title=\"\">2025b</a>; Qi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib29\" title=\"\">2025</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib42\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib41\" title=\"\">a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib9\" title=\"\">2025</a>; Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib37\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib38\" title=\"\">b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib21\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib49\" title=\"\">2025a</a>)</cite>, recent research has increasingly focused on integrating audio perception with LLMs to enhance audio understanding and reasoning. Existing methods can be broadly grouped into two categories: Large Audio Language Models(LALMs) and Omni Language Models(OLMs).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most LALMs combine a pre-trained audio encoder with an LLM backbone, where the two modalities are aligned via large-scale text-audio joint training. Notable models include LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib15\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib33\" title=\"\">2024</a>)</cite>, MidashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib10\" title=\"\">2025</a>)</cite>, Audio Flamingo series <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib13\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib14\" title=\"\">2025</a>)</cite>, Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib6\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib7\" title=\"\">2024</a>)</cite>, Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib39\" title=\"\">2025</a>)</cite> and Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib40\" title=\"\">2025</a>)</cite>. These models have achieved remarkable performance across a wide range of audio understanding tasks, including automatic speech recognition(ASR), spoken question answering(SpokenQA), and automated audio captioning(AAC). In parallel, OLMs extend this paradigm to unify multimodal understanding with representative examples such as Qwen-2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib43\" title=\"\">2025</a>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(AI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib3\" title=\"\">2025</a>)</cite>,MiniCPM-O <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib48\" title=\"\">2024</a>)</cite>, Phi-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Abouelenin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib1\" title=\"\">2025</a>)</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib2\" title=\"\">2023</a>)</cite>, and Gemini 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib8\" title=\"\">2025</a>)</cite>. Notably, they also achieve impressive performance on audio understanding and reasoning, highlighting their potential to bridge multimodal perception and advanced audio intelligence.</p>\n\n",
                "matched_terms": [
                    "salmonn",
                    "midashenglm",
                    "gemini",
                    "across",
                    "qwen25omni",
                    "gpt4o",
                    "audio",
                    "flamingo",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing audio benchmarks illustrate the rapid progress of multimodal evaluation but also expose limitations. AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib36\" title=\"\">2024</a>)</cite> and AIR-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib45\" title=\"\">2024</a>)</cite> primarily focus on tasks such as automatic speech recognition (ASR), spoken question answering (SpokenQA), and audio captioning (AAC). These settings tend to reduce audio understanding to transcription or description, thereby neglecting the broader spectrum of acoustic reasoning. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib30\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib27\" title=\"\">2025</a>)</cite> further expand the scope, yet their results reveal an inherent weakness: LLMs with audio captions can achieve comparable performance to advanced LALMs, suggesting that these benchmarks probe little beyond language-level semantics. MMAU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib20\" title=\"\">2025</a>)</cite> attempts to add temporal and spatial reasoning, but its scope is restricted to single-audio temporal reasoning and single static spatial reasoning.</p>\n\n",
                "matched_terms": [
                    "static",
                    "results",
                    "spatial",
                    "audio",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the LALMs evaluation, multimodal benchmarks in video question answering <cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib5\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib47\" title=\"\">2025c</a>)</cite> and embodied AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib46\" title=\"\">2025b</a>)</cite> have emphasized temporal and spatial reasoning. However, these frameworks are predominantly grounded in the visual modality, leaving the audio modality underexplored. Real-world audio understanding frequently requires integrating information across multiple sound streams and reasoning about subtle changes in intensity, phase, or frequency&#8212;capabilities that existing benchmarks scarcely capture.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark aims to address these gaps by introducing tasks that require <span class=\"ltx_text ltx_font_bold\">multi-audio input and cross-audio reasoning</span>, such as comparing or integrating information across multiple sound inputs, as well as <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-temporal deep reasoning</span>, such as tracking how acoustic patterns evolve with underlying physical changes. Rather than being limited to surface-level semantics, the benchmark is designed to assess whether models can leverage raw audio cues to perform physically grounded reasoning across spatial and temporal dimensions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tracking",
                    "spatial",
                    "audio",
                    "reasoning",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prompt for Gemini 2.5 Pro audio captioning: &#8220;Please provide a detailed description of the audio, including speech, music, environmental sounds, and any other noticeable elements. Be as specific as possible.&#8221;</p>\n\n",
                "matched_terms": [
                    "audio",
                    "gemini",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated binaural recordings for foundational perception tasks (azimuth, elevation, distance) in Pyroomacoustics <cite class=\"ltx_cite ltx_citemacro_citep\">(Scheibler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib31\" title=\"\">2018</a>)</cite> across three rectangular rooms&#8212;small (4.0&#215;3.5&#215;2.8 m), medium (8.0&#215;6.0&#215;3.5 m), and large (20&#215;15&#215;8 m)&#8212;each with a frequency-independent wall absorption coefficient of 0.25. Image-source reflections were modeled up to order 10 at 44.1 kHz (matched to the HRTF sampling rate). For each room, we evaluated two listener positions (distinct Cartesian coordinates) and oriented the head toward the +x axis. Binaural reception used a co-located two-microphone array at the listener position with ear-specific directivity derived from a measured SOFA HRTF<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa\" title=\"\">https://sofacoustics.org/data/database/mit/mit_kemar_normal_pinna.sofa</a></span></span></span> (MIT KEMAR, &#8220;normal pinna&#8221;; interpolation order 12, 1000 points), loaded via a local SOFA reader and applied to the left/right channels.</p>\n\n",
                "matched_terms": [
                    "each",
                    "rate",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each condition (room &#215; listener), sources were placed on a sphere centered at the listener (radii 1&#8211;10 m; configurable azimuth/elevation), and ear-specific BRIRs were computed. Mono source signals were drawn from three curated audio clips (&#8220;alarm,&#8221; &#8220;applause,&#8221; &#8220;telephones&#8221;), downmixed if necessary. Rendering was performed by convolving each dry signal with the left/right BRIRs after an early/late mix to emphasize distance cues: we preserved the first 80 ms and attenuated the late tail by 0.5. We then applied global peak normalization across the batch to avoid clipping while preserving inter-position level differences.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Absolute azimuth:</span>\nEight angles\n<math alttext=\"\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mn>30</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;60</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;120</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;150</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;210</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;240</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;300</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;330</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{30^{\\circ},\\,60^{\\circ},\\,120^{\\circ},\\,150^{\\circ},\\,210^{\\circ},\\,240^{\\circ},\\,300^{\\circ},\\,330^{\\circ}\\}</annotation></semantics></math>.\nFor each angle we rendered all combinations of 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, yielding\n<math alttext=\"8\\times(3\\times 2\\times 2)=96\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>96</mn></mrow><annotation encoding=\"application/x-tex\">8\\times(3\\times 2\\times 2)=96</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute elevation:</span>\nSix angles\n<math alttext=\"\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mo>&#8722;</mo><msup><mn>75</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>45</mn><mo>&#8728;</mo></msup></mrow><mo rspace=\"0.337em\">,</mo><mrow><mo>&#8722;</mo><msup><mn>15</mn><mo>&#8728;</mo></msup></mrow><mo>,</mo><msup><mn>&#8201;15</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;45</mn><mo>&#8728;</mo></msup><mo>,</mo><msup><mn>&#8201;75</mn><mo>&#8728;</mo></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{-75^{\\circ},\\,-45^{\\circ},\\,-15^{\\circ},\\,15^{\\circ},\\,45^{\\circ},\\,75^{\\circ}\\}</annotation></semantics></math>.\nPer angle we rendered 3 rooms <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 listener positions <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m7\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> 2 source clips, for\n<math alttext=\"6\\times(3\\times 2\\times 2)=72\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m8\" intent=\":literal\"><semantics><mrow><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>72</mn></mrow><annotation encoding=\"application/x-tex\">6\\times(3\\times 2\\times 2)=72</annotation></semantics></math> utterances.\n<span class=\"ltx_text ltx_font_bold\">Absolute distance:</span>\nRadii from 1&#8211;10 m with a nonuniform allocation to emphasize near-field cues:\nfor 1&#8211;7 m we generated 6 utterances per meter (42 total),\nand for 8&#8211;10 m we generated 3 per meter (9 total),\ngiving <math alttext=\"42+9=51\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m9\" intent=\":literal\"><semantics><mrow><mrow><mn>42</mn><mo>+</mo><mn>9</mn></mrow><mo>=</mo><mn>51</mn></mrow><annotation encoding=\"application/x-tex\">42+9=51</annotation></semantics></math> utterances per (room <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p4.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> listener) set.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All questions in <span class=\"ltx_text ltx_font_smallcaps\">STAR-Bench</span> are presented as clear multiple-choice questions with well-formatted options. We adopt classification accuracy as the evaluation metric. To determine the correctness of a response, we employ string matching to extract either the chosen option label (e.g., <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>A<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>) or the full text content of the option from the model&#8217;s output.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we implement a robust evaluation strategy to ensure rigorous and reliable results. For perception and spatial tasks, we adopt the CircularEval method from MM-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#bib.bib23\" title=\"\">2024b</a>)</cite>. Specifically, each question is presented to the model <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> times (<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of options), with the option order cyclically rotated in each run to mitigate potential positional biases. For temporal tasks, we conduct three runs per question with different temporal segment orders to evaluate the model&#8217;s robustness to sequence variations.\nNote that due to the significant API costs, GPT-4o Audio was evaluated only once per question.\nThis strategy yields two key metrics: Average Accuracy (AA), the mean accuracy across all evaluation runs, and All-Correct Rate (ACR), the proportion of questions answered correctly in every single run, which serves as a stronger indicator of model reliability.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "across",
                    "every",
                    "all",
                    "rate",
                    "accuracy",
                    "model",
                    "allcorrect",
                    "results",
                    "spatial",
                    "proportion",
                    "gpt4o",
                    "average",
                    "run",
                    "each",
                    "audio",
                    "runs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models that do not support multi-audio input (only Audio Flamingo 3 and its Think variant among the models we evaluated), we concatenate the audios with a 2-second silence and specify this in the prompt. In contrast, for models that support multiple audio inputs, we feed them sequentially with textual indices.</p>\n\n",
                "matched_terms": [
                    "think",
                    "audio",
                    "flamingo",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To establish a human performance baseline, we conduct a human evaluation on a randomly sampled subset of approximately 10% of the data from each task. This evaluation is performed by 10 university students, from whom we explicitly exclude anyone involved in data annotation or with domain-specific expertise, thereby ensuring a general, non-expert perspective.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reliability of model outputs on our benchmark is notably low, as evidenced by the stark contrast between their Average Accuracy (AA) and All-Correct-Rate (ACR) scores. Even the top-performing model, Gemini 2.5 Pro, exhibits an average drop of 25.01 percentage points from its AA to its ACR. This issue is even more pronounced for the majority of open-source models, which record an ACR near zero. This score indicates a complete failure to maintain consistent predictions under minor input perturbations. For these models, the instability often manifests as a tendency to concentrate predictions on a specific option, suggesting a reliance on superficial biases rather than genuine understanding.</p>\n\n",
                "matched_terms": [
                    "acr",
                    "gemini",
                    "accuracy",
                    "average",
                    "pro",
                    "model",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several case studies of error analysis, including temporal reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F12\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Figs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F14\" title=\"Figure 14 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F15\" title=\"Figure 15 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F16\" title=\"Figure 16 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F17\" title=\"Figure 17 &#8227; Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>) and spatial reasoning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24693v1#A6.F18\" title=\"In Appendix F Case Study &#8227; STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">18</span></a>).</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "reasoning"
                ]
            }
        ]
    }
}